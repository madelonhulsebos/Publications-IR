It is of the form   ' <Equation_1>   'where Q is a query containing key terms T, tf is the frequency of occurrence of the term within a specific document, qtf is the frequency of the term within the topic from which Q was derived, and w (1) is the Robertson/Spark Jones weight of T in Q.
Approximation is common (and necessary for some cases) in scaling up algorithms to Web scale on distributed clusters .
Generating candidate rules proceeds in two steps: finding the head words and modifiers of attributes, and generalizing modifiers to concepts nodes from H. We rely on in-house Natural Language Parsing technology to identify the head words and modifiers in an attribute.
Pseudocode that measures co-occurrence of two persons is shown in Fig.
Some of our optimizations are adopted from WebKit: Hash tables.
Scheduling is orthogonally handled as follows: We define parallel traversal functions that invoke layout calculation functions (semantic actions [11]).
Approximation algorithm.
  Pseudocode for the simulated bidding agent appears in Figure 5.
With high probability , if two points are close in the 2D space, they will also be close in the Hilbert transformation.
Globally, they form a concise graph description that has found several applications for the web [4, 21] as well as social and biological networks [29, 22] .
Globally, each subgraph count is in exact proportion with the same subgraph counted from a different vertex automorphism.
The ratio depends on the subgraph's degree distribution:   ' <Equation_14>   ' <Equation_15>   ' Global symmetry makes the equation for F8 and the system (3) linearly dependent.
We redefine the Global Gain as:   ' <Equation_12>   ' <Equation_13>   'This should be compared with the original Global Gain (Eq.
Bootstrapping large type systems and semantic relations from the Web [10, 28, 11, 6] is an active research area, but efficient index and query support are needed to fully realize their potential.
 In this study, we propose a method for personalized recommendation of news stories for commenting.
Global constraints are updated asynchronously and independently.
Stochastic gradient descent is a common way of solving this nonconvex problem.
This modifies only the local copies of X   ' <Equation_8>   ' These updates are analogous to standard matrix factorization and can be obtained using stochastic gradient descent (SGD) with the following gradient for X   ' <Equation_9>   ' <Equation_10>   ' <Equation_11>   'Algorithm 2 Synchronous Optimization Require: Matrix Y ∈ R n×n , rank r, accuracy Ensure: Find a local minimum of (5) over X and Z 1: Global Server 2: Initialize Z ∈ R n×r at random 3: repeat 4: Z ← Z 5: for all k do send Zv x to machine k 6: Wait until all machines finish local optimization 7: for all k do receive X (x) from machine k 8: Update Z by solving (9) 9: until i Z i − Zi 2 ≤ 10: Send termination signal to all machines 11: Machine k 12: repeat 13: Receive global factors Zv k and match X (k) to it 14:   ' <Equation_12>   '16: for all nodes i ∈ v k do update X   '(x) i using (8) 17:   ' <Equation_13>   'i 2 ≤ 18: Send X (k) to the global server 19: until Global server signals termination Latent Synchronizer Update: A second phase minimizes Z given X.
For each client k, we need to keep 2(|O k | + |B k |) copies of factor   'Algorithm 3 Asynchronous Distributed Optimization   'Require: Matrix Y ∈ R n×n , rank r, accuracy Ensure: Find a local minimum of (5) over X and Z 1: Initialization 2: for all i do Initialize Zi randomly 3: for all k, i ∈ O k do X (k) i ← Zi 4: for all k, i ∈ B k do X   ' <Equation_22>   '← Zi 5: Client k Optimizer Thread 6: repeat 7: for all nodes i ∈ V k do 8: Update X   '(x) i using the gradient from (11) 9: end for 10: until Termination 11: Client k Send Thread 12: repeat 13: for all nodes i ∈ V k do 14: Send X   '(k) i to the Global server 15: end for 16: until Termination 17: Client k receive (Zi)   ' <Equation_23>   'i ) 20: Update Zi using the gradient form (10) 21: Send Zi to machine k vectors: one for the local copies X (k) i and one for the cached copies of the global variables, Z i , for i ∈ O k ∪ B k .
The Dual Inversion (DI) system [9] supports 21 types, among the largest type catalog used in Web-scale entity  search.
Aliasing occurs in Web transactions when different request URLs yield replies containing identical data payloads.
Aliasing is most common among ble 5 shows that MIDI accounts for under 2% of all traffic and under 1% of all transactions.
 Aliasing can arise in several different ways, e.g., deliberate mirroring , aliasing within a single site, and identical content available at different sites.
Aliasing is not the only cause of redundant payload transfers, so we describe a simple and completely general way to eliminate all redundant transfers, regardless of cause: Duplicate Transfer Detection .
Breadth-first search (BFS) can be adapted to enumerate all the matched paths, which we call breadth-first match (BFM).
Dijkstra's algorithm can be adapted to this problem by placing additional constraints of the path pattern, which we call Dijkstra's Matching (DijkstraM) algorithm .
For example, methods adapted from Dijkstra's algorithm (DijkstraM ) as mentioned in Section 2.3 can effectively avoid enumerating all the paths.
"Each assignment had multiple parts (which combined for a total of 42 coding problems), in which students were asked to implement algorithms covered in lectures such function [Figure 1: (a) Example code submission to the   Gradient descent (for linear regression)   problem; (b) Example abstract syntax tree for linear regression gradient expression: X * (X * θ − y); (c) Example subtree from the AST from Figure 1(b); (d) Context of the subtree with respect to the same AST."
Figure 1 (a) shows a correct attempt at the   Gradient descent (for linear regression)   problem assigned in the first homework.
"First, Equation 4.1 can be written equivalently as: L ← QueryIndex(B) ; L ← QueryIndex(B ) ; Initialize count and denominator to 0; Z ← 0; foreach AST Ai ∈ L do if Ai\\B = A i \\B for some A i ∈ L then wi ← ci * c i ; count ← count + wi if F [Ai] = F [A i ] ; denominator ← denominator + w ; return count/denominator ; Algorithm 1: Pseudocode for estimating the semantic equivalence probability between two subtrees , B and B ."
Global constraints on message ordering and occurrence are central to a choreography.
The most basic session definition comes with Time Oriented Heuristics[8, 12, 23]  which are based on time limitations on total session time or page-stay time.
In this work, we compare the accuracies of the three web usage mining processes employing Smart-SRA, Navigation Oriented Heuristic and Time Oriented Heuristic using total session duration time threshold.
We didn't plot the results for Time Oriented Heuristics using page stay time threshold since its accuracy is very close to Time Oriented Heuristics using total session duration time.
On the average, the maximal patterns obtained from Smart- SRAs sessions are at least 30% more accurate than patterns obtained from sessions of Time and Navigation Oriented Heuristics.
In this paper we propose GlobeDB, a system for hosting Web applications, which handles distribution and partial replication of application data automatically and efficiently.
Section 3 presents GlobeDB's architecture and Section 4 describes the design of the data driver, the central component of GlobeDB.
Section 6 presents an overview of GlobeDB implementation and its internal performance.
Section 7 presents the relative performance of GlobeDB and different edge service architectures for the TPC-W benchmark.
In such scenarios, GlobeDB can be very useful as it places the data in only those servers that access them often.
GlobeDB does not guarantee full transaction semantics , but enforces consistency for each query individually.
GlobeDB enforces consistency among replicated data units using a simple master-slave protocol: each data cluster has one master server responsible for serializing concurrent updates emerging from different replicas.
GlobeDB assumes that each database transaction is composed of a single query which modifies at most a single data unit.
Note that even existing solutions over commercial database systems such as DBCache [6] and MTCache [17] face similar issues and guarantee the same level of consistency as GlobeDB.
Edge Server 1 Edge Server m Origin Server   'Web Clients   ' <Equation_2> ''The data driver is the central component of GlobeDB.
For the placement of data, we use a cost function that allows the system administrator to tell GlobeDB his/her idea of optimal performance.
As we explain in detail in this section, GlobeDB uses this function to assess the goodness of its placement decisions.
 As we mentioned earlier, in GlobeDB, data units with similar access patterns are clustered together.
In GlobeDB, we represent overall system performance into a single abstract figure using a cost function.
In GlobeDB we use the cost function as a decision making tool to decide on the best server placements and master server for each cluster.
In GlobeDB, we use heuristics to perform replica placement and master selection (discussed in the next subsections ).
Instead, in GlobeDB, administrators are just expected to define their preferred performance tradeoffs by choosing the weight parameters of the cost function.
We measured the execution latencies of read and write queries using the original PHP driver and the GlobeDB PHP driver for different throughput values.
"In both cases, the requested data is available locally; the only difference is that the GlobeDB driver needs to check its cluster membership and cluster-property tables before each data access."
 The experiments presented in the previous section show that the overhead introduced by GlobeDB's driver in a single edge server is low.
However, it does not offer any insight into the performance gains of GlobeDB.
In this section, we study the performance gain that could be obtained using GlobeDB while hosting an ecommerce application.
We chose the TPC-W benchmark and evaluated the performance of GlobeDB in comparison to other existing systems for different throughput values over an emulated wide-area network.
As the experiment results presented in this section will show, GlobeDB can reduce the client access latencies for typical e-commerce applications with large mixture of reads and write operations without requiring manual configurations or performance optimizations.
In our experiments, we evaluate GlobeDB for the ordering scenario, as it generates the highest number of updates.
In our experiments, we want to compare the performance of GlobeDB with traditional centralized and fully replicated scenarios .
So, for a fair comparison with GlobeDB, we implemented this optimization manually for the fully replicated system.
In addition to these fields, it also stores five integer fields that have identifiers of books that are closely related to it and have a similar customer Requests/second Original GlobeDB   '(b) Write Accesses ' 'We evaluated the performance of four different systems (shown in Figure 6): (i) a traditional centralized architecture where the code and the database is run in a server across a WAN (Centralized), (ii) a simple edge server architecture where three edge servers run a Web server and the database is run in a server across the WAN (SES), (iii) our proposed system with 3 edge servers (GlobeDB)   'System   'Origin Server   'Origin Server    Origin Server and (iv) a full replication system (Full) which is similar to the GlobeDB setup -the only difference being that the tpcw item and tpcw customer tables are fully replicated at all edge servers unlike GlobeDB.
GlobeDB performs better than SES and centralized system as it is able to save on wide-area network latency for each data access as it is able to find many data records locally.
The difference in WIRT between the GlobeDB and Full setup varies from 100 to 400ms.
This is because the GlobeDB system is capable of performing local updates (the server that writes most to a database cluster is elected as its master) but the Full setup forwards all updates to the origin server.
On the other hand, the Full setup gains in the fact it can handle some complex queries such as search result interactions locally, while GlobeDB forwards it to the origin server.
It is obvious that the fully replicated system produces more update traffic than GlobeDB as it propagates each update to all edge servers.
In this experiment, we found that GlobeDB reduces the update traffic by a factor of 6 compared to Full replication.
This is because GlobeDB prevents unnecessary update traffic by placing data only where they are accessed.
GlobeDB attains a throughput of 16.9 req/sec and is 2 WIPS better than the Full setup and 8 WIPS better than SES.
While the Full setup has the same number of database servers as GlobeDB, the latter sustains higher throughput as a server does not receive updates to data that it accesses rarely.
In conclusion, the results show that the GlobeDB's automatic data placement can reduce client access latencies for typical ecommerce applications with a large mixture of read and write operations .
The three systems evaluated are: (i) GlobeDB (0, 0, 1): a system with weights (α, β, γ) =(0,0,1), which implies the system wants to preserve only the bandwidth and does not care about latency, (ii) GlobeDB (1, 1, 0): a system whose weights are set such that the system cares only about the client latency and does not have any constraints on the amount of update bandwidth.
(iii) GlobeDB ( 2, 2, 1): a system that prefers to optimize latency twice as much as the update bandwidth.
For example, GlobeDB(0, 0, 1) is useful for a system that cannot afford to pay for the wide-area bandwidth consumed in its remote edge servers and GlobeDB(1, 1, 0) is useful for a CDN that values its client quality of service (QoS) more than its maintenance costs consumed for maintaining consistency among replicas.
As seen in the figure, GlobeDB(0, 0, 1) performs the worst in terms of WIRT, as it leads to a placement where the data are not replicated at all.
With respect to update traffic, GlobeDB(2, 2, 1) performs better than GlobeDB(1, 1, 0) and reduces update traffic by a factor of 3.5.
Note that, while GlobeDB(1, 1, 0) and a fully replicated system have similar goals, the former yields better WIRT as it is able to perform local updates.
We also note that these works can be combined with GlobeDB to scale the database in a single edge server.
In this paper, we presented GlobeDB, a system for hosting Web applications that performs efficient autonomic replication of application data.
The goal of GlobeDB is to provide data-intensive Web applications the same advantages CDNs offered to static Web pages: low latency and reduced update traffic.
In our evaluations, we found that GlobeDB significantly improves access latencies and reduces update traffic by a factor of 6 compared to a fully replicated system.
The system can learn (from our collection ) the transition probabilities with the Baum-Welch algorithm [24] and decode the collection sequence with the Viterbi algorithm [24].
We force the model to start with state E, and use the Baum-Welch algorithm to learn the transition probabilities and the output probability for E. Once all the parameters are estimated, we use the Viterbi algorithm to decode the collection sequence.
We count the frequency of all possible n-grams up to a Algorithm 1: Dynamic programming algorithm to compute top k segmentations certain length (n = 1, 2, · · · , 5) that occur at least once in the corpus .
, 'Input: query w1w2 · · · wn, frequencies for all n-grams not longer than m Output: frequencies (or their lower bounds) for all n-grams in the query C[i, j]: frequency (or its lower bound) for n-gram wi · · · wj   ' <Equation_14>   'Algorithm 2: Dynamic programming algorithm to compute lower bounds for n-gram frequencies   'There are two serious problems with this approach, however.
One possibility is to invoke a Semantic Web Service on this mRNA sequence, such as BLAST, a search engine commonly used in the bioinformatics community to locate similar sequences [35].
The user could click on the BLAST command to invoke the service (we have simulated this with a mocked up Semantic Web Service called NotQuiteABlast).
"This is in contrast to how services such as BLAST are made available from the Web today; in general, either an identifier or a DNA sequence string would need to be copied and pasted manually into the form."
It may be:   DFS (depth-first search) The strategy is FIFO and the data structure is a stack   BFS (breadth-first search) The strategy is LIFO and the data structure is a queue   DEG (higher degree) The most pointed URL is chosen.
which algorithm is chosen:   '  For DFS algorithm, the Unvisited Set is a stack (FIFO)   '  For BFS algorithm, it is a queue (LIFO)   '  For DEG algorithm, it is a priority queue (heap)   '  For RND algorithm, a random page is extracted from the Unvisited Set   'Because the average out-degree of a page is large enough, the crawling process will not stop unless almost all pages have been crawled.
Sequential pattern mining has raised great interest in data mining research field in recent years.
Sequential pattern mining has also shown its utility for Web data analysis, such as mining Web log data[2] and identifying comparative sentences from Web forum posting and product reviews[3].
Heuristics 1 and 2 are data-set specific, in the sense that they relate to the Freebase data model.
Heuristics 3 and 4 are movie-domain specific and would be different in other domains.
 Sequential pattern mining is the problem of finding frequent patterns of labels in sequences of data items (see [12] for a survey).
Ukkonen's algorithm is argued for lack of space efficiency in building a suffix tree [6].
However, with the trade off of space cost, Ukkonen's algorithm makes it possible to build a large incremental suffix tree online, which allows us to insert a document into the suffix tree and remove it dy- namically.
UPGMA (unweighted pair group method using arithmetic averages), a traditional agglomerative clustering method, is arranged to perform topic detection process because it achieves the best performance in our experiments.
New topics generated are then clustered into new news issue candidates using the UPGMA algorithm described in   ' <Equation_5>   ') is used to control the clustering process.
We surveyed the cluto toolkits [9] before we select UPGMA as our clustering method.
As shown in Figure 1, the best performance is achieved by the   agglo-upgma   algorithm, which is a traditional agglomerative clustering method based on UPGMA.
We consider that in news story datasets, the nearest neighbors of a story are always of the same class, which may be the reason why UPGMA performs best among all.
So we add a threshold d θ when implementing the UPGMA algorithm to decide when the clustering process stops, as mentioned in Section 3.
al., [6] compares these similarity measures for HAC algorithms, and finds UPGMA to be the best.
Sampling is typically worse than greedy, but unlike the partition algorithm, its performance improves dramatically as c becomes larger, and does not worsen as quickly when a gets larger.
Sampling Greedy Partition ' We present the sampling algorithm for the (c, a)-recommendation subgraph formally below.
We prove the following theorem about the performance of the Sampling Algorithm.
Since each vertex in L is in exactly c matchings, each vertex in L has degree c. It follows that except for a o(1) probability there exists a (c, a)-recommendation subgraph in G.   'Approximation Algorithm Using Perfect Matchings:   'The above result now enables us to design a near linear time algorithm with a (1 − ) approximation guarantee to the (c, a)-recommendation subgraph problem by leveraging combinatorial properties of matchings.
To summarize, our synthetic experiments show the following strengths of each algorithm: Sampling Algorithm: Sampling uses little to no memory and can be implemented as an online algorithm.
Bandwidth usage: The bandwidth usage for each link cannot exceed the link capacity.
As with the Greedy algorithm of Section   '3.3.1,   'the algorithm assumes we start with a shortest-path distribution tree T with depths d(v) for each node in V , but now we are not required to restrict all our paths to T .
8 Formally, we need to have the interval   ' <Equation_10>   'Most Influential Greedy Heuristic Input: (U, B, pu).
Next we see that even in simple linear influence models with B = n, there are bad examples for the two greedy algorithms Adaptive Hybrid Heuristic Input: (U, B, pu).
The optimal value Two-stage Heuristic Input: (U, B, pu).
, n. Arguments identical to those made above allow us to conclude that for all i, we have S Θ i+1 = ˆ S Θ i ∪ {s}.
Approximation algorithms are provided for two variants of coordinate costs: diameter cost and Steiner cost (cost of the minimum Steiner tree where the team members are the terminal nodes).
Factoring in the fact that 1 − 1: {Given (Λgiven,Σgiven,G,ca) where G = (N, E) is the network graph, Λgiven,Σgiven are the incomplete sets of active and inactive nodes and ca is an approximate value of |Λ| } 2: Λ pred = Λgiven 3: Create a refined graph G ′ that consists of nodes in N − Σgiven 4: Select a node ni at random from Λ 5: Tstein = min Steiner tree rooted at ni in G ′ covering Λgiven 6: Nstein = nodes in Tstein 7: Λ pred = Λ pred ∪ Nstein 8: while |Λ pred | ≤ ca do 9: N choose = ni ∈ N − Σgiven − Λ pred 10: Λ pred = Λ pred ∪ {argmaxn∈N choose {deg(n)Λ pred }} 11: Output Λ pred Figure 6: Heuristic to identify Λ pred is known to be inapproximable within o( p n/logn) [38] .
Algorithm 2 Greedy algorithm for recovering nested logit tree structure.
Let S be the set of old nodes returned by the Greedy algorithm for the k-budgeted cover on H t where t is the index of the most recent complete recrawl.
Greedy algorithms.
Dynamic programming on trees.
The Greedy algorithm provides us with an upper bound for the potentially cacheable content, and the TTL-Cache algorithm the lower bound if all the optional parameters are followed explicitly.
We found with our Greedy algorithm that 92.2% of requests and 67.9% of bytes could have been served from cache.
Global constraints can be classified based on the complexity of solving them (i.e.
For example, a natural greedy algorithm is Max-Rate Greedy algorithm: Initially set the winning set to the empty set, and then iteratively, add a bid on a query with the highest ratio of marginal profit over the marginal cost, or the query with the highest ratio of marginal value over marginal cost.
S. Yamazaki is the president of SEL (Semiconductor Energy Laboratory ).
For instance, in [22] the authors used a two-pass K-means clustering algorithm and a KL-divergence distance metric to organize a document collection into 100 topical clusters (or shards) and demonstrated the benefits of selectively searching only a few shards per query.
Online algorithms propose solutions to problems where the input is not known in advance and must be processed as it arrives in a sequential   online   fashion [10].
Job Scheduling) problems.
Sampling-based approaches, such as video magnifier [20] and Minivideo [25], usually extract keyframes from videos in a random or uniform manner.
  'Algorithm 2 Greedy algorithm for shot subset selection 1: Input:v, y 2: Initialize prediction ¯ y ← ∅   ' <Equation_27>   '5: end for 6: return ¯ y satisfying the condition that ¯ y ∪ {y} is the shot set with the highest score.
  'Algorithm 2 Greedy algorithm using the independence graph 1: Input:x, y 2: Initialize prediction ¯ y ← ∅ 3: for i = 1, 2, .
For a fixed T and b, O(T, b) defines a fractional Knapsack problem over the set T .
O(N (St ∪ {x}), t) is the optimal solution to the fractional Knapsack problem (8) with budget t and can be computed in time min( t p min     n) by iterating over the list of nodes in N (St ∪ {x}) in decreasing order of the degrees.
Analysis of Approximation Algorithm.
The former task can be done using LSH algorithm in O(n e ) time for some exponent e ≺ 1[11], while the latter task can be done using e.g., the 2-approximate GMM algorithm [10] in O(|Ra|k) time.
This is done by using GMM algorithm during the offline process of dLSH * (lines 5-7, algorithm 4).
Arguments similar to those presented in the above probabilistic argument, applied to the case in which one random subset of S * is initially selected, can be used to prove that the weight of the optimal restricted solution for that class is at least 1/2 of the weight of the optimal unrestricted solution .
BFS: starts a Breadth First Search algorithm in an arbitrary node of the graph and assigns a value of f (i) = 1 to all nodes it encounters until it reaches a count of μn nodes.
  'Scheduling the monitoring tasks.
M ← m/s for i ∈ [0..M − 1] do add to P, the substring C s [i(n/M)] return pool P Regular Sampling.
Sampling.
Sampling is an essential component of big-data algorithms , so initially we explored alternatives to regular sampling.
Reservoir Sampling.
We apply the Chu-Liu/Edmonds' algorithm[6] to solve the problem.
They took a similar philosophy of maximizing likelihood over all the possible tree structures and employ the Markov Chain Monte Carlo Sampling algorithm[10] to get the optimal structure.
Gibbs sampling is a simple and widely applicable Markov chain Monte Carlo algorithm.
  ' 4.2 Depth-first search for candidate genera- tion   'Using the seed bigrams described in the previous section, we attempt to generate higher order n-grams that will finally serve as candidate micropinions.
Greedy algorithms are known to generate good solutions when maximizing submodular functions with a cardinality constraint and were used in [1, 14].
"In order to ensure faster GMM fitting, we optimized model fitting procedure; the EM model was parameterized using K-means clustering."
We extract the following features from the images in our data set:   Autocorrelation: Autocorrelation measures the coarseness of an image by evaluating the linear spatial relationships between texture primitives.
 Legendre and Zernike Momemts:   'Global moments are employed as the invariant global features of an image in pattern recognition due to their ability to recognize shapes.
  ' 2.3 Extending Static Representation with Temporal Signals   'Our approach is inspired by the desire to augment text representation with massive amounts of temporal world knowledge .
 'Sampling is the process where we decide which users get assigned into what variant.
  ' Facts and Hypotheses.
Here, we just give an example for facts about the disambiguation prior of the wic Elvis@D29: disambPrior(Elvis@D29, ElvisPresley, 0.8)[1] disambPrior(Elvis@D29, ElvisCostello, 0.2)[1]  Hypotheses.
Approximation Algorithms.
Isotonic regression.
Variations in segmentation accuracy on both datasets with changing values of penalty are plotted in Figure 3.
Sorting: In this approach, proposed by Silvestri in [21], we simply sort the collection by their URLs, then assign docIDs according to this ordering.
  hepph: a co-authorship network among arXiv High Energy Physics publications.
Weighting: The Sampling step uses only the token weights from R 2 for the sampling, ignoring the weights of the in the other relation, R 1 .
Section 5.1 addresses the Sampling step, while Section 5.2 focuses on the Weighting and Thresholding steps for the asymmetric versions of the join.
 'Given the RiWeights relations, we now show how to implement the Sampling step of the text join approximation strategy (Section 4.2) in SQL.
Finally, a pure-SQL   simulation   of the Sampling step deterministically defines that each tuple will result in Round(S · RiWeights.weight   'RiSum.total   ' )   successes   after S trials, on average.
In the rest of the paper –to keep the discussion close to the probabilistic framework– we use the cursorbased approach for the Sampling step.
  ' 5.3 Implementing a Symmetric Text Join Approximation in SQL   ' Up to now we have described only an asymmetric text join approximation approach, in which we sample relation R 2 and weight the samples according to the tuples in R 1 (or vice versa).
Sampling has been utilized in a variety of tasks of database interest including data mining, estimation and optimization of queries, and query answering.
Variational approximation was often adopted (3) under some unwarranted meanfield assumptions.
To address these two different limitations, we present a novel scalable solution to do posterior inference using Gibbs Sampling which avoids any restricting assumptions and can be easily scaled up to capture thousands of topics from millions of documents in a parallel environment, while also being faster on single machines .
To deal with the non-conjugacy in the model, within the Gibbs Sampling framework, we use recent developments in Stochastic MCMC and use Stochastic Gradient Langevin Dynamics (SGLD) (28) to sample the logistic normal parameters by observing only a mini-batch of the document set.
Hence, we intend to try a more adaptive approach such as AdaGrad (11), or use more recent and sophisticated Stochastic Gradi-ent Monte Carlo methods such as SGHMC (9) and SGNHT (10) for inference.
Our algorithm is a novel combination of Stochastic Gradient Langevin Dynamics and Metropolis- Hastings sampler using Alias tables in a blockwise Gibbs Sampling framework.
  '.., vn)   'where vi, vj ∈ V for 1 ≤ i, j ≤ n, i = j except possible 1 = n.   'Definition 1 (Betweenness Centrality).
(Betweenness Centrality Update Theorem)   'By Lemma 1 and Lemma 2, we can compute the betweenness centrality of a vertex vi, c(vi).
Since this learning framework is similar to that of Parameter servers [9] and Petuum [10] or Spark [11], D-NOC could be implemented on these promising platforms without major modification.
We focus on Markov chain Monte Carlo sampling [28] inference than variational inference [32], since the former has the preferable advantages such as easy implementations and sampling speed.
 'With reference to the graphical model shown in Figure 2, the generative procedure of NOC is described as follows:   ' 3.3.1 Global hierarchical topic tree structure gener- ation   ' Each node in the tree defines a CRP over children and corresponds to an atom (topic) independent of G0 using Eq (4):   ' <Equation_16>   ' <Equation_17>     where u\\ is the suffix of u consisting of all but the earliest word.
This is why we call this algorithm an approximated Gibbs Sampling.
Although we require Memcached to accelerate the I/O performance in the system build on Hadoop, we would not use it in the the system build on Spark.
Consequently, D- NOC can process large data sets while preserving full generative model performance, and offers the approximated Gibbs Sampling of NOC.
Variations of the same measure have also been successfully used for web-spam detection [11], automated image colorization [13] and image segmentation with user feedback [7].
Inverted indexes are usually stored in highly compressed form on disk or in main memory, such that each list is laid out in a contiguous manner.
However, to be efficient, computations need to the carefully structured to conform to the programming model offered by the GPU, which is a data-parallel model reminiscent of the massively parallel SIMD models studied in the 1980s.
Query Stealing and Scheduling: One disadvantage of the above query assignment policy is that when the query characteristics change temporarily or when processing time is not accurately predicted, the load on CPU and GPU may become unbalanced such that one processor has too much work and the other too little.
Inverted indexes are usually stored in highly compressed form on disk, or sometimes in main memory if space is available.
The data is then transferred to the GPU Global Memory, which also maintains its own cache of index data in order to decrease the time for main memory-to-GPU data transfers.
"Multi-level tree-based algorithms for prefix sums on GPUs were discussed in [4] based on earlier SIMD algorithms; we adapted and fine-tuned these for unary and b-bit binary data."
Hashing methods are approximate nearest neighbor search methods , and are popular due to their speed.
Gradient descent: The weight matrix W is updated by averaging gradient estimates given by a set of queries and taking a step along the average gradient.
Dealing with Aggressive Fingerprinting.
Realtime password locator: this component maintains a 16 character FIFO that stores the last 16 keys typed while the browser had focus.
when a typed sequence in the FIFO matches a password in the PPL) it checks whether the URL of the current server is among the URLs previously associated with the password.
By tabulating all possible password lengths from 7 to 16 (no passwords shorter than length 7 generate entries in the PPL, and the realtime locator FIFO is length 16) and all combinations of the 4 different types we find a unique mapping between the quantized password strength Figure 9: Different types of passwords as a function of length averaged across all sites.
We then run an association rule miner [24] 1 (which is based on the Apriori algorithm in [1]) to find those frequent itemsets.
The Apriori algorithm works in two steps.
The Apriori algorithm handles each transaction and frequent itemset as a bag of items (or words) without the notion of sequence.
We can then determine the sequence of words of the frequent itemsets (which are lexicographically sorted by our Apriori algorithm implementation) by hashing each of them into the hash table containing the lexicographically sorted transactions.
For example, the study in [25] did reasoning over pD* that includes rules for handling owl:sameAs, owl:IFP and owl:FP axioms.
If both of them have counterparts in these frequent property combinations, say (Sampling.
By applying the Apriori algorithm to perform association rule mining, we discovered 9,610 frequent property combinations with confidences greater than 0.98.
For example, we can directly implement steps (i)∼(v) in the GraphX of Apache Spark 3 .
Query optimization is a fundamental and crucial subtask of query execution in database management systems.
Selectivity Estimation Heuristics.
Heuristics are used to weight the nodes and edges of a BGP abstraction.
Heuristics Without Pre-computed Statistics.
 Heuristics With Pre-computed Statistics.
Sampling is potentially useful for large RDF datasets at the cost, however , of a loss of accuracy.
Heuristics based on the probabilistic framework (Section 3.3) implement wrappers around these two methods and, hence, provide selectivity information for the optimization of basic graph patterns.
: Query optimization time)   'RDF quads) are done on a small fraction of the subjects of the first range check.
Randomized algorithms (e.g.
Genetic algorithms (e.g.
, 'WWW 2007 / Track: Search Session: Personalization ' 'Spreading activation has been proposed for searching in graphs for over a decade [19, 20, 16].
Clipping at k is reasonable , because, in applications, users are generally not adversely affected by erroneous ranking low in the ranked list.
Moreover, these schemes do not systematically interweave tasks from di↵erent batches, and thus present also the same shortcomings as FIFO.
One popular approach currently used in Hadoop/Yarn is Fair Scheduling (FS) [16] .
This approach is the crowd-equivalent of Delay Scheduling [39].
  FIFO order is [.
In each run, we use a different scheduling algorithm from: FIFO, FS, RR, and SJF, with 10 \uf8ff |workf orce| \uf8ff 15.
We see how FIFO just assigns tasks from a batch until it is completed.
"To summarize, the main observations that we draw from our experiments are:   Large HIT batches are preferred by crowd workers; Thus, large batches attract a larger workforce which implies a higher throughput;   Individual workers perform slightly better when working on homogeneous batches (compared to batches regrouping di↵erent types of HITs);   HIT-BUNDLE have overall a positive impact on task latency as they tend to attract bigger workforces;   Scheduling techniques make it possible to prioritize HIT batches as needed while being fair with all running batches and all involved workers; In particular, FS and WCFS equally distribute the available workforce over the di↵erent batches;   The techniques we evaluated can be applied on top of existing micro-task crowdsourcing platforms in a scalable fashion without the need of new push crowdsourcing mechanisms or systems, thus leveraging large crowds of people already engaged on existing plat- forms."
  'Task Assignment and Scheduling.
Scheduling tasks for   'the crowd has been recently discussed in the context of work quality mostly, while we focus on eciency.
Thus, we explored the problem of scheduling HITs using weighted Fair Scheduling algorithms, where priority is expressed as a function of price.
This paper presents GlobeTP, a database replication system that exploits the fact that the database queries issued by typical Web applications belong to a relatively small number of query templates [2, 19, 27].
We however return briefly to this issue in Section 6.2 to   'WWW 2007 / Track: Performance and Scalability Session: Scalable Systems for Dynamic Content   'show how availability constraints can be taken into account in GlobeTP.
We show in Section 5.5 how GlobeTP can easily be coupled with a widearea database caching system so that both throughput and latency can be improved at the same time.
Instead, GlobeTP treats each read and write query as independent operations.
However, our system can also be easily coupled with a distributed database query cache such as DBProxy [2] and GlobeCBC [27].
Query routing gains a higher significance in GlobeTP.
The query router used in GlobeTP thus differs from the traditional query routers used in fully replicated databases in the following aspects.
 'In this section, we compare the performance of full database replication to GlobeTP for two well-known Web application benchmarks.
In addition to these experiments, we also evaluate the benefit of adding a database query caching layer to GlobeTP.
Since GlobeTP only operates at the database query level, the proportion of update interactions is irrelevant here.
  ' One important goal of GlobeTP is to reduce the replication degree of individual database tables to reduce the number of UDI queries to be processed.
Since searches are implemented as very expensive read queries, removing them from the workload mechanically improves the cost ratio of UDI queries and thereby the gains to be expected from GlobeTP.
For these, GlobeTP will not provide any improvement unless the application itself is updated (see Section 6.1).
  'WWW 2007 / Track: Performance and Scalability   ' 5.3 Effect of Partial Replication and Template- Aware Query Routing   'To illustrate the benefits of GlobeTP, we measured the query execution latencies of read and UDI queries together using different configurations.
For each of the two benchmarks , we compared the performance of full replication, GlobeTP using RR-QID query routing, and GlobeTP using cost-based query routing.
As one can see, in both cases GlobeTP processes queries with a much lower latency than full replication.
For example, in RUBBOS GlobeTP processes 40% more queries than full replication within 10 ms.
In TPC-W, GlobeTP processes 20% more queries within 10 ms than full replication.
In RUBBoS, GlobeTP combined with cost-based routing outperforms both other configurations.
One should note that GlobeTP has greater effect on the latency in the case of RUBBoS than for TPC-W.
As we will see in the next section , the throughput improvements that GlobeTP provides are significantly greater for TPC-W than RUBBoS.
On the other hand, GlobeTP can sustain up to 150% higher throughput while using identical hardware resources.
In RUBBoS, GlobeTP again performs better than full replication, yet with a lower difference.
With 4 and 8 servers, GlobeTP sustains 120 more EBs than full replication, which accounts for 57% of throughput improvement.
  As noted in Section 3.2, GlobeTP can easily be coupled with a database query caching system as most query caching systems rely on the same assumption as GlobeTP regarding the explicit definition of query templates.
However, GlobeTP focuses on improving the throughput of the application's origin database, while query caching systems aim at reducing the individual query execution latencies.
In our experiments, we use our own in-house query caching solution, GlobeCBC [27].
GlobeCBC acts as a very simple object cache: unlike other similar systems it does not attempt to merge multiple query results into a single view of the database.
Instead, GlobeCBC stores the result of each query independently from the others.
  In this paper we have presented GlobeTP, which exploits table-granularity partial database replication to optimize the system throughput.
To increase the system's scalability even further, a natural extension is to combine GlobeTP with a database query caching system such as GlobeCBC, as both systems rely on the same definition of query templates.
These systems complement each other very well: query caching improves the execution latency in a wide-area setting by filtering out many read queries, while GlobeTP shows its best potential for improving throughput under workloads that contain many UDI queries.
The instances of these classes and properties, modeling the DW contents to be further analyzed, are intensionally defined in the schema, following the well-know   Global As View   (GAV) approach for data integration [19].
One other feature that is common to all production rule systems is that they are all based on (a version of) the Rete algorithm [10]—a combination of the semi-naive bottom-up computation [27] commonly used in Datalog systems with a certain heuristic for common expression elimination [6].
Like Drools, it is mainly a bottom-up system, based on an enhanced version of the Rete algorithm, but it also has certain top-down inference capabilities.
    Recursion is perhaps the most important feature that distinguishes rule-based systems from traditional database management systems (SQL:1999 extensions notwithstanding).
RW-TR is built on the distributed graph processing system Signal/Collect [24].
2 Signal/Collect is similar to Pregel [19] , GraphLab/Pow- erGraph [6], and Trinity [22].
  'RW-TR is built leveraging the large-scale, parallel and distributed graph processing framework Signal/Collect 4 [24, 25].
In contrast to other frameworks, Signal/Collect allows for asynchronous execution, multiple vertex types, and the ability to change the graph structure during the execution.
Conceptually, Signal/Collectvertices can be seen as actor-like active elements, where the framework handles messaging, parallelization and distribu- tion.
The core idea of RW-TR is to build a triple store with three types of Signal/Collect vertices: Each index vertex corresponds to a triple pattern, each triple vertex corresponds to an RDF triple, and query vertices coordinate query execution.
The index graph is optimized for efficient routing of query descriptions to data and its vertices are addressable by an ID, which is a unique [ ' 'As mentioned before, RW-TR is a triple store with three types of Signal/Collect vertices: Triple vertices (level 4, Fig.
These vertices are added to Signal/Collect, which turns them into parallel processing units.
After the query optimizer determines the execution order of the triple patterns, the query gets processed as follows: ' 'The query vertex emits a single query particle, which is routed (by Signal/Collect) to the index vertex that matches its first unmatched triple pattern.
Signal/Collect uses the same mappings for vertex addressing and for routing messages to (potentially non-existent) index vertices.
Pipelining, however, comes at the cost of more complexity in both the operators and their coordination.
A full efficiency evaluation of RW-TR's RWR capability or an extension to other graph analytics, which would be supported by the underlying Signal/Collect framework, is beyond the scope of this paper.
Rabin fingerprinting was used earlier by Manber [11]  to find similar files within a file system.
Bandwidth access is both expensive and limited in developing regions.
]}]}}, '_score': 8.869356}, {'_index': 'mm  '_type': 'pubs  '_id': 'conf_www_KellyM02  '_source': {'content': {'chapters': [{'paragraphs': ['Aliasing occurs in Web transactions when different request URLs yield replies containing identical data payloads.
  Aliasing: Many times different names or abbreviations are used for the same city name.
The cleanest formulation of which we are aware is given by Meila [12] , who proposes addressing many of the concerns with the above methods using a new measure called the Variation of Information (VI).
Sampling training instances and validating on held-out labeled data show that the classifier learns quickly, settling near its best accuracy within only 2–4% of the training data available.
For example, string decomposition techniques based on Karp-Rabin fingerprints have been used to remove redundant network transmissions in applications such as general network traffic [32], distributed file systems [25, 10], and web access [29] .
This problem is solved through the use of Karp-Rabin fingerprints [18].
Our work is most closely related to, and an extension of, the value-based caching technique in [29] , which itself employs Karp-Rabin fingerprints as introduced in [18] .
In our simplified example, we suppose that the manager is asked to update the affected loan requests by adding the LastVariation to the Established Rate attribute of the Loan Request related to an Installment Plan of the interested Country.
Further, each community has a probability score s   '(m) c representing the log probability vector of the c th row of the confusion matrix π (m) of community m. We apply a softmax operator to the community score s   ' <Equation_11>   ' for all the classes c to derive a normalised exponentiated version of s   ' <Equation_12>     which we refer to as the community confusion matrix π   ' <Equation_13>   ' here δ is the Dirac delta function imposing the equality constraint between π   ' <Equation_14>   'and the softmax of s   ' <Equation_15>   '.
The novel periodicity engagement metrics of user behavior (resulted from the Discrete Fourier transform of 4 state-of-the-art engagement measures) were applied in [8] to evaluate, by means of A/B experiments, different changes of the search engine ranking algorithm, changes of the user interface, and changes of the engine\'s efficiency.
"In order to get the average amount of information contained in daily user engagement series, we calculate entropy for our series TS d in the following ways: (a) the Shannon entropy Ent Sh as in [26] ; (b) the Permutation entropies Ent Perm as in [2], and the Sorting entropies Ent Sort as in [2]  of orders n = 2, .., |Tp| − 1; (c) the Approximate entropies Ent Ap as in [23, 24], and the Sample entropies Ent Smpl as in [24] for m = 2, .., |Tp| − 2."
Sampling Differential Privacy.
Furthermore, the number of records contributed by each user can be unbounded for many applications, as a Algorithm 1 Simple Random Algorithm (SRA) Input: raw dataset D, sampling factor l, privacy budget Output: sanitized answer˜q1answer˜ answer˜q1 and˜q2and˜ and˜q2 /* Simple Random Sampling */ 1: DR ← ∅ 2: for k = 1, .
In the domain of DE, it holds that GS(q1) = d. Greedy Sampling.
, n do 3: T k ← σuID=u k (D) /* T k : records of user u k */ 4: Random sample d record from T k , add to DE /* Generate Private Item Counts */ 5: q1(DE) ← compute count ci(DE) for every i 6: ˜ q1(DE) = q1(DE) + Lap( d 0   ') m /* Estimate Popularity */ 7: ˜ p ← normalize histogram˜q1histogram˜ histogram˜q1(DE) /* Greedy Sampling */ 8: DG ← ∅ 9: for k = 1, .
Determining periodicity: Our algorithm for detecting group chats uses Fourier analysis to help determine whether a hashtag has regular meetings.
Many different approaches have been proposed for periodicity detection in time-series data, for example, using Fourier analysis [25] and Wavelet transforms [20].
However, we chose the Fourier analysis based method instead, for two reasons.
"Second, Kleinberg\'s method does not detect whether or not the bursts are of a periodic nature, nor does it produce the period; both of these are natural outputs of the Fourier analysis based method."
Autocorrelations can be computed from the Fourier transform as:   ' <Equation_7>   ' <Equation_8> ' 'Given the timeline function f h for a hashtag, we first compute the Fourier coefficientsˆfcoefficientsˆ coefficientsˆf (ξj) for NF equally spaced frequencies ξj in a fixed range [−1/τF , 1/τF ] using (2).
Otherwise,   ' <Equation_18>   'So by Lemmas A.4 and A.8,   ' <Equation_19>   'A.1 Results about Autocorrelation   'Here we state results which say that the autocorrelation of the true period is high, and that the autocorrelations of certain other periods are low.
[22] studied the trade-off between time series compressibility and perturbation, developing two algorithms based on Fast Fourier Transform (FFT) and Discrete Wavelet Transform (DWT) respectively .
"Step response is defined as the time behavior of the output of a general system when its input suddenly changes from zero to one in a very short time; Rise time refers to the time required for the output to reach 90% of the reference value."
Therefore, to eliminate the effect of multipath on distance estimation, we devised mechanisms to extract the Energy of the Direct Path (EDP) [27]  and the Time-of-Flight of the Direct Path (TFDP) [24] .
  Energy efficiency: Apart from the AP to which the client is associated with, it may not respond to the other APs' data packets as its WiFi radio will mostly be off due to the use of power-save mode (PSM).
 'Signal strength-based distance estimation approach [18, 27– 31]  leverages the fact that the wireless signal suffers attenuation proportional to the distance between the transmitter and the receiver.
To address the adverse effect of multipath on distance estimation, our previous work [27]  primarily uses only the Energy of the Direct Path (EDP), ignoring the multipath reflections between the client and the AP.
Signal strength based ranging also relies on the knowledge of the transmit power of the client.
However, we also combine TFDP with Energy of the Direct Path (EDP) to ensure scalability.
We determine the Energy of Direct Path (EDP) and Time-of-Flight of Direct Path (TFDP) using the time-domain multipath characteristics [24, 27].
    Signal strength based approach: Most WiFi based indoor positioning systems utilize the RSSI to determine the client's location.
  Energy efficiency: CUPID2.0 imposes minimal energy overhead on client devices.
These features are usually generated based on mel-frequency cepstral coefficients (MFCCs) [7] by applying Fast Fourier transforms to the signal.
The most likely sequence estimated based on the acoustic model are identified using Viterbi algorithm.
Folding (see algorithm 1) is an approach that appreciates the original ranking, by assigning a larger probability of being a representative to higher ranked images.
  'Algorithm 1 Folding Input: Ranked list L of I Output: Clustering C 1: Let the image L1 be the first representative R1 2: for Each image Li do 3: if d(Li, Rj) > ǫ(*) for all representatives Rj then 4: add Li to the set of representatives R 5: for Each image Li / ∈ R do 6: Find representative Rj that is closest to Li 7: Assign Li to the cluster of Rj (*)ǫ is defined as the mean distance all images have to the average image in I To achieve this, it uses a maxmin heuristic on the distances between cluster representatives.
Variation of information uses the contingency table, and is based on the concept of conditional entropy.
Table 1presents performance of the methods averaged over all topics, and Table 1: Average performance over all topics and assessors.
  'Results on Variation of Information Metric   'A different relative performance is given by the variation of information criterion.
Folding shows a better performance according to the Folwkes-Mallows index, a performance measure that focuses on image pairs that can be formed with images from the same cluster.
Sorting all products b according to c * (b) in reverse order, we obtain the dissimilarity rank P rev c * .
The Folding-in method for SVD update can be used in order to compute authority weights for new sessions.
There exists some methods for SVD update in which the new included information contributes for the semantic latent structure identified, but there is a good trade-off between effectiveness and efficiency is obtained in using the Folding-in method and periodically reevaluating a new set of representative sessions.
Parsing has a profound effect on Word documents (1 and 2), the worst offenders in terms of proprietary style attributes, which to compound the problem are displayed in a space-wasting manner.
Get upgraded to a shiny new avatar   Leaderboard -You would automatically be advanced on The Global Leaderboard.
For example, Broder\'s shingle-based fingerprints [14] require 24 bytes per fingerprint (it boils down to checking whether two or more Rabin fingerprints out of six are identical).
Fingerprints are more or less equally spaced out.
Folding such displays lets users more quickly navigate such structure, which is particularly useful for large hierarchies.
[20]  were among the first to examine and model periodicities and bursts in Web queries using methods from Fourier analysis.
Figure  12shows the precision-recall results for our autocorrelation model (Autocorrelation) compared to the baseline model (STL).
The Autocorrelation method proposed in this work reaches the same maximum recall as the state-of-the-art STL autocorrelation method (around 0.85), and outperforms it in precision for every recall level by up to 15 percent.
Spectral methods described in [9] also lack scalability (i.e.
Simulated annealing either incurs excessive time, or the starting point of the algorithm must jump to different random locations, which is impractical for the physically moving AP.
At this point, the AP computes the best pixel among these N e pixels, where   best   is defined as an utility function of CSI:   'Mobility Planning Heuristic   ' <Equation_1>    where p denotes a pixel covered by the AP, i denotes the index of its own clients.
  'Improvements to the Heuristic   'We discuss a few optimizations to the core heuristic above.
Global element declarations appear at the top level of the schema document.
Full scale deployment of the system will therefore be dependent on negotiating a higher search limit with Google, or on adapting the system to use an alternative search engine.
ONLINE MUSIC DISTRIBUTION SER- VICES    The model focuses on three online music distribution tools.
Fingerprinting   techniques focus on finding almost-identical music recordings, while tolerating small amount of noise distor- tions [1] .
The ID-resolution system should operate with legacy identifiers such as ISBNs, ISSNs, UPC codes, EAN codes, iButton identifiers, MAC addresses, etc.
Energy is calculated over sliding windows of 50ms duration.
We use pt(i) to denote the magnitude of ith frequency bin of the normalized FFT at time t. We then summarize spectrogram by computing Spectral Centroid (SC), Bandwidth (BW), and Spectral Flux (SF) as follows:   ' <Equation_6>   ' <Equation_7>   'Both spectral centroid and bandwidth are computed for each one of the 31 FFTs over a single 1 second segment, resulting to 31 SC and 31 BW features.
Bandwidth is a measure of the width/range of frequencies in the computed FFT.
Spectral flux values can consistently differentiate the various acoustic sources.
Besides spectral centroid, bandwidth, and spectral flux, we also compute the Mel-Frequency Cepstrum Coefficients (MFCC) [18].
More importantly, the best feature set for every model includes at least one feature from every class of features (Energy, ZCR, spectrogram, and MFCC), highlighting the complementary nature of the extracted features.
Figure 14(a) and Figure 15(a) show the raw audio signal along with the computed ZCR and Energy features for an example business audio recording across all three devices.
Figure 14(b) and Figure 15(b) show the raw audio signals along with the computed ZCR and Energy features after scaling the amplitude of the audio signal, and properly adjusting the zero level in the iPad2 and Nexus S devices.
Sampling is included as a dummy variable.
In this work, we are not using the rarely-available produced Figure 2: Spectrogram of a segment of audio, with selected landmarks overlaid (as black squares).
As a result:   ' <Equation_30>   ')   'Approximation X * I * ∼J * is constructed for each submatrix XI * ∼J * in X.
PMF: The Bayesian PMF by Markov Chain Monte Carlo method in [25] is used.
Motivated by the above issues, we propose a novel Monte Carlo algorithm which utilizes the preference propagation between users to enhance the recommendation performance.
We design an efficient Monte Carlo algorithm on this random walk process, which can precompute the similarity between existing users in a recommendation system as the model.
  A Monte Carlo algorithm based on the random walk process is designed to estimate the user similarity.
In Section 4 we propose a Monte Carlo algorithm to efficiently estimate the user similarity and provide theoretical analysis.
Inspired from [3], we design the basic Monte Carlo algorithm to estimate the similarity vector cτ of uτ as fol- lows.
  ' 4.2 Monte Carlo Algorithm with Precompu- tation    In the previous section we have introduced the online Monte Carlo algorithm (Algorithm 1) to estimate the similarity vector for the target user.
Thus we design another Monte Carlo algorithm with a precomputation approach to build a model to estimate any target users' similarity vector.
 'Owing to the Monte Carlo method, our approach can be easily extended to handle dynamic updates in the recommendation system.
GlobCausal   ' has been found to be consistently more effective than IniCausal.
For example, the pair iPod→jogging is returned at the 68 th rank by the method GlobCausal, while AggrConcept and AggrPatt return it within the top 5 results.
Variations of user interactions and network transmissions have been removed.
2 YOUTUBE and YOUTUBEMUSIC.
YOUTUBEMUSIC is a subset of YOUTUBE, consisting of only the music videos.
In the YOUTUBE, YOUTUBEMUSIC, MAPCLICKS, and WIKICLICKS data, the slope parameter is at Figure 1: Probability distribution of the number of times an item is consumed by a user over all datasets.
However , repeat consumption in YOUTUBE and YOUTUBEMUSIC still shows a preference for videos seen in the last day.
Hypotheticals are used widely in the law [16] and may be used to  present, support and attack positions (e.g., by testing the consequences of a tentative conclusion, pressing an assertion to its limits, and exploring the meaning of a concept)  and to  factor a complex situation into component parts (e.g., by exaggerating strengths, weaknesses or eliminating features). 
"The equation for calculating the centrality Centrality sch_term (w) of a word w is as follows:   ') (   ' <Equation_6>   'where Co(w, sch_term) denotes the number of sentences where w co-occurs with the search term sch_term; and sf(w) gives the number of sentences containing the word w. We also use the inverted document frequency of w, idf(w), as a measurement of its global importance 2 ."
  'Centrality scores for all words appearing in the input sentences are calculated and those words whose scores exceed the average plus a standard deviation form a set of centroid words.
 'One of the most widely used approximate inference techniques for such models is Markov chain Monte Carlo (MCMC) sampling, where one samples from a Markov chain whose stationary distribution is the posterior of interest [20, 21] Markov chain is defined by the conditional distribution of each latent variable, has found widespread use in Bayesian models [20, 22, 23, 24].
   By definition, Monte Carlo algorithms depend on randomness.
Variational inference, given an initialization, is deterministic, which is more in line with MapReduce's system for ensuring fault tolerance.
Variational inference typically requires dozens of iterations to converge, while Gibbs sampling requires thousands (determining convergence is often more difficult for Gibbs sampling).
  'WWW   ' <Equation_0>    (b) Variational Figure 1 : Graphical model of LDA and the mean field variational distribution.
Variational methods, based on techniques from statistical physics, use optimization to find a distribution over the latent variables that is close to the posterior of interest [28, 29] .
Variational methods provide effective approximations in topic models and nonparametric Bayesian models [30, 31, 32].
Variational methods enjoy clear convergence criterion, tend to be faster than MCMC in high-dimensional problems, and provide particular advantages over sampling when latent variable pairs are not conjugate.
Variational inference fits the variational parameters Ω to tighten this lower bound and thus minimizes the Kullback-Leibler divergence between the variational distribution and the posterior.
Variational EM alternates between updating the expectations of the variational distribution q and maximizing the probability of the parameters given the   observed   expected counts.
Variational inference is an attractive inference technique for the MapReduce framework, as it allows the selection of a variational distribution that breaks dependencies among variables to enforce consistency with the computational constraints of MapReduce.
Variational inference is also attractive for its ability to handle online updates.
Hence, applying CS on the objects in M   '(training examples) would make SVM behave alike the Rocchio algorithm [3, 22], which is not going to provide much help because Rocchio is not as powerful as SVM..
"The Rocchio algorithm [3, 22] has been applied to this problem in [15]; and the Naïve Bayes (NB) algorithm [18] has been applied to this problem in [8], without exploiting information in the source taxonomy."
Invest in Geographically Global Projects.
Np is number of all investors for project p. With this definition, low geographic dispersion is associated with projects with investors who live close to the founder, while high dispersion    Hypothesis [H1] A project is likely to be financed by frequent investors if its founder: [H1.1] frequently updates the project after launching it.
Bandwidth could be further reduced by having peers send only changed bits [22] instead of re-sending the entire bloom filter with each change.
In more details:   Taste Profile Subset (TPS): contains user-song play counts collected by the music intelligence company Echo Nest.
The probabilistic model is s(q, c) Relevance score of query q that matches a given prefix in composition c p(q, c) Preference for query q in composition c Q (c) Set of top N queries ranked by s(q, c) w Signal parameter vector x, y Signals of installed apps and recently opened apps based on a combination of the relevance score and app-related signal score on mobile devices.
 'As mentioned in §3.2, for indexing convenience all the signal parameters βq,a and β k from (3.1) in any fixed order constitute the signal parameter vector w. Let wj be the j th element of vector w of dimension d. We denote the 1 and 2 norms of vector w as   ' <Equation_2>   'Signal parameter vector w is to be inferred based on maximum likelihood.
Signal parameters of value 0 indicate that their corresponding noisy signals are filtered out in (3.1).
 'Globally across queries, the textual description of each type t induces a language model.
If another event-handler is executing at that time, other event-handlers are placed on an active queue and are serviced by the browser generally using a first-in-first-out (FIFO) policy.
UI Events do not require any technical solution because of the FIFO policy of browsers.
Orbanz and Teh [24]  presented an overview of how Bayesian nonparametric models work for a variety of machine learning problems, and provided a few examples where the models can be employed.
Sampling from the DPM model is conducted by the following generative process:   ' <Equation_0>   'where F is a given likelihood function parameterized by θ.
In particular, we develop an efficient Markov chain Monte Carlo (MCMC) algorithm [26] , or more precisely a Gibbs sampler, to approximate the posterior for URM.
Sampling y: Let wua denote the a-th word in user u\'s tweets.
Sampling z: Gibbs sampling for retweet topics z is similar to that for tweet topics y.
Sampling β: Following the simulation of new tables in the CRF introduced in [25], the prior global proportions β can be sampled   ' <Equation_19>   '(b) UCM as a DP hierarchy by simulating how new topics are created for c uk draws from the DP with precision λβ k (dishes in the CRF), which is a sequence of Bernoulli trials for each u and k:   ' <Equation_20>   ' <Equation_21>   'A posterior sample of β is then obtained by:   'β ∼ Dirichlet(m1, .
Sampling π: Similarly, Equations (12) and (13) for sampling retweet topics z require the posterior samples of π.
Sampling y: The Gibbs sampling equation for topic assignment yua of the a-th word in user u's tweets is:   ' <Equation_37>   ' <Equation_38>   'whereas a new value knew is sampled for yua based on the following probability:   ' <Equation_39>    denotes the number of words in user u's tweets assigned to topic k, excluding the current assignment yua,   ' <Equation_40>    denotes the number of times word w is assigned to topic k across all tweets, excluding the current assignment.
Sampling z: For the b-th word in user u's retweets, a previously seen topic k is sampled from the distribution given by:   ' <Equation_41>   ' <Equation_42>   'whereas the probability that the topic assignment z ub takes on a new value knew is:   ' <Equation_43>    denotes the number of words in user u's retweets assigned to topic k, excluding the current assignment z ub , and g    Number of tweet topics  Gibbs sampling process we collected posterior samples after the Markov chain had converged.
(a1, a2) ∈ L1 ∧ (b1, b2) ∈ L2 ⇐⇒ ((a1, b1), (a2, b2)) ∈ E Figure 3 shows an example of the constructed PCG of two graphs.
The constructed PCG between them (Cf.
PCG can represent the linking relations of node-pairs between two graphs, we use PCG to capture the interaction of cross-lingual links between two Wiki knowledge bases.
Our proposed approach takes PCG of two Wiki knowledge bases as input and solves the knowledge linking problem by predicting the label (equivalent or inequivalent) of nodes in the PCG.
The solution to this is to use approximate estimation methods like Variational Methods [8], Expectation– propagation [28], and Gibbs Sampling [19].
Gibbs Sampling is a special case of Markov-chain Monte Carlo (MCMC) [18] and often yields relatively simple algorithms for approximate inference in high-dimensional models such as LDA [21].
The first use of Gibbs Sampling for estimating LDA is reported in [19] and a more comprehensive description of this method is from the technical report [21].
After finishing Gibbs Sampling, two matrices Φ and Θ are computed as follows.
, 'Statistics of the crawled Wikipedia data ' 'We estimated many LDA models for the Wikipedia data using GibbsLDA++ 4 , our C/C++ implementation of LDA using Gibbs Sampling.
Topic inference for documents in W also needs to perform Gibbs Sampling.
The last experiment with Web search snippets is to examine how Gibbs Sampling influences the classification accuracy .
To estimated parameters of each model, we ran 1,000 Gibbs Sampling iterations, and saved the estimated model at every 200 iterations.
As depicted in the figure, for those numbers of topics that give high performance (e.g., 30, 50 70, 90 topics), the accuracy changes slightly with respect to the different numbers of Gibbs Sampling iterations.
Although it is hard to control the convergence of Gibbs Sampling, we observed that it is quite fast and yields stable results after the   burn-in   period (about 200 iterations).
For example , estimating the Wikipedia data using GibbsLDA++ with 50 topics and 2000 Gibbs Sampling iterations took about 8 hours on a 2GHz processor.
 'Sampling approximately uniformly at random (uar) is a key enabling technology that we use in this work, and we review sampling techniques in some detail here.
We call this the Sampling walk, whereas the PageRank walk with d = 0 is called the Wander walk.
"We will use only Sampling walks here; Wander walks will be used in §4."
Hence we add a PageRankstyle random jump parameter to the original Bar-Yossef algorithm [2], set to 0.01–0.05 throughout our experiments, i.e., with this probability at every step, we jump uar to a node visited earlier in the Sampling walk.
Berg confirms [4] that this improves the stability and convergence of the Sampling walks.
showed that the samples collected by a Sampling walk have degree distributions that converge quickly (within a few hundred distinct page fetches) to the   global   degree distribution (obtained from the Internet Archive).
Perform separate Sampling walks starting from each of them.
Ideally, we should use one Sampling walk for collecting each sample page (i.e., keep only the last page reached in every Sampling walk), but walking is expensive (mainly because of backlink queries which need to be polite to the search service).
  'In our first experiment, we pick some 20 topics from our 482-topic Dmoz collection and one representative URL from each topic as a starting point for a Sampling walk.
It indicates that there is a welldefined background topic distribution and we are being able to approach it with suitably long Sampling walks.
  '(a) Sample a page u nearly uar from the Web using the Sampling walk and the virtual hop technique.
Figure 7: Energy savings of SCORE relative to oracle.
To mimic the two-step filtering process described above, we propose the Link Bootstrapping Sampling (LBS) model as a variation of induced subgraph sampling [15].
We propose a two-step model, called the Link Bootstrapping Sampling (LBS), which is a variation of the induced subgraph sampling pro- cess [15].
In a directed network of arbitrary degree distribution p(j, k), a GCC exists if [22]:   ' <Equation_5>   ' For the copied network induced by the Link Bootstrapping Sampling , the average of the degree and joint degree distribution of the subgraph G(S) sampled by the Link Bootstrapping Sampling method from a network G with joint distribution ps(j, k), can be calculated, respectively, as jk = p 2 e jk and k = pek .
  'Thus the link copy probability pe at which a GCC emerges in the sampled subgraph under Link Bootstrapping Sampling depends in a simple and intuitive manner on the properties of the source network.
The Link Bootstrapping Sampling with uniform node and edge sampling preserves the clustering coefficient of the source network, up to a multiplicative factor corresponding to the link sampling probability.
Hypothesis 2 is about measures that evaluate shilling attacks based on knowing exactly which items are being attacked, which the operator of the recommender system will not know.
Hypotheses (a)–(c) are expressed via edges, with edge weights indicating strength of belief.
According to Kass and Raftery's interpretation table of log-Bayes factors [21], we find that all differences are decisive which is why we refrain from presenting explicit Bayes factors.
This property of Bayes factors has been seen as a limitation in the past—as originally pointed out by Kass and Raftery [21].
Since we know the type of answer we are looking for, we can use this as an additional constraint, by discarding all incorrectly typed answer entities (e.g., THE MOTHERS OF INVENTION and MUSICAL ENSEMBLE in Fig.
Error analysis led us to conclude that the effect is due to the QA system: result snippets that mention the subject's children often also mention their spouse, to the extent that, in some cases, the spouse appears more often in the snippets than the children themselves, so our QA system, which uses frequency of occurrence among its main features, may return the spouse in place of the children.
A special case of this framework is Global Collaborative Ranking (GCR) where global low-rank structure is assumed, as is usually done in the ranking counterpart of Probabilistic Matrix Factorization (PMF)-type methods [1, 25, 31, 33], although equal importance is given to the whole ranked list.
  ' 3.1 Baseline 1: Heuristically Labeled Negative Examples   'Perhaps the simplest approach is to use a non-traditional classifier to distinguish positive from unlabeled examples.
"The decision variable of the optimization problem latency vs. bandwidth costs is the update schedule between sites, denoted by t [t] n,k ; the number of updates of user un sent to site S k during time bin t. We denote by t [t] n,k the size of the updates of user un sent to site S k during time bin t. Transmission of updates of a given user to a given S k follows a FIFO policy to ensure temporal consistency."
  'Optimization metric: Bandwidth costs   ' The incoming and outgoing traffic volumes of each site S k depends on the upload strategy and updates:   ' <Equation_0>   'In general, a peak-based pricing scheme is used as a cost function (p k (·)).
  'We describe two solutions – push/FIFO and a pull based approach that mimic various cache-based solutions (including CDNs) that can be used to distribute long-tailed content.
  'Immediate Push/FIFO:   'The content is distributed to different PoPs as soon as it is uploaded.
Assuming there are no losses in the network, FIFO decreases latency for accesses as content will always be served from the nearest PoP.
In order to calculate penalties, we first note that for FIFO and TailGate there are no penalties, while for PULL, penalties is simply the number of uploads that get transmitted to different PoPs, as all of these uploads will face higher latency.
Using large traces gleaned from an OSN, we show that TailGate can reduce costs by as much as 80% over a naive FIFO based mechanism and as much as 30% over a pull based approach that is employed by CDNs.
Defense against Stateless Fingerprinting.
Variations result when articles select different subparts of the same statement, or choose different paraphrases of the quote.
  '(1) Global rounding: This rounding tries to align the ratio of trust to distrust values in F to that in the input M .
Rounding (3 cases): Global, Local, or Majority.
Global constraints are defined by opposition to local constraints.
Recursion is implicit.
For such pages with a known structure, a simple Topic Parsing algorithm uses regular expressions which were manually devised for specific fields in the pages, based upon their performance on test data.
[4] presented a prototype system, named Pulse, for mining topics and sentiment orientation jointly from customer feedback.
Heuristic model performs the worst of the three for all NDCG levels, while the enhanced probabilistic method that incorporates random walks provides small but consistent improvements over the basic probabilistic model.
This is exactly what a system called Spark is doing [3].
Spark provides extended browsing capabilities through the knowledge graph exploiting public knowledge bases from the Semantic Web, in combination with proprietary data, to provide related entity suggestions for web search queries.
This is a path that can be potentially explored by a user following suggestions made by Spark.
"Starting from the entity query   selena gomez   , Spark is triggered recommending several related entities for exploring, one being Kevin James, which itself also triggers Spark to suggest Henry Winkler ; this process continues with the following sequence of clicks on recommended entities, Robin Williams, Al Pacino, and Robert de Niro, to finally reach Marlon Brando."
We perform a large-scale analysis on search logs of 2M users on the existing Spark system [3].
We analyze the types of queries and entities that users interact with, exploring who are the users interacting with Spark results, the characteristics of their sessions, and the interplay between the typical search results and Spark entity recommendation results.
Our analysis clearly indicates that Spark is able to promote an explorative behavior.
Based on our analysis, we develop a set of query and user-based features that reflect the click behavior of the users and explore their impact in the context of click prediction on Spark (i.e., predicting future interactions with entity recommendations).
The focus of our study is Yahoo\'s Spark system [3].
Spark extracts several signals from a variety of data sources, including user sessions, Twitter and Flickr, using a large cluster of computers running Hadoop.
Spark [3] falls in this type of systems.
This is the focus of this paper studying user behaviors for queries for which Spark returns recommended entities, for users to browse during their search session.
Spark uses Yahoo Search, Twitter, and Flickr as sources to extract the co-occurrence information .
Spark uses learning to rank approaches to derive an efficient ranking function for entities related to a query entity.
Figure 1shows the related entity recommendations made by Spark in Yahoo Web Search for the query   barcelona spain   .
This is a place, so Spark returns   related places   .
After clicking one of the related entities a new query is launched with the related entity leading to a new search result page, which can potentially contain more recommendations by Spark based on the new query.
The effectiveness and efficiency of the Spark system has been described recently in [3].
We study how users interact with the related entity recommendations returned by Spark as it performing a large-scale analysis on search logs of 2M users.
 'We performed a large-scale analysis exploring the click behavior of users submitting entity queries that trigger Spark.
Then, we study the users, investigate the impact of their demographics (age and gender) on the observed click behavior, and attempt to characterize sessions (e.g., in terms of duration) that lead to a successful interaction with Spark (i.e., at least one click on a related entity).
We also distinguish between different navigation patterns of users while interacting with Spark.
 'We collected a sample of 2M users focusing on their activity on queries that trigger Spark to recommend related entities.
Spark aims to promote exploration.
For a given query, we define the search CTR (respectively the Spark CTR) as the total number of clicks on a search (respectively Spark) result returned as an answer to this query divided by the total number of times that the query was issued (respectively, triggered Spark).
We leave this for future work, as, in this paper, we want to understand what makes users click on the entities returned to them by Spark.
We therefore focus on CTR, as our metric to study user exploration with Spark results.
We also restrict ourselves to sessions where Spark was triggered to recommend entities for users to explore as part of their search session.
As discussed in the previous section, not all queries submitted by users lead to entities returned by Spark.
Although it would be interesting to compare users\'   explorative behavior   when returned or not returned Spark results, our focus is to gain an understanding on what makes users explore the entities recommended to them by Spark, and whether this can be predicted.
We distinguish between the following user actions: (1) the user clicks on one or more search results situated in the central page panel (search click on organic results), (2) the user clicks on one or more related entities suggested by Spark on the right page panel (Spark click ), (3) the user does not click.
  'Search versus Spark CTR.
We look into the relationship between search and Spark click-through rate per query.
First, queries having a relatively low CTR for search results tend to also have a low CTR for Spark results.
Then, we identify a mutual growth area where we find queries with relatively high search and Spark CTR.
Finally, queries with the highest search CTR values tend to have the lowest values of Spark CTR.
Overall this indicates that queries with an average search CTR (neither low nor high) are those for which the returned Spark recommended entities are more likely to   attract   user attention.
However, as the plot depicts, the search CTR does not directly indicate its Spark CTR value.
In Spark, each entity is associated with a type, such as   person   ,   location   and   movie   .
In this case, Spark may be triggered recommending a set of related entities, as in the case of a query containing only the entity   jennifer aniston   .
As we observe in Figure 2(c), the user who submits an entity query without any surrounding context is more likely to click on Spark results (denoted as   E only   ).
However, a user who submits an entity query with the surrounding context such as   news   or   photo   is less likely to click on a Spark result.
"The same holds for the context   news   ; returning related entities when users are interested in reading about news does not seem to trigger an explorative behavior through Spark entities."
Last, the context   movie   , aiming most probably to identify the entity of a query (e.g.,   walk the line movie   ), leads to a relatively higher CTR on Spark.
To conclude, queries with average CTR are more likely to have their Spark recommended entities clicked, the entity type (person versus location) affects whether the corresponding Spark recommended entities are clicked, and finally, queries associated with a genre (e.g., news) or media context (e.g., image) are less likely to have their Spark recommended entities clicked.
First, we study whether the demographics of the users affect their click behavior on Spark and also explore whether specific characteristics of a user session such as its overall duration affect this behavior.
  'Spark & search CTR by demographics.
In Figure 3(a), we depict the average CTR values for search and Spark results across users of different ages.
The plotted values are normalized by the maximum average value separately for search and Spark CTR values.
As we observe, there is a different effect on search versus Spark click-through rate as the age increases.
In case of Spark, users of a younger age tend to have a relatively higher CTR than the users of the other groups.
Figure 3(b) depicts the difference of the Spark and search CTR rates between male and female users.
While for search no difference is observed, for Spark, male users tend to exhibit a higher CTR than female users.
Figure 3(c) shows how the duration of a user session affects the average Spark CTR and the average search CTR of the corresponding session.
Spark and search CTR values are normalized by their standard score and shifted by a constant value for visualization purposes.
The behavior is different for Spark CTR.
This suggests that when a user clicks on a Spark result she is willing to explore the recommendation, and may continue to do so for a while.
"The fact that we observe a difference in the curves of search CTR and Spark CTR clearly shows that Spark is indeed returning results that entice users to explore; when users do so, they become more engaged as they interact with the Spark recommendations; it is not anymore about satisfying an information need, but satisfying curiosity."
Finally, we studied how users interact with Spark in terms of navigation.
This suggests that users, when returning to the initial search result page after clicking on a Spark result, are less likely to engage in further exploration.
However, in the context of gender, it is not clear if this comes from the type of queries triggering Spark (to return entities) or from the user gender itself.
"However, what is clear is that Spark results have the potential to lead users in explorative behavior; when they start engaging with Spark search results, it is more likely that the session is longer and the navigation deeper."
 'We now study whether Spark and search CTR varies for different days of the week or different times of the day.
For both search and Spark, CTR is significantly higher on weekends (t-test, p < 0.001).
We recall here that our dataset only contains sessions where the query triggered Spark to recommend entities.
Therefore, this indicates that the types of queries that triggered Spark are those more likely to lead to users interacting with the results over the weekend than weekdays.
This is not surprising as the entities, and their relationships, covered by Spark (i.e., stored in Spark knowledge base) relate to persons, locations, movies and sports, many of which have an   entertainment   or   leisure   nature.
Moreover, the difference in CTR between weekdays and weekends is more accentuated with Spark CTR in the afternoon and at night.
Users are more likely to click on Spark results during these parts of the day on weekends.
Interestingly, both the Spark CTR and the search CTR do not increase significantly from weekdays and weekends in the evening (there is hardly any increase for Search CTR).
(b) Search CTR (normalized) 1945 1955 1965 1975 1985 1950 1955 1960 1965 1970 1975 User birth year Clicked entity birth year any female male   ' <Equation_0>   '(c) Birth year correlation Breadth Overall, the   entertainment   or   leisure   nature of the queries that trigger Spark (e.g.
actress, movie) results in users who are more likely to engage with the Spark results (and search results) when their entertainment and leisure activities are planned (a movie trip on the weekend) or happening (browsing the web in the evening).
It would be interesting to see whether increasing the coverage of Spark, by including entities for which older users would relate, could increase their interaction with Spark (e.g., increasing the CTR on Spark results from older users).
 'In this section we studied user behavior, with respect to user clicks on related entities suggested by Spark.
Spark results are not there to fulfill this need, but to entice users to explore entities that are related to their initial query.
Our analysis clearly shows that Spark is able to promote this explorative behavior.
When users decide to engage with a Spark recommendation, they often end up navigating through the recommendations, clearly engaging in explorative search.
Contrary to standard search behavior , where a satisfactory experience is when users find the information they are looking for as soon as possible (the search sessions are short), users interacting with Spark entity recommendations seem to happily explore through these results, leading to longer sessions.
Next, we look at how these insights can be used to build a model that can predict whether users will click on Spark results.
 'We studied the click behavior of users after issuing an entity query for which Spark recommends a set of related entities.
We only consider actions related to entity queries triggering Spark.
To capture the notion of   new users   interacting with Spark for the first time, we only consider users that do not have any action related to Spark during a period of one month and then at some point perform a click on a related entity suggested by Spark.
An interaction consists of the query issued by the user and the action that followed (a click on a Spark result, a click on a search result, or no click at all).
Given a user, her previous interactions, and an issued entity query, we want to predict whether the user will interact with the Spark module or not.
Since our focus is on predicting Spark interactions, we consider this as the   positive event   and any other interaction is considered as the   negative event   (e.g., a click only on organic search results).
We focus only on interactions concerning entity queries triggering Spark and ignore any other activity of the user.
Recall that given a user issuing an entity query we want to predict whether the user will click or not on the Spark module (positive vs. negative class).
This suggests that session length is a good indicator of whether users will click or not on Spark.
This suggests that users who are more recently engaged with the Spark module are more likely to continue engaging with it.
In other words, users who have clicked on Spark are easier to be enticed to explore its recommended entities when triggered.
We also dig further into the effect of specific features in Q, specifically Spark CTR (Q3), Spark views (Q1), query CTR category (Q4) and query type (Q5).
Interestingly, we see that features related to Spark CTR are the most discriminating to predict the future click behavior of the user.
The extent to which the fact that demographics do not help in the prediction task is caused by the coverage of Spark knowledge base should be investigated.
  'We developed features based on the characteristics of queries and users that reflect the click behavior on related entities recommended by Spark given a query and a user who issues that query.
Session length can help in the prediction, indicating that users who spend time on the search application may be in   explorative mood   and are more likely to interact with entities recommended to them by the Spark engine.
However, in all cases, recall is relatively low showing that overall the particulars under which a user will engage with the Spark module and a recommended entity are diverse and cannot be captured easily.
First, we study which relationships promote explorative search exploiting the Spark system, which has been deployed large-scale as part of Yahoo Search, as a use case.
Given a query submitted to Yahoo search engine, Spark displays related entity suggestions exploiting Yahoo knowledge graph.
Secondly, we perform a large-scale evaluation of Spark, with real information needs that then translate into queries by actual users of Yahoo Search.
Our focus is on the queries that trigger Spark to recommend entities.
Analyzing a sample of 2M users, we studied how users interact with results returned by Spark, in terms of submitted queries, user demographics, investigating also the interplay between typical search results and Spark recommendations.
Our results show that Spark is able to promote an explorative behavior.
Often, when users decide to engage with a Spark recommendation they end up navigating through the recommendations engaging in explorative search as our introductory example suggests, where users navigated from   Selena Gomez   to   Marlon Brando   .
Among other findings, we show that longer sessions result in a higher explorative activity and engagement, and that when users navigate through Spark they favour following paths of different entities rather than exploring multiple entities related to a single entity.
We develop a set of features based on the characteristics of queries and users that reflect the click behavior on related entities recommended by Spark given a query and a user who issued it.
The main contribution for this improvement originates from the historical information on past user behavior related to their interaction with Spark results.
However, recall is relatively low showing that the specifics under which a user will interact with Spark are diverse and cannot be easily captured easily.
In this work, we focused on the interplay between search results and Spark recommended entities.
All these compete for user attention and it will be important to situate the explorative search experience promoted by such systems like Spark within the full search context, and not only with respect to the standard search results, as done in this paper .
Also, Spark returns entities from a regularly updated KB.
Finally , our work focused on queries for which Spark returned entities as recommendations for users to explore.
On the contrary, we argue that the naming of class labels is strongly influenced by the very properties shared among the instances of the class: Hypothesis 1: Let S be a semantic class, {I} its set of instances , and {P } the properties shared among the instances.
The naming of its class label takes into account both old properties and new properties: Hypothesis 2: Let P ′ be a new property applied to the set of instances {I} of the class S. The subset of instances {I ′ }⊂{I} that satisfies the additional property P ′ corresponds to a more specific class S ′ .
Hypothesis 3: Let S1 and S2 be two semantic classes, {I1} and {I2} their sets of instances, and {P1} and {P2} the properties shared among the instances.
  'Our first dataset contains 100 randomly selected papers taken from the SIGIR 2012 conference proceedings, while our second dataset contains the same number of recent (2012) articles taken from the High Energy Physics (hep-ph) section from the arXiv.org pre-print repository.
  To evaluate our scheme, we implemented a prototype that acts as a stand-alone server that distributes RC4 brute-force key search to browsers of the users in exchange of accessing content.
  ' In our prototype implementation, the outsourced microcomputations consist of N sets of {Kr, C ⊕ P } where P is a given plaintext, C is its corresponding ciphertext encrypted with RC4 using a key k, and Kr is the assigned key-search space.
The aggregate time that all users spent in performing the computations was minutes, which enabled them to test for approximately 2 26 RC4 keys.
Sampling Random Users: Our goal is to collect random samples of G+ users for our analysis.
  ' 5.2 Local versus Global: Measuring Focus, Entropy , and Spread   'Using these three spatial properties, we now analyze the properties of hashtag propagations.
Parsing user provided data values ensures that the stored RDF triples conform to the XML specification, and allows us to perform further operations on the data, as described below.
From left to right: PEOPLE, MUSIC, IT, LANGUAGE AND LITERATURE, HISTORY, SCIENCE, RELIGION, DESIGN AND TECHNOLOGY, CITIZENSHIP, ART, BUSINESS STUDIES, MATHEMATICS, EVERYDAY LIFE, GEOGRAPHY.
Generality in Tag Similarity Graph (Closeness Centrality / Cosine Similarity) In [13] , the authors describe an algorithm developed to overcome the limited success in producing hierarchical structures from the tagging data by means of hierarchical clustering .
Generality in Tag Similarity Graph (Degree Centrality / Co- Occurrence) In [3] , the authors describe an extension of the algorithm presented in [13].
If the data source provides its data in a proprietary format, embedded in HTML, or using a custom library, the user can either write a wrapper themselves in Javascript, or use a third-party service such as Dapper( .net/open), Babel (http://simile.mit.edu/babel), or Yahoo Pipes to convert data into an RSS feed first.
EnergyStar develops metrics for many different types energy consuming devices, including servers [19], displays [17] and personal computers (PCs) [13].
For laptops and desktops, we assume this is 50 J based on our own scoping experiments with a modern EnergyStar-rated laptop.
  'To estimate the power consumption of laptop, desktop and monitors, we take power consumption of models as measured by EnergyStar [13], for the one hundred most popular models on Amazon, and calculate the average.
Electrical Energy and Carbon Footprint   'Energy [MWh] Carbon ' 'Having used our model to calculate current emissions from GNM digital products, we now turn to assessing six potential interventions on the product.
 'This work was conducted as part of the SYMPACT project, funded by the RCUK Digital Economy and Energy programs (EPSRC EP/I000151/1).
1 After this exchange has occurred, the clients use RC4 to encrypt the data packets.
, Xt−1, Kt−1 = B]   ' <Equation_5>   'Hence, we conclude that the sequence {Xt} is a martingale, and this implies via the Optional Sampling Theorem that   ' <Equation_6>   'Besides having the correct expectation, a good estimator should be close to the correct value with high probability.
The optimal choice for the set S to minimize P i∈S Ni is obtained by solving the following integer program:   ' <Equation_20>   'or, equivalently,   ' <Equation_21>   'which can be interpreted as a Knapsack problem.
Instead, what we do is generate a set of independent , random seed values, one for each MinHash function (as per the discussion above, p × q), and map each news-story to a hash-value computed using the Id of the news story and the seed value.
We would like to emphasize here that, while from a sociological perspective the question of whether peer influence affects the check-ins of a pair of friends anywhere in the world might seem absurd, we begin with this question in order to smoothly introduce Figure 4: Global influence can possibly explain only up to 2.32% of the global similarity between friends.
Bootstrapping begins by instantiating a set of extraction rules and queries for each predicate from generic rule templates, and also generates a set of discriminator phrases from keyword phrases of the rules and from the class names.
Bootstrapping selects seeds by first running an extraction cycle to find a set of at least n proposed instances of the class, then selecting m instances from those with highest average PMI.
Bootstrapping selects the best k discriminators to use for its Assessor, favoring those with the best split of positive and negative instances.
To address this problem, KNOWITALL terminates extractions for a class when it reaches its Signal-To-Noise ratio (STN) cutoff on the most recent 100 extractions for that class (Section 2.5).
In the first mode it was executed without language focus restriction using a pure FIFO crawl queue while the second mode was with language focus restriction using a priority queue from which the crawler fetched the next crawl URL.
Contextual Hypothesis for Sense [37]  states that the context in which a word appears can be used to determine its sense.
Misleading Topic Signals from Users\' Activity.
Algorithms that infer authority based on users\' activity would either ignore these players or assign Figure 1: Normalized frequency of the five most used hashtags by a group of well-known basketball players in one month period.
Observation 1 in conjunction with Hypothesis 1 is akin to the concept of preferential attachment [4] along the topical lines.
 ' Algorithm 1 ensures that F tu is high if u is a known authority on t, in keeping with Hypothesis 1.
Bootstrapping methods [1, 9, 15, 25, 38] to relation extraction are attractive because they require markedly fewer training instances than supervised approaches do.
Bootstrapping methods are initialized with a few instances (often referred to as seeds) of the target relation [1, 25, 38] or general extraction templates [15].
The implemented dataflow network is similar to a parallel version of the Rete algorithm [12].
Parallel processing of two or more corpus shards would lead to serious access conflicts Yahoo!LDA addresses this problem by locking accesses to conflictin shards, but this locking mechanism degenerates its scalability performance when the number of threads increases.
However, computing this posterior is intractable because the denominator contains intractable integration,   ' <Equation_1>   'So, VB infers an approximate variational posterior based on the variational EM algorithm [17]:   Variational E-step:   ' <Equation_2>   ' <Equation_3>   '  Variational M-step:   ' <Equation_4>   ' <Equation_5>   ' In variational E-step, we update μ w,d (k) andˆθandˆ andˆθ d (k) until convergence , which makes the variational posterior approximate the true posterior p(θ, z|x, φ, α, β) by minimizing the Kullback-Leibler (KL) divergence between them.
Maximizing the joint probability p(x, z|α, β) is intractable (i.e., there are K ntokens configuration that increase exponentially), an approximate inference called Markov chain Monte Carlo (MCMC) EM [17] is used as follows:   ' <Equation_8>   '  MCMC M-step:   ' <Equation_9>   ' <Equation_10>   ' <Equation_11>   ' In the MCMC E-step, GS infers the topic posterior per word = 1 from this posterior.
Modern CPU provides Streaming SIMD Extension (SSE) instructions that can concurrently run floating-poin multiplications and additions.
We see that PBEM or PIEM converges around 1.2 or 1.3, 1.1 or 1.2, 1.4 or 1.4 times faster than PBEM-noScheduling or PIEM-noScheduling on Pubmed, Wiki and Nytimes, respectively.
We then propose a Smart Hill-Climbing algorithm based on the ideas of importance sampling and Latin Hypercube Sampling (LHS).
Simulated annealing is a search heuristic commonly used to solve global optimization problems, especially in the presence of many false minima.
In the next section, we propose a Smart Hill-Climbing algorithm based on the ideas of importance sampling and Latin Hypercube Sampling (LHS).
 ' We have extended the standard Latin Hypercube Sampling algorithm to take into account knowledge about the correlations between parameters and system performance.
The correlation information can be combined with the Latin Hypercube Sampling to generate skewed random search samples that are likely to lead to more efficient searches.
We need to draw the point ξj such that,   ' <Equation_13>   ' After standard algebraic manipulations, the expression for ξj be- comes: Figure 3illustrates that for a particular parameter with a positive correlation with ρ ∼ 0.6, the importance sampling strategy using the truncated exponential density function (3.2), would therefore divide the sampling space [1, 120] The weighted LHS algorithm for generating K random vectors (or configurations) of dimension N can be summarized as follows:   ' <Equation_14>   '1.
We then sample a set of n points using the weighted Latin Hypercube Sampling around a pre-specified neighborhood of Xmin.
The weighted Latin Hyper Sampling strategy guides much more samples from the neighborhood close to the local or global minima.
The first set illustrates the power for the Latin Hypercube Sampling procedure.
We implement the three black-box algorithms with and without the Latin Hypercube Sampling.
We then proposed a Smart Hill-Climbing algorithm using ideas of importance sampling and Latin Hypercube Sampling.
Other RDFS or OWL fragments that can be approximated by materializing finitely evaluatable inference rules, like ρdf [18] on the lower end and the Horst's pD* rule set[24] on the upper end, could easily be plugged into our framework by similar operators, currently we support the latter by employing Jena OWL reasoner in the OWL operator.
This caching happens both internally at execution level, a process also known as memoization, and externally: as the invocation happens using the REST model it would be straightforward to arbitrarily increase the aggregated performance of a Semantic Web Pipelining cluster simply by using multiple servlets and roundrobin Web request routing techniques.
Inverted index compression is used in all major engines, and many techniques have been proposed [26, 29].
Arguments needed by the processors are given either as attributes or child elements of the processor.
Arguments can then be given using the children of an fx:args element of the fx:lambda element.
Turing proved long ago that all computers instantiate one universal formal model of computation, the Turing machine, which the Church-Turing Hypothesis notes is equivalent to the λ calculus that f XML is built upon [5].
Sorting by the expected number of instances is generally the most useful, in that the patterns at the top of the list appear many times, have high confidence , or both.
Compared to using alternative platforms like Spark and Graphlab that also offer highly sophisticated data-or model-parallel systems, or designing bespoke ground-up solutions like PL- DA and YahooLDA, we suggest that our intermediate approach that leverages both simple-but-critical algorithmic innovation and lightweight ML-friendly system platforms stands as a highly costeffective solution to Big ML.
Sampling-based algorithms are known to yield very sparse updates that make them well-suited to settings with a massive number of topics and distributed implementation.
Tuning the Model Scheduling Scheme.
B. SYN Flooding.
More specifically, the browser (client) keeps a FIFO queue of pending HTTP requests for each connection, and handles them one by one, as follows.
Multi-core processors are now available in most computing platforms (desktops, laptops, tablets, and smart phones) since hardware trends favor parallel architectures.
Ray tracing is computationally intensive and has minimal shared state.
Since SRC algorithm can generate clusters with highly readable names, we use these names as our candidate phrases, and rank them according to the following two criteria: Maximum cluster size criterion.
A simplified MPEG-7 description containing an adaptation sequence consisting of two variations might look like this: <Description xsi:type= VariationDescriptionType > <VariationSet> <Source xsi:type= VideoType > [..] <ComponentMediaProfile id= ES1 > [..] <ComponentMediaProfile id= ES2 > [ The   source part   describes the internal structure of the MPEG- 4 video.
As a special case of the Metropolis-Hastings algorithm [18], Gibbs sampling is a Markov chain Monte Carlo algorithm and usually applies when the conditional probability distribution of each variable can be evaluated.
Enron email dataset was made public by the Federal Energy Regulatory Commission during its investigations and subsequently made available [20].
Sampling approaches have been employed for triangle counting with good success [28, 34, 36, 35, 31].
  'The Sampling Problem.
We prove the accept-reject process is a good approximation to Po(Eij = 1|f (xi, xj), ΘM, ΘF ), as the probability is small, due to the Binomial Approximation [23] (Proof in Appendix C).
PROOF OF THEOREM 1   'The Binomial Approximation [23] states that for values z close to 0, (1 + z) α = 1 + αz.
Sampling results in the sort-merge producing fewer IPs as well as fewer sites sharing any IP.
 ' Random permutations are needed to implement this Jaccard estimator.
Sampling from the whole web, however, is a more difficult problem , and therefore all the known algorithms suffer from severe bias.
Sampling from p can be done easily (see Figure 1): we repeatedly select queries from P uniformly at random, submit them to the search engine, and extract the valid results from each such query.
Inverted indexes might be suitable for problems when the sizes of documents are small and each record only contains few words.
(Locality Sensitive Hashing) A family   'H is called (S0, cS0, p1, p2) sensitive if for any two point x, y ∈ R D and h chosen uniformly from H satisfies the following:   if Sim(x, y) ≥ S0 then P rH(h(x) = h(y)) ≥ p1   if Sim(x, y) ≤ cS0 then P rH(h(x) = h(y)) ≤ p2 For approximate nearest neighbor search typically, p1 > p2 and c < 1 is needed.
Here we use Gibbs Sampling as an approximate inference method.
Compared with other approximate inference methods such as Variational Inference, Gibbs Sampling is easy to extend and has been proved to be quite e↵ective in avoiding local optima.
In Gibbs Sampling, each hidden variable will be iteratively sampled and the corresponding marginal probability can later be estimated with the samples.
Algorithm 2 describes the blocked Gibbs Sampling process .
 'is sampled to the   master   topic with: p(t di = 2, z di = ems|w w w, z z z di , t t t di , ) / |w = w di , tw = 2, w 2 d| + ms |tw = 2, w 2 d| + V · ms · (|tw = 2, w 2 d| + 3)   ' <Equation_4>   'Sampling functions for documents in DC can be derived similarly, according to the corresponding generative process.
Given a query document, we predict its word labels z z z using the incremental Gibbs Sampling algorithm described in [17] .
Sampling queries according to their volume is tricky, though, because we do not know a priori the volume of queries.
Otherwise, empirical methods are used to set B. Monte Carlo methods with approximate weights.
All Monte Carlo methods assume that the trial distribution p is known, up to normalization.
Thus, by applying any of the Monte Carlo methods on the samples from the match distribution , the PB sampler obtains near-uniform samples.
We implemented our algorithm in a system called INCAP- PROX based on Apache Spark Streaming [5], and evaluated its effectiveness by applying INCAPPROX to various micro-benchmarks .
Our evaluation using real-world case-studies shows that IN- CAPPROX achieves a speedup of ∼ 2× over the native Spark Streaming execution, and ∼1.4× over the individual speedups of both incremental and approximate computing.
Given these advantages, the batched streaming model is widely adopted by many stream processing frameworks including Spark Stream- ing [5], Flink [2], Slider [19, 20], TimeStream [65], Trident [8], MapReduce Online [32], Comet [47], and NOVA [26].
(Note that our implementation is based on Spark Streaming [5], which is a generic extended version of MapReduce.)
  'Approximation for aggregate functions.
 'We implemented INCAPPROX based on the Apache Spark Streaming framework [5].
In this section, we first give a brief necessary background on Spark Streaming, and next, we present the design details of the implemented modules.
Spark Streaming     is a scalable and fault-tolerant distributed stream processing framework.
Thereafter, the input data is processed using Apache Spark [71], a distributed data-parallel job processing framework similar to MapReduce [34] or Dryad [49].
Spark Streaming is built on top of Apache Spark, which uses Resilient Distributed Datasets (RDDs) [71]  for distributed dataparallel computing.
Spark Streaming extends the RDD abstraction by introducing the DStreams APIs [72], which is a sequence of RDDs arrived during a time window.
Our implementation builds    on the Spark Streaming APIs to implement the approximate and incremental computing mechanisms.
For that, we reuse the caching mechanism available in Spark to memoize the intermediate results for the tasks.
For the reduction operations, we adapt a windowing operation in Spark Streaming, namely reduceByKeyAndWindow() to incrementally update the output.
Finally, the dependence graph is maintained at Spark's job controller.
In general, our modifications in Spark Streaming are fairly straightforward, and could easily be adapted to other batched streaming processing frameworks (described in § 2.3).
Figure 6(a) shows the throughput comparison between Approx, Inc, INCAPPROX, and native Spark Streaming.
The individual throughput for approximate computing (Approx) and incremental computing (Inc) is 1.41× and 1.43× higher than native Spark Streaming execution, respectively.
Approx and Inc achieve 1.49× and 1.51× higher throughput than native Spark Streaming.
"We discuss three different approaches that could be adopted for fault tolerance if memoized results are unavailable:   '(i) we could continue processing the window without using any memoized items, albeit with lower efficiency; (ii) we could use a similar approach for fault tolerance as provided in Spark [71], where the lineage of memoized RDDs is used to recompute only the lost RDD partitions; (iii) we could make use of underlying distributed fault tolerant file-systems (HDFS [4]) to asynchronously replicate the memoized results."
Approximation techniques   'such as sampling [15, 41], sketches [33], and online aggregation [48] have been well-studied over the decades in the context of traditional (centralized) database systems.
We implemented our algorithm in a data analytics system called INCAPPROX based on Apache Spark Streaming.
Random sampling, -greedy method, the upper confidence bound (UCB) algorithms and Thompson sampling have been explored in solving the problem [7].
Given a new email enew, we define the two sub problems mentioned above as: Sampling Users for Feedback Given U, E, I, enew and time interval T f eedback in which we collect users' feedback , select the subset S of k users from U whose feedback in T f eedback maximizes the prediction accuracy of I U−S,enew .
Particle filters approximate the posterior distribution over the latent variables up until document td.
"Sampling topic indicators: For the topic of word i in document d and epoch t, we sample from:   ' <Equation_4>   'where rest denotes all other hidden variables,C −i tdk refers to the count of topic k and document d in epoch t, not including the currently sampled index i; C −i sk is the count of topic k with story s, C −i kw is the count of word w with topic k (which indexes the story if k = K + 1; traditional dot notation is used to indicate sums over indices (e.g."
Sampling story indicators: The sampling equation for the storyline std on a high level decomposes as follows:   ' <Equation_6>   'where the prior follows from the RCRP (2), w K+1 td are the set of words in document d sampled from the story specific language model φs td , and the emission terms for w K+1 td , etd are simple ratios of partition functions.
For each relationship, (v, v ), and a new multinomial if w di is an instance of a concept c ∈ KB then Algorithm 2: SamplingWord() distribution ϑ vv is constructed by combining the two multinomial θv and θ v specific to the two users v and v .
Confronted with the massive size of web search streams, conventional parameter inference methods such as collapsed Gibbs Sampling (GS) [24] and Variational Bayes (VB) [2] are inefficient for training WSSM because of their high computational cost.
Sorting the residuals in Equation (9) is computationally expensive because the number of non-zero residuals is very large.
While we believe that our framework can handle variable update schedules of many different methods, in this paper we will primarily focus on Collapsed Gibbs Sampling (CGS).
Sampling can be performed in O(log T ) time and maintaining the data structure requires only O(log T ) work.
Collapsed Gibbs Sampling (CGS) [8] is a popular inference scheme for LDA.
  LSearch: Linear search on p. Initialization: Compute the normalization constant cT = P t pt.
  Alias method.
Unlike the Alias method, there is no extra space required in the F+tree initialization in addition to F. Sample Generation.
Sampling using a F+tree can be carried out as a simple top-down traversal procedure to locate z = min n t :   P s:s≤t ps   > u o for a number uniformly sampled between [0, P t pt).
Note that to deal with a similar change in p, LSearch can update its normalization constant cT ← cT +δ in a constant time, while both BSearch and Alias method require to re-construct the entire data structure (either c = cumsum(p) or the Alias table: alias and prob), which costs O(T ) time in general.
Clearly, LSearch has the smallest update cost but the largest generation cost, and Alias method has the best generation cost but the worst maintenance cost.
Sampling procedures for the first two terms have very low chance to be performed due to the observation that most mass of pt is contributed from the third term.
For both terms, Alias method is applied to perform the sampling.
AliasLDA: the approach that uses Alias method to do the sampling with document-wise sampling order.
Matrix Approximation.
We consider now the problem of matrix approximation:   'Problem Definition 1 (Matrix Approximation) Given: a sparse matrix R ∈ R N ×M with indicator matrix I Find: a model M with parameters θ, such that the size of θ is small, |θ| R, and M(θ) approximates R well:   ' <Equation_1>   'Symbol Definition N, M Number of rows (users) and columns (movies) R Data matrix ∈ R N ×M (with missing values) I Indicator matrix ∈ {0, 1} N ×M for R S Number of stencils k   ' <Equation_2>   'Number of user and movie clusters in stencil   ' <Equation_3>   'm for stencil c () Vector of user assignments ∈ {1, .
As mentioned above, we can efficiently improve the approximation accuracy by using multiple stencils:   ' <Equation_6>   'Algorithm 1 Matrix Approximation Require: matrix R, indicator matrix I, clusters kn, km, max stencils S 1: ˆ R ← R 2: for = 1 to S do 3:   ' <Equation_7>     km, L ) {Columns} 5: for all a, b ∈ {1, .
σn) the singular values of R.   ' Theorem 3 (Approximation Guarantees) Using k clusters for both rows and columns, the matrix R can be approximated with error at most   ' <Equation_14>   'Proof.
Various distance measures can be used, as long as they satisfy the following properties: (a) D is positive and strictly convex, (b)   ' <Equation_2>     This optimization problem can be solved using numerical methods (e.g., Newton's method), and for certain distance measures (e.g., D(x)   ' <Equation_3>     only one iteration is required, making the calibration process very efficient.
 'Sampling is fundamental to statistics and employed when there is a need to study a population and direct analysis of the entire population is infeasible due to sheer size and inaccessibility.
 'The Snowball approach to Expansion Sampling (XSN) is shown in Algorithm 1.
The Snowball approach to Expansion Sampling always produces connected samples.
 'An alternative approach to Expansion Sampling is Markov Chain Monte Carlo simulation (MCMC), a standard technique to sample and evaluate probability distributions (see Chapter 29 in [26] for an excellent introduction to Monte Carlo methods).
As can be seen, the Expansion Sampling approaches (XSN and XMC) outperform other sampling methods.
We also see that the Expansion Sampling methods not only have the highest composite score, but also dominate on each of the measures individually.
These results again show that Expansion Sampling produces samples most representative of community structure in the larger network and points to an intriguing yet previously uninvestigated approach to community inference in complex networks.
Surprisingly , the Expansion Sampling approaches seem to match the clustering coefficients and degree distributions better on larger datasets than smaller ones.
We see that, although the Expansion Sampling algorithms outperform other methods in terms of community representativeness , they do not do as well in matching degree distributions and clustering coefficients of the larger network as other sampling methods.
We referred to this approach as Expansion Sampling and described two different methods following this approach: a Snowball Expansion Sampler (XSN) and an MCMC Expansion Sampler (XMC).
One approach to this is to couple Expansion Sampling with other methods known to sample different properties well.
In other experiments we have conducted, we found that samples produced by Expansion Sampling exhibited a significantly high expansion quality with relatively small sample sizes (smaller than 15% in many cases).
Gibbs sampling (GS) [12]  belongs to an accurate class of approximate inference algorithms known as Markov chain Monte Carlo (MCMC).
GS is possible even without this relationship, but is more Algorithm 5: Gibbs Sampling for learning groups Sampling step:   ' <Equation_9>   ' <Equation_10>   ' complicated and not assuming this relationship may have implications on how fast GS mixes and converges.
Sampling step:∀d ∈ D, ∀r ∈ d do   ' <Equation_12>    φ computation: λg(e) ∝ β + ne,g, P e∈E λg(e) = 1, if ∃r s.t.
In a knowledge Algorithm 7: Gibbs Sampling with λgs and φes Sampling step: we ← g, e ∼ λg(e)φe(w)(n − d,g + γg), ∀d ∈ Dtest, ∀w ∈ d base with about 16, 500 entities, one can hardly expect more than 5000 groups (which is possible but one hopes unlikely).
Sampling large networks is a fundamental data mining problem.
Sampling has been successfully used for many network-related measurements, ranging from estimating the size of the network, the number of edges and the average degree, or even higher-order properties such as the clustering coefficient, the number of triangles and small subgraphs, and more.
Sampling in general, and network sampling in particular, has been studied ex-tensively in both theoretical and applied research communities, and there is a rich body of literature on these topics.
Sampling in networks.
"With high probability, G will have the following properties: (i) davg = (1 ± o(1)) · d; (ii) if c = head, then G has no degree 1 nodes and |V | = n; (iii) if c = tail, then the number of degree 1 nodes in G is (1 ± o(1)) · n, and |V | = (2 ± o(1))n;   'and (iv) tmix \uf8ff O ⇣ log n log d ⌘ ."
Sampling in the context of network parameter estimation has been extensively studied in several papers.
Inference uses Gibbs Sampling.
For inference [17] use Variational EM.
Global: No personal distribution over the tree structure, we assume that all tweets are generated by a fictitious user and no personal preferences are incorporated.
The first observation is that Topic, Global and Full, which utilize hierarchical structures of regions are better than other methods.
We can see the effect of regional language models by focusing on Global where no personal distributions over the tree is introduced.
  We derive an efficient inference and learning algorithm based on Variational EM (expectation-maximization), which makes use of the constraint to significantly reduce the computational time to a linear cost in the size of the network, allowing for 
§3 presents the Constrained Latent Space Model (CLSM), and describes the Variational Expectation Maximization algorithm.
In the following, we describe an approximate method for learning and inference based on Variational Expectation Maximization (EM).
For the experiments below, we set the precision of the Dirichlet prior α to 1, as low as 1e-30, and run the Variational EM algorithm until its convergence.
To tame the algorithmic complexity of such a task, we suggested an efficient inference strategy based on Variational Expectation Maximization, which scales linearly with the size of the network.
Variations on the testing procedure.
Related work [8, 11] has hypothesized that the relative impact of spam reviews Table 3 : Signal costs associated with six online review communities, sorted approximately from highest signal cost to lowest.
An overview of these factors for each of the six review communities is given in Table 3  Hypothesis 2 : Increasing the signal cost will decrease the prevalence of deception.
In agreement with Hypothesis 1 (given in Section 6), it is clear from Figure 2 that deceptive opinion spam is decreasing or stationary over time for High posting cost review communities (blue graphs, a–d).
With high probability, all subsets of a small constant fraction, say γn, of nodes in an Erdös-Rényi graph will have average degree less than 2 + γ where γ depends on γ and goes to 0 as γ does.
Sampling is performed through Google Base API's as described in Section 4.2.
Sampling from the recommender distribution: As was mentioned earlier, one of the challenges of fitting such a model correctly is approximating our recommender distribution to perform the Metropolis-Hastings sampling.
The Random Node Sampling (RNS) method, in which the crawler uniformly re-download at random webpages in each download cycle, was used as the baseline algorithm.
  ' <Equation_0>   'where t denotes the index of the current word in Gibbs Sampling procedure.
For each value of K, the model is estimated using 200 Gibbs Sampling iterations.
  'Parameter Sampling with Adaptive Precision.
Hill climbing starts from a random (potentially poor) solution, and iteratively improves the solution by making small changes until no more improvements are found.
Hill climbing does not work well for nonconvex spaces, however, since it will terminate when it finds a local maxima.
K-means clustering is then used on original graph nodes to assign them to different groups.
Graph Sampling for Fast Clustering: Graph sampling (also known as   sparsification   or   filtering   ) has attracted more and more focus in recent years due to the explosive growth of network data.
Autocorrelation is a common characteristic of relational and social network datasets, which refers to a statistical dependency between the values of the same variable on related entities.
Relational Autocorrelation Let PR = {(vi, vj) : eij ∈ E} be a set of related instance pairs in G. Let X be a binary attribute defined on the nodes V .
 'In this subsection, we use Gibbs Sampling for inference.
Sampling r: What are drawn from global measure G = ∞ k=1 r k δ φ k are the dishes for customers (tweets) labeled with (x=0, y=0) for any user across all time epoches.
Sampling ψt, βi, πit: Fraction parameters can be sampled in the similar way as r. Notably due to the specific regulatory framework for each user and time, the posterior distribution for Gt, Gi and G t i are calculated by only counting the number of tables in correspondent user, or time, or user and time.
Take βi for example: as βi ∼ DP (γ, r), and assume we have count variable {T k i }, where T k i denotes the number of tables with topic k in user i's tweet corpus , the posterior for βt is given by:   ' <Equation_12>   ' Sample zv: Given the value of xv and yv, we sample topic assignment zv according to the correspondent Gx v ,yv given by:   ' <Equation_13>   'The first part P r(v|xv, yv, zv, w) denotes the probability of current tweet v generated by topic z, described in Appendix and the second part denotes the probability of dish z selected from Gx v ,yv :   ' <Equation_14>   ' <Equation_15>    Sampling M k , T k i : Table number M k , T k i at different levels (global, user or time) of restaurants are sampled from Chinese Restaurant Process (CRP) in Teh et al.
Sampling xv and yv: For each tweet v, we determine whether it is public or personal (xv), time-general or time-specific (yv) as follows:   ' <Equation_16>   ' <Equation_17>   ' <Equation_18>    where E (x,y) i denotes the number of tweets published by user i with label (x,y) while M (·,y) i denotes number of tweets labeled as y by summing over x.
In order to leverage these contextual factors, we rely on the refined Experience Sampling methodology (rESM) [7] (Sec.
 ' In order to address the challenge of users\' retrospective recall for PII valuation, we use a refined version of the Experience Sampling Method (rESM).
Experience Sampling involves asking participants to report on their experiences at specific points throughout the day.
We attempt to overcome these issues through the use of the refined Experience Sampling Method and a truth-telling auction mechanism that incentivizes users to participate honestly.
Another approach is to use a Markov chain Monte Carlo algorithm for inference with LDA, as proposed in [14].
Gibbs sampling is an example of a Markov Chain Monte Carlo algorithm [13].
Sampling with such model is fast and in practice convergence can be achieved in time similar to that needed for standard LDA implementations.
Global topics of MG-LDA are not supposed to capture ratable aspects and they are not of primary interest in these experiments.
Sampling an edge from the alias table takes constant time, O(1), and optimization with negative sampling takes O(d(K+ 1)) time, where K is the number of negative samples.
These methods are based on agglomerative clustering , min-cut based graph partitioning, centrality based and Clique percolation methods (CPM) [12][2] .
The Gibbs Sampling updates for TURCM-1 are similar with user u replaced by a user-recipient pair ur.
The Gibbs Sampling updates to infer TURCM 2's parameters contain an additional update for the recipient draw.
Hypotheses are tested by running trial campaigns, new hypotheses are generated, tested, and the process repeats.
  'Sampling   'for REG:.
Sorting is possible on any column.
The inference is performed by Variational EM.
For inference , this work uses Gibbs Sampling and the evaluation is done by showing anecdotal results, by measuring Deviation Information Criteria (a model complexity criterion similar to BIC), as well as classification accuracy using manually labeled data.
The inference is done by Variational EM and the evaluation is done by measuring the accuracy of predicted location and showing anecdotal results.
Sampling r does not require us to re-estimate the values of mean and covariance matrix in the M-step and hence reduce the computation cost of the inference algorithm.
Globally based community finding algorithms.
Sampling method can effectively solve the memory consumption issue when one wants to find a local community within a large graph, since the whole graph does not have to be loaded into memory.
, '  Random seeding: pick |S| vertices in C randomly.
Pulse demonstrate that, by coupling friendship with interest , FIP achieves much higher performance on both tasks.
Pulse in terms of both interest targeting and friendship prediction.
Pulse (pulse.yahoo.com) is a social network site that allows users to create profiles, connect to friends, post updates, and respond to questions, as in other social networks .
Pulse, so as to simultaneously propagate interest and friendship .
Pulse for about one year, involving hundreds of millions of users and a large collection of applications, such as games, sports, news feeds, finance, entertainment, travel, shopping, and local information services.
"As the user population in the   'WWW 2011 – Session: Temporal Dynamics March 28–April 1, 2011, Hyderabad, India   'Pulse social network is huge, it is prohibitive to take all the users as candidate friends and generate a total ordering of the whole user set for each user to evaluate the prediction performance; similarly, the models relying on neighborhood information (e.g."
This observation indicates that there is strong evidence of homophily in the Pulse social network such that users with similar interests are truly interested in each other.
Pulse social networking system.
Thus, we use an efficient Markov chain Monte Carlo inference procedure based on Gibbs sampling [17] to approximate the posterior distributions in the following way.
[24] proposed Maximal Semantic Trajectory Pattern Similarity(MSTP-Similarity) to measure the semantic similarity between trajectories.
To establish the extent that this is likely to occur, we measure spatial autocorrelation within the neighbourhood of each point using local Moran's I, or Local Indicators of Spatial Autocorrelation (LISA) [2]: Figure 5maps the local Moran's I of average wealth in the neighbourhood of each sample point at the spatial scale corresponding to the approximate distance at which the level of spatial clustering reduces to that expected from randomly placed points (i.e., 29 km in Côte d'Ivoire and 64 km in Senegal, indicated by the vertical lines in the correlograms of Figure 4).
Gravity Residuals.
Then using the solution for the special case, we develop two approximate inference algorithms for the entire problem – Gibbs, an approach based on Gibbs Sampling [4] , and EMA, an approach based on Expectation- Maximization.
Once missing/multiple updates are addressed, in the discrete case, Z can be inferred exactly using standard Viterbi algorithm for HMM(HSMM) inference.
We can use Viterbi algorithm [12]  to get the best mapping (see algorithm 1).
We assume that errors of temporal transition do not occur, and assume that errors in observation are Gaussian for simplic- ity:   ' <Equation_10>    Case 2: Trajectory estimation of a typhoon.
We set z t = Algorithm 1 Particle filter algorithm 1.
Similarly to Case 1, we assume for simplicity that errors of temporal transition do not occur, and that errors in observation are Gaussian, as   ' <Equation_17> ' 'A particle filter is a probabilistic approximation algorithm implementing a Bayes filter, and a member of the family of sequential Monte Carlo methods.
The Sequential Importance Sampling (SIS) algorithm is a Monte Carlo method that forms the basis for particle filters.
Particle filters perform well compared to other methods.
Then, for each sentence ti in a review document d from training set, we infer its most probable topic z k from HTSM via the Viterbi algorithm.
Heuristic treatments such as document pooling [19] or contextualization [24] have to be applied to improve the performance of topic modeling on short text, both of which require the availability of additional context information (e.g., authors) beyond individual documents.
Indeed, the convergence of CVB0 has been theoretically proven to be stable [21], and its practical performance is better than the Collapsed Gibbs Sampling Algorithm [12] and the Collapsed Variational Bayes Inference Algorithm (CVB) [25].
  'APPENDIX   'Derivations of Zero-Order Collapsed Variational Bayes Inference   ' <Equation_30>   'x, y}.
  'Variational Bernoulli distribution for   'α: Minimizing (7) with respect tô a jk , we getâ getˆgetâ jk   ' <Equation_33>     cancelling those appearing in both numerator and denominator, and applying the Gaussian approximation to the above equation as well as [25], we can get:   ' <Equation_34>   'jk and ¬jk means without α jk .
  'Variational   'Bernoulli distribution for β: Similar as calculation above,the update equation for variational parameterˆbterˆterˆb kr is:   ' <Equation_35>   ' <Equation_36>   'Variational Multinomial distribution for z:   'The same as zero-order collapsed variational bayes inference (CVB0) for LDA [2] , we can directly derive the the distribution to update γ ijk :   ' <Equation_37>   'where all notations have been mentioned before.
Sampling action types from a different distribution incurs a cost to the user, based on how far the distribution is from his preferred one.
  ' <Equation_19>   'are two expectations of S. The value of S can be obtained naturally using approximated inference algorithms , such as Loopy Belief Propagation (LBP) algorithm [39], Gibbs Sampling [4] or Contrastive Divergence [3].
Energy model.
Energy needed to download and render each image is measured for all sizes shown on the x-axis with the energy along the y-axis.
 ' In the case of the standard SMAB problem, bandit algorithms following the Bayesian approach, namely, Bayesian- UCB [19] and Thompson sampling [20] , have stronger theoretically grounded guarantees and achieved better performance in experiments with other tasks [19, 20] than the pointwise alternatives such as UCB-1.
We further describe Bayesian bandits algorithm, which generalizes both Bayesian-UCB and Thompson sampling .
In the case of α low (t) = αup(t), this algorithm coincides with Bayesian-UCB [19], and if α low (t) = 0, αup(t) = 1, it reduces to Thompson sampling [20].
Then, any threshold value t defines a binary relation R t on the set of all web pages, where for two pages p 1 and p 2 , they are in the relation R t (p 1 ,p 2 ) iff content similarity between p 1 and p 2 is greater than t. Transitive closure of this relation gives a set of cDocs.
Parsing an application's logs to recreate a click trail allows us to incorporate these behaviors into a value of α.
The general framework of the problem is computing an estimate of Figure 5: Three fitted distributions that largely characterize empirical teleportation parameters on the entire web (Global), Wikipedia, and Hel- loMovies.
In general, topic models require a long training process because they are typically trained using iterative techniques such as Gibbs Sampling or variational inference.
, 'WWW 2010   Full Paper April 26-30   Raleigh   NC   USA   'Global Recommendation   'Let T d 1 and T d 2 be the respective density matrices of the manuscript d1 and a document d2 from the corpus D. We define sim(d1, d2), the overall relevance of d2 to d1 to be the probability that a random context drawn from the uniform distribution over contexts is relevant to both d1 and d2.
  ' (6) Global Top-K Ranking — After a predefined TTL (Time-to- Live), P j (j=1…m) will send local top-K rank value to P i , who will then sort on all the returned rank value and send the final lower bound T final to P j (j=1…m).
To the best of our knowledge, the only scalable personalized PageRank algorithm that supports the unrestricted choice of the teleportation vector is the Monte Carlo method of [11] .
The algorithms presented in Section 2 outperform the Monte Carlo method by significantly reducing the error.
Dynamic programming, in contrast, moves bottom up by computing the trivial PPRw vector, then all the PPRv i , then finally averages all of them into PPRu.
We used a semi-external memory implementation for rounded dynamic programming , partitioning the intermediate d PPR   ' <Equation_65>   'u (w) vectors along the coordinate w. Using a single vector allowed us to halve the memory requirements by storing the intermediate results in a FIFO like large array, moving the d PPR   ' <Equation_66>   'u being updated from the head of the queue to its tail.
 'We used three test sets for our study: queries obtained as follows: Sampling starts from a frequency-weighted sample of Q, but during sampling, we only admit new queries to the test set if they cover a type that has not been covered yet.
For example, Handcock and Raftery\'s [11]  model for social networks incorporates assumptions about transitivity in link structure (if A is connected to B and B is connected to C, then A is likely connected to C), and attribute homophily (if A and B have similar attributes, they are likely to be connected).
The specific features used are as follows:   Global Features.
Sampling strategies may affect the performance of the model to some extent.
The variational parameters learned in this step   'Algorithm 2 LTP-EM: Variational EM Algorithm for LTP 1: Input Training data-set (π, σ)1,2,··· ,m 2: Output Values (λ ′ , µ ′ ) that maximize Equation 5 3: Initialization Randomly initialize (λ (0) , µ (0) ) s.t.
  'Algorithm 2 MapReduce CopyCatch 1: Require: Preset parameters ∆t, m, and ρ 2: C, P = Initialize() 3: repeat 4: C = C, P = P 5: C, P = MapReduceJob(C , P ) 6: until C = C ∧ P = P 7: return [C, P] Procedure 3 UserMapper(Null, (Li, * , Ii, * )) 1: Globals: C, P 2: for k = 1 .
s do 3:   ' <Equation_5>   'j , Li,j) 4: if σ ≥ ρ|P k | then 5: emit k, (Li, * , Ii, * ) 6: end if 7: end for Procedure 4 AdjustCluster-Reducer(k, U ) 1: Globals: C, P 2: Initialize c = 0, p = 0, v = 0 3: for all map values (Li, * , Ii, * ) ∈ U do 4: for j = 1 .
Residual E[ǫ|α]   ' <Equation_2>   'where fixed treatment effect θ can vary from user to user but θ is uncorrelated to the noise ǫ.
Based on this representation, our goal is equivalent to decomposing a given Term-Document matrix into multiple matrices , including Term-LocalTopic, Term-GlobalTopic, and LocalTopic-Location matrices.
"In Figure 3, there are another two matrices that we should learn, i.e., GlobalTopic-Document matrix  ' In the LT model, each location is represented by , a multinomial distribution over local topics, with symmetric Dirichlet prior; each document is associated with , a multinomial distribution over global topics, with symmetric Dirichlet prior."
Variational Bayesian inference is an approximate method for obtaining a strict lower bound of the true (log) joint posterior.
Some tunable parameters in our pipeline include, for example, the dimensionality of local spectral subspace l, the number of short random walk steps k, the minimum cluster size n, the maximum size of the sampled subgraph N , the degree threshold dmax for sampling the subgraph, the edge weight threshold m.   'Algorithm 2 MapReduce Leas   'Globals:graph G = (A, E, W), configuration parameters 1: InitializeReplica() 2: for s ∈ S do 3: if deg(s) ≤ dmax then 4: Sample subgraph Gs 5: V k,l = LocalSpectral(Gs, s) compute local spectral subspace 6: Solve the optimization objective y in Section 4.3 7: C = SweepCut(y) 8: emit s, accomplice C 9: end if 10: end for   'The core of the MapReduce Leas algorithm can be seen in Algorithm 2.
"In relevance feedback, the search system extracts information from a user in order to improve its results, this can be: explicit feedback such as ratings and judgements given directly by the user, for example the well known Rocchio algorithm [26]; implicit feedback such as clickthroughs or other user actions; or pseudo relevance feedback, where terms from the top n search results are used to further refine a query [7] ."
"We compare the IES algorithm with a number of methods representing the different facets of our technique: a baseline which is also the underlying primary ranking function, the BM25 ranking algorithm[34]; a variant of the BM25 that uses our conditional model update for the second page ranking which we denote BM25-U; the Rocchio algorithm[26], which uses the baseline ranking for the first page and performs relevance feedback to generate a new ranking for the second page; and the Maximal Marginal Relevance (MMR) method[8] and variant MMR-U which diversifies the first page using the baseline ranking and our covariance matrix, and ranks the second page according to the baseline or the conditional model update respectively."
"  'Recall@ '  After setting the value of λ for each data set according to the optimal values discovered in the previous section (and also setting for the MMR variants λ = 0.8 for WT10G, λ = 0.8 for Robust and λ = 0.9 for TREC8 after some initial investigation), we sought to investigate how the algorithm compared with several baseline algorithms, each of which shares a feature with the IES algorithm; the Rocchio algorithm also performs relevance feedback; the MMR diversifies results; and also some variants that use our conditional model update."
To calculate SS score, we first extract a Semantic Fingerprint Vector (SFV) for each account, which essentially contains several representative terms in its tweets based on the TF-IDF algorithm [31], a widely used metric in the information retrieval community to measure the representativeness of terms.
  Degeneracy, the largest k for which the k-core is non-empty.
These are passed through a Unix FIFO to the enqueuer which is responsible for the removal of duplicates and the recording of the annotations along with the associated URL.
The use of a FIFO enabled convenient repeated debugging with the same test set.
We describe these three approaches below: Global model: In this setup we learn a global regression function fg parametrized by a global weight vector wg.
In analogy to Section 5.1, we are interested in the following three cases:   'Global model: In this setup we learn a global classification function hg parametrized by a global weight vector wg.
9 We perform prediction as follows based on the model: Global models: We use the same weight vector wg on both seen and unseen users.
The query possible results are sampled (hence the Monte Carlo algorithm) in each database instance that is defined by the rules to be   clean   (i.e.
Small Buckets Heuristic Much of the support of valid DUST substring substitution rules is likely to belong to small buckets.
  'WWW   'Similarity Likeliness Heuristic   'The likely similar support of a valid DUST rule is large.
We identify five classes of features: Local, Global, Trending , Headline and User.
Global similarity GS a,h,γ : Distinguishes between general and topic specific hashtags.
Global hashtag frequency GF h,γ : Captures global popularity of usage for a given hashtag.
We test different combinations of the five types of features (Local, Global, Trending, Headline, and User, 14 features in total), and compare the classification performance.
Basic refers to using Local and Global article-hashtag content and popularity features.
  ' Temporal Variations around Bonuses.
Energy consumption: We found a marginally significant difference between infected and clean devices in terms of remaining battery life.
  Energy vs. infection: Investigating whether malware infection has an impact on the expected battery life and whether another proxy for energy use could be predictive or indicative of infection.
Global models attempt to jointly disambiguate all mentions in a document based on the assumption that the underlying entities are correlated and consistent with the main topic of the document.
The Gerbil platform already integrates the methods of Agdis- tis [36], Babelfy [23], DBpedia Spotlight [20], Dexter [3], Kea [33], Nerd-ML [29], Tagme2 [9], WAT [25], Wikipedia Miner [22] and Illinois Wikifier [27] .
The parameters k1 and b are optimized using Powell's method [27].
Global personalized recommender would not adapt directly to the recent purchase (the digital camera) but would recommend items this user likes in general.
Query expansion benefits for the same reason, as unrelated terms in the page outside the article can be ignored—[24] shows, also using VIPS, that results are improved by segmenting a page and selecting expansion terms from only the   best   blocks.
Our work is similar to all previous work in a sense that we also use Simulated Annealing algorithm, however we combine it with the downhill Simplex algorithm and systematically examine the algorithm in comparison with other state-of-the-art algorithms, and apply it in a different application, i.e., ranking ads in contextual advertising.
In N-dimensional minimization, the downhill Simplex algorithm starts with a guess, i.e., (N+1) points, which define an initial simplex.
  'Author Consistency Hypothesis:   'The hypothesis is that reviews from the same author will be of similar quality.
  'Trust Consistency Hypothesis:   'We make the assumption that a link from a user u 1 to a user u2 is an explicit or implicit statement of trust.
  'Co-Citation Consistency Hypothesis:   ' The hypothesis is that people are consistent in how they trust other people.
, 'WWW 2010   Full Paper April 26-30   Raleigh   NC   USA   'Link Consistency Hypothesis:   ' The hypothesis is that if two people are connected in the social network (u 1 trusts u2, or u2 trusts u 1, or both), then their quality should be similar.
This supports the Trust Consistency Hypothesis that when u i trusts uj, the quality of ui is usually lower than that of   ' <Equation_20>   'The remaining three distributions are all symmetric with mean zero.
This supports the Co-Citation and Link Consistency Hypotheses that reviewers are more similar in quality (quality difference closer to zero) if they are co-trusted by others, or linked in a trust graph regardless of direction.
proposed the Rank-Energy Selective Query forwarding (RESQ) algorithm, designed to work with partially replicated indices [29].
  'Energy Model.
    This research was funded by National Science Foundation IIS-0713111, the Petascale Data Storage Institute under Department of Energy award DE-FC02-06ER25768, Los Alamos National Laboratory under ISSDM, and the industry sponsors of the Information Retrieval and Knowledge Management Lab at University of California Santa Cruz.
The second measure that we have decided to employ to compare partitions is the Variation of Information (VI) proposed in [16] .
Enron (enron): This dataset was made public by the Federal Energy Regulatory Commission during its investigations: it is a partially anonymised corpus of e-mail messages exchanged by some Enron employees (mostly part of the senior management ).
Rate of convergence.
Variation in F (b) comes from both the variation in these quality scores across searches and from the search-specific set of competing advertisers (determined by standard match, advanced match and targeting settings described in Section 2).
To be concrete, we specify a function f of p and α,   ' <Equation_19>   ' <Equation_20>     and use Newton's method to find its first and second derivatives so that during each iteration, both p and α are optimized .
Among them, one type of RNNs is referred to as Echo State Network (ESN) where a large set of dynamical reservoir is randomly constructed from many neurons with fixed structure and input weights.
"Therefore , for all other methods, we train two models for each of them: Global Only: for each type of tasks, aggregate data across all users and train a single model; and Target Only: for each user, train individual models for each tasks using only that user's data."
Inference: We use Gibbs Sampling [12] , which is a standard inference technique for topic modeling.
Autocorrelation refers to the correlation of a time series with its own past and future values.
With high probability, any node of P inserted after time φn, for any constant φ > 0, will be connected to a hub via a preferential attachment edge.
  'Algorithm 1 Local algorithm to calculate cpr ≡ ppr(·, c) adfactor of the conversion node c. Initialization: cpr(u) ⇐ 0, resid(u) ⇐ 1 if conversion node otherwise 0 Main Loop: while ∃u such that | resid(u)| ≥ ε do Pushback(u, α) end while Pushback (u, α): cpr(u) ⇐ cpr(u) + α resid(u) {accumulate α fraction of the current residual} for every incoming edge w −→ u do resid  ' <Equation_14>   '{distribute 1 − α fraction of the current residual to neighbors} end for resid(u) ⇐ 0 Using Propositions 3 and 5, one can see that calculation of the MIα and REα, and passthrough (Pass) adfactors requires estimating both the Pagerank contribution vector for conversion (i.e, the ppr(·, c) value for every node in the graph) and the Personalized PageRank vector of the begin node (the ppr(b, ·) value for every node in the graph).
EMPIRICAL PROPERTIES OF ADFAC- TORS   ' In this section, we present some interesting empirical properties of the adfactors in our dataset.
Such mechanism is called Heuristic Expiration.
  'Heuristic and Same Content are the top two causes to NM for both Top and Random.
For Random, Heuristic is the most significant issue while Same Content is in the second place.
The distributions of Heuristic and Same Content are not affected by RI.
For Heuristic, as we consider the worst case, RI is not an influencing factor and thus it occupies the same proportion in each RI.
We next analyze Same Content, Heuristic Expiration, and Conservative Expiration Time in detail.
 'We next investigate the Heuristic Expiration issue, which often happens when developers are unaware of the importance of cache and do not explicitly configure cache parameters.
Heuristic Expiration is an important issue for the Random websites because their developers may be unskilled.
However, Heuristic Expiration also takes the second place in the Top websites.
Table 6shows the proportion of Heuristic Expiration resources as well as the average updating cycle (the interval between two updates).
Totally, 6% and 33.7% of Top and Random resources are configured as Heuristic Expiration.
Since different Web browsers have different algorithms to deal with Heuristic Expiration, we just consider the worst case in the previous analysis.
Considering all the Heuristic Expiration resources, the cycle is 5.9 hours for Top while it is 107 hours for Random.
This observation implies that resources configured as   Heuristic Expiration   can benefit a lot from caching if their expiration time is set to daylevel .
In contrast, the proper expiration time for Heuristic Expiration resources of Top is hour-level.
"Based on the model, (1) we have examined the ideal cacheability and found that caching is substantially helpful for mobile Web browsing; (2) we have measured the actual performance and identified a big gap between ideality and reality; (3) we have investigated three main root causes – Same Content, Heuristic Expiration, and Conservative Expiration Time."
6% and 33.7% of Top and Random resources are configured as Heuristic Expiration, but the average updating cycle is 5.9 hours and 107 hours for Top and Random, respectively.
It is better to set an explicit expiration time than using the Heuristic Expiration.
"Our results show a continuum of behavior between FIFO and SJF request scheduling; Web site operators can use the mechanism to implement a policy enforcing a particular behavior."
Otherwise, the servlet is deferred to execute later, and is placed in the admission queue in FIFO order.
Scheduling cannot improve response times if the workload is completely homogeneous.
When there is no admission control and when admission control is in effect with FIFO scheduling , requests that fail or time out during overload are distributed uniformly across all request types.
"Three curves are presented: the original system without admission control, marked   original   ; the system using the Gatekeeper proxy with FIFO scheduling, marked   gatekeeper-FIFO   ; and the system using the Gatekeeper proxy with shorted-job-first scheduling , marked   gatekeeper-SJF.  "
Throughput is unaffected by changing the scheduling algorithm in Gatekeeper from FIFO to SJF.
The system using the Gatekeeper proxy with FIFO scheduling gives better response time, as it provides better throughput and thus lower service and waiting times.
For example, with 250 clients, the original system has an average response time of roughly 11 seconds , whereas with Gatekeeper, using FIFO scheduling provides a response time of 9 seconds, and using SJF scheduling gives a response time of about half a second.
Scheduling can help stave off degrading response time under overload, but cannot prevent it.
The original system has the worst response time, the system using Gatekeeper with FIFO scheduling is better, and the system using Gatekeeper with SJF scheduling performs the best.
Figures 11, 12 and 13 show response times under both the FIFO and SJF policies , distinguishing execution time from waiting time.
The   Exec Search   request waiting time shrinks from over 8 seconds using FIFO to 99 milliseconds using SJF, and the average waiting time falls from 8.8 seconds to 225 milliseconds .
Figure  14shows response times using Gatekeeper with FIFO, SJF with no aging, and SJF with aging, all when locking is done in the application server.
This restrictive policy produces a graph that is closest to the FIFO policy.
The response time is lower (better) than FIFO because the scheduler still has a degree of freedom to reorder the requests in such a way to reduce average response time.
As can be seen, a continuum of behaviors is available between FIFO and SJF.
The mechanism allows decisions of where in the continuum between FIFO and SJF a Web site should operate based on operator policy.
These null hypotheses are:   'Hypothesis One: H 1 0 =   The mean score for group A is no greater than the mean score for group B on good programs   If this hypothesis is refuted, then it means that group A is doing a better job than group B in recognizing good programs, which would be a strong indicator that group A does a better job grading good programs.
Hypothesis Two: H 2 0 =   The mean score for group A is no less than the mean score for group B on bad programs   If this is refuted, it means that group A does a better job than B recognizing bad programs, which is again indicative that group A is doing a better job.
Hypothesis Three: H 3 0 =   The median comment length for group A is no greater than the median comment length for group B on good programs   If this is refuted, it means that group A writes longer comments on high-quality submissions than group B.
Hypothesis Four: H 4 0 =   The median comment length for group A is no greater than the median comment length for group B on bad programs   Comments on bad programs are the most important feedback that a struggling student will receive, and so this is also an important hypothesis.
Hypothesis Five: H 5 0 =   The fraction of people doing a \'bad job\' in group A is no less than the fraction doing a \'bad job\' in B     'We define someone who has done a   bad job   to be a grader that either (a) gets the grade wrong, and gives a perfect score to a program that our code analysis engine thinks is flawed (or gives a non-perfect score to a program that our engine can find no fault with), or (b) writes no comment across all rubric items.
  'Hypothesis Six: H 6 0 =   The fraction of people doing a \'really bad job\' in group A is no less than the fraction doing a \'really bad job\' in group B   We define someone who has done a   really bad job   similarly to the way we define someone who has done a   bad job,   but we replace the or with an and.
We started to investigate the scalability of SimRank in [12], and we gave a Monte Carlo algorithm with the naive representation as outlined in the beginning of Section 2.
The key idea of achieving scalability by Monte Carlo algorithms was inspired by the seminal papers of Broder et al.
Monte Carlo algorithms with simulated random walks also play an important role in a different aspect of web algorithms, when a crawler attempts to download a uniform sample of web pages and compute various statistics [18, 29, 2] or page decay [3].
We refer to the book of Motwani and Raghavan [25] for more theoretical results about Monte Carlo algorithms solving combinatorial prob- lems.
 ' In this section we give a new SimRank variant with properties extending those of Minimax SimRank [20] , a nonscalable algorithm that cannot be formulated in our framework .
Suppose that a Monte Carlo algorithm assigns N independent sets of fingerprints for the vertices and for any pair u, v the similarity function sim(u, v) equals the expected value of the similarities of the fingerprints.
For the ground-based observations, we use publicly-available daily snowfall and snow depth observations from the U.S. National Oceanic and Atmospheric Administration (NOAA) Global Climate Observing System Surface Network (GSN) [1].
DeLa [17]  extends that approach to support nested repetition, such as   (AB*C)*D   .
  Energy Consumption: We provide the first detailed power model on radio bundling that characterizes contributions of individual components (dual-core CPU and two radio interfaces).
  Performance vs. Energy Tradeoff: Finally, our measurements and analysis show that performance gains from bundling are heavily dependent on traffic partitioning algorithms, and naive approaches can actually perform worse than single radio operation
Incorporating Signal Strength.
Radio Energy Tail.
Designing Energy-efficient Bundling.
Smartphone Energy Characterization.
Identify Signal Tweets.
Identify Signal Clusters.
After being trained, both annotators labeled all the top-ranked candidate rumor clusters extracted by either the Popularity method, the Decision Tree method, or the Correction Signal method, from the first week of the GARDENHOSE data set and the first two days and the eighth day of the BOSTON data set.
Therefore, we compare the proposed methods with only Baseline 3 (Correction Signals) on the GARDENHOSE data set.
Signal detection theory is applied to detect abnormal patterns indicating occurring events.
 'In this section, we first discuss how to explicitly model the two categories of emotional signals and propose a novel framework to make use of the Emotional Signals for unsupervised Sentiment Analysis (ESSA).
The basic idea here is to build a latent connection to make sentiment labels of two words as close as possible if they are close in the graph G v , and this goal can be achieved by minimizing the following loss function:   ' <Equation_14>   ' <Equation_15>   ' <Equation_16>   ' 5.3 Exploiting Emotional Signals for Unsupervised Sentiment Analysis   'Supervised learning methods have been widely used for sentiment analysis [25].
, '  Methods Incorporating Emotional Signals:   'Although we first present a quantitative study of emotional signals and provide a unified model to incorporate emotional signals, some efforts have been made to use some specific emotional signals, i.e., emoticons and sentiment lexicons.
Parameters of the Four Signals: In previous subsections , for general experiment purposes, we empirically set equal weights for the emotional signals.
We find that a nonparametric model fits best to the conditions given in open Web service markets and adopt the methods of Wavelet Analysis to an exemplary scenario.
Wavelets have long become a popular tool in time series analysis [31].
In contrast to the Fourier expansion, which allows only for a global spectral analysis, Wavelets are bases functions localized in time and frequency, enabling us to analyze signals and time series locally on different scales.
This is realized through iterative smoothing on different scales: Applying a Wavelet decomposition of level J to a signal implies J times an iterative partition of the signal into a smooth part Sj, and a detailed part Dj , j = 1, .
By analyzing the data on different scales via Wavelet techniques one does not encounter these problems, as the approximation relies only on the observed data itself.
This is illustrated in Figure 3 (c) where we did not assume an explicit parameterized trend function (due to the reasons pointed out above), but compared the weekly mean and the Wavelet approximation .
The approximation was derived by reconstructing the scaling coefficients obtained via a level 9 discrete Wavelet decomposition using the Daubechies-10 (D10) Wavelets (for technical details see [10]).
Since we are mainly interested in the analysis and synthesis of periodic signals, the Wavelet approximation reveals a flaw at this point, since even if we analyze a signal over several periods, the final approximation will not be periodic (as we did not even choose a period, but rather the scale of the decomposition).
The idea we propose is then, to use Wavelets as a preprocessing technique prior to the Fourier expansion: Having computed SJ , we perform a periodic approximation , with period length P , solving the least squares problem   ' <Equation_6>   'WWW   ' <Equation_7>   'and choose k sufficiently large, until the preselected error limit ǫ is reached.
Observable daily and weekly patterns encourage to repeat the above procedure, only changing the approximation level and Wavelet base according to the desired scope of analysis.
 'Global static ranking measures such as PageRank [7] usually have to be computed globally for the entire graph during a lengthy batch process.
Argumentation mining focuses on fine-grained analysis of arguments and on discovering the relationships, such as support and premise, between different arguments [34].
Signal strength, handover and traffic let j w increase and different with the characteristic of wire's jitters.
Global Risk Under global risk, we analyze the price of anarchy and the structure of equilibria when each agent is limited to extend credit to at most one other agent.
In this model, an agent u partitions the set of agents V into two sets using H: neighbors in H and nonneighbors in H. For any network s, agent u estimates risk exposure   ' <Equation_6>    This model assumes agents are willing to interact only with their neighbors in H. For any credit network s formed under this model, cuv(s) = 0 if (u, v) / ∈ E. Global Risk.
Here, as in the Global Risk model, each agent v has default probability δv, but this information is not publicly known.
Global features yield the largest feature coverage.
This framework allows us to leverage the well understood PageRank [3] and Centrality Analysis [4]  approach for Webpage ranking.
Eigenvector Centrality provides a principled method to combine the   importance   of a vertex with those of its neighbors in ranking.
As an example of a successful application of Eigenvector Centrality, PageRank [3] pre-computes a rank vector to estimate the importance for all of the webpages on the Web by analyzing the hyperlinks connecting web documents.
Eigenvector Centrality is defined as the principle Eigenvector of a square stochastic adjacency matrix, constructed from the weights of the edges in the graph.
Global features like color histograms and shape analysis , when used alone, are often too restrictive for the breadth of image types that need to be handled.
Table 2Table 2: Signal-to-noise ratio of a blog post widget for the five top featured Wordpress themes, measured measured by diving the number of fields displayed by the number of DOM nodes used to display them.
3) Canvas fingerprinting accounts for a method of generating unsafe data using the HTML5 canvas element.
Fingerprinting based on exploiting javascript, including strict Canvas fingerprinting, amounts for 15.5% of the observed unsafe requests.
"With high probability, an entity   X   that can be used as   debug X   can also be used as   X developer   ; the other direction doesn\'t hold."
The European cellular system GSM (Global System for Mobile Communications) [35] has received an unprecedented acceptance and spread rapidly over the globe.
For example, the feature   DPU   in previous feature set is extended into   DTP-U,     DTTP-U,     D*PTTU,   and so on.
Interestingly ,   D*UE   is also among the most important features, but with a positively correlated with likerate (r = 0.02).
Notice that the   D*UE   requires a long interval (≥ 72 hours), so the users must have kept the app for some time.
  Global: the Global Prediction method proposed in [12].
  'Note that, we are aware of two other methods in addition to the Global method that can output aspect-dependent sentiment scores.
"But the idea in [1] is similar to the Global method; and the other method [20] has a strict requirement that each text should come with all k aspects, which is not realistic and does not hold in our data sets."
Thus, we only include the Global method here as a representative of state- of-the-art.
The recall of MPQA and INQ is significantly lower than other methods that take context into consideration (Global and OPT).
  In comparison, the Global method, gives a better balance of precision and recall and thus better F-measure.
  Our method OPT further improves the Global method in both precision and recall significantly (and thus F-measure too, by almost 15%).
  The Global method still performs well on both precision and recall.
Editors can explicitly signal their proficiency level in different languages on their user pages using the Babel template [21].
  'We also included editors who made an edit in both French and English Wikipedia in the 12 months before the experiment (regardless of their use of the Babel template), assuming that these editors would be proficient in both languages.
There might be several alternative explanations of why members with high rating today are expected to deliver higher scores in the future:   '  Hypothesis Ia: Higher rated members are those with more inherent skills and abilities and therefore they deliver better solutions.
, '   Hypothesis Ib: Higher rated members are those who inherently care more about their rating and therefore consistently    Hypothesis II: Higher rated members are those with more accrued experience and therefore they deliver better solutions.
, '  Hypothesis III: The rating is   addictive   , members that achieved high rating today tend to contribute more in the future to keep their status high (this is similar to Huberman et al.
  Hypothesis IV: Higher rated members experience less competition in the project choice phase, therefore they can afford to choose easier, better paying or less competitive projects and deliver higher scores.
  Hypothesis V: Higher rated members expect fiercer competition from opponents and therefore have to deliver better solutions in order to win.
Experience of the contestant is not a significant predictor of the contestant\'s performance 18 , therefore we suggest that the Hypothesis II does not hold in our sample.
Overall, our empirical results support Hypothesis IV that higher rated members face less competition in the project choice phase and behave strategically to exploit this competitive advantage.
  ' 4.4 Variation of Patterns in Temporal Mentions over Time    The next observation is the apparent stability of temporal attention both for the future and for the past, if one neglects the effect of major calendar events such as the Christmas and the New Year's Eve.
We cast our model in a hierarchical Bayesian framework and estimate it using Markov chain Monte Carlo methods (see [19] for a detailed review of such models).
ResponseTime and Bandwidth are both objective and observable , but usually take continuous values.
For most applications , however, clients are indifferent between values that fall within some range, and therefore, they can be discretized: e.g., Bandwidth ∈ { DialUp, DSL, T1 }.
Dynamic programming can be used to give close approximations [13, 18], but with tens of thousands of macro-level pages, each with hundreds of DOM nodes, something even simpler is needed.
This stands in contrast to findings of a prior study by Rafter and Smyth [30] who showed for one specific task type that display time is correlated with user interest , especially after individually adjusting the measure.
[Parsing Steps] As per step 2, the new activity has m = 3 steps.
Jimminy bases its suggestions on what the user is reading or writing on the heads-up display, as well as on Global Positioning System data and active badge data indicating what other people are nearby.
(2) Incorporation of Heuristic Rules: there are numerous heuristic rules that can be incorporated.
HTMLParsing (or Parsing), StyleFormatting (or Style) and Scripting process HTML documents , style constraints, e.g., Cascading Style Sheets (CSS), and JavaScript, respectively, and attach results to the internal representation (IR).
If we know nothing about the sequences , we could (and should) try using Fourier, Wavelets, AR, Kalman filters and the other time series analysis tools.
 'The related work falls into the following large subgroups: Similarity search and forecasting: There is a lot of interest in mining time series and data streams [42, 1, 20, 53, 11, 48].
Similarity search and pattern discovery in time sequences have also attracted huge interest [52, 25, 50, 48, 9].
  Wavelets and Fourier transforms (i.e., DWT, DFT, DCT) focus on a single time sequence, and cannot detect interaction between multiple co-evolving sequences.
Global Optimization   ' Two general approaches exist for the QoS-aware service composition: local selection and global optimization.
  'Global Optimization:   'The global optimization approach was recently put forward as a solution to the QoSaware service composition problem [24, 25, 5, 6].
The global selection problem can be modeled as a Multi-Choice Multidimensional Knapsack problem (MMKP), which is known to be NP-hard in the strong sense [20].
The WS HEU for example, is an improvement of the original heuristic algorithm for solving general Multi-Choice Multidimensional Knapsack problems named M-HEU [1].
, ' <Equation_0>   'Aggregation type Examples Function   'Summation Response time q (CS) = n j=1 q(sj)   'Price Reputation q (CS) = 1/n n j=1 q(sj)   'Multiplication Availability q (CS) = n j=1 q(sj ) Reliability Minimum Throughput q (CS) = min n j=1 q(sj) '  Global QoS constraints represent user's end-to-end QoS requirements.
There are three factors that determine the size of the composition problem: the number of required service classes n, the number of service candidates per class l, which we assume to be equal for all classes, and the number of global QoS constraints m. As the problem can be modeled as a Multi- Choice Multidimensional Knapsack problem (MMKP), which is known to be NP-hard [20] , the time complexity of any exact solution is expected to be exponential.
  'Approximation hardness.
 'Approximation algorithm.
¨   ' Bandwidth first principle.
SignalToNoise evaluates the fraction of URLs that are worth further consideration (for Discovery and Sitemaps), while UniqueCoverage, IndexCoverage and PageRankCoverage compare the URLs from Sitemaps compared to what we know globally about the domain.
FIFO ordering delivers all messages sent by a group member in FIFO order.
FIFO order is used in passive and semi-active replication to send the state or the result of the non-deterministic actions.
Since this information is only sent by one replica, the application of those messages in FIFO order will guarantee that all the replicas reach the same state.
The table below summarises these systems, together with the learning style preferences that they utilise and the research upon which they are based: Witkin and Goodenough [48] Arthur [25] Audio, visual, tactile and text Sarasin [40] ILASH [2] Summarising, questioning Hsiao [29] iWeaver [49] Global, analytical, impulsive, reflective, visual, auditory, kinaesthetic   'Dunn and Dunn [18] MANIC [44] Abstract, concrete, graphic, text Stern et al [43] MOT [42] Diverger, converger Kolb [31] AHA!
[42] Activists, pragmatists, reflectors, theorists INSPIRE [26] Reflector, activist Honey and Mumford [28] CS-383 [11] Global, sequential, sensing, intuitive, visual, verbal, active, reflective LSAS [3] Global, sequential Tangow [35] Sensing, intuitive   'Felder and Silverman 1 [20] It is clear from Table 1that a multitude of web systems exists, that represent a diverse assortment of learning styles, some based on work by psychologists such as Witkin, Dunn and Dunn, Kolb, and Honey and Mumford.
Global (or wholist/holist) information is arranged so the student can gain an overview of the subject before studying the finer detail, to gain a broad conceptual view.
Second , they internally use a FIFO transmit queue and achieve maximal transmission rates only if fed several packets in advance.
(Ordinarily, FreeBSD uses a single FIFO queue per output link.)
Several Sensemaking, Decision Support, and Argumentation tools allow a user to annotate a document with structured information that they may then share with other users.
On a 2.33GHz Core2 processor, Dispute Finder is able to check for disputed claims on the New York Times front page in 50 milliseconds, and is able to check the Wikipedia page on Global Warming in 127 milliseconds.
We prototyped and tested two different ways of showing a user alternative points of view to the disputed claim they are looking at:   'WWW 2010   Full Paper April 26-30   Raleigh   NC   USA   'Argumentation graph: When the user clicks on a snippet that makes a disputed claim, Dispute Finder shows them a simple user-editable argumentation graph.
Participants were seated at a workstation with the Firefox browser Figure 12: Argumentation graph interface for a disputed claim mented with the Dispute Finder extension.
The correct behavior is to mark the same text with two different claims, but several participants tried to create a new compound claim such as   Global warming will cause X and Y   .
For example, if a snippet says   Global temperatures will rise by X degrees by 2050   then is that making the claim   Global temperatures will rise   , or should the claim include the extra information?
For example   Global warming is causing more hurricanes   does not support   Global warming is causing rising sea levels   , but both support   Global warming is causing problems   .
In particular, we apply the A* search algorithm against a trie built from the query log, where each node is further annotated with the best score of all descendent queries.
To achieve this, we propose to apply the A* search algorithm against a trie representing the query language model.
We then present the A* search algorithm, followed by discussions on the pruning and thresholding techniques necessary to improve the efficiency and quality of the suggestions.
 'We use the A* search algorithm to find the top corrected query completions for the prefix ̅ , given the query trie and transformation model .
The A* search algorithm can also be configured to perform exact match for offline spelling correction by simply substituting the probabilities in line W with line K. In effect, we continue to penalize transformations involving additional unmatched letters even after finding a prefix match.
The A* search algorithm is configured to deal with partial queries, so that online search is possible.
These bits are coded in a Hamming code manner.
 Similarity search in a collection of objects has a wide variety of applications such as clustering [17], semi-supervised learning [22], information retrieval [6], query refinement on websearch [1], near duplicate detection [21], collaborative filtering, and link prediction [15].
 'Similarity search has been a topic of much research in recent years.
Hash functions that result in minhash equality with probability J(v, w) can be found in [10, 13, 17].
When an object is opaque, the use of Rabin fingerprints [8, 30] to detect common data between two objects has been successfully shown in the past by systems such as LBFS [26] and CASPER [37].
Rabin fingerprinting uses a sliding window over the data to compute a rolling hash.
As the locations of boundaries found by using Rabin fingerprints is stochastically determined, they usually fail to align with any structural properties of the underlying data.
In the extreme case, Rabin fingerprinting might be unable to find any similar data due to the way it detects chunk boundaries.
Tuning Rabin fingerprinting for a workload can also be difficult.
Rabin fingerprinting also needs two computationally-expensive passes over the data: once to determine chunk boundaries and one again to generate cryptographic hashes for the chunks.
The performance comparison in Section 6 shows that Ganesh's row-based algorithm outperforms Rabin fingerprinting.
Given that previous work has already shown that Rabin fingerprinting performs better than gzip [26] , we do not compare Ganesh to compression algorithms in this paper.
At 1600 test clients, Figure 8(    In this section, we address the second question raised in Section 4: How important is Ganesh's structural similarity detection relative to Rabin fingerprinting-based similarity detecting?
As Ganesh always performed better than Rabin fingerprinting, we only present a subset of the results here in the interests of brevity.
   Two microbenchmarks show an example of the effects of data reordering on Rabin fingerprinting algorithm.
As Table 2 shows, Ganesh's row-based algorithm achieves a 97.6% reduction while the Rabin fingerprinting algorithm, with the average chunk size parameter set to 4 KB, only achieves a 1% reduction.
The reason, as shown earlier in Figure 2 , is that with Rabin fingerprinting , the spans of data between two consecutive boundaries usually cross row boundaries.
With the order of the rows changing in the second result and the Rabin fingerprints now spanning different rows, the algorithm is unable to detect significant similarity.
SelectSort2, another micro-benchmark executed the same queries but increased the minimum chunk size of the Rabin fingerprinting algorithm.
While one can partially address these problems by dynamically varying the parameters of the Rabin fingerprinting algorithm, this can be computationally expensive , especially in the presence of changing workloads.
Our initial goal in this work was to study bandwidth-efficient index updates over a network, and we were particularly influenced in this direction by rsync [37] and the Low-Bandwidth File System [25].
In particular, we consider two different sharing policies:   ' ¯ Global Sharing: In this policy, we allow unrestricted redundancy elimination across pages.
  'Query Processing with Global Sharing:   ' In this case, we process a query in a similar manner as in a standard index, by scanning lists from start to end.
Robust Discretization [15] uses three offset grids of grid-square size 6 × 6 to guarantee that for every point in the image, there exists at least one grid whereby the point is -safe.
Centered Discretization [16] determines the grid for a point such that the point is the center of a grid-square of the grid.
Centered Discretization produces a smaller grid-square than Robust Discretization without impacting the usability of the system [16].
Optimal Discretization [17] is the same as Centered Discretization when offset is used, and suffers from the edge problem of discretization (i.e., a small perturbation may result in a wrong grid-square when the click-point is near a grid line) when no offset is used.
As a consequence, multiple trials of possible grid-squares due to acceptable clickvariation are used during authentication to address the edge problem of discretization when offset is not used in Optimal Discretization.
Our security analysis on two representative discretization schemes, Robust Discretization and Centered Discretization, indicates that discretization does have significant security implications: it leaks information about password click-points, and thus leads to weaker security.
Our experimental studies on PCCP show that our purely automatic dictionary attacks using dictionaries each with approximately 2 entries guessed 69.2% of the passwords when Centered Discretization was used, and 39.4% of the passwords when Robust Discretization was used, whereas the full password space was of 2 entries.
In addition, for Centered Discretization, our attack still successfully guessed 50% of the passwords when the attack dictionary size was reduced to approximately 2 .
Our dictionary attacks on PCCP using both Robust Discretization and Centered Discretization are described in Section 5.
Heuristic patterns of click-points in a password and salient regions detected using a visual attention model are used to select likely combinations of the predicted click-points in building attack dictionaries.
Since Optimal Discretization [17] is the same as Centered Discretization [16] when the edge problem of discretization is avoided and the image-dependent discretization scheme [18] lacks detailed information for an actual implantation, our security analysis will focus only on Robust Discretization and Centered Discretization.
 'security when both have the same size of grid-squares, and that Centered Discretization is more secure when they are both -safe.
 'In this section, we present a security analysis of Robust Discretization and Centered Discretization.
Additionally, if the location of a click-point is known within a grid-square as in Centered Discretization or predictable as in the image-dependent discretization scheme [18], salient points can be rank-ordered by their distances to the click-point's locations inside their respective grid-squares: a salient point with a shorter distance is more likely to be the click-point.
 'For each click-point in a password, once salient points are detected, the discretization information stored in the system is exploited to remove salient points that are unlikely to be the clickpoint for Robust Discretization, or to rank-order combinations of salient points by their predicted probabilities to be the password for Centered Discretization.
 'In Centered Discretization, a click-point is the center of some grid-square of the grid associated with the click-point.
 'In Robust Discretization, different schemes can be used to select a grid when more than one grid satisfies the guaranteed tolerance range .
This optimal Robust Discretization was used in [16] to study security of Robust Discretization, and has been adopted in our studies too.
The optimal Robust Discretization leads to an eligible region that a click-point may lie in.
For Centered Discretization, the recovered click-point is the center of the clicked grid-square.
For Robust Discretization, the recovered click-point is the point with a given bias from the center of the clicked grid-square, with the bias being the coordinates of the click-point relative to the center of the grid-square it lies in.
(5) depends only on the coordinates of a click-point, which does not change for different grid-square sizes, a participant would create a single password valid for both Centered Discretization and Robust Discretization with different gridsquare sizes, greatly simplifying our experiments.
"Each guess in a personalized dictionary was tested in the following order until the password was found, the dictionary was exhausted, or the termination condition was met: randomly for Robust Discretization; from highest probability to the lowest probability for Centered Discretization using the probability model; or from lowest layer to the highest layer and randomly within the same layer for Centered Discretization using the zooming-out window model."
 'For Robust Discretization, only the grid-square size of 19 × 19, with = 3 pixels, was studied since a smaller grid-square size leads to poor usability according to [16].
  'To find out how effectively Robust Discretization has helped reduce dictionary sizes, we constructed the attack dictionaries traversing all combinations of the detected salient points, i.e., without exploiting Robust Discretization.
4 bits, and Robust Discretization has further reduced the search space by a factor of 2 = 2 2 ⁄ , i.e., 4 additional bits.
  'On the other hand, Robust Discretization removes an average of 1.0 − 2 2 ⁄ = 42.6% of grid-squares per image when constructing attack dictionaries, which roughly agrees with the 36% area ratio of the eligible region to the grid-square size (see Section 4.4.2).
 'For Centered Discretization, grid-square sizes ranged from 9 × 9 to 19 × 19 were used, leading to = 4.5, 5.5, … , 9.5 pixels.
To compare the success rates for both Robust Discretization and Centered Discretization with the same 19 × 19 grid-square size, our attack on Centered Discretization produced a much higher success rate for similar dictionary sizes or required much smaller dictionaries to achieve similar success rates.
The significantly different success rates between the two discretization schemes for the same grid-square size can be explained by the fact that the discretization schemes leak clickpoint information at different uncertainty levels: whether a clickpoint is in the center of a grid square in Centered Discretization vs. whether a click-point is inside an eligible region, i.e.
36% of a grid square in Robust Discretization.
That is, Centered Discretization leaks much more click-point information than Robust Discretization does, and thus the former suffers more from our dictionary attacks.
  'On the other hand, for the same , Centered Discretization tends to have a better security than Robust Discretization, as the former allows a much larger theoretical password space.
For example, when = 3, our attack guessed 39.4% of the passwords when Robust Discretization (with grid-square size of 19 × 19 ) was used, and 7.9% of the passwords when Centered Discretization (with grid-square size of 9 × 9) was used, both used dictionaries each of approximately 2 entries.
 ' We have not conducted experimental studies on the Image- Dependent Discretization (IDD) scheme [18] due to lack of details to implement the scheme.
However, it is possible to apply our ideas for attacking Centered Discretization to mount dictionary attacks on this IDD as well.
In this case, we can actually use the same techniques as we used for Centered Discretization to rank-order sequences of Voronoi polygon tiles that contain salient points.
 'We have shown that two representative discretization schemes, Robust Discretization and Centered Discretization, both leak information on password click-points, weakening the security of click-based graphical password systems.
In our experiments, our attack successfully guessed 69.2% of the passwords in PCCP when Centered Discretization was used, and 39.4% of the passwords when Robust Discretization was used, with attack dictionaries each of approximately 2 entries whereas the full password space was of 2 entries.
In addition, for Centered Discretization, our attack still successfully guessed 50% of the passwords in PCCP when the attack dictionary size was reduced to approximately 2 .
We note that in such worst-case scenario, denormalization is actually equivalent to the way GlobeTP [14] would have hosted the application.
"Fingerprinting is a probabilistic technique; there is a small chance that two URLs have the same fingerprint."
The orders and the combination of the systems and the topics were randomized according to Latin square design to control for any possible learning and fatigue effects.
Thus the evidence does not support our hypothesis that users are more satisfied using this system over the baseline (Hypothesis 2-1) At the same time the lack of significant difference is a result that speaks in favor of the experimental System.
Regarding other hypotheses at the subjective level, most subjects in the exit interviews said they found the ability to view the task model more useful than the overall neutral rating from the questionnaires would indicate, thus providing support for Hypothesis 2-3.
In support of Hypothesis 2-4, most subjects also found the highlighting of terms from the task model and/or query helpful in the early stages of a search task, but less so toward the end.
The tasks in the main part of the evaluation were similar to the ones from the training session: for each exhibition- UI combination we asked 4 questions related to the content of the exhibitions (presentation of variables was counterbalanced by means of Latin square design).
Lemma 6 (Random seed sets).
Regular expression inference.
The Viterbi algorithm or Viterbi path [28] is a dynamic programming approach for finding the optimal path through a HMM for a given input sequence.
After running the Viterbi algorithm for our running example , the disambiguated resources are: {sider:sideEffect, diseasome:possibleDrug , diseases:1154} and consequently the detected segmentation is: {side effect, drug, Tuberculosis}.
We use the idea behind Prim's algorithm [3] , which starts with all vertices and subsequently incrementally includes edges.
, 'The second set of 15 additional dimensions accounts for compositional and subject isolation aspects not covered by the previous features:   Wavelet-based texture (f10 to f22): Texture richness is normally considered as a positive aesthetic feature, since repetitive patterns create a richer sense of harmony and perspective depth.
Selection algorithms select for each task of a workflow one actual service such that the QoS of the workflow are optimized.
"  ' Genetic algorithms have been successfully used for multiobjective optimization; see, e.g."
Ontology development is difficult and time-consuming, and this is even worsened by the fact that ontologies are mostly created from scratch, resulting in the   Babel of Ontologies   problem.
Fingerprints also can be used to find out when accounts are being illegally shared by more than one persons.
In our current implementation, we use an A* search algorithm to seek those rules with sufficient support and confidence , and use the support threshold to prune the search space.
, zm) = arg min   ' <Equation_1>   ' <Equation_2>   'Variations of MDS with slightly different distortions (1) and objective functions (2) may be found in [3].
RMQ can be supported in constant time by pre-computing the Cartesian tree of A, which can be encoded using BP into 2n + o(n) bits [13].
As each node score now represents the largest score among all strings starting with the prefix corresponding to the node, we can apply it as an exact heuristic function in a variation of the A* search algorithm [32] to find the best completion path from a node representing the prefix.
In addition, it requires a Cartesian tree to perform Range Minimum Queries, which takes a further 2.7 bits per string.
  'Global colours.
  'WWW 2012 – Session: Social Interactions and the Web   ' Personal characteristics: Hypothesis B.
One possible explanation for the inconsistency of our observations with P speaker is the effect of personal characteristics suggested in Hypothesis B from §2.
Figure 2(b) shows that even in the conversations they had on their RfA pages, the admins-to-be were coordinating more to the others than the failedto-be , providing evidence for a strong form of Hypothesis B.
  'Revisiting status: Hypothesis P speaker .
We now return to the issue of status, and describe a method of partially controlling for personal characteristics so as to evaluate the following modification of Hypothesis P speaker : To study P speaker , we create two populations for comparison: the interactions of each admin before his or her promotion via RfA (i.e., when they were admins-to-be), and the interactions of each admin after his or her respective promotion.
   In §5 we saw that the interaction between personal characteristics (which form the basis for Hypothesis B) and power differentials (which form the basis for Hypothesis P) can lead to complex effects.
We also consider a second basic example that raises an interesting challenge for distinguishing between Hypotheses P and B: the effect of gender on coordination, using the fact that gender information is available for participants in the Supreme Court dataset.
Given the extensive history of work exploring culturally-embedded status and power differences based on gender [3, 48], one interpretation of this finding is directly in terms of Hypothesis P. However, since it is also potentially related to theories of gender-based communication differences [26] and even gender-based language adaptation differences [40], the question of separating Hypotheses P and B becomes challenging here.
For example, it is unlikely that a data record starts from TD* and ends at TD# (Figure 2).
Regular expressions ϕ can be used as unary patterns in the following way: ϕ selects those nodes v on a tree t whose sequence of labels on the path from the root to v satisfies ϕ.
To solve this problem, many methods have been using feedback-loops to iteratively adjust prototype vectors, such as Dragpushing method [34], Hypothesis Margin method [34], and Weight Adjustment method [29] .
To improve performance , many studies have attempted using feedbacks to adjust term weight in prototype vectors, such as Dragpush- ing [32], Hypothesis Margin [34], and CentroidW [29] .
" 'Centrality-based link prediction functions are defined as products of centrality measures of the two vertices i and j; different choices of centrality measures lead to different link prediction functions."
Based on the block size |B|, we know the exact size of tuples   'WWW 2007 / Track: XML and Web Data Session: Parsing, Normalizing, and Storing XML    and tiers in our topology layer.
5 For convenience, we introduce the following notation   ' <Equation_10>   'where βν,n(q) = n−1 ν · q ν · (1 − q) n−1−ν , denotes the Bernstein basis polynomial and Sn (q) is the Bernstein polynomial approximation of the status function S (·).
By properties of Bernstein polynomials (see [27]), if S (·) is a strictly decreasing function then Sn (·) is also strictly decreasing and if S (·) is convex or concave then so is Sn (·).
In this setting, the revenue function R(q) = q ·F −1 (1−q) = q(1−q) and the expected per-player contribution of the leaderboard mechanism converges to 0:   ' <Equation_40>   ' where the interchange of the limit and the integration is justified by the uniform convergence of Bernstein polynomials.
Globally, this generates a map showing the landscape of topics over time as in Figure 1obtained from the ACM corpus.
Bootstrapping methods [25, 6, 14], which require a few seeds (ca.
The aux portion of each key (which usually contains the text of URLs) from the i-th bucket is kept in a separate file Q T i in the same FIFO order as pairs <key,value> in Q H i .
  'A.5 Estimating Global Offset   'Since the global offset b is not subject to regularization, we simply estimate b as the mean deviation of each pairwise (dis-)similarity scores in the training objective:   ' <Equation_55>   'A.6 Estimating Feature Transform   'We now describe how to estimate the feature transform S for the transformed feature-based model described in Section 3.2.
The entire formula can thus be seen as a compact recursive notation for a infinitely nested formula of the form: Recursion allows expressing global properties.
Similarity search and pattern discovery in time sequences have attracted huge interest [38, 26, 2, 37, 35, 29, 24, 5] .
Wavelets (DWT) and Fourier transforms (DFT) can detect bursts and typical patterns, but they cannot detect interactions between multiple co-evolving sequences.
Most importantly, we are interested in capturing the above patterns from the following perspectives,   (Global), i.e., world-wide level: general trends and patterns.
 'Until now, we have focused on individual/location-specific trends with four properties, i.e., B, C , S, D. Our final step is to capture these four properties from both (Global) and   ' (Local) perspectives .
"5 We digitize the floating number so that it has the optimal code length, i.e., cF = log(bF ), where bF is the number of buckets/digits and bF = argmin b ′   ' <Equation_17>   'Algorithm 2 GLOBALFIT (X ) 1: Input: Tensor X (d × m × n) 2: Output: Global-level parameters M = {B, C, S, D}; 3: Compute average volumes X (d × n), i.e., X = { ¯ x i } d i=1 4: Initialize parameter set M 5: /* (I) Estimate individual parameters */ 6: for i = 1 : d do 7: M i = TETRAFIT(i, ¯ x i , M); // Parameter fitting for i using ¯ shows the overall procedure."
Considering all path weights are nonnegative, we exploit Dijkstra's algorithm to solve the shortest-paths problem in Eq.
This is because, systems where peers communicate asynchronously with unbounded FIFO message queues can simulate Turing Machines [3].
 'The core problem we study in this paper is the following: Given a set of peers (individual services) that interact via asynchronous messaging (i.e., messages are sent and received through unbounded FIFO message queues), does the interaction behavior change when asynchronous communication is replaced with synchronous communication?
Hypothesis testing decides, at a given confidence level, whether the data supports rejecting the null hypothesis.
Efficient inference is performed with a novel combination of Variational Message Passing (VMP) and Expectation Propagation (EP) (Section 3.1).
For training, inference is achieved by a novel combination of Variational Message Passing and EP.
Heuristically, we found that if the head of the category name is a plural word, the category is most likely a conceptual category.
Parsing Person Names.
In this paper, we apply the model of Biased Minimax Probability Machine (BMPM) to the problem of imbalanced text classification, and propose a new training algorithm to tackle the complexity and accuracy issues in BMPM learning task.
Biased Minimax Probability Machine (BMPM) attempts to determine the hyperplane a T z = b with a T z > b being considered as class x and a T z < b being judged as class y to separate the important class of data x with a maximal probability while keeping the accuracy of less important class of data y acceptable.
 'Most of recent studies on BMPM are usually based on the Fractional Programming problem (we name it BM P MF P ) which could be solved by Rosen Gradient method.
The involved traditional algorithms are the Support Vector Machine (SVM), k -Nearest Neighbor (k NN) and Minimax Probability Machine (MPM).
Transitive closure computation is one of the most basic inference capabilities for an RDF repository.
Note that due to the FIFO eviction policy, (xi, x i ) must be present in the cache.
(Pair (xi, y) may get evicted afterwards by the FIFO policy if xi is the least recently used element).
 'Inverted index is based on the concepts of documents, fields, and terms.
2, the unfolded XPath expression is /drinkers/beers/barname Recursion can also be expressed in XSLT through templates.
GLUE [4], QOM [5], OLA [6], S-Match [7], ASCO [14], PROMPT [19] and HCONE-merge [12]  the emergence of the Semantic Web, there exist a number of algorithms and tools for schema matching in the field of database, e.g., Artemis [1], COMA [3], Cupid [16] and Similarity Flooding (SF) [17].
 'To assign labels to the columns of the data table containing the extracted data objects, i.e., to understand the meaning of the data attributes, we employ the following four heuristics:   'Heuristic 1: match form element labels to data attributes The search form of a web site through which users submit their queries provides a sketch of the underlying relational database of the web site.
  'Heuristic 2: search for voluntary labels in table headers   'The HTML specification [22] defines some tags such as <TH> and <THEAD> for page designers to voluntarily list the heading for the columns of their HTML tables.
Heuristic 4: label data attributes in conventional formats Some data have a conventional format, e.g., a date is usually organized as   dd-mm-yy   ,   dd/mm/yy   , etc., email usually has the symbol   @   , price usually has the symbol   $   , etc.
For example, most of the web sites for job advertisements list their data objects in HTML tables with table headers, so Heuristic 2 worked well for them.
Similarly, Heuristic 1 worked well for the web sites for book shopping and car advertisement because book and car data objects tend to have a clear data structure.
The performance of Heuristic 4 is quite stable, because the three categories of web sites all have data attributes, such as price and date.
Global scope may be undesirable in some cases.
In the Monte Carlo algorithm , we initialize π   ' <Equation_51>   'l,n increases by ∆t in each step of the loop   for t = 1, · · · , N   .
It partially constructs a 2-hop cover index and combines it with bidirectional Dijkstra's algorithm to answer each query.
   Weighted graphs: To handle weighted graphs, for both indexing and updating, we conduct or resume pruned Dijkstra's algorithm instead of pruned BFSs.
With high probability, Algorithm 1 returns estimatesˆqestimatesˆ estimatesˆq, such that error(ˆ q) ≤ exp −O   ' <Equation_14>   'When the average reliability ¯ w is some constant bounded away from 0 (i.e., users are good on average), then error(ˆ q) scales as exp(−O(∆)  ').
(d) NLP Users Figure 1: Error analysis on real datasets: (a) and (b) measure error in item ratings estimates as % of incorrect items, and (c) and   '(d) measure error in user reliabilities using correlation coefficient.
Lines 3 to 8 describe the environment element which specifies that when the event called BandwidthChecker reports a value less than 5120 (which is used to report the current bandwidth of the network connection, in bps), then the chain map will come into effect.
In the onStart() method, the mobilet master begins by subscribing to the BandwidthChecker event by registering with the WebPADS event service.
On subscribing to the event, the mobilet master acquires the bandwidth information from the system by accessing the BandwidthChecker handler and referencing to its primeValue.
Variations of the general model are effective for solving real world text mining problems, such as author-topic analysis and spatial topic analysis.
With high probability, the event F0 holds.
With high probability, the event F1 holds.
In this section, we give a survey on different notions of centrality and their measurements, which can be classified into three categories: Degree Centrality, Shortest-Path-Based Centrality and Eigenvector Centrality.
"For example, the salience of an actor can be measured by the maximum or total distance from it to others, which are called Graph Centrality [12] denoted by CG and Closeness Centrality [22] denoted by CC respectively; or the ratio of shortest paths across the actor in the network, which is called Betweenness Centrality [10] denoted by CB."
Approximation methods aim to reduce the dimensionality and/or size of the input vectors.
Parsing the policy file using sunxacml and loading the policy ontology took 2.1 seconds.
  'Bootstrapping.
Variations in dwell time might be explained by considering the category of the ad (e.g.
Variations such as nibble-oriented coding have been proposed [21], but with only slight additional benefits.
Regular expressions can express a number of strings that the be language cannot, but be types can be generated from type recognizers that can be far more complex than regular expressions.
Nearest neighbor search, based on TFxIDF (Term Frequency multiplied by Inverse Document Frequency), outputs a similarity score between two documents utilizing the following simple formula:   ' <Equation_0>   'where v(Di), v(Dj) are the term vectors describing documents Di, Dj using TFxIDF, as within the Vector Space Model [3], and w k,t represents the actual TFxIDF weight associated to term t within document D k .
Graph kernels are described in [2, 14, 16] .
Centrality and trust measures based on the graph Laplacians are described in [21] for graphs with only positive edge weights.
Centrality in the broadest sense is a measure computed for each node in a graph, denoting to what extent the node is central to the graph.
Centrality measures can be defined by taking a given distance measure on the graph, and measuring the average distance from all other nodes.
Bandwidth costs for the mashup author are also increased by the proxy approach, particularly if the size of the mashup\'s code is small relative to the amount of cross-domain data being proxied.
Sorting/Max algorithms with errors Another line of work similar to ours involves sorting networks, in which some comparators can be faulty.
  'Sorting in steps   'Work has also been done in sorting in a particular number of rounds (or steps as we defined them).
Emphasis is on joint processing of the different modalities and on generic algorithms for linking content.
In the empirical study, we will show that the proposed algorithm for ranking refinement significantly outperforms the standard relevance feedback algorithm (i.e., the Rocchio algorithm) over several datasets.
  ' Rocchio: This algorithm extends the standard Rocchio algorithm for user relevance feedback and it creates a new query vector by linearly combining the query vector and vectors of feedback documents.
Graph kernel measures the similarity between two graphs.
Graph kernels based on random walk is one of the most successful choices [5].
To this end, we define a novel abstract process profile, the Abstract Process Profile for Globally Observable Behavior (APPGOB, cf.
In order to do so, we take the existing APPOB and introduce a new profile, the Abstract Process Profile for Globally In the following, we introduce the above mentioned rules.
Results on Bandwidth: In a mobile handheld environment , the bandwidth of the wireless network poses constraints on the amount of data that can be transmitted.
Sorting test cases will take O(m log m) time.
Sorting m test cases takes O(m log m) time.
  'Difference of click−through rates   'Hidden:   'Observed:   ' <Equation_7>   'Hidden:   'Observed:   ' <Equation_8>   '(a) Examination Hypothesis (b) Intent Hypothesis  It order to represent the intent hypothesis in a probabilistic way, we first introduce some symbols.
   Bootstrapping is a procedure through which a node becomes part of the system.
Variations can be caused by spanning cells created using <ROWSPAN> and <COLSPAN> tags.
  ' <Equation_1>   'such that ∀m ∈ M, ∀n ∈ N Im,n = 0 or Im,n = 1 (5) ∀m ∈ M, ∀n ∈ N Rm,n = 0 ⇒ Im,n = 0 (6) ∀m ∈ M, ∀n ∈ N Im,n = 0 ⇒ Lm,n = 0 (7) ∀m ∈ M, ∀n ∈ N Lm,n ≥ 0   ' <Equation_2>   '∀m ∈ M X n∈N Lm,n ≤ ωm   ' <Equation_3>   ' This problem is a variant of the Class Constrained Multiple- Knapsack problem [12, 13] .
(2) Residual memory and residual CPU are likely to co-locate on the same set of machines.
Intuitively, this model is characterized by a set of global states, describing the composition during its execution, and a set of (FIFO) queues that store the messages exchanges among partners 1 .
The values of the information-theoretic measure are real numbers between 0 and 1, with 0 meaning   completely redundant information   and 1 corresponding to no   'WWW 2007 / Track: XML and Web Data Session: Parsing, Normalizing, and Storing XML   'dancy at all.
We briefly review some normal forms   'WWW 2007 / Track: XML and Web Data Session: Parsing, Normalizing, and Storing XML   'defined for relational and XML data and refer the reader to surveys [7, 22, 9], texts [2, 24, 28], and papers [4, 38] for additional information.
  'WWW 2007 / Track: XML and Web Data Session: Parsing, Normalizing, and Storing XML   'Definition 5.
If B k Σ (I, p) is the probability space ([1, k], P k ), then Ric k I (p | Σ) is the conditional entropy:   ' <Equation_26>   'Since the domain of B k Σ (I, p) is [1, k], we have 0 ≤ Ric k I (p | Σ) ≤ log k. To normalize this, we consider the ratio Ric k I (p | Σ)/ log k. The key observation of [5] is that for most reasonable constraints Σ (certainly for all definable in first-order logic), this sequence converges as k → ∞, and we thus define   ' <Equation_27>   'WWW 2007 / Track: XML and Web Data Session: Parsing, Normalizing, and Storing XML ']}]}}, '_score': 0.8718425}, {'_index': 'mm  '_type': 'pubs  '_id': 'conf_www_JiSCH16  '_source': {'content': {'chapters': [{'paragraphs': [ Through tweets (i.e., short messages up to 140 characters), users share their opinions, emotions, daily life activities and other information .
Regular expression matching is naturally computationally expensive.
 'Scheduling with load balancing: A well-studied problem since the work of Graham [12], is the scheduling of jobs on a set of machines with the goal of minimizing the maximum load on a machine.
Competitive analysis compares an online algorithm against an offline adversary who knows the entire input sequence in advance and serves it optimally.
Heuristics to solve this problem (and, thus, the overall scheduling problem) in the ECT model are described in Section 5.
   With non-negative edge weights, single-source shortest paths can be computed using Dijkstra's algorithm in time proportional to the square of the number of vertices reachable from the source.
To describe the overall complexity and homogeneity of the image texture, we extract the Haralick's features from the Gray-Level Co-occurrence Matrices, namely the Entropy, Energy, Homogeneity, Contrast, similar to [22].
Similarity search Similarity searches return documents with chemical formulae with similar structures as the query formula, i.e., a sequence of partial formulae si with a specific ranges i , e.g.
Similarity search A scoring function like a sequence kernel [9] is designed to measure similarity between formulae for similarity search.
The Sniping Heuristic.
Services communicate using asynchronous message passing via firstin-first-out (FIFO), error-free channels as illustrated in Fig- ure2.
  'Pattern matching:   'As mentioned previously, we adopt VERT for pattern matching.
Sorting by multiple properties works in the way that the system sorts tuples by the first property, and if two or more tuples have the same value on the first property, then it sorts them by the second property, and so forth.
  ' Length difference Besides matching letters, words, hashtags , and URLs, we also calculate the difference in length between two tweets and normalize it by Lmax:   'length difference = abs(|tweeta| − |tweet b |) 140   ' <Equation_1>   'Hypothesis H6: The smaller the difference in length between two tweets, the higher the likelihood of them being duplicates and the higher their duplicate score.
Therefore, we confirm our Hypothesis H1 made in Section 4.1.1.
This result supports Hypothesis H2.
It has the largest positive weight which  means that pairs of tweets with high overlap in Word- Net concepts are more likely to be duplicates, confirming Hypothesis H10 (Section 4.1.2).
However, we can still conclude that Hypothesis H13 holds (see Section 4.1.4).
  Global/local: In this split, we considered the interest for the topic across the globe.
  'Balance and Status: From Local to Global.
  'Searching for Evidence of Global Balance and Status.
, 'WWW 2010   Full Paper April 26-30   Raleigh   NC   USA   'Balance   'Evaluating Global Balance and Status.
Adversary model.
OZSpec == P(Globaldef Classdef ) Finally, the web browsing environment is modeled as:  1 Attributes of a class are partitioned into primary and secondary .
   Consider these three commonly occurring search scenarios across different types of intermittent networks in rural developing regions: Shared Budget-constrained Low Bandwidth Networks: In this scenario several users (100−1000 users) in an institution connect to the Internet using a low-bandwidth connection (say 128 Kbps) with a usage based charging model.
Our current implementation supports only a simple FIFO allocation where the responses to requests are returned according to when they have been successfully fetched.
For MSNBC, the dataset with the longest documents, the second best annotator is AIDA with the CocktailParty and Local algorithms, followed by Wikipedia Miner, Illinois Wikifier and DBPedia Spotlight.
As communication among web services is asynchronous, each peer is equipped with a FIFO queue to store incoming messages.
Heuristically, there are two possible options for doing this.
Energy-minimizing cuts formulation.
In asynchronous communication, when a message is sent, it is inserted to a FIFO message queue, and the receiver consumes (i.e.
"Operationally , we perform the optimization the following way: we use the Local Spectral method which starts from a seed node and then explores the cluster structure around the seed node; running Local Spectral from each node, we obtain a millions of sets of nodes of various sizes, many of which are overlapping; and then for each such set of nodes, we compute the community score f (S) and find the best cluster of each size."
Their box plots are presented in Figure 3, where ResidualsCC and ResidualsSC are the residuals for the cross-company and singlecompany models respectively.
The box plots show that the spread of the distribution for ResidualCC is much wider than that for ResidualSC.
In addition, ResidualCC has most of its values greater than ResidualSC's values, indicating that residuals based Apart from using box plots, we also applied the Paired T-test and the Wilcoxon Signed Ranks test for two related samples to check if both sets of residuals came from the same population.
Betweenness Centrality.
"The new abstractions provide an ideal world where there is only one client and the client 509 (World) W ::= (Σ, κ, κ) (Global Env) Σ ::= { a : τ = v} (Closure) κ ::= (σ, C) | σ (Local Env) σ ::= { x : τ = v} (T ype) τ ::= int | str (Command) C ::= skip | a := E | x := E | C; C | if E then C else C | while E do C | formS( a : s) | formM( a : s) | clear (Exp) E ::= a | x | i (integers) | s (strings) | op( E) (V alue) v ::= i | s Figure 5: BASS syntax is well behaved."
Isotonic regression has been used in many domains such as epidemiology , microarray data analysis [1], webpage cleaning [2], and calibration of classifiers [19] .
 'We denote µI as the maximum throughput that TP I can accept when it can still fulfill the quality qI , defined as   ' <Equation_7>   ' <Equation_8>   'For instance, if the quality metric is the average queueing delay under M/G/1 systems and TP I implements a FIFO scheduling policy, by the Pollaczek-Khinchine mean formula, QI λI , νI =   ' <Equation_9>   'the expected residual service time of jobs.
We observe that average price drop from 2007 to 2011 is approximately 20%, which coincides with the price drop surveyed in the Global Internet Geography [4] report.
Global topics correspond to global properties of the product (e.g., product brand) and local topics are related to the product aspects.
Algorithm 1 E-step of Variational Inference for FLDA 1: initialize µ 0 puni = 1/k for all p, u, n and i 2: initialize η 0 punj = 1/5 for all p, u, n and j 3: initialize σ 0 pi = αi + (N × U )/k for all p and i 4: initialize 0 pij = χij + (N × U )/(k × 5) for all p, i, j 5: initialize o 0 ui = δi + (N × P )/k for all u and i 6: initialize τ 0 uij = γij + (N × P )/(k × 5) for all u, i, j 7: repeat 8: for p = 1 to P do 9: for u = 1 to U do 10: if (p, u) == 1 then 11: for n = 1 to N do 12: for i = 1 to k do 13:   ' <Equation_2>   ') 14: end for 15: normalize µ t+1 puni to sum to 1 16: for j = 1 to 5 do 17:   ' <Equation_3>   ') 18: end for 19: normalize η t+1 punj to sum to 1 20: end for 21: end if 22: end for 23: end for 24: for p = 1 to P do 25:   ' <Equation_4>   ') 27: end for 28: for u = 1 to U do 29:   ' <Equation_5>   ' where the Dirichlet parameters σ, , o and τ , and the multinomial parameters µ and η are free variational parameters.
In this paper, we introduce Tributary-Delta, a novel approach that combines the advantages of the tree and multi-path approaches by running them simultaneously in different regions of the network ... tree, network, algorithm, item, rate, factor, efficient, simultaneously, stream, data, … Bullet: High Bandwidth Data Dissemination Using an Overlay Mesh Abstract In recent years, overlay networks have become an effective alternative to IP multicast for efficient point to multipoint communication across the Internet.
Requests within each class can be processed either in FIFO order or in order of their service times.
Further, when requests within a class are examined in order of service times instead of FIFO, the complexity increases to O(r · log(r)) due to the need to sort requests .
Heuristics used in different search engines are often different from each other emphasizing some aspects, de-emphasizing others, not necessarily sustaining the same quality across varying types of queries.
We have generates about 100 k-workspaces from the disaster portal sites such as Global Volcanism Program [8], Wikipedia [9], and the other RSS-based News Sites on the Web.
Local concept representations are a) the centroid of a concept seed set S or b) the top 100 sentence , query or document contexts ranked by average PMI (Pointwise Mutual Information) with the elements of S. Global representations are 2597 semantic clusters computed using the Cluster-by-Committee algorithm [2] on 1/5th of the 2008 Web crawl and automatically correlated with the lexicons and concepts of interest based on the relative overlap in the element set or centroid feature set.
2   ' (3) Enhancements of other heuristics: Heuristics for combining rankings are motivated by some underlying principle .
Relation depOnArgument(L, X , M, A) determines if variable X defined at L is (transitively) dependent on the M th formal argument A of the residence function.
Relation depOnGlobal(L, X, G) determines if X defined at L is (transitively) dependent on a global variable G. The first two rules are similar to those for depOnArgument.
The third rule describes that if X is dependent on an argument A and there is a call to the function that provides an actual argument dependent on a global variable G, then X is dependent on G. Relation def Global(F, G) represents the global variable definition set of a function.
Recursions do not impose problems for datalog inference as a fixed point would be reached at the end as our analysis is monotonic.
Our second dataset is a repository of the emails exchanged internally among the employees at the Enron Corporation, obtained through a subpoena as part of an investigation by the Federal Energy Regulatory Commission (FERC) and then made public.
Query expansion was first introduced to overcome the problem of word mismatch, a problem fundamental to Information Retrieval.
Most of the commercial systems create special web content for the mobile devices, for example, web Clipping [17], NTT i-Mode [18], AvantGo [16].
Query expansion [17] is used to augment the textual query with related terms, like synonyms.
Median cut uses the median of the indicator vector r as the splitting value to produce equally sized partitions while ratio cut chooses one such that the resulting partitions have the lowest isoperimetric ratio of the graph indicating optimal partitioning.
Bootstrapping methods [5, 1, 7] significantly reduce the number of training examples by iteratively discovering extraction patterns and identifying entity relations with a small number of seeds, either target relation tuples [1] or general extraction templates [7].
Bootstrapping methods [5, 1, 7] significantly reduce the number of training examples by iteratively discovering new extraction patterns and identifying entity relations with a small set of seeds, either target relation tuples [1]  or general extraction templates [7].
Online-tunable control parameters: Controls such as rate of admitted requests, buffer size, rate of upstream request dispatch, maximum number of pending upstream requests , queue priorities, queuing disciplines (FIFO, LIFO), and multi-queue scheduling policies (round robin, priority ) are provided for use by overload control mechanisms.
[30]  exploited Predicate- Argument Structure (PAS) for Question Classification, relevance detection and answer re-ranking.
Figure 3: Global and cluster weights visualization for the various models.
Global ranking: Once we find the   consensus   preference matrix, we find the   optimal   global ranking consistent with it (Section 4).
Ideal experiments would test the following three hypotheses:   Hypothesis 1: In query-only scenarios, Volant does not perform significantly worse than conventional retrieval approaches.
  '   Hypothesis 2: In combined query/navigation scenarios , Volant selects high-quality starting points.
 'As a test of Hypothesis 1 (Section 5.1), we measured mean average precision (MAP) scores on the unambiguous test set (Section 5.2).
 'Our main experiment tests Hypothesis 2 (Section 5.1), i.e., how well Volant performs in scenarios for which it was conceived.
Therefore, we ourselves designed and conducted a user study in order to validate Hypothesis 2.
Figure 2: Signaling cellu- lar scale as well as diversity of apps has posed several challenges towards desired app performance and operational efficiency in cellular networks.
Figure 23: Signaling improvement Signaling for a device, however, is not dictated by a single app but by a mix of apps.
Recursion.
Scheduling such uploads is delegated to data channels.
Any incoming calls that are received during a state transition are placed in the control queue, and any requests in the queue are processed in a FIFO manner until the queue is empty.
Belief propagation functions via iterative message passing between nodes in the network.
Figure 4 : Global network of frequent herb-drug interactions , with drugs represented by nodes with size/font proportional to degree, interactions represented by edges,and drug communities represented by distinct colors.
To estimate the Centrality of a web document di, there are two types of measures: AverSim and CenterSim.
Note that achieving the highest Centrality score does not necessarily mean the web document is the topic initiator, because a following web document may contain more detailed information about the topic, and has higher Centrality score than the topic initiator.
For a date st j , let D(st j ) = {d i |t i = st j } as the set of web documents whose publication date is st j , We define M CS as the maximum content score of those documents,   ' <Equation_17>   'For simplicity, Centrality is used as the ContentScore.
Centrality measures 1 are then applied to analyze the resulting service network in order to detect sources of potential redundancy.
Normalizing the Global and Local Relevance Scores: The global and local relevance scores have different value ranges when they are computed by the five models introduced in Sections 3.2 and 3.3.
Global Relevance Score Analysis: The CE model promotes the rankings of the questions in categories with larger global relevance scores.
To see the effects of the category-specific local IDF, we compare the results of the model VSM+VSM using local IDF (denoted as VSM+VSM.Local) and global IDF (VSM+VSM.Global).
We can see that VSM+VSM.Local performs better than VSM+VSM.Global.
We compare the results of LM when using local smoothing, denoted as LM.Local, and when using global smoothing, denoted as LM.Global.
First, we see that in the results of both VSM+VSM.Local and VSM+VSM.Global, the global relevance component promotes the question in category   Pets.Fish.  
This is because in the method VSM+VSM.Local, the words   giving   and   birth   are more important than the word   guppies,   while the opposite is true in the method VSM+VSM.Global.
   Pulser.
We automate the task of querying the database for genres with Pulser, a script which utilizes Selenium Webdriver.
Pulser submits a query web page, scrapes the returned genres, and logs them in a database table.
 'The data gathered by Pulser allows each site a i to be labeled with a set of genres.
Global term weighting schemes use global document information such as document frequency and total term frequency over documents .
There are some choices of global term weighting schemes such as Residual IDF (RIDF) [8] and gain [32] other than IDF.
There are many alternatives such as x I [5], z-measure [18], Residual IDF (RIDF) [8] and gain [32].
Wavelet tree is a succinct data structure that has recently been used for various purposes.
Given a collection of geo-tagged photos related to festival with tags and locations in Flickr, the desired geographical topics are the festivals in different areas, such as Cherry Blossom Festival in Washington DC and South by Southwest Festival in Austin, etc.
  'Edge Detection Line Detection   'Global Texture Extraction Categorization Type Figure Figure 1: The process to categorize figures.
If B k Σ (I, p) is the probability space ([1, k], P k ), then Ric k I (p | Σ) is the conditional entropy:   ' <Equation_26>   'Since the domain of B k Σ (I, p) is [1, k], we have 0 ≤ Ric k I (p | Σ) ≤ log k. To normalize this, we consider the ratio Ric k I (p | Σ)/ log k. The key observation of [5] is that for most reasonable constraints Σ (certainly for all definable in first-order logic), this sequence converges as k → ∞, and we thus define   ' <Equation_27>   'WWW 2007 / Track: XML and Web Data Session: Parsing, Normalizing, and Storing XML ']}]}}, '_score': 1.4919727}, {'_index': 'mm  '_type': 'pubs  '_id': 'conf_www_Zhang0ZLLZM15  '_source': {'content': {'chapters': [{'paragraphs': [ The vast amount of online products in various e-commerce websites make it an essential task to develop reliable Personalized Recommender Systems (PRS) [24].
  'Document Sorting:   'The documents are sorted by the timestamp.
 'The Local Low-Rank Matrix Approximation (LLORMA) model [26] assumes that the space of (row, column) pairs Φ = {(u, i) : u = 1, .
To date, numerous organisations across the Globe have developed thousands of commercial and/or educational Web applications.
Variations of PageRank and HITS have already been applied in many contexts, especially for propagating reputation and finding experts in the mutual reinforcement process.
"Currently, a number of initiatives and efforts in the lexical semantic community have been started to extend WordNet to cover multiple languages; see the Global WordNet Association 9 ."
 'Step Approximation 0 0.100, 0.100, 0.100, 0.100, 0.100, 0.100, 0.100, 0.100, 0.100, 0.100 1 0.415, 0.049, 0.075, 0.075, 0.113, 0.070, 0.049, 0.049, 0.049, 0.049 2 0.232, 0.100, 0.051, 0.062, 0.078, 0.072, 0.100, 0.100, 0.100, 0.100 3 0.391, 0.066, 0.070, 0.049, 0.097, 0.056, 0.066, 0.066, 0.066, 0.066 4 0.283, 0.093, 0.054, 0.056, 0.076, 0.063, 0.093, 0.093, 0.093, 0.093   ' <Equation_38>     0, 0, 0, 1/2, 1/2, 0, 0, 0, 0.
Query expansion involves adding new words and phrases to the existing search terms to generate an expanded query.
Global analysis is one of the first techniques to produce consistent and effective improvements through query expansion.
Query expansion using query logs is only one application of web log mining.
Therefore, we would not identify the mention   Sony PCG F150   .
\uf0b7 Heuristic training rankings (H): In our preliminary study [29], three heuristic rankings using feature mixtures were proposed to generate the training ranks for learning to rank methods.
  'Hypothesis: We assume that starting from structured content with assigned presentation properties, CSS code can be generated automatically that does not take second place to manually authored code in aspects of code quality.
Sorting can be based on the time when the comment was made, as well.
"In the second step, we propose to predict the aspect ratings using two different approaches, both un-supervised: Local Prediction uses the local information of the overall rating of a comment to rate the phrases in that comment; Global Prediction rates phrases based on aspect level rating classifiers which are learned from overall ratings of all comments."
Our results also show that Global Prediction generates more accurate rating prediction, but Local Prediction is sufficient at predicting a few representative phrases in each aspect.
  ' <Equation_21>   'Intuitively, the phrase rating classifier of Global Prediction should work better than that of Local Prediction.
"Suppose a comment says   slow shipping   while rated as maximum score: Local Prediction would blindly rate the phrase a maximum score; but Global Prediction could potentially tell   slow   is a low-rating on shipping, because   slow   should appear in more lowly rated comments than highly rated comments about shipping ."
With the globally learned classifiers, Global Prediction should be able to accommodate more noisy data, where some comments do not totally agree with their overall rat- ings.
 'After we classify each phrase into different rating values using either Local Prediction or Global Prediction, the rating for each aspect Ai can be calculated by aggregating the rating of the phrases that are clustered into this aspect.
The second column is the predicted ratings for different aspects using Global Prediction.
Since the aspect rating prediction also depends on the quality of aspect clusters, we compare our two methods of rating prediction (Local Prediction and Global Prediction) using three different aspect clustering algorithms proposed in Section 3.1.
  The prediction algorithm Global Prediction always performs better than Local Prediction at correlation for both Unstructured and Structured PLSA aspect clustering .
This indicates that the ratings predicted by Global Prediction are more discriminative and accurate in ranking the four DSRs.
  The ranking loss performance of our methods Unstructured PLSA/Structured PLSA + Local Prediction/Global Prediction is almost always better than the baseline.
If we exclude this aspect and take the average of the other three ranking losses, average ranking loss performance of each algorithm improves and the best performance is achieved by Structured PLSA + Global Prediction at 0.1534 compared with 0.2365 by the baseline.
(2) Local Prediction always outperforms Global Prediction, independent of the underlying aspect clustering algorithm.
This indicates that Local Prediction is sufficient and even better than Global Prediction at selecting only a few representative phrases for each aspect.
But Global Prediction provides rating prediction with more discrimination in ranking different aspects.
Thus, we can write: Parsing involves an inverse mapping from a string to a corresponding tree.
"We can write: Parsing is expressed as given a string s find a meaning d such that there is a tree t and:   ' <Equation_3>   ' <Equation_4>   'Generation is expressed as given a meaning d find a string s such that there is a tree t and:   ' <Equation_5>   'Some parsers choose to build the tree t explicitly; streaming parsers choose not to."
Linear algebraic techniques can be used to verify many properties, e.g., place invariants, transition invariants, and (non-)reachability.
7   'This notion of automated composition of Web services with macros is analogous to AI planning in systems such as Blackbox [19] or Graphplan [6] where we have complete information about the initial situation [22].
  'Given the fact that model I and model II are not nested, besides R 2 -adjusted, we calculated Bayesian Information Criterion (BIC) of both models to compare the two models, using the formula for linear regression models introduced in Raftery (1995):   ' <Equation_0>    : the sample size, in our case, n = 417 log: natural logarithm p: the number of independent variables included in each model It turned out that BIC of model I (R 2 = 0.588) is about 15 points lower than that of model II (R 2 =0.567).
Use Nelder-Mead Simplex algorithm to update the parameter values for θ = (αI , βT ) and go to Step 2, until minimizing the GMM objective function.
Then, we use the Nelder-Mead Simplex algorithm [21] to search for the optimal   ' <Equation_29>   'that minimizes the GMM objective function.
To compute a   content-based weight vector   , a variety of techniques were used, such as the Rocchio algorithm, Bayesian classifiers, and Winnow algorithm [1].
It is of the form   ' <Equation_1>   'where Q is a query containing key terms T, tf is the frequency of occurrence of the term within a specific document, qtf is the frequency of the term within the topic from which Q was derived, and w (1) is the Robertson/Spark Jones weight of T in Q.
Globally consistent registration.
Query expansion with click-through data is motivated by the well-known relevance feedback techniques, which modify the queries based on users' relevance judgments of the retrieved documents [1, 7, 10].
  'Signaling versus Transport:   ' Since the bulk of P2P traffic is related to file downloads and not due to file searches (signaling ) we chose to concentrate our efforts on identifying signatures for file downloads rather than the signaling part of P2P protocols.
Typically the following two phases are involved if a requester desires to download content: Signaling: During the signaling phase a client searches for the content and determines which peers are able and willing to provide the desired content.
: Trajectory of grid cells that the user's mouse passed over while searching for the query ski.
Parsing is a computation process.
This distributed object architecture is similar to that proposed in the Globe system [36] .
For correctness, the system must guarantee FIFO consistency [31]  (aka PRAM consis- tency [24]) in which writes by the backend are seen by each edge server in the order they were issued.
Enforcing FIFO consistency guarantees that, for example, if the backend server creates an object and then updates a page to refer to that object, the system ensures that an edge server that reads the new page will also see the new object.
Note that because only one node issues writes, FIFO consistency is equivalent to sequential consistency [26].
Also note that although FIFO consistency provides strong guarantees on the order that updates are observed, it does allow time delays between when an update occurs and when it is seen by an edge server.
Furthermore, this implementation provides FIFO/PR- AM consistency for shared catalog information using a straightforward approach.
Variations of the catalog object may be useful for other applications that require one-to-many scatter semantics.
Bandwidth constrained update: Applications that have high write/read ratio with large data objects might not want to use a push-all strategy for propagating updates because it would take a lot of bandwidth to send all updates to all edge servers.
In another study we examine a self-tuning one-to-many data replication algorithm that maximizes availability given a bandwidth constraint by sending FIFO updates for some objects but sending FIFO invalidations for others [26].
Therefore, orders from the same edge server maintain FIFO consistency at the backend server but different servers' orders can be arbitrarily interleaved.
For example, since it supports FIFO consistency for updates from the same machine, we can use it to gather the system logs in distributed systems to, for example, gather user click patterns at a web site.
Strengthening consistency from the underlying FIFO/PRAM propagation of updates to provide stronger semantics such as casual consistency (which may require more complex communication mechanisms such as Bayou\'s log exchange pro- tocol [29] ) or sequential consistency (which may require locking ).
, ' <Equation_0> ' 'Our general approach is similar to that of the Globe system [36] which proposes a uniform framework for distributed computing in wide area networks based on the concept of distributed shared objects .
A distributed Globe object is built from a set of local objects in different address spaces, and each local object interacts with local objects in other address spaces.
Our specific implementation differs from Globe in that we do not follow the same uniform internal structure of Globe objects that separate   semantics object   from   replication object   .
Global Ranking: This keeps track of all the scores computed in previous iterations and makes sure the best candidates are also ranked high globally (say top 2%, or equally ranking threshold of 0.02).
, 'Now that for each state we computed all potential k-trustworthiness levels and corresponding conversations (from which to derive conversation policies), the access control enforcement system can proceed through the following phases:   '  Bootstrapping phase   ' – This phase occurs when the subject has its first contact with the Web service.
  Residual placement is based on the following intuitive rule.
  'WWW   'Heuristic 1 IdentifyBotFromUserAgent Input: ua, a user agent string.
Heuristic rules based detection system can be more easily manipulated by the spamers.
  'We demonstrate the process of constructing the models on a particular blog:   Guided By Echoes.  
3 Heuristic approaches such as Reverb do not allow for such flexibility (in our example, Reverb extracts P2 only).
We evaluate the degree of susceptivity using the following algorithm:   ' <Equation_3>   'Out(q)   ' <Equation_4>   '6: For each page p 7: P R(p ) = P R(p ) + Residual V ' 'We evaluated the impact of our noise removal techniques on the link database of the TodoBR search engine.
  'Cache replacement policies attempt to keep pages that have a high probability of being requested in the near future, cached.
Global tokens are assigned a fixed set of codes in all contexts and are unambiguous in all situations.
Global tokens are used to encode inline data (e.g., strings, entities, opaque data, etc.)
For this study we used the logs of ReachOut deployment in two IBM internal communities – inside Haifa Research Lab (Haifa) and in IBM Global Services sub-division (IGS).
We are grateful to IBM Haifa Labs and to the IBM Global Technical Sales Support team worldwide for using the ReachOut tool and letting us use the data for this work.
Variational inference is an important class of approximate inference algorithms [13].
  Snippet Truncation.
Emphasis can be placed on keywords by purposely repeating them.
Simulated annealing[2] is a stochastic optimization technique that enables one to find 'low cost' configuration without getting trapped by the 'high cost' local minima.
Global instructions (attribute-set, namespace-alias, etc.)
  'Consider, for example, a user seeking to find the   Global Technology Services   homepage.
On the other hand, a priori, if the Global Technology Services homepage had been identified and associated with the term   gts   , the task of the search engine at query time is extremely straightforward.
As an example, several pages in the IBM intranet related to   Global Technology Services   mention the phrase   global technology services intranet   in the title.
To illustrate, let us revisit our Global Technology Services example.
Several pages within the Global Technology Services website contain the phrase   Global Technology Services Intranet   in the title.
Therefore all of these pages will be identified by LAt as candidate home pages with a feature value   Global Technology Services   .
For example, the Diversity Councils home page is located within the website of the Global Workforce Diversity program which in turn is located under the main human resources website.
The titles for these three pages are, respectively:   You and IBM   ,   You and IBM | Global workforce diversity   , and   You and IBM | Global workforce diversity | Initiatives | Diversity Councils   .
Global analysis operates by first creating groups of related features using an appropriate grouping function.
Thus, the navigational feature   Global Technology Services   will result in a variant   gts   that will be indexed along with the original feature.
As a means to include a wider variety of popular pages, we also obtained the Alexa   Global Top 500   list of 500 globally popular pages, which include both English and non-English pages.
Using the same methodology for 2667 unique ad URLs obtained from pages in the Alexa Global 500 list we obtained similar results with the top 10 accounting for roughly 35%, the top 20 for roughly half and the top 50 for over 70%.
If B k Σ (I, p) is the probability space ([1, k], P k ), then Ric k I (p | Σ) is the conditional entropy:   ' <Equation_26>   'Since the domain of B k Σ (I, p) is [1, k], we have 0 ≤ Ric k I (p | Σ) ≤ log k. To normalize this, we consider the ratio Ric k I (p | Σ)/ log k. The key observation of [5] is that for most reasonable constraints Σ (certainly for all definable in first-order logic), this sequence converges as k → ∞, and we thus define   ' <Equation_27>   'WWW 2007 / Track: XML and Web Data Session: Parsing, Normalizing, and Storing XML ']}]}}, '_score': 2.4428282}, {'_index': 'mm  '_type': 'pubs  '_id': 'conf_www_YinTL11  '_source': {'content': {'chapters': [{'paragraphs': ['Recently we have witnessed more and more usage of structured data by search engines, and a most visible feature is the direct answers to fact lookup queries.
Similarity search in a collection of objects has a wide variety of applications such as clustering [17], semi-supervised learning [22], information retrieval [6], query refinement on websearch [1], near duplicate detection [21], collaborative filtering, and link prediction [15].
Recursion is supported, but often unnecessary as well.
If the notification condition had been content-based (e.g.,  whenever the number of records returned increases by 100 since the last time the query was run   ), the CQServer would parse both the current and previous XML-formatted query result documents using the Java API for XML Parsing (JAXP) 7 and the Apache Xerces XMLparser 8 , a standard XML parsing component, in order to analyze whether the specified condition had been met.
We also offer a web service called Babel [1] through which authors can convert various formats to the Exhibit JSON format and even preview the results in a generic exhibit.
Babel can currently convert between RDF/XML, N3, Bibtex, Tab Separated Values, Excel files, and Exhibit JSON.
The reader is encouraged to upload his or her own data to Babel, or cut and paste rows of tab-separated values from a spreadsheet into Babel, to experiment with Exhibit.
  If XML's essential strength – decentralized evolution of new tag sets — was also its essential weakness, then there would be little to be gain by simply renaming the problem of Babel by encouraging random mutation of new HTML class names.
FIFO, LIFO) which will presumably depend on the application semantics.
Scheduling is more complicated than any one of its perspectives.
"Workload Scheduling Schemes: We choose two wellknown scheduling schemes to study: random dispatching, which doesn't make use of any information of the servers and just sends each incoming job to one of s server uniformly with probability 1/s; Least workload Left (LWL) scheme, which tries to achieve load balancing among servers by making use of the per-server workload information and assigns the job to the server with the least workload left at the arrival instant."
In contrast to centralized information retrieval engines, leveraging a P2P network to distribute the global index among a large number of participating peers offers a solution * The work presented in this paper was (partly) carried out in the framework of the EPFL Center for Global Computing and supported by the Swiss National Funding Agency OFES as part of the European projects BRICKS (507457) and ALVIS (002068).
Second , unions are also used when multiple entities satisfy a set of constraints or projections (like the collection 'provenanceGlobal' in 7.1).
In case multiple molecules are used to construct the final results, the system keeps track of the local provenance of the molecules by performing a union of the local provenance data using a global provenance structure (provenanceGlobal.union).
It also indicates that the Algorithm 1 Simplified algorithm for provenance polynomials generation Require: SPARQL query q 1: results ← NULL 2: provenanceGlobal ← NULL 3: getMolecules ← q.getPhysicalPlan 4: constraints ← q.getConstraints 5: projections ← q.getProjections 6: for all getMolecules do 7: provenanceLocal ← NULL 8: for all constrains do 9: if checkIfTripleExists then 10: provenanceLocal.join 11: else 12: nextMolecule 13: end if 14: end for 15: for all projections do 16: entity = getEntity(for particular projection) 17: if entity is NOT EMPTY then 18: results.add(entity) 19: provenanceLocal.join 20: else 21: nextMolecule 22: end if 23: end for 24: if allConstrainsSatisfied AND allProjectionsAvailable then 25: provenanceGlobal.union 26: end if 27: end for first projection was processed with elements having a lineage of l6 or l7, while the second one was processed with elements from l8 or l9.
The work described in this paper has been supported, substantially, by a grant from the Research Grants Council of the HKSAR, China [']}]}}, '_score': 2.34779}, {'_index': 'mm  '_type': 'pubs  '_id': 'conf_www_ChakrabartiP15  '_source': {'content': {'chapters': [{'paragraphs': [' Similarity search in a collection of objects has a wide variety of applications such as clustering [17], semi-supervised learning [22], information retrieval [6], query refinement on websearch [1], near duplicate detection [21], collaborative filtering, and link prediction [15].
3) Variation is the measure of the mean formatting diversity under each subtree.
   A new service was launched by the EMPIRE market on March 7, 2014.
Hypothesis 2: Figure 2 illustrates a simulation run for REMIND- IN' with a random contribution of 20%.
Hypothesis 3: We cannot confirm hypothesis three, at least not for the basic algorithm without random contribution.
Hypothesis 4: Figure 3nicely exemplifies the effect of the query relaxation algorithm.
Hypothesis 5: Figure 3 contrasts the effects of different parameters and query relaxation algorithm on the peer selection.
Hypothesis 6: Most of our hypotheses were supported with most strength when we combined our algorithm with a proportion of randomly selected peers.
   Global variables.
The content-type, and thus the   'WWW 2007 / Track: XML and Web Data Session: Parsing, Normalizing, and Storing XML    tion rule, for any particular sequence of character data is determined by the type definition for its enclosing element.
  'WWW 2007 / Track: XML and Web Data Session: Parsing, Normalizing, and Storing XML      ASSERT ATTR CONTENT QNAME ID TYPE ID : Check the current tag's attributes for the given attribute QName, and if present, validate its content using the built-in simple type handler given by the type ID.
scan for exactly one tag, or scan a given simple type) are available to the interpreter, we may further assume that the actual work carried out by these primitives will be   'WWW 2007 / Track: XML and Web Data Session: Parsing, Normalizing, and Storing XML   'alent to the work required for a source-code compiled XML validator.
Concerning the discovery (Decentralized), identification (Global), and binding (Dynamic) facets, we do not observe any difference due to the properties of the HTTP protocol.
Afterwards, we merge all valid properties into Global Preference.
Preference Management will decide which Helper class to use by merging them in Global Preference.
Clipping enables the end-user to make a selection within the target web page and is followed by a training session which generates several valid extraction patterns capable of identifying the clipped content.
Clipping is available in the tool in the same manner as several other HTML controls, so the incorporation of the target web clip into the user's web page is provided through a simple drag-and-drop operation or by a menu-selection.
Clippings were generated from the 25 web sites to assess the effectiveness of these extraction patterns.
The derivation of these plans follows a simple algorithm: IF C parameterOf GlobalRequest AND x instanceOf C AND pick(o) achieves fillin(x) => assert  if (C==x) pick(o)  In a similar manner, we address cases where many-toone and one-to-many relations or value transformations are required for expressing the mapping from form elements to domain concepts.
Bootstrapping   such a system is always complex.
 ' This research has been supported by NEDO (New Energy and Industrial Technology Development Organization) as the project ID of 04A11502a.
Many of these hits actually represent sites that present a certain topic in a sequence of web pages and commented links like, e.g., the guided tour of the Visual Human (www.madsci.org/~lynn/VH), guided tour on Wind Energy (www.windpower.dk/tour) and the Chalmers Library guide (educate2.lib.chalmers.se/demopath.
Heuristics [2]  and EM-based learn- ing [5] have been proposed for finding attributes in HTML tables.
Heuristics are used to determine the parent-child relationships between data items, for instance table names, field names, and values.
Answers: Astronomy, Global Warming, and Philosophy, and filter out all questions which have only one answer.
  'Hypotheses ' ' The IMDb was used frequently by one participant, occasionally by nine participants, and never by eight participants.
The same process was followed in both evaluations and can be summarised as fol- lows: Hypothesis If the users of the framework would be able to create a travel object inventory, classify travel objects and the results would be consistent between users , then we would be able to design a tool to support this framework.
  'This work was supported in part by Grant-in-Aid for JSPS Fellows (#09J55302),  Informatics Education and Research Center for Knowledge-Circulating Society  (Project Leader: Katsumi Tanaka, MEXT Global COE Program, Kyoto University), and by MEXT Grant-in-Aid for Scientific Research on Priority Areas:  Cyber Infrastructure for the Information explosion Era   Contents Fusion and Seamless Search for Information explosion  (Project Leader: Katsumi Tanaka, A01-00-02, Grant#: 18049041).
We rewrite the quantity θ as follows:   ' <Equation_50> ']}]}}, '_score': 1.6054012}, {'_index': 'mm  '_type': 'pubs  '_id': 'conf_www_FondN10  '_source': {'content': {'chapters': [{'paragraphs': ['Autocorrelation is a common characteristic of relational and social network datasets, which refers to a statistical dependency between the values of the same variable on related entities.
BlogPulse 2 , a blog analysis service, use the percentage of all posts concerned with a topic of interest to show trends in blogs.
As illustrated in Figure 1, WebDAD consists of five components: SQL Parsing, ATOMIZE Method Generator (AMG), ATOMIZE Method Handler (AMH), Response Generator, and Result Producer.
When a user asks for an SQL query, the SQL Parsing parses that query.
If B k Σ (I, p) is the probability space ([1, k], P k ), then Ric k I (p | Σ) is the conditional entropy:   ' <Equation_26>   'Since the domain of B k Σ (I, p) is [1, k], we have 0 ≤ Ric k I (p | Σ) ≤ log k. To normalize this, we consider the ratio Ric k I (p | Σ)/ log k. The key observation of [5] is that for most reasonable constraints Σ (certainly for all definable in first-order logic), this sequence converges as k → ∞, and we thus define   ' <Equation_27>   'WWW 2007 / Track: XML and Web Data Session: Parsing, Normalizing, and Storing XML ']}]}}, '_score': 2.969118}, {'_index': 'mm  '_type': 'pubs  '_id': 'conf_www_YinTL11  '_source': {'content': {'chapters': [{'paragraphs': ['Recently we have witnessed more and more usage of structured data by search engines, and a most visible feature is the direct answers to fact lookup queries.
We modified Firefox in three key aspects:   Parsing.
