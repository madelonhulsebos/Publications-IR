"Introduction 1
The Formosan Language Archive at Academia Sinica 2 , Taipei, is part of the Language * Institute of Linguistics, Academia Sinica, Taipei, Taiwan E-Mail: {hsez, harryyu}@gate.sinica.edu.tw 1 Earlier drafts of this manuscript were presented in different occasions and among others, at The Fourth Workshop on Asian Language Resources, March 25, 2004, Sanya, Hainan Island, China and at the Tuesday Seminar at the Institute of Linguistics, University of Hawai'i at Mānoa on Feb.1, 2005. We are thankful to all the participants for their helpful suggestions and comments.We are also grateful to two reviewers for their insightful comments that helped us revise an earlier version of this manuscript. 2 The Formosan Language Archive is located at: http://formosan.sinica.edu.tw/. The project work team (headed by E. Zeitoun) includes/included* the following assistants and members. @BULLET Language analysis: E. Zeitoun, Hui-chuan Lin, *Tien-hsin Hsin (
Table 1. Digitized texts in Chinese and English, as of June 2005 
Language 
This paper is organized as follows. In section 2, we discuss problems related to the transcription of different corpora. In section 3, we deal with annotation rules and standards. In section 4, we turn to the notion of text structure. In section 5, we discuss problems related to analytic and programming consistency. Conclusions are drawn in section 6. 
Transcriptions
In this section, we first deal with the orthographic system adopted in the Archive and then discuss IPA conversions from one operating environment (Word) to another (Web). 
Orthographic system adopted in the Archive
We first outline the phonemic inventory of the Formosan languages. We then provide an overview of the diverse writing systems that have been used to transcribe the Formosan languages. Finally, we deal with the problems raised by these writing systems, and explain our preference for using IPA for standardized transcription. 
Outline of the phonemic inventory of the Formosan languages
The Formosan languages exhibit fairly simple phonemic inventory systems consisting usually of no more than twenty consonants and four vowels, which typically include a series of voiceless and voiced stops: /p, t, k, q, /, b, d, g/; an affricate: /ts/; fricatives: /s, z/; a series of nasals: /m, n, N/; liquids: /l, r/; and four vowels: /a, i, u, ´/. Of course, there is great variation among these languages which has arisen through phonological changes. They will not be detailed in the present paper. Most noticeably, Paiwan has developed a series of palatals: /c, Ô, ¥/; Rukai, Paiwan and Puyuma exhibit a partial/full series of retroflexes: /ˇ, Í, Ò/. Atayal, Seediq, Bunun, Paiwan and Thao distinguish between velar and pharyngeal sounds, while 
Amis differentiates glottal and epi-glottal sounds 
f ∏ T s S ß x X  h fricative +vd B v D z Z Ω ƒ "" nasal m n N liquid l ¬ [lh] ¥ Ò trill/flap r R [r] glide w y 【VOWELS】 front central back high i i u u mid e ´, ø o low Q a 
The basic syllable structure in most languages is CVC, though both Rukai and Tsou now exhibit a CV syllable structure. Consonant clusters occur in only a few languages (e.g., Tsou, Maga Rukai, Thao and Atayal). Stress is usually non-phonemic. 
Writing systems adopted to transcribe the Formosan languages during the past four hundred years
Different writing systems (alphabetic, syllabic and logographic) have been adopted to transcribe the Formosan languages during the past four hundred years. Four stages can be distinguished that reflect the history of Taiwan. The last of them is the most complex. 
Dutch colonization (1629-1661): 
The Roman alphabet was first used in Taiwan in the 17th century by Dutch missionaries to record Siraya and Favorlang. They devised a Romanization system based on the Dutch spelling, which at the time had not yet been standardized. 
Chinese colonization (1661-1895): 
With the colonization of Taiwan by the Chinese, many land contracts, songs, place or family names and reports were transcribed with Chinese characters. The phonetic value of these Chinese characters is somewhat complex, sometimes referring to Mandarin Chinese and at other times to the Minnan pronunciation. 
Japanese colonization (1895-1945): 
From 1895 to 1945, Taiwan was a Japanese colony. Aboriginal children were enrolled in schools (up to the age of 12) and learnt Japanese, so they were able, in later years, to transcribe their own language in katakana. 
Post-1945: 
With the arrival of the Nationalist Chinese under the leadership of Chiang Kai-shek, the Chinese government imposed Mandarin Chinese as the sole official language. The Zhuyin fuhao system more popularly known as Bopomofo, was introduced and used in textbooks, dictionaries etc. At one time, it was also used to transcribe the Formosan languages (e.g., the Bible, songs and textbooks). Bopomofo consists of 37 symbols derived from Chinese characters, and some of these symbols were slightly altered to convey sounds recorded in the Formosan languages that are not found in Chinese. Different writing systems (all Romanized) were devised by the Catholic and the Protestant Church and used during the same period. The lack of adherence to common principles had the unfortunate consequence of producing different writing systems for different tribes. Diacritics were introduced: in Amis, for instance, ^ is used to represent a glottal stop. 
In 1991, Prof. Li Jen-kuei 
In 2002, linguists were asked by the Council of 
Problems raised by more recent Romanized writing systems
We will not discuss problems with earlier writing systems (the Dutch-based transcription system and the use of Chinese characters and symbols) as these have been addressed elsewhere (see, for instance, Adelaar 
t v v k k D dh q q z ' z z ? ' rh / ' Ω z ^ "" R b m m ∫ b n n B N ng d d d Î ¬ l dr lh Í D l l rh Ò lr r L dj l Ô dê ¥ l d l g g lj ƒ r r f f R ∏ w w T th j y c 
To overcome the problem of non-standardization in the current writing systems, we decided to record or re-edit texts in IPA, a recommended standard used in many Archive projects (e.g., the Rosetta Project). However, to preserve the integrity of earlier recorded data, we keep intact original materials recorded with certain Romanization systems and produce new versions of these based on our own standardized format. It became necessary for us to make changes in our corpus, as we were including more and more languages. The first languages we started to digitize and to annotate were Rukai, Atayal and Tsou. The commonly accepted use of c in Formosan linguistics as a replacement for 
Using Unicode IPA symbols
To convert IPA symbols from Word documents (in which texts are typed) to the Web, we make use of the Unicode encoding system, which offers the possibility of displaying symbols uniformly across browser platforms. In Unicode, each IPA character is assigned a standardized encoding number so as to avoid using the same code for two different symbols. In theory, Unicode represents the best way to display IPA characters on the Web. In practice, it requires an initial configuration. Displaying IPA symbols on certain platforms is sometimes difficult as will be shown below (Webster 
This section discusses how we use IPA in our two working environments (Word and the Web) and how we convert IPA symbols into a computer-readable form. 
Three things are required to convert IPA symbols from word processing documents into 3. A Unicode-compliant application (e.g., Microsoft Word or Internet Explorer). 
Creating Word processing documents
All the texts included in the Formosan Language Corpora contain different kinds of information: metadata information, utterance identifications, orthographic transcriptions, interlinear word-glosses and free translations. Specific IPA symbols are introduced in the files whenever necessary. We make use of the Unicode-compliant font SILDoulosIPA, made available through SIL. The data is typed as follows: 
(Strictly speaking, a Word document is not an ASCII text file, as it may contain formatting code (e.g., indenting, italics, etc.) and IPA symbols, which are challenging for computer processing. It is thus necessary to convert these phonetic symbols into computer-readable forms. Thus the interoperability can be achieved on another application or platform. A macro can be used to transliterate IPA symbols as decimal numeric entities. For instance, the D character is rendered by the HTML code &#240. Each IPA symbol is automatically converted into its corresponding numeric reference entity throughout a document. When this operation is finished, we import these alphanumeric characters into the textual database. Once the database has been established, the query operation can be performed as desired. 
Creating Unicode IPA Web pages
To display IPA symbols in Web pages, some preliminary work must be done by the user, i.e., his/her computer must be configured with a Unicode IPA font and a Unicode-compliant browser for viewing IPA symbols on the Web. Internet Explorer automatically views web pages encoded with UTF-8, an encoding standard, provided that an appropriate font is In order to display Unicode IPA Web pages, we declare that the HTML page is using: 
(2) <head> <meta http-equiv=""Content-Type"" content=""text/html; charset=utf-8""> 
... </head> 
Then, we need to either specify the name of the font locally, e.g., Lucida Sans Unicode as: 
(Our database keeps track of all the graphs and symbols in each corpus. In other words, not only the Romanization systems but also the numeric reference entities for IPA symbols are stored. This means that when a query is issued from the user's machine, the request is then sent to the server application, which sends the query command to the back-end database, producing a query result that satisfies the initial criteria. The result is then sent back to the server program, which finally produces the HTML output for the user. Our web application is oriented to both browsing and searching the corpus. Either method displays the HTML output, including the IPA codes (if any), and finally displays it in the client browser. If the client computer has the appropriate font installed, e.g., Lucida Sans Unicode, then the IPA symbols are guaranteed to be displayed correctly; if not, the user's web browser will display ""????"" or empty boxes . 
Keyword search with IPA symbols
As briefly outlined above (see section 1), the Formosan Language Archive not only permits the browsing of texts, but also allows for searching based on (i) keywords, (ii) list of affixes and (iii) lexical categories. While the search through affixes and lexical categories is rather simple, as the user browses a separate database 7 , keyword search is one of the most important features of the Formosan Language Archive. The search can be made by typing a word in any of the Formosan languages included in the corpora, its Chinese or English translation or glosses. Of interest for us is searching performed by typing a word in a specific Formosan language. Since each corpus includes IPA symbols, the type of search must also handle these. 
The two applications we are using, Microsoft Word and Internet Explorer, do not allow the automatic insertion of Unicode IPA. However, it is easier to insert manually IPA symbols in Word than in Internet Explorer. The insertion of IPA symbols will first be discussed here with respect to these two environments. We will then explain how we devised a keyboard mapping mechanism that allows the insertion of IPA symbols on the Web. In Microsoft Word (e.g., 2000/XP), there are several ways to insert a Unicode IPA symbol. The first is the well-known Insert…Symbol menu command. After Insert…Symbol is chosen, a Unicode font is then selected, the pull-down list on the right displays all of the Unicode code points (such as "" IPA Extension "" ) included in that font. The second method consists of using the AutoCorrect feature, which is designed to replace mnemonic abbreviations with their Unicode IPA equivalents. This method is handy, but a constraint is placed on codes. They must all begin and end with a non-alphabetic character (see Webster 
Figure 1. IPA Keyboard Mapping 
Annotation rules and standards
Ontology of different Formosan languages
The use of language codes is necessary when constructing the ontology of different Formosan language families included in the corpora. Our coding system is actually based on the latest version of Ethnologue, which was developed by the Summer Institute of Linguistics and is available on the Internet (e.g., DRU for Rukai and BNN for Bunun). As the SIL website does not provide abbreviated names for dialects. We use a two-letter code based on the dialect name itself (e.g., Mn from Mantauran Rukai). Thus, the language and the dialect codes form distinct entries in our database. 
The codes used for the Formosan languages (along with the dialects they include) that are being archived are shown in 
Rules for annotating the corpus in English and Chinese
The Formosan languages are morphosyntactically heterogenous, and though the literature on a number of Formosan languages is now much more abundant than it used to be, many grammatical phenomena have yet to be clarified or need to be further investigated. This poses a challenge for the analysis of each Formosan language corpus that we deal with, as will be explained below. As pointed out by Zeitoun et al. 
Since we started our research in 2001, we have applied a morphemic analysis to annotate all the texts that have been recorded or re-analyzed by ourselves. This method has many advantages in spite of its shortcomings (see below). First, the linguist can annotate the corpus consistently, i.e., words are not "" contextually "" glossed but their "" core "" meaning is sought. Second, it helps to determine the distribution and meaning of nearly each affix, thus allowing construction of an affix database. Third, it deepens one's understanding of the grammar of a specific language, making it easier to identify major lexical and syntactic categories (also included in a database). The first corpus was annotated in 2001 and focused on only one dialect of Rukai, Mantauran. Over the past four years, as different languages have been annotated, we have been obliged to add more abbreviations to our original list, taking into account morphosyntactic distinctions that exist in these languages. This does not pose a problem, as far as linguistic analysis is concerned, because we know that the Formosan languages exhibit much typological variation. As our abbreviation list was discussed in Zeitoun et al. 
The addition of new abbreviations has had two different consequences: (i) the use of particular glosses for a single language, and (ii) the insertion of new symbols to distinguish different types of affixes. We will discuss these two consequences in turn below. Some of these abbreviations are (so far) only used for one language. In Atayal, for instance, there is a distinction between the immediate progressive and remote progressive (cf., nyux vs. cyux). As progressive auxiliary verbs have grammaticalized from earlier existential verbs that still co-occur productively in this language, the same immediate/remote distinction is also found in these existential verbs. This dichotomy has been reported in Seediq, a language from which collections of texts ready for digitization have not yet been retrieved. Atayal is, thus, the only language in our corpora that makes use of these four abbreviations. Other abbreviations, e.g., AF, PF, Red and LocNmz, are much more common and widely spread cross-linguistically. One of the most important changes we have had to make has been the insertion of brackets <>, commonly used to delimit infixes and recommended by the Max Planck Institute, Leipzig 9 . Initially, that symbol was not used in our glosses because in the languages that we were annotating (Rukai, Tsou, Atayal and Saisiyat), two infixes barely co-occur simultaneously. In Saisiyat, for instance, though the combination S<om><in>B´t 
Originally, if we had a word like SomB´t 'beat' to annotate, we would use hyphens to show its word formation, cf., S-om-B´t [beat-AF-beat], following a common practice among Formosanists. The introduction of two new languages, Bunun and Paiwan, forced us to use brackets instead, as the occurrence of two infixes in these languages is quite productive. Our newest abbreviation list is shown in 
Table 5. Abbreviations used in the Corpora 
ABBREVIATION 
AnnoTool: An Annotation Tool for Formosan Languages
To help with annotation of the corpora, a program called AnnoTool (see 
Labels can be translated from English into Chinese, or vice versa. To do so, the user must first select a single term or an entire line from a document and then switch to AnnoTool and click on English→Chinese (or Chinese→English) in the Translate menu. Accordingly, the selected sequence in Word can be translated into one of these two languages. We are conscious that one limitation of AnnoTool is that it has been programmed to handle a specific terminological set. It does not deal with the literal translation of lexical words or phrases. Nevertheless, using this tool makes our linguistic analysis easier than it used to be. 
Figure 3. Using AnnoTool with Word 
Affixes and lexical categories
For the tagging of major lexical categories, we follow – though with some reservations – the standardization established by CKIP in charge of the Academia Sinica Chinese Corpora. Not all of the lexical categories devised by CKIP are found in the Formosan languages, and conversely, some lexical categories not listed by CKIP are necessary to describe the Formosan languages, as illustrated in Tables 6 and 7. The set of lexical categories has been improved since two more languages (Atayal and Saisiyat) other than Rukai 10 were tagged. 
Text structure
In this section, we deal with linguistic "" recognition "" of clause/sentence and paragraph boundaries, and the programs that have been developed to obtain from the Internet a parallel alignment of words, glosses and sentences both in Chinese and in English. 
Table 6. A comparison of existing lexical categories in Chinese and in Formosan languages 
: lexical category found in Rukai or in other Formosan languages 
Clause/sentence and paragraph boundaries
As far as linguistic data is concerned, two major factors help in the recognition of clauses/sentences: intonation and syntactic structure. We transcribe every text based on voice files that are recorded and digitized. Though we have not taken into account nor have we tried to provide the duration of each word, intonation plays quite an important role in the detection of clause/sentence boundaries. The analyst's knowledge of the language also helps him/her determine the beginning and the end of a clause vs. that of a sentence. To give but one example, in Tona Rukai, si 'and' can appear at the end of a sentence or between two nouns or two clauses. Syntactically speaking, it thus functions as a phrasal or causal coordinator/conjunction. In terms of discursive practices, it is used to mark a pause. That pause can be perceived as "" long "" (as in (5)), in which case the analyst has to treat the clause as a full sentence, or as short (as in (6)), in which case, two clauses will be treated as being coordinated and forming a longer sentence. 
Design of programs to recognize words, sentences and paragraphs
In accordance with annotating conventions, the transcription of a text is divided into sentences, which are further segmented into space-delimited words. There are two types of translations: glosses at the word level and free translations at the sentence level. Sentences are numbered for reference purposes. The encoded format of the reference number is xx-xxx-x, where the first part refers to the text id, the second indicates the paragraph id, and the third corresponds to the sentence id 11 . 
Each utterance or sentence contributes to the concept of "" one block. "" A block thus includes: (i) the reference information, (ii) the original utterance or sentence, (iii) word glosses and (iv) free sentential translations. 
The annotated data has a three-level hierarchy. It includes the ""text, "" the ""word"" and the ""sentence."" Transcriptions, glosses and translations are associated with one of these three levels. Metadata is associated at the text level. The structure is hierarchical in that a text contains sentences and words. Based on this hierarchical structure, it is easy for a computer to handle a text as an object (see Jacobson et al. 
Figure 4. Sentence-level database 
Figure 
Word-level database
There is no translation at a higher level than the sentence, so there is no need for a paragraph-level table. The free, sentence-level translations can be strung together and arranged in the original order, and they serve as intelligible, if not always smooth or elegant, translations of the whole text. 
Bilingual translation and alignment
Figure 6. XML markup of a linguistic text 
Sentence alignment
According to our conventional notations, sentences have been aligned since the first corpus (that for Mantauran Rukai) was initially built on a sentence-by-sentence basis. Then the Chinese and English translations were appended. They are clearly distinguishable for distinct line position in the file: We did not have anything. (Zeitoun and Lin 
Word alignment
In the Formosan Language Corpora, each uttered word is space-delimited and owns its bilingual glosses appear below it. If no gloss is available, then an asterisk * replaces it: Interlinear morpheme-by-morpheme glosses provide most of the information necessary to build a word alignment database. In the database design, each record is based on a transcribed word. This lexical unit includes important pieces of information, such as a unique identifier (here called a location), a spelled form, a specific word order, and glosses. Word order plays a major role in word arrangement. It allows words (along with their glosses) to be pieced together and to reappear in the same order as in the original format. 
Word alignment provides a basis for the extraction of bilingual lexicons. Using the alignment database, we can get the full index of a particular language. However, as each word is deliberately cut into pieces corresponding to morphemes rather than being given a literal meaning, it is impractical to put them together in the order of the source language since the result would be incomprehensible gibberish. That is why we provide a lexical category search, which allows the user to browse the meaning of each word, and a reference to its word formation (see section 3.1). Our aligning strategy thus consists quite simply of arranging words with their been/are being adopted for other Rukai dialects and other languages whenever necessary. 
Morphology
Morphology plays a crucial role in understanding the Formosan languages, and the morphemic method we have adopted to annotate each corpus has forced us to deal even more carefully with word formation. The analyst is confronted with two major problems, (i) the incorrect identification of morpheme boundaries, and (ii) the restricted distribution of certain affixes or roots that might render their use and functions opaque. 
Morpheme boundaries
Blust 
Different analyses from ours are found in the literature, and we must take them into consideration. In Saisiyat (Chu 
Distribution of affixes and roots
Some morphemes are invariable. Because their distribution is very much restricted and their morphophonemic/morphemic alternations are nonexistent, it might be difficult to determine their roots, their origins, their lexical categories. This is the case with Mantauran Rukai tila! which translates as 'Leave/Go away' but is actually formed with a first person plural pronoun t(a)-adjoined to what was originally the root ila. This type of analysis can only be drawn on external evidence, and as mentioned above, necessitates a good understanding of the language being investigated. Likewise, some affixes are very non-productive, and it might be difficult to determine their meaning. This is the case with Mantauran Rukai ta/aDa/an´'an´'house warning' (< Da/an´' an´'house'); the meaning of ta/a is still poorly understood. 
Syntactic structures
The major problem that the linguist must be aware of regarding syntactic structures has to do with typological diversity. For instance, in Mantauran and Labuan Rukai, though subordinate temporal clauses are superficially identical, in the former, the subject is marked by the genitive, and in the latter, it is marked by the nominative. 'Yesterday, after I had eaten, I left.' 
Programs developed to remedy analytic inconsistencies
From the processing perspective, a hyphen is used as a morpheme boundary and as such provides morphemic information that can be used to parse word tokens (e.g., om-ia-nai 'Dyn.Fin-so-1PE.Nom') without difficulty. To remedy inconsistencies in transcriptions and glosses, all the words can be extracted from the corpus data to create an index. This index list (or finderlist) enables the analyst to compare all the words in order to minimize incorrect spelling or glosses. This program can also output a frequency list of morphemes (Hockey 
Word-by-word alignment consistency checker
At a very early stage in the development of the Formosan Language Archive, a program called 
Chkgloss was designed to verify the rigid structure of the corpus by comparing the number of orthographic words with that of their glosses (see 
Figure 7. A Screenshot of Chkgloss 
Chkgloss is helpful for identifying errors because it provides the consistency rate between (i) each tagged word and its gloss and (ii) each sentence and its bilingual translation. In most cases, a corpus has to undergo back-and-forth processing several times before it can be deemed to be valid (
Figure 8. The workflow of using Chkgloss 
Conclusion
The Formosan Language Archive is a useful tool for conducting research on the Formosan languages. The multilingual comparable corpora have begun to find their way in linguistic applications and natural language processing. As far as linguistic applications are concerned, each corpus features well-analyzed data that can serve as a basis for more in-depth studies. There are a number of advantages in providing word alignment, sentence alignment, linguistic annotations and bilingual translations. Computer-aided linguistic research is being carried out using tools and techniques that improve the work of the analyst. Applications that were developed for the Formosan Language Archive include Unicode IPA symbols, AnnoTool, 
Chkgloss and Indexer. 
Drawbacks are inevitable, however. If suitable electronic text versions had been available, progress would have been more rapid. Admittedly, a lot of time has been spent on reformatting the legacy data to make it computer readable. In addition, electronic versions of earlier published materials have to be made from scratch, since there were previously no electronic files (e.g., Li 
"
"Introduction
 The most challenging problem in natural language processing (NLP) is programming computers to understand natural languages. For humans, efficient syllable-to-word (STW) conversion and word sense disambiguation (WSD) occur naturally when a sentence is understood. In a natural language understanding (NLU) system is designed, methods that enable consistent STW and WSD are critical but difficult to attain. For most languages, a sentence is a grammatical organization of words expressing a complete thought 
This paper is arranged as follows. In section 2, we describe in detail the auto-generation of NVEF knowledge. Experiment results and analyses are given in section 3. Conclusions are drawn and future research ideas discussed in section 4. 
2. Development of a Method for NVEF Knowledge Auto-GenerationFor our auto-generate NVEF knowledge (AUTO-NVEF) system, we use HowNet 1.0 
 Since 1999, HowNet has become one of widely used Chinese-English bilingual knowledge-base dictionaries for Chinese NLP research. Machine translation (MT) is a typical application of HowNet. The interesting issues related to (1) the overall picture of HowNet, (2) comparisons between HowNet 
Definition of NVEF Knowledge
The sense of a word is defined as its definition of concept (DEF) in HowNet. 
Knowledge Representation Tree for NVEF Knowledge
 To effectively represent NVEF knowledge, we have proposed an NVEF knowledge representation tree (NVEF KR-tree) that can be used to store, edit and browse acquired NVEF knowledge . The details of the NVEF KR-tree given below are taken from 
(place), 位置類(location), 時間類(time), 抽象類(abstract) and 數量類(quantity). Appendix A provides a table of the fifteen main noun features in each noun-sense subclass. As shown in 
(1) Major Event (主要事件): The content of the major event parent node represents a noun-sense subclass, and the content of its child node represents a verb-sense subclass. A noun-sense subclass and a verb-sense subclass linked by a Major Event function node is an NVEF subclass sense-pair, such as LandVehicle|車 and VehicleGo|駛 shown in 
(3) Test Sentence (測試題): The contents of test sentence children consist of the selected test NV-sentence that provides a language context for its corresponding NVEF knowledge. 
Figure 1. An illustration of the KR-tree using 
(
Process 2. Initial POS sequence generation: 
This process will be triggered if the output of process 1 is not a NULL segmentation. It is comprised of the following steps. 
1) For segmentation result w 1 /w 2 /…/w n-1 /w n from process 1, our algorithm computes the POS of w i , where i = 2 to n. Then, it computes the following two sets: a) the following POS/frequency set of w i-1 according to ASBC and b) the HowNet POS set of w i . It then computes the POS intersection of the two sets. Finally, it selects the POS with the highest frequency in the POS intersection as the POS of w i . If there is zero or more than one POS with the highest frequency, the POS of w i will be set to NULL POS. 
2) For the POS of w 1 , it selects the POS with the highest frequency in the POS intersection of the these sets, we have the POS intersection {STRU/36, V/35}. Since the POS with the highest frequency in this intersection is STRU, the POS of 了 will be set to STRU. Similarly, according to the intersection {V/16124, N/1321, ADJ/4} of the preceding POS/frequency set {V/16124, N/1321, PREP/1232, ECHO/121, ADV/58, STRU/26, CONJ/4, ADJ/4} of 了 and the HowNet POS set {V, N, ADJ} of 生, the POS of 生will be set to V. 
Process 4. NVEF knowledge auto-confirmation: 
 In this stage, AUTO-NVEF automatically confirms whether the generated NV knowledge is or is not NVEF knowledge. The two auto-confirmation procedures are described in the following. 
(a) NVEF accepting condition (NVEF-AC) checking: Each NVEF accepting condition is constructed using a noun-sense class (such as 人物類
(b) NVEF enclosed-word template (NVEF-EW template) checking: If the generated NV knowledge cannot be auto-confirmed as NVEF knowledge in procedure (a), this procedure will be triggered. An NVEF-EW template is composed of all the left side words and right side words of an NVEF word-pair in a Chinese sentence. For example, the NVEF-EW template of the NVEF word-pair 汽車-行駛(car, move) in the Chinese sentence 這(this)/汽車(car)/似乎(seem)/行駛(move)/順暢(well) is 這 N 似乎 V 順暢. In this study, all NVEF-EW templates were auto-generated from: 1) the collection of manually confirmed NVEF knowledge in , 2) the on-line collection of NVEF knowledge automatically confirmed by AUTO-NVEF and 3) the manually created NVEF-EW templates. In this procedure, if the NVEF-EW template of a generated NV word-pair matches at least one NVEF-EW template, then the NV knowledge will be auto-confirmed as NVEF knowledge. 
Experiments
To evaluate the performance of the proposed approach to the auto-generation of NVEF knowledge, we define the NVEF accuracy and NVEF-identified sentence ratio according to Equations (1) 
and 
(2), respectively: In Equation (1), meaningful NVEF knowledge means that the generated NVEF knowledge has been manually confirmed to be a collection of NVEF knowledge. In Equation (2), if a Chinese sentence can be identified as having at least one NVEF word-pair by means of the generated NVEF knowledge in conjunction with the NVEF word-pair identifier proposed in 
User Interface for Manually Confirming NVEF Knowledge
A user interface that manually confirms generated NVEF knowledge is shown in 
Principles for Confirming Meaningful NVEF Knowledge
Auto-generated NVEF knowledge can be confirmed as meaningful NVEF knowledge if it satisfies all three of the following principles. 
Experiment Results
For our experiment, we used two corpora. One was the of the NVEF knowledge were generated based on NVEF accepting conditions (human-editing knowledge), and 49% were generated based on NVEF-enclosed word templates (machine-learning knowledge). Tables 5a and 5b show that the average accuracy of NVEF knowledge generated by NVEF-AC and NVEF-EW for news and specific texts reached 98.71% and 97.00%, respectively. These results indicate that our AUTO-NVEF has the ability to simultaneously maintain high precision and extend NVEF-EW knowledge, similar to the snowball effect, and to generate a large amount of NVEF knowledge without human intervention. The results also suggest that the best method to overcome the Precision-Recall Tradeoff problem for NLP is based on linguistic knowledge and statistical constraints, i.e., hybrid approach 
Analysis and Classification of NVEF Knowledge
Table 6a. An illustration of four NV-position types of NVEF knowledge and their ratios. The English words in parentheses are provided for explanatory purposes only. [ ] indicate nouns and <> indicate verbs. 
Table 6c. The Top 5 single-character verbs in N1V1 and N2+V1 word-pairs in manually-edited NVEF knowledge for 1,000 randomly selected ASBC sentences and their percentages. The English words in parentheses are provided for explanatory purposes only. [ ] indicate nouns and <> indicate verbs. 
2 是(be) / [它]<是>做人的根本 8.8% 有(have) / 是不是<有>[問題]了 15.5% 3 說(speak) / [他]<說> 7.7% 說(speak) / 而談到成功的秘訣[妮娜]<說> 3.9% 4 看(see) / <看>著[它]被卡車載走 4.4% 到(arrive) / 一[到]<陰天> 3.6% 5 買(buy) / 美國本土的人極少到那兒< 買>[地] 3.3% 讓(let) / <讓>現職[人員]無處棲身 2.5% 
Table 6d. The Top 5 multi-character verbs in N1V2+ and N2+V2+ word-pairs in manually-edited NVEF knowledge for 1,000 randomly selected ASBC sentences and their percentages. The English words in parentheses are 
Error Analysis -Non-Meaningful NVEF Knowledge Generated by AUTO-NVEF
One hundred collections of manually confirmed non-meaningful NVEF (NM-NVEF) knowledge from the experiment results were analyzed. We classified them according to eleven error types, as shown in 
Table 7. Eleven error types and their confirmation principles for non-meaningful NVEF knowledge generated by AUTO-NVEF. 
Conclusions and Directions for Future Research
In this paper, we have presented an auto-generation system for NVEF knowledge (AUTO-NVEF) that fully and automatically discovers and constructs a large amount of  According to our estimation, the auto-acquired NVEF knowledge from the 2001 UDN corpus combined with the NVEF word-pair identifier could be used to identify 54% and 60% of the NVEF-sentences in ASBC and in the 2001 UDN corpus, respectively. Since 94.73% (9,345/9,865) of the nouns in the most frequent 60,000 CKIP lexicon are contained in NVEF knowledge constructions, the auto-generated NVEF knowledge can be an acceptably large amount of NVEF knowledge for NLP/NLU systems. We found that the remaining 51.16% (5,122/10,011) of the noun-senses in HowNet were caused by two problems. One was that words with multiple noun-senses or multiple verb-senses, which are not easily resolved by WSD (for example, fully-automatic machine learning techniques), especially for single-character words. In our system dictionary, the maximum and average word-sense numbers of single-character words are 27 and 2.2, respectively. The other problem was corpus sparsness. We will continue expanding our NVEF knowledge through other corpora so that we can identify more than 75% of the NVEF-sentences in ASBC. AUTO-NVEF will be extended to auto-generate other meaningful content word constructions, in particular, meaningful noun-noun, noun-adjective and verb-adverb word-pairs. In addition, we will investigate the effectiveness of NVEF knowledge in other NLP and NLU applications, such as syllable and speech understanding as well as full and shallow parsing. In 
董振東，語義關係的表達和知識系統的建造，語言文字應用，第 
Appendix A. Sample Table of Main Noun Features and Noun-Sense Classes 
Main noun features Noun-sense classes bacteria|微生物 微生物(bacteria) 
"
"簡介
因此給定一個形聲字，我們依據漢字構詞資料庫所拆解成的二至三個構件，分別計 算這些構件與原本漢字的發音相似度，查閱聲母與韻母的相似度比較公式表，求算聲母 與韻母的總和，取相似度大者構件，做為聲符的預測。因此形聲字 w 的聲符即可選取與 w 發聲最為相似的構件 c。 
( ) arg max ( , ) c w PC w Similarity w c ∈ = (2) 
舉例來說，漢字「校」
發音相似度最佳化
由於前述發音相似度比較公式表是由人工制訂，這些值是否能有效的做為聲符預測的參 數，還是有更佳的值可以推測形聲字聲符？另外，聲母、韻母及聲調三者所占的比重為 何？則是本節所要探討的問題。我們嘗試採用限制型最佳化方法計算聲母和韻母之發音 相似度。假設一組已知聲符的形聲字 T，依照發音相似度比較公式，我們可以為每一個 形聲字 w∈T 列出 w 的聲符構件與原字發音相似度必須大於非聲符構件與原字發音相似 度的限制條件。以前例漢字「校」來說，其構件為「木」和「交」 ，而其已知聲符為「交」， 因此 Similarity(木,校)≤Similarity(交,校)，也就是 0.1+lb ≤ b+1。 在我們的問題中，可以將聲母發音相似度參數 ub, a, b, c, d 以及韻母發音相似度參數 x, y, z, lb， 拿來做為最佳化問題中的變數。由於當限制條件多於變數個數時，系統可能 無解，因此我們對每個不等式的聲符部份加上一個額外的變數ε i, ≥0，也就是 d+0.1≤ b+1+ 
ε i ，再以 p i i ε ∑ 做為最小化的目標函數，確保聲符與原字的發音相似度大於非聲符構件 以最佳化及機率分佈標記形聲字聲符之研究 151 與原字的相似度。舉例而言，若是聲符與原字的相似度小於非聲符構件與原字的相似度， 則ε i 必須大於 0 才足以讓條件成立，反之若聲符與原字的相似度已大於非聲符構件與 原字的相似度，則ε i 在最小化的目標下自然會是 0。因此若有 m 個已知聲符的漢字，則 可化為以下最佳化問題： 1 2 p i i 0.1 1 0.1 1 min s.t. M 1 , , , , , , , , , 0 T i 
ε ε ε ε ε ⎧ + <= + + ⎪ + <= + + ⎪ ⎪ ⎨ ⎪ + <= + + ⎪ ⎪ >= ⎩ ∑ (3) 
其中 假設 S 代表某些漢字所形成的集合，ƒ(S)、g(S)、h(S)分表示其聲母、韻母及聲調的 分佈機率。令 A 表示所有漢字所成的集合，則ƒ(A)、g(A)、h(A)分別表示漢字的聲母、 韻母及聲調的分佈機率。同理對於一個漢字構件 b，我們可以找出包含 b 的所有漢字 B， 同時求得其聲母、韻母及聲調的分佈機率ƒ(B)、g(B)、h(B)。若是 b 發音集中度較高，則 其聲母分佈ƒ(B)與ƒ(A)就會有較大的差異。因此我們採用 Kullback–Leibler divergence 的 方法來計算兩個分佈的距離。Kullback–Leibler divergence 的公式如下: 
i P(i) (P||Q) P(i)log Q(i) KL = ∑ (4) 
因此我們可以計算 KL(ƒ(B) || ƒ(A))做為構件 b 聲母強度，同理計算 KL(g(B) || g(A)) 做為 b 韻母強度，以及計算 KL(h(B) || h(A)) 做為聲調強度。以下我們以調號以下我們以 聲母為例，計算構件「火」及「包」的聲母強度，我們從漢字構詞資料庫中找出所有標 示注音的字共|A| = 14598；我們同時統計含有構件「火」的字共有|B|=259，含有構件「包」 的字共有|C|=32，其聲母分佈如下: 
154 張嘉惠 等 聲符構件則取發音相似度值較大者），並定義 pcdiff(w)為兩者的差： \ ( ) ( , ) ( , ) w w c w pc pcdiff w Similarity w pc similarity w c ∈ = − (6) 
表五、發音相似度比較法判別準確率 圖一為所有字的 pcdiff 分佈圖（Histogram）。每一個藍色長條圖 M i 代表 pcdiff 為 i （i=…, -0.2, -0.1, 0, 0.1, 0.2, ….）的字的個數（黑色字），同時我們也統計每一藍色長 條中有多少個字的聲調與其聲符構件相同但與非聲符構件不同，我們用 N i 來表示（綠色 字）；另外我們也統計有多少個字的聲調與其聲符構件不同但與非聲符構件相同，我們 用 L i 來表示（紫色字）。這兩部份的字集分別代表的是在採用方程式(5)來計算發音相似 度時，pcdiff 會增加或是減少的字數。因此如果我們採用方程式(5)來計算發音相似度， 將會有新增加 N 0 +N -0.1 +… +N -δ+0.1 可被正確預測的字（橘色轉換），但是同時會有 L 0.1 +L 0.2 +…+L δ 的字會從正確預測轉為錯誤預測或無法判斷 （藍色轉換） 。因此若 N 0 =125, L 0 =73, N -0.1 =45, N -0.2 =10, L 0.1 =25, L 0.2 =14（如圖二）, 當我們設δ=0.2，則我們可新增 125+45=170 個可正確判斷的字，但失去 25+14=39 原可正確判斷的字。整體來說，我們 可多預測 170-39=131 個正確的字，而無法判別的字則減少 125+73-10-14＝174 個字 (442-174=268)。 圖一、聲符發音相似度與非聲符發音相似度差分佈圖 以最佳化及機率分佈標記形聲字聲符之研究 
發音相似度最佳化
"
"Introduction
Bilingual corpora are very important for building natural language processing systems 
Much work reported in the computational linguistics literature has focused on aligning English-French and English-German sentences. While the length-based approach [
Figure 1. The relationships between German-English [Gale and Church 
Punctuation across languages
According to the Longman Dictionary of Applied-Linguistics 
The traditional Chinese writing system does not have punctuation, and it is up to the reader to demarcate the text while reading. With the influx of Western culture in the eighteenth century, punctuation systems similar to the one used with Roman script was adopted in China and Japan. The punctuation includes the period, comma, colon, dash, etc. Although most of those forms of punctuation look similar to Roman ones, they are usually coded as double-bytes and tend to be used differently. The full stop in Chinese and Japanese is a small empty circle, quite different in appearance from the Roman period. Quotes are also very different, shaped like a Greek letter Γ, upright or upside down. There are forms of punctuation that have no counterparts in Roman text. For instance, "" 、 "" is the pause symbol, which is used somewhat like the comma but only when separating items in a list. On the other hand, there are several uses of the Roman comma which do not occur in Chinese texts. A few examples are given below: 
(Parenthetical expressions) 
(1e) Evolution, as far as we know, doesn't work this way. 
(1c) 我們所知道的進化論不是如此的。 
(Appositives) 
(2e) His father, Tom, is a well-known scholar. 
(2c) 他的父親湯姆是一位有名的學者。 
Yang 
Punctuation and Sentence Alignment
Punctuation Marks in English and Chinese
In this section, we will describe how punctuation in two languages can be used to measure the likelihood of mutual translation in sentence alignment. We will use an example in the following to illustrate the method. A formal description also follows: If we keep punctuations in the above examples in the original order and strip everything else out, we have ten pieces of punctuation from the English part (3e) and eight from the Mandarin part (3c) as follows: 
(4c) ， ， 「 ， ， 」 ， 。 (4e) , . "" , , , , "" . 
They can be arranged into different match types as shown below. 
1-1 ， , 1-1 ， . 1-1 「 "" 1-1 ， , 0-1 , 1-1 ， , 2-2 」， , "" 1-1 。 . 
Figure 2. The correspondence between two punctuation strings 
There are several frequently used punctuation forms in Chinese text that are not available in English text, for example, the punctuation forms ""、"" and ""。"". These punctuation forms often correspond to the English punctuation forms "","" and ""."", respectively. It is not difficult to see that the two punctuation strings above match up quite nicely, indicating that the corresponding texts are mutual translations. Roughly, the first two commas in Chinese correspond to the first two English punctuation marks (comma and period), while the Chinese open quote in the third position corresponds to the English open quote also in the third position. The two Chinese commas inside the quotes correspond to two of the four commas within the quotes in English. The two consecutive marks (」 ，) correspond to (,""), forming a 2-2 match. These correspondences can be unraveled via a dynamic programming procedure, much like sentence alignment. See 
Punctuation marks as Good Indicators of Mutual Translation
Based on our initial observation, the portion of the identifiable punctuation matches between two parallel texts in Chinese and English is over 50%. Examining 
c n m γ = , 
(1) 
where γ = the punctuation compatibility factor, c = the number of direct punctuation matches, n = the number of Chinese punctuation marks, m = the number of English punctuation marks. 
We took aligned English-Chinese sentences that had the same punctuation count (which is the denominator of Equation 1), take ten for example, in order to determine how well punctuation works as an indicator of mutual translation of English and Chinese sentences. We also took the same English sentences and matched them up with randomly selected Chinese sentences to calculate the compatibility of punctuation marks in unrelated texts. 
The results obtained indicated that the average compatibility of pairs of sentences, which were mutual translations, was about 0.67 (with a standard deviation of 0.170), while the average compatibility of random pairs of bilingual sentences was 0.34 (with a standard deviation of 0.167). 
Figure 3. English punctuation across aligned sentences 
Aligning Parallel Bilingual Corpora Statistically with Punctuation Criteria 
Figures 4 through 6 show the compatibility results based on punctuation counts of eight, ten and twelve respectively. These graphs were constructed by analyzing around 50,000 aligned sentences found in the Sinorama Magazine (1990-2000). 521, 259, and 143 sentences were selected to obtain values of n and m equal to 8, 10, and 12, respectively. The solid lines simply connect data points for easier observation. 
Figure 6. Compatibility of translation pairs vs. random pairs with n=m = 12 
Intuitively, as the number of punctuation marks increases, the reliability of the compatibility function does also. Overall, if the punctuation marks are softly matched in ordered comparison across the two languages, they indeed provide useful information for effective sentence alignment. Analyzing the Sinorama corpus, we found that the percentage of matched sentences having the same number of punctuation marks was 21.42%. We selected and analyzed aligned sentences having different numbers of punctuation marks to get more insight into the distinction between matched and random sentences. The analysis also helped us to determine the proper use of the binomial distribution function for sentence alignment. Sentences with eight, ten, and twelve punctuation marks were arbitrarily chosen for analysis. It appears that the distinction between mutual translations and unrelated texts indeed becomes more prominent for sentences that have larger numbers of punctuation marks. 
Punctuation Alignment Model
Instead of one-to-one hard matching of punctuation marks in parallel texts as used in the cognate approach of 
( , ) ( , ) ( , ) i j k k k k A P EP CP P Cp Ep P Cp Ep ≈ ⋅ ∏ (2) 
where which ranges from 0 to 2, We observe that in most cases, the links of punctuation do not cross each other, much like the situation with sentence alignment. Therefore, it is possible to use the dynamic programming procedure to softly match punctuation across languages. 
( , ) k k P Cp Ep = the probability of translating k Cp into k Ep , and ( , ) k k P Cp Ep = 
In order to explore the relationship between punctuation in pairs of Chinese and English sentences that are mutual translations, we selected a small set of manually aligned texts and investigated the characteristics and the statistics associated with the punctuation. Information from around 500 manually analyzed sentences was then used as the initial parameters to bootstrap a larger corpus. An unsupervised EM algorithm and dynamic programming were used to optimize the punctuation correspondence between a text and its translation counterpart. The steps in the standard EM algorithm which we used included initializing model parameters with manually analyzed punctuation matching probabilities, assigning probabilities to missing punctuation, estimating model parameters from completed data, and iterating the process until convergence was reached. The EM algorithm converged quickly after the second iteration of training. 
We observed that, in most cases, the links of punctuation did not cross each other, much like the situation with sentence alignment. Therefore, we were motivated to use the dynamic programming procedure to softly match punctuation across the languages by finding the Viterbi path using the punctuation translation function ( 
, ) k k P Cp Ep and fertility function ( , ) k k P Cp Ep . 
The translation probability functions corresponding to 1-1, 2-2, 1-0, and 0-1 English-Chinese punctuation matches are shown in Tables 1 to 4, respectively. It should be noted that the calculated probability was the conditional probability of each punctuation mark, therefore, the sum of the probability in each table does not equal to one. The punctuation match types (also known as the fertility functions) obtained through training are summarized in 
： 1-1 6 0.015666 , ： 1-1 5 0.007485 ? ， 1-1 5 0.131552 , ； 1-1 4 0.005988 . － 1-1 4 0.007828 : ： 1-1 4 0.222101 ; 。 1-1 4 0.153791 ) ） 1-1 4 0.997159 , ． 1-1 3 0.004491 . ． 1-1 
Punctuation-based Sentence Alignment Model
Unlike the method Simard et al. 
P C E Α Α , 
where A is an alignment and C and E are the source and target texts, respectively. A further approximation encapsulates the dependence of a single parameter b, which is a function of CP and EP: P(A|Ci,Ej) = P(A|b(CP,EP)) . Since it is easier to estimate the distribution for the inverted form, we apply Bayes' Rule to further simplify the calculation: P(A|b) = P(b|A)P(A)/P(b) , where P(b) is a normalizing constant that can be ignored during minimization. P (A) is the match type, and its values are shown in 
( | , ) ( | , ) i j P A C E P A C E Α ≈ ∏ 1 ( ) ( , ) (1 ( , )) k k k t k r n r k k k k k k k n P A P Cp Ep P Cp Ep r − = ⎛ ⎞ ≈ ⋅ − ⎜ ⎟ ⎝ ⎠ ∏ , (3) 
where P Cp Ep = the probability of the existence of a compatible punctuation mark in both languages; 
( ) k P A = 
the match type probability of aligning 
k i E , and k j C , ; t = 
the total number of sentences that are aligned. From the data, we have found that about 66% of the time, a sentence in one language matches exactly one sentence in the other language (1-1). Three additional possibilities should be also considered: 1-0 (including 0-1), and many-1 (including 1-many). Chinese-English parallel corpora are quite noisy, reflecting from wider possibilities of the match types. Here, we used the same probabilistic figures as proposed by Chuang and Chang 
A Hybrid Punctuation-based and Length-Based Sentence Alignment Model
The 
( | ) P match δ 
can be estimated using the Gaussian assumption following Gale and Church 
( | , ) ( ) ( | ) ( , )(1 ( , )) k k t k n r k k k k k k n P A C E P A P match P CP EP P CP EP r δ − = ⎛ ⎞ ≈ ⋅ ⋅ − ⎜ ⎟ ⎝ ⎠ ∏ . (4) 
The same dynamic programming optimization can then be used. Again, the computation and memory costs are very low when both the length-based and punctuation-based criteria are employed. The average slopes of c l and e l , and the associated standard deviations are estimated in an adaptive manner for each corpus being evaluated 
Experiments and evaluation
To explore the relationship between the punctuation marks in pairs of Chinese and English sentences that are mutual translations, we prepared a small set of 200 pairs of sentences aligned at the sentence and punctuation levels. We then investigated the characteristics of and the statistics associated with the punctuation marks. We derived estimates of the punctuation translation probabilities and fertility probabilities from the small set of hand-tagged data. This seed information was then used to train the punctuation translation model on a larger corpus via the EM algorithm. The probability of a punctuation mark having a translation counterpart was estimated as p = 0.670 with a standard deviation 0.170. For random pairs of bilingual sentences, p = 0.340, with a standard deviation 0.167. There appears to be marked differences between the two distributions, indicating that, indeed, soft and ordered comparison of punctuation marks across languages provide useful information for effective sentence alignment. 
In order to assess the performance of punctuation-based sentence alignment, we randomly selected five bilingual articles from the Sinorama Magazine Corpus and Scientific American (US and Taiwan editions), and several chapters from the novel Harry Potter. These were subjected to an implementation of the proposed method. Some experimental results are shown in appendices A and B. It should be noted that in Appendix A, the first English sentence and the first Chinese sentence are both title sentences, and that they are aligned based on the carriage return deliminater, even though no punctuation marks are found in the English sentence. We found that, in general, there were more periods in the English text than that in the Chinese text for a given bilingual corpus, especially in the case of a text translated from Chinese into English. As an example, 112 periods were found in a Chinese article 
Figure 7. The punctuation distribution for a bilingual corpus 
This observation prompted us to establish a special rule that the combination of a comma and an open quote in a Chinese sentence should be considered as being equivalent to a full stop. Applying this rule, we found that the sentence count increased from 112 to 126 for the Chinese text mentioned in the above example. This empirical rule helped to improve the The precision rate of the length-based approach [
Table7. Baseline sentence alignment performance achieved using the length-based approach 
Articles Additionally, we evaluated our method on a larger corpus the Scientific American Corpus. We used all of the English and Chinese articles from January 2003 to December 2003. There were 67 articles, 1523 English sentences, and 1599 Chinese sentences. Every article included both an English text and its corresponding Chinese text. The punctuation-based sentence alignment method achieved alignment precision rates of over 93%. Inferior performance was achieved when the hybrid punctuation and length-based method was used as compared with the punctuation-based method alone, as shown by the results listed in Tables 8 and 9. This phenomenon may be attributed to the strong dependence of the length-based method on the average length of the sentences being analyzed. Apparently, length-based methods do not perform well in the case of a corpus that is composed of shorter sentences. Therefore, a length-based method may achieve poorer performance when it is combined with a punctuation-based method. Consequently, caution should be exercised in interpreting these precision rates. 
Our approach has been proven to be effective, and it has been used to construct a concordancer system called TotallRecall 
Discussion
We achieved a striking improvement over the length-based baseline for bilingual text alignment when punctuation was used alone or in combination with lexical information. Combining punctuation and length information, we could get slightly better overall performance. However, the improvement was not entirely consistent. Thus we need to experiment on a longer parallel text in order to be more certain about it. Although word alignment links cross each other quite often, punctuation links do not. It appears that we can obtain sub-sentential alignment at the clause and phrase levels from the alignment of punctuation. For instance, after we align the punctuation in examples (3c) and 
(3e), we can extract the following finer-grained bilingual analyses: 
(5c) 逐漸的 
Conclusion and Future Work
We have developed a very effective sentence alignment method based on punctuation. The probability of the finding matches between different punctuation marks in source and target texts is calculated based on a large bilingual corpus. The punctuation-based measure of mutual translation can be modeled by the binomial distribution. We have implemented the proposed method on the parallel Chinese-English Sinorama Magazine Corpus. The experimental results show that the punctuation-based approach outperforms the length-based approach with precision rates exceeding 93%. We have also demonstrated that the alignment method can be applied to other bilingual texts, without the need for a priori linguistic knowledge of the languages, like Japanese and English. This general approach has been found to be fast, easy to set up, and universal. We believe that this method can be easily applied to many different languages. A number of interesting future directions for researches present themselves. First, punctuation alignment can be exploited to constrain word alignment and reduce error rates. Second, punctuation alignment makes possible a finer-grained level of bilingual analysis of sub-sentential alignment and can provide a strikingly more effective translation memory and bilingual concordance for more effective example-based machine translation (EBMT), computer assisted translation and language learning (CAT and CALL). 
Appendix A 
Some experimental results of sentence alignment based on length and punctuation are presented here. Shaded parts indicate imprecision in alignment results. We calculated the precision rates by dividing the number of un-shaded sentences (counting both English and Chinese sentences) by the total number of sentences proposed. Since we did not exclude aligned pairs using a threshold, the recall rate should be the same as the precision rate. The experimental results indicate that when non 1-1 matches next to each other tend to fail the length-based aligner. However, the punctuation-based aligner appears to handle such cases more successfully. to pay to put their children through university is certainly not that they hope they will become passionate seekers after truth, but to enable them to find good careers,"" says Providence University president Li Chia-tung bluntly. 教育不能「窄化」成只為經濟服務， 但現實的狀況是， 11 Yet in reality, ""the reason most parents are willing to pay to put their children through university is certainly not that they hope they will become passionate seekers after truth, but to enable them to find good careers,"" says Providence University president Li Chia-tung bluntly. 「大部分家長之所以肯花錢讓孩子來 念大學，絕不是希望孩子以後熱衷於 真理的追求，而是為了使孩子將來能 找到好職業，」靜宜大學校長李家同 明白地說。 
Sentence alignment based on length 
念大學，絕不是希望孩子以後熱衷於 
Appendix B 
More English-Chinese alignment results. 
Sentence alignment based on length 
31 ""The advocacy of core curriculum teaching is in itself a very important education for teachers."" Lin Ku-fang says that when NHMC was set up it made broad-based education one of its founding principles, but discovered that attitudes were very hard to change, because ""people today feel they are respected for their profession rather than their personality."" Although when first studying an academic discipline one starts from a general outline, nonetheless one must be very well versed in a subject to teach it well. 「通識本身的提倡，對老師就是很重 要的教育，」文化評論者林谷芳指出， 南華成立時就把通識教育視為創校理 念，但還是發現觀念問題最難突破， 因為「現代人常覺得自己被尊重是因 為我的專業，而不是我的人。」 discipline one starts from a general outline, nonetheless one must be very well versed in a subject to teach it well. 
論，但真的得弄通，才教得好， 
11 ""There is a great sense of challenge about core curriculum teaching, but many people make the mistake of thinking it is very simple,"" says Lin. 
「通識挑戰意味很濃，但大家都誤以 為很簡單。」 
21 The scope of core curriculum teaching appears very broad, but it still has to start from the basics. In Lin Ku-fang's view, any branch of academic learning can be viewed on the three levels of ""man and nature,"" ""man and man,"" and ""man and the supernatural, or that which transcends self."" 通識範圍看起來很廣，但還是由基礎 出發，林谷芳認為，任何學科都可以 從「人與自然」 、 「人與人」 、 「人與超 自然或自我」三個層次來看。 
Appendix C 
Sentence alignment of English-Japanese parallel texts based on punctuation. 
Sentence alignment based on punctuation 
Type English text Japanese Text 11 Liu Tseng-kuei, of Academia Sinica's Institute of History and Philology, once analyzed over 570 female names used during the Han dynasty in hopes it might shed some light on what the people of that time hoped to see in a woman. 
中央研究院歴史語言研究所の副研究 員である劉増貴さんは、 漢代において 女性に何が期待されていたかを理解 するために、570 名余りの漢代の女性 の名前を研究したことがある。 , , . 、、。 12 It turns out that about two-thirds of the names examined were suitable for either women or men. その結果、 ３分の２の名前が男でも女 でも通用するものであることがわか った。 漢代の女性の名前には実に力強 いものも少なくない。 . 、。。 21 Wang Mang, who usurped the throne in 9 AD, named his daughter Jie (""nimble and quick""). The daughter of the emperor Huan Di (132-167 AD) was named Jian (""solid and resolute"") while her mother, the empress Deng, had the even more emphatic name of Mengnu, which means ""fierce woman""! 王莽の娘の名は「倢」 、後漢の桓帝の 娘の名は「堅」といい、桓帝の時の皇 后の名は、より直接的な「猛女」とい うものだったのである。 , , ( "" "" ) . 
( ) ( "" "" ) , , 
, "" "" ! 「」 、 「」 、、 「」 。 11 Says Liu, ""These names show that society at that time had not yet come to hold the two sexes to such very different standards."" 「この現象は、 男性と女性の道徳行為 に対する社会の要求が、 あまり違わな かったことを示しています」 と劉増貴 さんは言う。 , "" . "" 「、、」 。 11 Although they were gradually beginning to use specifically feminine names alluding to a gentle and submissive nature, such traits as a resolute spirit and an agile, tough body were also seen as virtues in a woman. 
当時、 いわゆる女性的な名前もしだい に増えており、 女性を低く見るという 観念も確かにあったが、 それでも女性 が強くたくましくあることも肯定さ れていたのである。 , , . 、、、。 11 ""The notion of the ideal woman being soft and weak was not so universally accepted then as it would later come to be."" 「女性は弱くておとなしい方が良い とする考えは、 後の時代のように絶対 的なものではなかったようです」 と劉 増貴さんは言う。 "" . "" 「、」 。 
"
"Introduction
Statistical language model plays an important role in natural language processing. It has a wide range of applications in many domains, such as speech recognition 
The ngram model takes the word sequence as a Markov chain. It makes the Markov hypothesis on the sequence so as to simplify the probability inference. There are actually two hypotheses implied by the Markov hypothesis, named the limited history hypothesis and the stationary hypothesis [
The most obvious extension to the traditional ngram model is simply to enlarge the number of history words and build up the higher-order ngram model 
The remaining part of the paper is organized as follows. The related works are outlined in section 2. In section 3, the NS ngram model is proposed and several related issues are discussed in detail. In section 4, the data sparseness problem of the NS ngram problem is addressed and three smoothing approaches are proposed. The experimental results and discussions are presented in section 5 and the conclusion is drawn in section 6. 
Related Works
There are many ways to improve the performance of the ngram model. The most obvious way is to relax the limited history hypothesis and build up the high-order ngram model, which has been discussed in the above section. Another way is to construct the skipping ngram model 
The class-based ngram model is constructed based on word cluster instead of word. The syntax and semantic information can be well captured in this way. Meanwhile, the parameter space is reduced greatly and the data sparseness problem is alleviated. However, the predictive capability of the class-based ngram model is much lower than the traditional ngram model due to its small parameter space. It usually achieves limited improvements by interpolating with the traditional ngram model. 
The cache-based ngram model 
Non-Stationary Ngram Model
This section firstly reviews the traditional ngram model briefly. Secondly, it defines the NS ngram model formally. Thirdly, the word positional information is formalized. Finally, the estimation method is provided for the conditional probability of the NS ngram model. 
Ngram Model
Language model aims to determine the probability of the sequence of words. The sequence probability is usually decomposed into the conditional probabilities of words which are composed of sequences. For the sequence of 1122 ,,, ... 
mm 
()(|,...) iiii m lplplplp i Pspwwww -- = = Õ , 
(1) 
where , ij lp w is the i th word in the lexicon and appears at the j th position in sequence S. 
The ngram model makes the Markov hypothesis on the sequence so as to simplify formula (1). The procedures are described in formula 
(2): 111111 ,,, 11 ()(|...)(|...) iiininiiiini mm lplplplll ii Pspwwwpwww -+-+---+- == »» ÕÕ . 
(2) 
Actually, there are two hypotheses implied by the Markov hypothesis: 1. The limited history hypothesis: the probability of current word is dependent only on the previous n-1 words, but irrelevant to the whole history of words. 
2. The stationary hypothesis: the word transition probability is determined only by the words which consist of the transition probability, but irrelevant to the positions where these words possess in the sequence. Formula (1) is firstly simplified by the limited history hypothesis, resulted in the second item of formula (2). Then, the stationary hypothesis is applied on it and the final form of the ngram model is obtained, as represented by the last item of formula (2). The paper substitutes 
i l w for , ij lp w 
since the conditional probability is irrelevant to word position. In literature, the limited history hypothesis is referred to frequently, but seldom is the stationary hypothesis. 
The most obvious way to extend the ngram model is simply to relax the limited history hypothesis and involve more history information of words. The higher-order ngram model is built up. However, the high-order ngram model suffers from the curse of dimensionality. As the model order increases, the parameter space explodes at an exponential rate. The data sparseness problem becomes very severe which hampers its applications gravely. From another point of view, the paper relaxes the stationary hypothesis and enhances the ngram model by the exploitation of the word positional information. The NS ngram model is proposed. It is described in the following sections. 
NS Ngram Model
As presented in section 1, the occurrence of words is relevant to their positional information in sentence. It is beneficial for the language model to exploit the positional information to determine the word probability. However, the In the NS ngram model, formula (1) is simplified merely by the limited history hypothesis, rather than the stationary hypothesis. The conditional probability of the current word is determined not only by history words but also by the words' positions in sentence. The paper uses a single positional variable of t to denote the word positional information in formula (3). The traditional ngram model is a special case of the NS ngram model in which t is a constant. 
Important things for the NS ngram model are how to calculate the value of t and how to estimate the conditional probability of word in formula (3). 
Representation of t
Since t denotes the word positional information in a sentence, it is a natural way to take the word position index as the concrete value of t. However, there are two serious problems with this method. Firstly, index has different meanings in sentences of different lengths. For example, there are two English sentences: "" Yesterday I saw you "" and "" Yesterday I saw you were looking around here "" . In both of the sentences, the word "" you "" has the same position index -4. However, "" you "" appears at the end of the first sentence, while it is in the middle in the second. It possesses completely different positional information in these two sentences. Secondly, since a sentence may have arbitrary length, the t value can be any natural number. 
But computer can not deal with infinite value. 
A refined method is to use the ratio of the word position index to the sentence length, which maps t into a real number in the range of 
1. Calculate the ratio of the word position index to the sentence's length, which maps t into the range of 
Figure 1. Calculation of the t value in NS ngram model 
From the above procedures, the more number of bins it divides of the word sequence, the more accuracy of the positional information is extracted from the sentence. 
Training Method
The section discusses how to estimate the conditional probability in formula (3), which is the In order to calculate the probability of a sentence, the t value is firstly obtained for each word. Then, the conditional probability of word is computed according to formula (4). Finally, the sentence probability is calculated by formula (3). The traditional ngram model is a special case of the NS ngram model in which there is only one bin. 
Smoothing Techniques
As shown in section 3.4, there are totally k traditional ngram models in the NS ngram model the traditional ngram model. Data sparseness problem is an inherent and severe problem in the traditional ngram model . Therefore, it is more severe in the NS ngram model. 
Figure 2. Data sparseness problem in NS ngram model 
In 
As n (or k) increases, the problem becomes more severe, and the estimated probability becomes more unreliable. It is necessary to start with these two factors to solve the data sparseness problem of the NS ngram model. Considering the factor of the model order which is represented as the vertical axis in 
The First Approach
Since the NS ngram model is composed of several traditional ngram models, each of these component ngram models can be smoothed separately by the traditional smoothing techniques. 
The traditional smoothing techniques have been well studied before. Many smoothing algorithms have been proposed, such as the additive smoothing 
Additive smoothing: 
1 1 1 ~ (,,)1 (|,) (,) ii ii i ll ll l Cwwt Pwwt Cwtl - - - + = + (5) 
t is the positional variable which is defined in section 3.3; l is the lexicon size; and ~ p is the smoothed probability of the NS bigram model. 
Back-off smoothing: 
11 1 1 ~ ~ (|,)(,,)0 (|,) (,)(,) iiii ii ii GTllll ll ll PwwtifCwwt Pwwt wtPwtotherwise a -- - - > ì ï = í ï î (6) 
P GT is the probability of the NS bigram model which is smoothed by the Good-Turing method. 
It is formalized as below: 
1 1 1 (,,) (|,) (,) ii ii i GTll GTll l Cwwt Pwwt Cwt - - - = (7) and 1 11 1 ((,,)1) (,,)((,,)1) ((,,)) ii iiii ii ll GTllll ll ECwwt CwwtCwwt ECwwt - -- - + =+´( =+´(8) E(C) 
is the expectation of the number of the bigram items which occurs C times in the corpus. 
In reality, N(C) is usually substituted for E(C). N(C) is the concrete number of the bigram items which actually occurs C times in the training corpus. Formula (8) is reformulated as below: 
1 11 1 ((,,)1) (,,)((,,)1) ((,,)) ii iiii ii ll GTllll ll NCwwt CwwtCwwt NCwwt - -- - + =+´( =+´(9) 
However, N(C) can not be estimated reliably for some large values of C. At this time, formula 
(9) can not work properly and problems occur in the Good-Turing method. In particular, when C reaches its max value in the training corpus, 1 
(,,) ii GTll Cwwt 
is calculated to be zero according to formula (9) because N(C+1) is equal to zero. It is obviously wrong. In this paper, a simple strategy is adopted to address the problem. Formula (7) and formula (9) are adopted only for the small value of C (i.e. below a threshold). For the large value of C, it is regarded that the bigram probabilities can be estimated reliably according to the word frequencies and they need not to be smoothed. The MLE principle is applied on them directly. 
In formula (6), α is the coefficient for normalization and it is calculated as below: 
11 1 11 ~~ :(,)0:(,)0 (,)(,) (,) (,)1(,) 
ii 
bb a -- - -- => == - åå (10) and 11 1 :(,)0 (,)1(|,) iii lll iii lGTll wCwwt wtPwwt b -- - > =- å (11) 
Linear interpolation smoothing: 
11 ~~ (|,)()(|,)(1())(,) iiiii lllll PwwttPwwttPwt ll -- =´+-´ (12) 
P is the probability of the NS bigram model which is estimated by formula (4); l(t) is the coefficient which is a function of t and can be estimated by the EM algorithm on the held-out corpus. 
The Second Approach
As shown in 
Jinghui Xiao et al. 
ngram model (k=1) to smooth the NS ngram model (k>1). However, the traditional ngram model also suffers from the data sparseness problem. Actually, the paper utilizes the smoothed traditional ngram model in this approach. 
Totally, three smoothing methods are investigated. They are the back-off method, the linear interpolation method and the hybrid method. The formulas are listed as below. 
Back-off smoothing: 
11 1 11 ~ ~ 1 (|,)(,)0 (|,) (,)(|) iiii ii iii GTllll ll lll PwwtifCwwt Pwwt wtPwwotherwise a -- - -- > ì ï = í ï î (13) 
α 1 is the coefficient for normalization, and it can be calculated as below: 
11 1 11 11 11 1 ~~ :(,)0:(,)0 (,)(,) (,) (|)1(|) ii i iiii llllll iiiiii ll l llll wCwwtwCwwt wtwt wt PwwPww bb a -- - -- -- => == - åå (14) and 11 1 1 :(,)0 (,)1(|,) iii lll iii lGTll wCwwt wtPwwt b -- - > =- å (15) 
In formula (13), 
1 ~ (|) ii ll Pww 
is the traditional bigram probability smoothed by the back-off method, and it is calculated as below: 
11 1 1 ~ ~ 2 (|)()0 (|) ()() iiii ii ii GTllll ll ll PwwifCww Pww wPwotherwise a -- - - > ì ï = í ï î (16) 
α 2 is the coefficient for normalization, and it can be computed as below: (
)() () ()1() ii i ii llllll iiiiii ll l ll wCwwwCww ww w PwPw bb a -- - -- => == - åå (17) and 11 1 2 :()0 ()1(|) iii lll iii lGTll wCww wPww b -- - > =- å (18) 
Linear interpolation smoothing: 
111 ~~ (|,)()(|,)(1())(|) iiiiii llllll PwwttPwwttPww ll --- =´+-´ (19) 1 ~ (|) ii ll Pww 
is the traditional bigram probability smoothed by the linear interpolation method, and it is calculated by formula (20): 
11 ~ (|)(|)(1)() iiiii lllll PwwPwwPw qq -- =´+-´ (20) 
The coefficients of l(t) and q can be optimized by the EM algorithm on the held-out corpus. 
Hybrid smoothing: 
111 ^~~ (|,)()(|,)(1())(|) iiiiii llllll PwwttPwwttPww ll --- =´+-´ (21) 1 ~ (|,) ii ll Pwwt 
is the NS bigram probability smoothed by the back-off method, and it can be 
The Third Approach
The above sections provide two smoothing approaches for the NS ngram model. They are mainly based on the traditional smoothing techniques. This section proposes a novel smoothing method and constructs a more compact model to solve the data sparseness problem of the NS ngram model. As shown in 
The first step is to solve the data sparseness problem which is brought forth by modeling the word positional information. Some statistical variables are constructed to substitute for the concrete positional information. A more compact model is built up. The second step is to solve the data sparseness problem which is inherited from the traditional ngram model. The traditional smoothing techniques are utilized. After describing the motivation and the technique sketch, the formula is presented as below: 
Jinghui Xiao et al. 
2 11 1 () ~~ ((())) 1 (|,)(|) () l i l i iiii i Vw tEw llll l pwwtepww Zw a b -- - ´ -+ =´( =´(22) where l t 
is the positional variable. 
l 1 ~ (|) ii ll pww -is 
the smoothed traditional bigram probability. Any smoothing algorithm, such as the back-off algorithm and the linear interpolation algorithm, can be applied. 
l 1 () i l Zw -is 
the factor for normalization and it is defined as below: 
2 11 () ~ ((())) 1 ()(|) l i i l i iii i Vw ll tEw lll l Zwepww a b -- ´ = -+ = =´å=´å (23) l 
l is the size of the lexicon 
To smooth the word positional information, the paper aims at reducing the parameter number of the NS ngram model. Different from the clustering technique in the class-based ngram model , the paper constructs the statistical variables of the word positional information to substitute for the concrete value of t in the NS ngram model. Two statistical variables are calculated: the expectation and the variance. The weight is computed for the bigram probability according to these variables. Such an assumption is made that more weight should be awarded if the current word position fits in better with the training corpus, and less weight vice versa. According to the assumption, the term of t-E(w li ), which defines the difference between the current word position and its average position in the training corpus, is adopted in formula (22). As the value decreases, t fits in with the training corpus better and more weight should be awarded. Henceforth, the weight function is descendent with the value of t-E(w li ) as formula (22) shows. Moreover, the weight function is ascendant with the should be noticed that the way to constructing the weight is a purely empirical method. There is no theoretic foundation on it. However, it performs pretty well in the experiments, as presented later in section 5.4.3. In the second step, the traditional smoothing techniques can be adopted to solve the data sparseness problem which inherits from the traditional ngram model. 
The paper investigates two smoothing techniques: the back-off smoothing and the linear interpolation smoothing. Moreover, the coefficients of α and β can be optimized by some automatic methods on the held-out corpus. The genetic algorithm is adopted in this paper. It is presented as below: Step 2: selection 
Step 3: crossover 
Step 4: mutation 
Step 5: if termination criterion is met The actual performance of formula (22) on the held-out corpus is taken as the fitness function in the above algorithm. Until now, a compact NS ngram model has been built up in the section. The parameter space is reduced by substituting the statistical variables for the concrete positional information, which results in a space complexity of O(l n +2l+2). The data sparseness problem is alleviated. However, the predictive capability is also lowered to some extent due to the small parameter space, which is the limitation of this smoothing approach. To overcome the above drawback, the paper constructs the statistical variables for the word ngram other than for the word itself. It results in a larger space complexity of O(3´l n ), and therefore yields a more powerful predictive capability. In addition, the compact model has a slight higher time complexity than the normal NS ngram model by calculation of the weight function. 
Experiments and Discussions
This section evaluates the NS ngram model and its smoothing techniques on the pinyin-to-character conversion task which is the core technique of the Chinese keyboard input method. The section is organized as follows. Firstly, the task and the data set are described. Secondly, the non-stationary property of words is investigated in a statistical way so as to verify the motivation of the paper. Thirdly, the performance of the NS ngram model is presented and compared with the traditional ngram model. Finally, the smoothing algorithms proposed in the paper are evaluated and the performances of the smoothed NS ngram model are provided. 
Task and Data Set Description Task Description
The standard keyboard is initially designed for native English speakers. In Asia, such as China, 
Japan and Thailand, people can not input their language through the standard keyboard directly. Asian text input becomes the challenge for the computer users in Asia. Asian language input method is one of the most important techniques in Asian language processing. 
The pinyin-based input method is the most important Chinese text input method. There are over 97% of Chinese computer users using pinyin to input Chinese text 
The pinyin-to-character conversion task can also be taken as a simplified automatically speech recognition task 
Text Corpus 
The The paper chooses the large scale of corpus for the NS trigram model since its parameter space is much larger than that of the NS bigram model. In what follows, the paper presents the distributions of the lengths of the sentences in those corpora. The information is crucial to evaluating the NS ngram model which exploits the positional information of word in the sentences. The distributions are presented in 
Figure 3. Distributions of the sentence length in text corpus 
According to 
Pinyin Corpus 
The pinyin corpus is necessary for evaluating the NS ngram models on the pinyin-to-character conversion task. The paper gets the pinyin corpus from the above text corpus by a conversion toolkit 1 which yields 99.7% accuracy evaluated on a golden corpus. When the NS ngram models are evaluated, the pinyin corpus is firstly converted into the text corpus by the NS ngram model. Then, the converted results are compared with the standard text corpus and the error rate is calculated. As the pinyin corpus is not a golden corpus, the errors in the pinyin corpus could lead to the conversion error of the NS ngram model. Therefore, the actual error rate of the NS ngram model is a little lower than the reported results in the paper and the NS ngram model could get a little better performance in the real system. However, since there are not many errors in the pinyin corpus because of the high precision of the conversion toolkit, the reported error rate of the NS ngram model can be regarded to be close enough to the actual error rate. 
Non-Stationary Property of Words
Section 1 has provided some intuitive examples for the non-stationary property (NS property) of words. However, the intuition is not enough for our motivation of the paper. The section will further present some statistical evidences. The NS property assumes that word behaves differently in different portions of sentences. 
Then their probability distributions would be different in different portions. The more differences between these distributions, the more positional information has been implied by word. The section investigates the probability distributions in the NS bigram model, and presents their differences by comparing them with the distribution in the traditional bigram model. The Kullback-Leibler (KL) distance [
Table 2. The KL distances between the traditional bigram model and the NS bigram model 
Experiments of NS Ngram Model
This section evaluates the un-smoothed NS ngram model on the pinyin-to-character conversion task. Two sets of experiments, the close test and the open test, are carried out. The test on the training corpus is referred to as the close test; and the test on the testing corpus is referred to as the open test. In order to avoid the zero-probability problem in the open test, the paper adds a small value 2 to the zero-frequency words when estimating their probabilities. 
The un-smoothed traditional ngram model is taken as the baseline model. Both the NS bigram model and the NS trigram model are investigated. The experimental results of the NS bigram model are firstly presented in 
Table 3. Experimental results of the NS bigram model 
Bin Moreover, as the value of k increases, the error rate of the NS bigram model in the close test is reduced constantly, proving that the improvement of the NS ngram model is due to the increasing positional information of word. However, in the open test, the error rate stops decreasing after k=2, because the data sparseness problem becomes more severe as k 
increases. 
The NS trigram model is also investigated. The experimental results are presented in 
To sum up, the NS ngram model achieves great improvements by exploiting the word positional information; however, it suffers from severe data sparseness problem. The following sections will investigate the smoothing techniques presented in section 4, and provide the experimental results of the smoothed NS ngram model. Without loss of the generality, all the following experiments are carried out on the NS bigram model. 
Experiments of Smoothing Techniques
This section firstly investigates the three smoothing approaches separately. Then, these techniques are compared to each other and some conclusions are drawn. Finally, it investigates the performance of each probability distribution of the smoothed NS bigram model so as to gain further insight. All the experiments are carried out in the open test since the data sparseness problem occurs only on the unseen data. 
The First Approach
This approach smoothes the probability distributions in the NS bigram model by the traditional smoothing techniques. Totally three smoothing algorithms are investigated: the additive smoothing, the back-off smoothing and the linear interpolation smoothing. The techniques have been well presented in section 4.1. The un-smoothed NS bigram model is taken as the baseline model from which the error rate reduction is calculated. The experimental results are provided in 
Table 5. Experimental results of the first smoothing approach 
Bin 
The Second Approach
In the second approach, the paper smoothes the NS bigram model by the traditional bigram model. Three smoothing algorithms are provided. They are the back-off method, the linear interpolation method and the hybrid method, as described in section 4.2. The experimental results are presented in 
The Third Approach
The third approach smoothes the NS bigram model by reducing its parameter space and building up a more compact model. The statistical variables are utilized to substitute for the concrete positional information. A weight is calculated from these variables for the traditional bigram probability. The traditional smoothing techniques are utilized to smooth the bigram probability. Two smoothing techniques are investigated in the section: the back-off smoothing and the linear interpolation smoothing. The coefficients of α and β are optimized by the genetic algorithm on the held-out corpus. The settings of the genetic algorithm are presented in Firstly, according to the experimental results, this approach can smooth the NS bigram model effectively. It achieves as much as 29.78% error rate reduction which is slightly lower than the second approach's (31.8%), whereas much higher than the first one 's (15.77%). This smoothing approach can not achieve the best performance because the compact model has a smaller parameter space and its predictive capability is lower than that of the NS bigram model. Secondly, the error rate of the smoothed NS bigram model decreases along with the k value constantly. It proves that the approach can really solve the data sparseness problem of the NS bigram model, just as the second approach does. Finally, the performance of the smoothed NS bigram model becomes stably after k=2, which indicates that a small number of bins are enough to estimate the statistical variables and get the performance improvements. 
Comparisons
This section compares the performances of the three smoothing approaches with each other. In each approach, it presents the smoothing algorithm which yields the best experimental results. The smoothed traditional bigram model is also presented for comparison. The results are summarized in 
Figure 4. Comparison of the three smoothing approaches 
Performance of Each Distribution in NS Bigram Model
In section 5.2, it has presented the NS property of words by investigating the probability distributions in the NS bigram model. In order to gain more insight, this section presents the performance of each probability distribution in the NS bigram model and evaluates their contributions to the ultimate performance of the NS bigram model. Generally speaking, it can not tell exactly which probability distribution in the NS bigram model leads to a certain error in the pinyin-to-character conversion process. An approximate method is then provided. The section simply divides each sentence of the test corpus into several bins according to the method in section 3.3, and then calculates the error rate in each bin separately. Each error rate corresponds to the performance of a particular probability distribution in the NS bigram model. All the following experiments are carried out in the open test. The hybrid algorithm in the second approach is utilized to smooth the NS bigram model. It yields the best experimental results in the above sections. The NS bigram model is built up on various values of k which are up to 8. The experimental results are summarized in 
Table 9. Performance of each probability distribution in the NS bigram model 
Bin Focusing on a certain column in 
Their positional information is much richer than words '. Therefore, the predictive capability of the probability distribution at the end position is much more powerful than other distributions in the NS bigram model, and it yields much higher performance. 
Conclusions
This paper enhances the traditional ngram model by relaxing the stationary hypothesis and exploring the word positional information. The non-stationary ngram model is proposed. Several related issues are discussed in detail, including the definition of the NS ngram model, the representation of the word positional information and the estimation of the conditional probability. In addition, three smoothing approaches are proposed to solve the data sparseness problem of the NS ngram model. Several smoothing algorithms are presented in each approach. In the experiments, the NS ngram model and its smoothing techniques are evaluated on the pinyin-to-character conversion task which is the core technique of Chinese text input method. According to the experimental results, several conclusions are drawn as follows: 
1. The NS ngram model outperforms the traditional ngram model significantly by the exploitation of the word positional information; however, it suffers from severe data sparseness problem. 
2. The traditional smoothing techniques are effective in smoothing the NS ngram model; however, they can only alleviate the data sparseness problem without solving it completely. 
3. The traditional ngram model is utilized to smooth the NS ngram model. Combined with the traditional smoothing techniques, this smoothing approach can solve the data sparseness problem completely and achieve the best experimental results. 4. The third smoothing approach can also solve the data sparseness problem of the NS ngram model, and it yields a comparable experimental result to the second approach at the cost of a smaller parameter space. 5. Among the probability distributions in the NS ngram model, the distributions in the marginal positions have more predictive capability than the middle ones, and therefore contribute more to the ultimate performance of the NS ngram model. 
"
"Introduction
Automatic speech-to-speech translation is a prospective application of speech and language technology [See JANUS III 
With the rising importance of parallel texts (bitexts) in language translation, an approach called translation spotting has been applied for proposing appropriate translations, referring to the TransSearch system 
n ssq L 1 = . 
Ÿ The output is a pair of sets of translation patterns 
)(),( TrSr qq 
: the SL answer and TL answer, respectively. the TL answer may possibly be empty (example 4) when there is no satisfactory way of linking TL patterns to the input. By varying the identification criteria, the translation spotting method can help evaluate units over various dimensions, such as frequency ranges, parts of speech and even speech features of spoken language. 
1. q : 待 幾 天 )( Sr q ={待,幾,天} )( Tr q ={doax,kuie,jit} 
我 明天 要 訂 兩 間 有 淋浴 設備 的 單人房 minafzaix goar bueq dexng lerng kefng u sea sengqw e danjiin paang 2. 
q : 我 要 訂 兩 間 單人 房 )( Sr q ={我,要,訂,兩,間,單 人房} )( Tr q ={goar
,bueq,dexng,lerng,kefng,danjiin paang} 請 問 你們 今晚 有 一 間 雙人房 嗎 chviar bun lirn ehngf u cit kefng sianglaang paang but 3. 
q : 今晚 有 […] 雙人房 嗎 )( Sr q ={今晚,有,雙人房, 嗎} )( Tr q ={ehngf,u
,sianglaang,paang,but} 有 包括 早餐 在 內? u zafdngx but 4. 
q : 包括 … 在 內 )( Sr q ={包括,在,內} )( Tr q ={φ } 
Multiple-Translation Spotting for Mandarin-Taiwanese Speech-to-Speech Translation 15 
However, translation spotting can only draw out the TL answer from the best translation; it can not handle an SL query whose word-tokens are distributed in different translations. Consequently, we propose conducting multiple-translation spotting of a speech input using multiple pairs of translation patterns. 
Figure 1. An example of multiple-translation spotting. 
Framework of the Proposed System
The proposed speech-to-speech translation system is divided into two phases – a training phase and a translation phase. In the training phase, the developed translation examples are imported to derive multiple-translation templates and develop speech data. In the following step, the developed speech data are applied to construct multiple-translation spotting models and synthesis templates. 
Data Training Phase
As for the task of translating Mandarin and Taiwanese language pairs, although these languages both belong to the family of Chinese languages, their language usages still have various development by language families and their origins, Mandarin belongs to Altaic 
Multiple-Translation Spotting for Mandarin-Taiwanese Speech-to-Speech Translation 17 
language family, and Taiwanese belongs to Sinitic language family 
Multiple Translation Template Construction
While translation templates can be fully constructed, one major issue in translation pattern exploitation, called "" divergence, "" makes straightforward transfer mapping extraction impractical. Dorr (1993) describes divergence in the following way: "" translation divergence arises when the natural translation of one language into another result in a very different form than that of the original. "" Therefore, we choose translations with no divergence to The translation template is composed of a translated example, an intention translation, and two variable translations. The example shows how a sentence in Mandarin (SL) that contains an intention "" 要 訂 "" with two variables, M p1 (我 朋友) and M p2 (房間), can be translated into a sentence in Taiwanese (TL) with an intention "" bueq dexng "" and two variables, T p1 (goarn pengiuo) and T p2 (pangkefng). According to the template, the number of variable translations should be expanded to improve the capability for spotting the speech input. From the preceding example, variable translation expansion can be illustrated as follows: 
Variable Translation Expansion: 
If M p1 ↔ T p1 , 我 ↔ goar 我 朋友↔ goarn pengiuo If M p2 ↔ T p2 , 房間 ↔ pangkefng 票 ↔ phiaux alignments 
Therefore, we can obtain corpus-specific multiple translations in a template constructed from three translation patterns, which are "" 我 朋 友 要 訂 房間↔goarn pengiuo bueq dexng pangkefng "" , "" 我↔goar "" , and "" 票↔phiaux "" . 
Spotting Model Construction
Taiwanese is a typical oral language and still has no uniform system of writing. In the literature, there are two ways to represent Taiwanese words: Chinese characters and alphabetic writing. 
Therefore, for the purpose of practical system construction, a collection of speech data is developed from derived text-form templates not only to obtain spotting models but also to transcribe text data as waveform-based representations. For one of the translating languages, the speech data, including intention speech and related variable speech, are used in chorus to construct spotting reference models for use in multiple-translation spotting. Such spotting reference models are embedded with latent grammars from the constructed templates. When dealing with Mandarin-Taiwanese speech feature models, we build the database by extracting LPCC features from recorded template speeches. Hence, when speech recognition is performed, the LPCC features are extracted from the recorded template speeches, and the LPCC features of speech input are used in combination to compute the degree of dissimilarity. 
After language pairs of both Taiwanese and Mandarin speech data are developed, the transfer mapping information for a pair of Taiwanese and Mandarin speech segments known to be similar in terms of text-form word alignment is constructed. 
Synthesis Template Construction
Both Mandarin and Taiwanese are tonal languages, and it is difficult to determine whether a morpheme will take its inherent tone or the derived tone when every word in a sentence is synthesized. 
Translation Phase
Multiple-Translation Spotting Method
To deal with the problem of spotting between a speech input 
L X 1 and a translation pattern set { } J j v j v j ts 1 )()( , = in the v-th translation template ( v r 
), we use the standard notation l to represent the frame index of 
L X 1 , Ll ≤ ≤ 1 
, j to represent the spotting pair ( 
)()( , v j v j ts ) index of v r , Jj ≤ ≤ 1 
, and k to represent the frame index of j-th spotting pattern 
)( v j s , 1 j Kk ≤ ≤ . Then 
for each input frame, the accumulated distance 
),,( jkld A is computed by ( ) ),,1(min),,(),,( 2 jmldjkldjkld A kmk A −+= ≤≤− . (1) For j Kk ≤≤ 2 , Jj ≤ ≤ 1 , where ),,( jkld 
is the local distance between the l-th frame of ) of each source pattern. At the speech pattern boundary, i.e., 1 = k , the recursion can be calculated as follows: 
( ) [ ] ),1,1(,),,1(minmin),1,(),1,( 1 jldmKldjldjld AmA Jm A − − + = ≤≤ . (2) 
The final solution for the best path is 
( ) [ ] jKLdd jA Jj v G ,,min 1 )( ≤≤ = (3) 
The details of the multiple-translation spotting algorithm are given below: 
/* Parameter descriptions { } J j v j 1 )( = τ : the spotting results of { } J j (v) j s 1 = , where    = otherwise. ,0 .by spotted is pattern speech SL if ,1 1 )( )( Lv j v j Xs τ ; { } Jjtw v j v jv ≤≤=← 1 ,1| )()( τ : the hypothesized TL synthesis patterns; */ /* Initialization */ 1 ← ← ← jkl ; 0 )( ← v j τ , Jj ≤ ≤ 1 ; { } φ ← v w ; /* v-th template spotting */ while ( Ll ≤ ) 
for each spotting pattern )( end while 
v j s while ( j Kk ≤ ) if (k = 1) )],,1()],,,1([minmin[),,(),,( 1 jkldjKldjkldjkld AjA Jj A −−+← ≤≤ )],,1()],,,1([minmin[arg),,( 1 jkldmKldjklp AmA Jm −−← ≤≤ else if (k > 1) )),,1((min),,(),,( 2 jmldjkldjkld A kmk A −+← ≤≤− )),,1((minarg),,( 2 jmldjklp A kmk −← ≤≤− else if (k = K j ) 1 ← k ; 
)],,([min 1 )( jKLdd jA Jj v G ≤≤ ← ; )],,([minargˆ1 minargˆ minargˆ1 jKLdj jA Jj ≤≤ ← ; 
/* Trace back and TL synthesis pattern extraction */ 
{ } )] ˆ ,,[(back tracê tracê 1 )( jKL τ j J j v j ← = ; /*, )( v j 
τ is assigned as 1 or 0*/ 
for each )( v j τ , j=1,2,…,J If ( 1 )( = v j τ ) { } )( v jvv tww ∪← ; end if end for return v w and { } J j v j τ 1 )( = ; 
Normalizing the Score and Ranking
The length of the matching sequence can severely impact the cumulative dissimilarity measurement, so a length-conditioning weight is applied to overcome this defect. Scoring methods that involve the length measurement 
( ) )( 1 , vL sX ∆ ( U J j (v) j v ss 1 )( = = ) [J.N.K. Liu and L. 
Zhou, 
),(minor )(,max(),( )( 1 )( 1 )( 1 vLvLvL sXsXsX =∆ , (4) )( 1 )( 1 ),( vLvL sXsX * =∆ , (5) 
Multiple-Translation Spotting for Mandarin-Taiwanese Speech-to-Speech Translation 21 
3/),(),(),( 11 )( 1 ∑∑ == +=∆ J j j J j j vL KLFKLNsX , (6) 
where )( , 
{ } J j v j s 1 )( = ; ),( 1 ∑ = J j 
),( )( 1 v sX w vL sX ∂=∆ , (7) 
where The experimental analysis shown in 
Figure 3. Time-conditioned weight convergence for dissimilarity measurement 
Smoothing the Hypothesized Template
The main weakness with the one-stage algorithm for multiple-translation spotting is that it provides no mechanism for controlling the resulting sequence length, that is, for determining the optimal token sequence of arbitrary length. The algorithm finds a single best path whose sequence length is arbitrary. Therefore, the hypothesized token sequence generally includes noise-like components. The components should be in the form of duplications, and their durations should be below a threshold. Based on this assumption, hypothesized token outputs with segmented durations below the threshold are considered for further smoothing. With Mandarin and Taiwanese, the duration of a syllable is 0.3 sec on average 
Target Speech Generation
Once the hypothesized target sequences have been determined, the target speech generation process is straightforward, similar to the waveform segment concatenation-based synthesis method. In this method, waveform segments are extracted beforehand from the recorded intention synthesis units and variable synthesis units of the synthesis template, and they are rearranged with adequate overlapping portions to generate speech with the desired energy and duration. The merits of the method are the small computational cost in the synthesis process and the high level of intelligibility of the synthesized speech. The generation process includes complete matching, waveform replacement, and waveform deletion; thus, it is similar to the example-base translation method [J. Liu and L. 
Experimental Results
The Task and the Corpus
We built a collection of Mandarin sentences and their Taiwanese translations that usually appear in phrasebooks for foreign tourists. Because the translations were made sentence by sentence, the corpus was sentence-aligned at birth. 
Table 2. Basic characteristics of the collected translated examples. 
In this work, the content of the high divergent example sentence pairs needed to be collated or sieved out to improve the accuracy and effectiveness of alignment exploration between word sequences and the derivation of multiple translation templates. 
Translation Evaluations
For the speech translation system, we found that the recognition performance of 39-dimension MFCCs and 10-dimension LPCCs was close. Therefore, we adopted 10-dimension LPCCs due to their advantages of faster operation and simpler hardware design. Speech feature analysis of recognition was performed using 10 linear prediction coefficient cepstrums (LPCCs) on a 32ms frame that overlapped every 8ms. For estimating the computational load of the proposed MTS algorithm, a complexity analysis is shown in 
Table 4. Complexity analysis of the MTS algorithm. 
Computational Load Addition Multiplication 
Distance computation 
( ) addLPCCOKL J j v j _ 1 )( ⋅⋅ ∑ = ( ) mulLPCCOKL J j v j _ 1 )( ⋅⋅ ∑ = Path selection ∑ = ⋅⋅ J j v j KL 1 )( 5 ∑ = ⋅⋅ J j v j KL 1 )( 3 Total for each template ( ) ( ) addLPCCOKL J j v j _5 1 )( +⋅⋅ ∑ = ( ) ( ) mulLPCCOKL J j v j _3 1 )( +⋅⋅ ∑ = Total for all templates ( ) ( ) ∑∑         +⋅⋅ = v J j v j addLPCCOKL _5 1 )( ( ) ( ) ∑∑         +⋅⋅ = v J j v j mulLPCCOKL _3 1 )( 
When input speech is being spotted, a major sub-problem in speech processing is determining the presence or absence of a voice component in a given signal, especially the beginnings and endings of voice segments. Therefore, the energy-based approach, which is a classic one and works well under high SNR conditions, was applied to eliminate unvoiced components in this research. The measurement results were divided into four parts: the dissimilarity measurement of linear prediction coefficient cepstrum (LPCC)-based (baseline), the baseline with unvoiced elimination (unVE), the baseline with the time-conditioned weight (TcW), and the combination of unVE and TcW considerations with the baseline. A given translation template is called a match when it contained the same intention as the speech input. The reason for adopting this strategy was that variables could be confirmed again while a dialogue was being processed, while wrong intentions could cause endless iterations of dialogue. The experimental results for proper template spotting are shown in Based on the constructed translation templates, when the template or vocabulary size increases, more templates would possibly lead to more feature models and more similarities in 
Multiple-Translation Spotting for Mandarin-Taiwanese Speech-to-Speech Translation 25 
speech recognition, thus causing false recognition results and lower spotting accuracy. Additionally, multiple speaker dependent results were obtained using three speakers. The first speaker's feature models (spotting models) were used to perform tests on the other two speakers, and the results are shown in 
Table 5. Average accuracy of baseline spotting and the improvement in Mandarin-to-Taiwanese Translation. 
Template Size 
Top 1Top 5Top 1Top 5Top 1Top 5Top 1Top 5
Table 6. Average accuracy of baseline spotting and the improvement in Taiwanese-to-Mandarin Translation. 
Conclusion
In this work, we have proposed an approach that retrieves identified target speech segments by carrying out multiple-translation spotting on a source input. According to the retrieved speech segments, the target speech can be further generated by using the waveform segment concatenation-based synthesis method. Experiments using Mandarin and Taiwanese were performed on Pentium ® PCs. The experimental results reveal that our system can achieve an average translation understanding rate of about 78%. 
Multiple-Translation Spotting for Mandarin-Taiwanese Speech-to-Speech Translation 27 
"
"Introduction
Starting in 1995, the Defense Advanced Research Projects Agency of the United States (DARPA) directed its research program for continuous speech recognition to focus on automatic transcription of broadcast news 
We have established a webpage for the corpus. On this webpage, there are tools that users can employ to query the corpus. Though the project is finished, we will continue to correct errors reported by users. Also, we have selected five one-hour shows as a development set and five more one-hour shows as an evaluation set, and conducted speech recognition experiments on them. The rest of this paper is organized as follows: The data collection procedures and the details of transcription and annotation are presented in sections 2 and 3, respectively. Then, a preliminary assessment of the 198-hour Mandarin Chinese broadcast news corpus is given in section 4. The corpus webpage and corpus tools are introduced in section 5. Speech recognition evaluation results are discussed in section 6. Finally, conclusions are drawn in section 7. 
Data Collection
The Public Television Service Foundation (Taiwan) 2 kindly agreed to share their broadcast news with us. The recordings spanned the period November 7, 2001 through June 30, 2003. A Digital Audio Tape (DAT) recorder, which was connected to a broadcasting machine using an XLR balanced cable, was set up in the TV broadcasting studio. That is, the broadcast news speech was recorded at the same time it was broadcasted to avoid any modulation effect. Recordings are made in stereo with a 44.1kHz sampling rate and 16 bit resolution. Each recording consists of a broadcast news episode 60 minutes long. Each DAT was manually processed to convert the digital speech samples into a single Microsoft Windows wave file and stored in a hard disk. Then, the signal was down-sampled to 16kHz with a resolution of 16 bits. During this operation, only the left channel was selected. Thus, broadcast news speech in mono, down-sampled to 16kHz with 16 bit resolution was used for further transcription and annotation. More than 250 one-hour broadcast news shows were recorded in this way. However, we were able to transcribe only 198 of them. 
Since video can provide visual clues to facilitate transcription and annotation, video recordings were also made simultaneously with the audio recordings. The recordings were made on VHS video tapes. Because we did not have enough space to store several hundred video tapes, each recording was first converted into an MPEG1 file and then stored on a CD-ROM. After conversion was completed, the video tape was reused again. With the video recording, the broadcast news speech corpus can be expanded into a video broadcast news corpus, though at this stage, we are only focusing on the audio track. 
Transcription and Annotation
The corpus has been segmented, labeled, and transcribed manually using a tool developed by DGA (Délégation Générale pour l'Armement, France) and LDC, called "" Transcriber "" 
The studio anchor speech always exhibits a high standard of fluency, good pronunciation, and good acoustic quality. Most of the field reporter speech also exhibits a high standard of fluency and good pronunciation, but sometimes the acoustic quality is low. Some of the interviewee speech is of very low quality and intelligibility with background speech and noises of various types, and the speech itself sometimes contains mispronunciations, particles, repetitions, repairs, etc. As a result, much more time was required to transcribe and annotate the interviewee speech. The segments containing dialects or foreign languages were annotated with the language identity and time stamps without orthographic transcripts. 
SGML structure of transcriptions
Owing to the complexity and hierarchical nature of the additional information needed in the transcripts, SGML was chosen by the DGA&LDC Transcriber as a suitable framework for formatting the text. The document structure used in all the transcripts is as follows 
Within each Section containing material to be transcribed, there are one or more ""Segment"" elements, corresponding to speaker turns within the Section; the Segment attributes identify the speaker, the speaking mode, the channel fidelity, and the points in time at which the speaker turn begins and ends. At any point within an Episode, a Section or a Segment where there is a change in the presence of music, background voices, or other noises, a ""Background"" element is inserted to mark the change; the Background attributes identify the type of background (music, speech, other, and shh) and the point in time at which the change occurs. 
Automatic generation of initial transcriptions
After we examined the anchor scripts on the website of the TV broadcaster, we found that they matched the content of the studio anchor speech rather well. This observation led us to design an automatic tool to generate initial transcription files for our transcribers to start with. The flowchart of the tool for generating initial transcriptions is depicted in 
The story segmentation module first performs speaker and environment change detection, followed by hierarchical clustering of audio segments. We assume that the largest cluster is the studio anchor cluster, and that every studio anchor speech segment is the first segment of a story. Thus, the number of studio anchor speech segments corresponds to the number of stories in the audio stream, and the starting time of a story is the starting time of its studio anchor speech segment. The speech recognition module transcribes all the studio anchor speech segments, and the anchor script alignment module aligns the anchor scripts with the recognition output. Here, we use a vector-space-model-based information retrieval approach for alignment. For each studio anchor speech segment, the recognition output is converted into feature vectors, where the weight of an indexing term (which can be a syllable, a character, a word, or an overlapping N-gram combination) is represented as tf×idf, the term frequency multiplied by the inverse document frequency. Each anchor script is also represented by feature vectors in a similar way. The Cosine measure is used to estimate the relevance between the recognition output and the anchor script. The anchor script with the largest relevance value is aligned with the studio anchor speech segment. Details about the story segmentation approach and the information retrieval approach to story alignment can be found in 
Corpus Assessment
A brief description
Each one-hour news show usually has two or three parts separated by advertisements. Sometimes, however, there is no advertising at all within a show. Each part starts with headlines with background music, followed by a number of stories. Because the news shows were collected from a non-profit public TV station, the advertising was composed of public service announcements and previews rather than commercials. Generally, a one-hour news show contains one to three headline sections, zero to two advertising sections, depending on the number of headline sections, a number of news stories, a weather forecast section, and an ending section. 
Preliminary statistical analysis
Some preliminary assessments of the 198-hour Mandarin Chinese broadcast news corpus have been conducted. There are seven distinct studio anchors; three are male, and four are female. The distribution of studio anchors is summarized in 
Figure 2. A partial transcription of a broadcast news show 
identified reporters. Therefore, the exact number of distinct field reporters may be much lower than 386 but slightly higher than 100. 
(2) There are around 5,900 distinct interviewees. Of these, around 4,000 interviewees are male. It is interesting to find that, unlike the situation with the field reporters, the percentage for male speakers is relatively high. Moreover, even though the identities of some interviewees are also unknown, it is very likely that the unknown interviewees in different stories are in fact different people. 
(3) The total lengths of the studio anchor speech, field reporter speech, and interviewee speech are around 27 hours, 71 hours, and 45 hours, respectively. The numbers of characters transcribed from them are around 493,000, 1,167,000, and 616,000, respectively. The speaking rates are 4.9, 5.1, and 4.5 syllables per second, respectively. Some segments contain pure music or noise. In addition, some speech segments contain foreign languages, dialects, or aboriginal languages. If these segments are omitted, then the total lengths of the studio anchor speech, field reporter speech, and interviewee speech are around 25 hours, 58 hours, and 35 hours, respectively. Some speech segments contain overlapping speech. 
(4) The frequency counts of the most frequently used tags in the corpus are shown in 
In addition to the 4,100 news stories, there are 581 headline sections (~5.5 hours), 652 advertising sections (~23.5 hours), 197 weather forecast sections (~10.3 hours), and 197 ending sections (~0.8 hours). All the headline sections and ending sections were carefully transcribed. The weather forecasts in 10 shows were also carefully transcribed, but the remaining 187 weather forecasts and all the advertising sections were simply annotated with time stamps without orthographic transcripts. The transcripts contain around 2.3 million Chinese characters in total. 
Table 2. The frequency counts of the most frequently used tags in the corpus. Min-Nan is a common dialect and Formosan denotes all the aboriginal languages used in the Taiwan area 
Studio 
Corpus Webpage and Tool
Though the project is finished, we will continue to correct errors reported by users and post the most up-to-date versions of transcriptions on the corpus webpage, http://sovideo.iis.sinica.edu.tw/SLG/corpus/MATBN-corpus.htm. Currently, on this webpage, two tools are available for querying the corpus. 
Speech Recognition Evaluation
The development and evaluation sets
Ten one-hour shows were selected from the 198-hour carefully transcribed broadcast news database to evaluate the speech recognition performance. That is, we spot-checked about 5% (10/198*100%=5.05%) of the database. We divided the shows into a development set and an evaluation set. The development set consisted of five 
The NTNU broadcast news transcription system
The NTNU broadcast news transcription system was employed here for speech recognition evaluation. Some features of the speech recognizer are reviewed below. 
Front-end signal processing
In the speech recognizer, spectral analysis is applied to a 20 ms frame of speech waveform every 10 ms. For each speech frame, 12 mel-frequency cepstral coefficients (MFCCs) and the logarithmic energy are extracted, and these coefficients and their first and second time derivatives are spliced together to form a 39-dimensional feature vector. In addition, to compensate for channel noise, utterance-based cepstral mean subtraction (CMS) is applied to the training and testing speech. 
Acoustic modeling
The acoustic models were trained with a database of 16 hours of broadcast news speech collected from several radio stations located in Taipei. The broadcast news data were recorded using a wizard FM radio connected to a PC and digitized at a sampling rate of 16 kHz with 16 bit resolution. The data collection period was from December 1998 to July 1999. The training database is a combination of two corpora: The first corpus contains two hours of field reporter and interviewee speech, and four hours of studio anchor speech. The manual transcripts have been time-aligned to the phrasal level. The second corpus contains ten hours of studio anchor speech. Each audio file is a short news abstract (lasting 50 seconds on average) produced by a studio anchor. Unlike the first corpus, only the orthographic transcripts were available for each audio file; detailed time alignment was unavailable. Due to the monosyllabic structure of the Chinese language, where each syllable can be decomposed into an INITIAL and a FINAL, the acoustic units used in the speech recognizer are intra-syllable right-context-dependent INITIAL/FINALs, including 112 context-dependent INITIALs and 38 context-independent FINALs. Each INITIAL or FINAL is represented by a Continuous Density Hidden Markov Model (CDHMM) with two to four states. The Gaussian mixture number per state ranges from 1 to 64, depending on the amount of corresponding training data available. In addition, the silence model is a 1-state CDHMM with 128 Gaussian mixtures trained with the non-speech segments. A total of 12,419 mixtures were obtained. 
Lexicon and language modeling
The lexicon contains 71,694 words, including 66,290 words selected manually from the CKIP lexicon and 5,404 new words or compound words extracted automatically from the language model training corpus. The word-based unigram, bigram, and trigram language models were trained using the newswire text corpus, consisting of 170 million Chinese characters collected from Central News Agency (CNA) in 2001 and 2002 (the Chinese Gigaword Corpus released by LDC). The language models were further processed with Katz backoff smoothing 
The speech recognizer
The speech recognizer was implemented with a left-to-right frame-synchronous tree search as well as a lexical prefix tree organization of the lexicon. At each speech frame, the so-called word-conditioned method was used to group path hypotheses that shared the same history of predecessor words into the same copies of the lexical tree, and to expand and recombine them according to the tree structure until a possible word ending was reached. At word boundaries, the path hypotheses among the tree copies that had the same search history were recombined and then propagated to the existing tree copies or used to start new ones if none yet existed. At each speech frame, a beam pruning technique, which considered the decoding scores of path hypotheses together with their unigram language model look-ahead and syllable-level acoustic look-ahead scores , was used to select the most promising path hypotheses. Moreover, if the word hypotheses ending at each speech frame had scores that were higher than a predefined threshold, then their associated decoding information, such as the word start and end frames, the identities of current and predecessor words, and the acoustic score, were kept in order to build a word graph for further language model rescoring. Once the word graph had been built, the Viterbi beam search with higher language model weighting 
Speech recognition experiments
We conducted speech recognition experiments on the development set and the evaluation set both separately and jointly. 
Discussion
Notice that, in a benchmark test, the model parameters optimally tuned for the development set are applied to the evaluation set directly. Further tuning conducted on the evaluation set is not allowed. In this study, we aimed to select development and evaluation data from the corpus so that users can experiment on it with the same setting. Therefore, we have only reported the baseline recognition accuracy of the development set and evaluation set. Furthermore, in this corpus, it is obvious that most of the anchor reporters and field reporters are female, while most of the interviewees are male. Therefore, it is difficult to design a benchmark test with a balanced number of female and male speakers using different types of speech. However, this may simply reflect the real situation in Taiwan. In that case, we do not need to worry about the gender issue very much. 
Concluding Remarks
Speech resources are crucially important for research and development in speech technology. But the development of a speech corpus used to be very tedious and time consuming. In August 2001, we started a 3-year project aimed at collecting a Mandarin Chinese broadcast news corpus in Taiwan. At the end of the project, we had labeled 198 one-hour news shows using the DGA&LDC Transcriber. In this paper, we have discussed the development and evaluation of the corpus. The speech corpus will soon be available through the Association for Computational Linguistics and Chinese Language Processing (ACLCLP). In the future, we will try to collect data from other TV broadcasters. We will also try to collect data from various programs. 
Appendix 
In the DGA&LDC Transcriber, the annotation tags are divided into 4 categories, namely, noise, pronounce, language, and lexical. We list them in 
Table A1. The tags used in the corpus (a) noise 
description example breathe, clear throat, click, cough, cry, hiccup, laugh, noise, pause, sigh, silence, smack, sneeze, sniffle, swallow, yawn, etc. 我從來沒想過 <Event desc=""laugh"" type=""noise"" extent=""instantaneous""/> 會來參加這個勞工遊行， cough, cry, laugh, yawn, etc. 這個倒 <Event desc=""laugh"" type=""noise"" extent=""begin""/> 沒什麼 <Event desc=""laugh"" type=""noise"" extent=""end""/> 。 particle 所以各位 <Event desc=""particle"" type=""noise"" extent=""begin""/> NE <Event desc=""particle"" type=""noise"" extent=""end""/> 在我們各自的工作崗位上， unrecognizable non-speech sound 我揣摩 <Event desc=""unrecognizable non-speech sound"" type=""noise"" extent=""begin""/> ... <Event desc=""unrecognizable non-speech sound"" type=""noise"" extent=""end""/> 笛子的 
(b) pronounce 
description example alternative 也就是待 <Event desc=""alternative dai1"" type=""pronounce"" extent=""previous""/> 在家裡 mispronunciation 所以根據不同可能發 <Event desc=""mispronunciation hua1"" type=""pronounce"" extent=""previous""/> 生的狀況， stutter, syllable contraction, uncertain, unrecognizable speech sound, etc. 尤其是派系比較嚴重的 <Event desc=""syllable contraction"" type=""pronounce"" extent=""begin""/> 這些 <Event desc=""syllable contraction"" type=""pronounce"" extent=""end""/> 地方， zhuyin 就連 <Event desc=""zhuyin"" type=""pronounce"" extent=""begin""/> ㄅㄆㄇㄈ <Event desc=""zhuyin"" type=""pronounce"" extent=""end""/> 他也是看不懂。 
(c) lexical 
description example abridged, cut, editing term, error, interrupted, discourse marker, new word, repair, repetition, restart, etc. 因為肌瘤切除術我們曉得 <Event desc=""repetition"" type=""lexical"" extent=""begin""/> 它的它的 <Event desc=""repetition"" type=""lexical"" extent=""end""/> 一些缺點 
(d) language 
description example English, Formosan, Hakka, Japanese, Min-Nan, Unknown, etc. 經濟部長林信義今天已經率領 <Event desc=""English"" type=""language"" extent=""begin""/> WTO <Event desc=""English"" type=""language"" extent=""end""/> 代表團出發前往卡達。 
"
"© The Association for Computational Linguistics and Chinese Language Processing 
Automatic Pronominal Anaphora Resolution in English Texts 
Tyne Liang * and Dian-Song Wu * 
Abstract 
Anaphora is a common phenomenon in discourses as well as an important research issue in the applications of natural language processing. In this paper, anaphora resolution is achieved by employing WordNet ontology and heuristic rules. The proposed system identifies both intra-sentential and inter-sentential antecedents of anaphors. Information about animacy is obtained by analyzing the hierarchical relations of nouns and verbs in the surrounding context. The identification of animacy entities and pleonastic-it usage in English discourses are employed to promote resolution accuracy. Traditionally, anaphora resolution systems have relied on syntactic, semantic or pragmatic clues to identify the antecedent of an anaphor. Our proposed method makes use of WordNet ontology to identify animate entities as well as essential gender information. In the animacy agreement module, the property is identified by the hypernym relation between entities and their unique beginners defined in WordNet. In addition, the verb of the entity is also an important clue used to reduce the uncertainty. An experiment was conducted using a balanced corpus to resolve the pronominal anaphora phenomenon. The methods proposed in 
Introduction
Problem description
Anaphora resolution is vital in applications such as machine translation, summarization, question-answering systems and so on. In machine translation, anaphora must be resolved in the case of languages that mark the gender of pronouns. One main drawback with most current machine translation systems is that the translation produced usually does not go beyond the sentence level and, thus, does not successfully deal with discourse understanding. Inter-sentential anaphora resolution would, thus, be of great assistance in the development of machine translation systems. On the other hand, many automatic text summarization systems apply a scoring mechanism to identify the most salient sentences. However, the task results are not always guaranteed to be coherent with each other. This could lead to errors if a selected sentence contained anaphoric expressions. To improve accuracy in extracting important sentences, it is essential to solve the problem of anaphoric references beforehand. Pronominal anaphora, where pronouns are substituted by previously mentioned entities, is a common phenomenon. This type of anaphora can be further divided into four subclasses, namely: nominative: {he, she, it, they}; reflexive: {himself, herself, itself, themselves}; possessive: {his, her, its, their}; objective: {him, her, it, them}. However, "" it "" can also be a non-anaphoric expression which does not refer to any previously mentioned item, in which case it is called an expletive or the pleonastic-it [
Tyne Liang and Dian-Song Wu 
a new, advanced and completely revamped version of his own knowledge-poor approach to pronoun resolution. In contrast to most anaphora resolution approaches, the system called MARS operates in the fully automatic mode. Three new indicators included in MARS are Boost Pronoun, Syntactic Parallelism and Frequent Candidates. In 
System Architecture
Proposed System Overview
( ) ∏ ∑ ∑ × ⎟ ⎟ ⎠ ⎞ ⎜ ⎜ ⎝ ⎛ − = k k j j i i agreement con rule pre rule ana can score , _ _ , 
(1) 
where can: each candidate noun phrase for the specified anaphor; ana: anaphor to be resolved; rule_pre i : the ith preference rule; rule_con i : the ith constraint rule; agreement k : denotes number agreement, gender agreement and animacy agreement. 
Main Components
POS Tagging
The TOSCA-ICLE tagger 
NP Finder
According to the part-of-speech result, the basic noun phrase patterns are found to be as follows: 
base NP → modifier＋head noun modifier → <article| number| present participle| past participle |adjective| noun> At the beginning, our system identifies base noun phrases that contain no other smaller noun phrases within them. For example, the chief executive officer of a financial company is divided into the chief executive officer and a financial company for the convenience of judging whether the noun phrase is a prepositional noun phrase or not. This could be of help in selecting a correct candidate for a specific anaphor. Once the final candidate is selected, the entire modifier is combined together again. 
The proposed base noun phrase finder is implemented based on a finite state machine (
Pleonastic-it Module
The pleonastic-it module is used to filter out those semantic empty usage conditions which are essential for pronominal anaphora resolution. A word "" it "" is said to be pleonastic when it is used in a discourse where the word does not refer to any antecedent. References of "" pleonastic-it "" can be classified as state references or passive references 
Passive references consist of modal adjectives and cognitive verbs. Modal adjectives (Modaladj) like advisable, convenient, desirable, difficult, easy, economical, certain, etc. are specified. The set of modal adjectives is extended by adding their comparative and superlative forms. Cognitive verbs (Cogv), on the other hand, are words like anticipate, assume, believe, expect, know, recommend, think, etc. 
Number Agreement
The quantity of a countable noun can be singular (one entity) or plural (numerous entities). It makes the process of deciding on candidates easier since they must be consistent in number. With the output of the specific tagger, all the noun phrases and pronouns are annotated with number (single or plural). For a specified pronoun, we can discard those noun phrases that differ in number from the pronoun. 
Gender Agreement
The gender recognition process can deal with words that have gender features. To distinguish the gender information of a person, we use an English first name list collected from 
(http://www.behindthename.com/) covering 5,661 male first name entries and 5,087 female ones. In addition, we employ some useful clues from WordNet results by conducting keyword search around the query result. These keywords can be divided into two classes： Class_Female= {feminine, female, woman, women} Class_Male= {masculine, male, man, men} 
Animacy Agreement
Animacy denotes the living entities which can be referred to by some gender-marked pronouns (he, she, him, her, his, hers, himself, herself) in texts. Conventionally, animate entities include people and animals. Since it is hard to obtain the property of animacy with respect to a noun phrase by its surface morphology, we use WordNet 
_ _ _ _ _ _ _ _ _ _ _ = . (4) 
Besides the noun hypernym relation, unique beginners of verbs are also taken into consideration. These lexicographical files with respect to verb synsets are {cognition}, {communication}, {emotion}, and {social} (
Figure 4. Thresholds of Animacy Entities. 
The process of determining whether a noun phrase is animate or inanimate is described below： 
Heuristic Rules
I. Syntactic parallelism rule 
The syntactic parallelism of an anaphor and an antecedent could be an important clue when other constraints or preferences can not be employed to identify a unique unambiguous antecedent. The rule reflects the preference that the correct antecedent has the same part-of-speech and grammatical function as the anaphor. Nouns can function grammatically as subjects, objects or subject complements. The subject is the person, thing, concept or idea that is the topic of the sentence. The object is directly or indirectly affected by the nature of the verb. Words which follow verbs are not always direct or indirect objects. After a particular kind of verb, such as verb "" be "" , nouns remain in the subjective case. We call these subjective completions or subject complements. He put it in the bottom of the closet. "" He "" (the subject) in the second sentence refers to "" The security guard, "" which is also the subject of the first sentence. In the same way, "" it "" refers to "" the uniform, "" which is the object of the first sentence. Empirical evidence also shows that anaphors usually match their antecedents in terms of their syntactic functions. 
II. Semantic parallelism rule 
This preference works by identifying collocation patterns in which anaphora appear. In this way, the system can automatically identify semantic roles and employ them to select the most appropriate candidate. Collocation relations specify the relations between words that tend to co-occur in the same lexical contexts. The rule emphasizes that those noun phrases with the same semantic roles as the anaphor are preferred answer candidates. 
III. Definiteness rule 
Definiteness is a category concerned with the grammaticalization of the identifiability and non-identifiability of referents. A definite noun phrase is a noun phrase that starts with the word ""the""; for example, ""the young lady"" is a definite noun phrase. Definite noun phrases which can be identified uniquely are more likely to be antecedents of anaphors than indefinite noun phrases. 
IV. Mention Frequency rule 
Recurring items in a context are regarded as likely candidates for the antecedent of an anaphor. Generally, high frequency items indicate the topic as well as the most likely candidate. 
V. Sentence recency rule 
Recency information is employed in most of the implementations of anaphora resolution. In 
VI. Non-prepositional noun phrase rule 
A noun phrase not contained in another noun phrase is considered a possible candidate. This condition can be explained from the perspective of functional ranking: subject > direct object > indirect object. A noun phrase embedded in a prepositional noun phrase is usually an indirect object. 
VII. Conjunction constraint rule 
Conjunctions are usually used to link words, phrases and clauses. If a candidate is connected with an anaphor by a conjunction, the anaphora relation is hard to be constructed between these two entities. 
The Brown Corpus
The training and testing texts were selected randomly from the Brown corpus. The Corpus is divided into 500 samples of about 2000 words each. The samples represent a wide range of styles and varieties of prose. The main categories are listed in 
System functions
The main system window is shown in 
Experimental Results and Analysis
The evaluation experiment employed random texts of different genres selected from the Brown corpus. There were 14,124 words, 2,970 noun phrases and 530 anaphors in the testing data. Two baseline models were established to compare the progress of performance with our proposed anaphora resolution (AR) system. The first baseline model (called the baseline subject) determined the number and gender agreement between candidates and anaphors, and then chose the most recent subject as the antecedent from the candidate set. The second baseline model (called baseline recent) performed a similar procedure, but it selected the most recent noun phrase as the antecedent which matched the anaphor in terms of number and gender agreement. The success rate was calculated as follows: The results obtained (
Table 6. Success rate of the AR system. 
heuristic are needed to decide whether the entities should be combined into one entity or not. In the case of an unknown word, the tagger may fail to identify the part of speech of the word so that in WordNet, no unique beginner can be assigned. This can lead to a matching failure if the entity turns out to be the correct anaphoric reference. 
Conclusion and Future Work
In this paper, the WordNet ontology and heuristic rules have been adopted to perform anaphora resolution. The recognition of animacy entities and gender features in discourses is helpful for improving resolution accuracy. The proposed system is able to deal with intra-sentential and inter-sentential anaphora in English texts and deals appropriately with pleonastic pronouns. From the experiment results, our proposed method is comparable in performance with prior works that fully parse the text. In contrast to most anaphora resolution approaches, our system benefits from the recognition of animacy agreement and operates in a fully automatic mode to achieve optimal performance. With the growing interest in natural language processing and its various applications, anaphora resolution is essential for further message understanding and the coherence of discourses during text processing. 
Our future works will be as follows: 
1. Extending the set of anaphors to be processed: This analysis aims at identifying instances (such as definite anaphors) that could be useful in anaphora resolution. 2. Resolving nominal coreferences: The language resource WordNet can be utilized to identify coreference entities by their synonymy/hypernym/hyponym relations. 
"
"Introduction
Building a knowledge base is time consuming work. The CKIP Chinese Lexical Knowledge Base has about 80 thousand lexical entries, and their senses are defined in terms of the E-HowNet format. E-HowNet is a lexical knowledge representation system. It extends the framework of HowNet (
In this paper, we take DMs as an example to demonstrate how the E-HowNet semantic composition mechanism works in deriving the sense representations for all DM compounds. The remainder of this paper is organized as follows. Section 2 presents the background knowledge of DM compounds and sense representation in E-HowNet. We'll describe our method in Section 3 and discuss the experiment result in Section 4 before we present conclusions in Section 5. 
Background
There are numerous studies on determiners as well as measures, especially on the types of measures 1 F 
Regular Expression Approach for Identifying DMs
Due to the possible infinite number of DMs, 
(Regular expression approach is also applied to deal with ordinal numbers, decimals, fractional numbers and DM compounds for times, locations etc.. The detailed regular expressions can be found in 
(2) a. NO1 = {○,一,二,兩,三,四,五,六,七,八,九,十,廿,卅,百,千, 萬,億,兆,零,幾}; b. NO2 = {壹,貳,參,肆,伍,陸,柒,捌,玖,拾,佰,仟,萬,億,兆,零,幾}; c. NO3 = {１,２,３,４,５,６,７,８,９,０,百,千
Lexical Sense Representation in E-HowNet
Core senses of natural language are compositions of relations and entities. Lexical senses are processing units for sense composition. Conventional linguistic theories classify words into content words and function words. Content words denote entities and function words mainly serve grammatical functions which link relations between entities/events. In E-HowNet, the senses of function words are represented by semantic roles/relations (
(3) because|因為 def: reason={}; which means reason(x)={y} where x is the dependent head and y is the dependent daughter of '因為'. In the following sentence (4), we'll show how the lexical concepts are combined into the sense representation of the sentence. 
(4) Because of the rain, all the clothes are wet. 因為下雨，衣服都濕了 
Automatic Sense Derivation for Determinative-Measure Compounds under the Framework of E-HowNet 
In the above sentence, '濕 wet', '衣服 clothes' and '下雨 rain' are content words while '都 all', '了 Le' and '因為 because' are function words. The difference of their representation is that function words start with a relation but content words have under-specified relations. If a content word plays a dependent daughter of a head concept, the relation between the head concept and this content word will be established after parsing process. Suppose that the following dependent structure and semantic relations are derived after parsing the sentence (4). 
(5) S(reason:VP(Head:Cb:因為|dummy:VA:下雨)|theme:NP(Head:Na:衣服) | quantity: Da:都 | Head:Vh:濕|particle:Ta:了)。 After the feature unification process, the following semantic composition result (6) is derived. The sense representations of dependent daughters became the feature attributes of the sentential head 'wet|濕'. 
(6) def:{wet|濕: theme={clothing|衣物}, aspect={Vachieve|達成}, manner={complete|整}, reason={rain|下雨}} 
In (5), the function word '因為 (because)' links the relation of 'reason' between head concept '濕 wet' and '下雨 rain'. The result of the composition is expressed as reason(wet| 濕)={rain|下雨}, since, for simplicity, the dependent head of a relation is normally omitted. Therefore, reason(wet| 濕 )={rain| 下 雨 } is expressed as reason={rain| 下 雨 }; theme(wet| 濕)={clothing|衣物} is expressed as theme={clothing|衣物} and so on in the expression (6). 
The sense representation for determiners and measures in E-HowNet
The sense of a DM compound is determined by its morphemes and the morphemes of DMs are determiners and measures which are exhaustively listable. Therefore, in order to apply a semantic composition mechanism to derive the senses of DM compounds, we first need to establish the sense representations for all determiners and measures. Determiners and measures are both modifiers of nouns/verbs and their semantic relation with head nouns/verbs are well established. We, thus, defined them by a semantic relation and its value like (7) and 
(8) below. For measure words, we found that some measure words contain content sense, but for some measure words, such as classifiers, their content senses are not important and could be neglected. So, we divided the measure words into two types, with or without content sense, with their sense representations being exemplified below: 
(
Semantic Composition for DM Compounds
To derive sense representations for all DM compounds, we study how to combine the E-HowNet representations of determiners and measures into a DM compound representation and make rules for automatic composition accordingly. Basically, a DM compound is a composition of some optional determiners and an optional measure. It is used as a modifier to describe the quantity, frequency, container, length, etc. of an entity. The major semantic roles played by determiners and measures are listed in 
Compounds under the Framework of E-HowNet 
If a morpheme B is a dependency daughter of morpheme A, i.e. B is a modifier or an argument of A, then unify the semantic representation of A and B via the following steps. Step 2: Unify the semantic representation of A and B by insert relation(A)={B} as a sub-feature of A. 
As exemplified in (9) and 
(10), a feature unification process can derive the sense representation of a DM compound if its morpheme sense representations and semantic head are known. 
(9) one 一 def:quantity={1} + bowl 碗 def: container={bowl|碗} one bowl 一碗 def: container={bowl|碗:quantity={1}} 
(10) this 這 def: quantifier={definite|定指} + 本 copy def:{null} this copy 這本 def: quantifier={definite|定指} There are, however, some complications that must be resolved. First of all we have to clarify the dependent relation between the determiner and the measure of a DM in order to construct a correct feature unification process. 
Head Morpheme of a DM Compound
In principle, a dependent head will take semantic representation of its dependent daughters as its features. Usually, determiners are modifiers of measures, such as '這 (this)' and '一 (one)' are modifiers of '碗 (bowl)' in the examples of 這碗, 一碗, 這一碗. For instance, Example (11) has the dependent relations of NP(quantifier:DM(quantifier:Neu:一|container:Nfa:碗)|Head:Nab:麵) 
Automatic Sense Derivation for Determinative-Measure Compounds under the Framework of E-HowNet 
Figure 1. The dependent relations of 一碗麵 "" a bowl of noddle "" . 
After the feature unification process, the semantic representation of "" 一 def: quantity={1} "" becomes the feature of its dependent head "" 碗 def: container={bowl|碗} "" and derives the feature representation of "" one bowl 一碗 def: container={bowl|碗:quantity={1}} "" . Similarly, "" one bowl 一碗 "" is the dependent daughter of "" noodle|麵 def:{noodle|麵} "" . After the unification process, we derive the result of (11). 
(11) one bowl of noodles|一碗麵 def:{noodle|麵:container={bowl| 
碗:quantity={1}}} 
The above feature unification process, written in rule form, is expressed as (12). The rule (12) says that the sense representation of a DM compound with a determiner D and a measure M is a unification of the feature representation of D as a feature of the sense representation of M as exemplified in (11). Nevertheless, a DM compound with a null sense measure word, such as "" this copy|這本 "" , "" a copy|一本 "" , or without measure word, such as "" these three|這三 "" , will be exceptions, since the measure word cannot be the semantic head of DM compound. The dependent head of determiners become the head noun of the NP containing the DM and the sense representation of a DM is a coordinate conjunction of the feature representations of its morphemes of determiners only. For instance, in (10), "" copy "" has weak content sense; therefore, we regard it as a null-sense measure word and only retain the feature representation of the determiner as the definition of "" this copy|這本 "" . The unification rule for DM with null-sense measure is expressed as (13). 
(13) Determiner + {Null-sense Measure} (D+M) def: Representation(D); 
If a DM has more than one determiner, we can consider the consecutive determiners as one D and the feature representation of D is a coordinate conjunction of the features of all its determiners. For instance, "" this one|這一 "" and "" this one|這一本 "" both are expressed as "" quantifier={definite|定指}, quantity={1} "" . Omissions of numeral determiner occur very often while the numeral quantity is "" 1 "" . For instance, "" 這本 "" in fact means "" this one|這一本 "" . Therefore, the definition of (10) should be modified as: 這本 def: quantifier={definite|定指}, quantity={1}; 
The following derivation rules cover the cases of omissions of numeral determiner. 
(14) If both numeral and quantitative determiners do not occur in a DM, then the feature quantity={1} is the default value of the DM. 
Another major complication is that senses of morphemes are ambiguous. The feature unification process may produce many sense representations for a DM compound. 
Sense Disambiguation
Multiple senses will be derived for a DM compound due to ambiguous senses of its morpheme components. For instance, the measure word "" 頭 (head) "" has either the sense of {頭|head}, such as "" 滿頭白髮 full head of white hair "" or the null sense in "" 一頭牛 a cow "" . Some DMs are inherent sense ambiguous and some are pseudo ambiguous. For instance, the above 
Automatic Sense Derivation for Determinative-Measure 29 Compounds under the Framework of E-HowNet 
example "" 一頭 "" is inherently ambiguous, since it could mean "" full head "" as in the example of "" 一頭白髮 full head of white hair "" or could mean "" one + classifier "" as in the example of "" 一 頭 牛 a cow "" . For inherently ambiguous DMs, the sense derivation step will produce ambiguous sense representations and leave the final sense disambiguation until seeing collocation context, in particular seeing dependent heads. Some ambiguous representations are improbable sense combination. The improbable sense combinations should be eliminated during or after feature unification of D and M. For instance, although the determiner "" 一 "" has ambiguous senses of "" one "" , "" first "" , and "" whole "" , "" 一公尺 "" has only the sense of "" one meter "" , so the other sense combinations should be eliminated. The way we tackle the problem is that first we find all the ambiguous Ds and Ms by looking their definitions shown in Appendix A. We, then, manually design content and context dependent rules to eliminate the improbable combinations for each ambiguous D or M types. For instance, according to Appendix A, "" 頭 "" has 3 different E-HowNet representations while it functions as a determiner or measure, i.e. "" def:{null} "" , "" def:{head| 頭 } "" , and "" def:ordinal={1} "" . We write three content or context dependent rules below to disambiguate its senses. 
(15) 頭 "" head "" , Nfa, E-HowNet: "" def:{null} "" : while E-HowNet of the head word is "" 動物({animate|生物}) "" and its subclasses. 
(16) 頭 "" head "" , Nff, E-HowNet: "" def:{head|頭} "" : while pre-determiner is 一(Neqa) "" one "" or 滿 "" full "" or 全 "" all "" or 整 "" total "" . 
(17) 頭 "" first "" , Nes, E-HowNet: "" def:ordinal={1} "" : while this word is being a demonstrative determiner that is a leading morpheme of the compound. 
The disambiguation rules are shown in Appendix B. In each rule, the first part is the word and its part-of-speech. Then, the E-HowNet definition of this sense is shown, followed by the condition constraints for this sense. If there is still remaining ambiguity after using the disambiguation rule, we choose the most frequent sense as the result. 
Simplification and Normalization for Sense Representation
Members of every type of determiners and measures are exhaustively listable except numeral determiners. Also, the formats of numerals are various. For example, "" 5020 "" is equal to "" 五零 二零 "" and "" 五千零二十 "" and "" 五千二十 "" . So, we have to unify the numeral representation into a standard form. All numerals are composed of basic numerals, as shown in the regular expressions (2). Their senses, however, are not possible to define one by one. We take a simple approach. For all numerals, their E-HowNet sense representations are expressed as themselves. For example, 5020 is expressed as quantity={5020} and we will not further define the sense of 5020. Furthermore all non-Arabic forms will be converted into Arabic expressions, e.g. "" 五千零二十 "" is defined as quantity={5020}. 
The other problem is that the morphological structures of some DMs are not regular patterns. Take "" 兩個半(two and a half) "" as an example. "" 半(half) "" is not a measure word. So, we collect those words, like "" 多 (many), 半 (half), 幾 (many), 上 (up), 大 (big), 來 (more) "" to modify the quantity definition. So, we first remove the word "" 半 "" and define the "" 兩個 "" as quantity={2}. As the word "" 半 "" means quantity={0.5}, we define the E-HowNet definition for "" 兩個半 "" as quantity={2.5}. For other modifiers such as "" 多 (many), 幾 (many), 餘 (more), 來 (more), "" we use a function over() to represent the sense of "" more "" , such as "" 十多個 more than 10 "" is represented as quantity={over(10)}. In E-HowNet, complex word senses are expressed by some limit number of basic or primitive concepts. Nevertheless, some certain domain concepts can hardly be expressed by primitive concepts, for instance "" 焦耳 (joule), "" "" 盧比 (rupee), "" "" 五千零二十 (five thousand and twenty), "" etc.. Therefore, we simplify our representations and consider many domain specific concepts as basic concept without further decomposing into primitive concepts. Appendix A shows the determiners and measures used and their E-HowNet definition in our method. Now, we have the basic principles for compositing semantics of DM under the framework of E-HowNet. 
The following steps show how we process DMs and derive their E-HowNet definitions from an input sentence. For an input Chinese sentence, we use the regular expression rules created by 
Experiments and Discussion
A corpus-based approach was adopted in developing our proposed method. We need a developing set to derive an exhaustive list of determiners and measures. We try to extract DMs and their morpheme components, i.e. determiners and measures, from the developing set and observe the instances of DM to decide their senses and sense representations. Furthermore, sense disambiguation rules will also be developed according to the context of sense ambiguous instances. First, we need to know how many DMs are sufficient to derive a list of determiners and measures with high coverage, if it is not exhaustive. Therefore, we extract DMs from different size subsets of Sinica Treebank and observe their character token coverage. The results are shown in 
Table 2. The character token coverage of different subsets of Sinica Treebank 
Sentences 
Chia-Hung Tai et al. 
Therefore, we randomly selected 16070 sentences from Sinica Treebank as our development set and 10000 sentences as our testing set. The development set contained 3753 DM tokens and the testing set contained 1604 DM tokens. We used the development set to derive lexical sense representations and to design disambiguation rules. A total of 405 determiner types and 211 measure types were found, in which 367 out of the 405 determiners were numerals. Since the numbers of numeral determiners are infinite, all numerals will be converted into their Arabic form automatically instead of representing their E-HowNet sense representations individually. The rest of the determiners and measures are encoded with their E-HowNet sense representations manually. For words with ambiguous senses, we also derived their disambiguation rules according to their contextual information shown in development corpus. Finally, a total of 40 disambiguation rules were developed, as shown in Appendix B. 
The sense representations of a DM compound will then be derived by a semantic composition process under the framework of E-HowNet. The evaluation of the sense derivation for DM compounds can be divided into two parts: the first part is the correctness of the semantic composition process, and the second part is the correctness of the sense disambiguation process. 
Automatic Sense Derivation for Determinative-Measure 33 Compounds under the Framework of E-HowNet 
morphemes. Ambiguous senses were found in 850 words out of the 1604 words. The quality of the result candidates is pretty good. 
Figure 4. The accuracy of composed sense for DM compounds. 
For checking sense correctness, after the disambiguation processes, the resulting E-HowNet representations of 1604 DM tokens in their context were judged manually. Among them, 850 token DMs were sense-ambiguous and the composition process failed to generate answers for 10 of them. Therefore, the composition rules cover 98.8% (840/850) of the ambiguous DM tokens and the precision of the disambiguation rules is 91% (765/840). In all, there are 1432 correct E-HowNet representations for 1604 DM tokens in both sense and format, i.e. the current model achieves 89% ((667+765)/1604) token accuracy. Among the 172 wrong answers, 57 errors are due to undefined ambiguous morpheme sense, 30 errors are unique but the wrong answer, and there are 85 sense disambiguation errors. 
After data analysis, we conclude the following error types. A. Unknown domain error: 七棒 "" 7 th batter "" , 七局 "" 7 th inning "" As there is no text related to the baseball domain in the development set, we get poor performance in dealing with text about baseball. The way to resolve this problem is to increase the coverage of sense representations and disambiguation rules for the baseball domain. 
B. Sense ambiguities: 
In the following parsed phrase, NP(property:DM:上半場 "" first half "" |Head:DM:二十 分 "" twenty minutes or twenty points "" ), the E-HowNet representation of 二十分 "" twenty minutes or twenty points "" can be defined as "" def:role={score| 分 數 :quantity={20}} "" or "" def:time={minute| 分 鐘 :quantity={20}} "" . More contextual information is required to resolve such kinds of sense ambiguity. 
For the type of unknown domain error, the solution is to expand the disambiguation rules and the sense representations for morphemes. For sense ambiguities, we need more information and better features to determine true senses. 
Conclusion
E-HowNet is a lexical sense representational framework and intends to achieve sense representation for all compounds, phrases, and sentences through automatic semantic composition processing. For this purpose, we defined word senses of the CKIP Chinese lexicon in E-HowNet representation. Then, we tried to automate semantic composition for phrases and sentences. Nevertheless, many unknown words or newly coined compound words may occur in the target sentences. In fact, DM compounds are the most frequently occurring unknown words. Therefore, our first goal was to derive the senses of DM words automatically. 
Compounds under the Framework of E-HowNet 
In this paper, we take DMs as an example to demonstrate how the semantic composition mechanism works in E-HowNet to derive the sense representations for all DM compounds. We analyze morphological structures of DMs and derive their morphological rules in terms of regular expression. Then, we defined the sense of all determiners and measures in E-HowNet format exhaustively. We created some simple composition rules to produce candidate sense representations for DMs. Then, we reviewed the development set to write some disambiguation rules. We used these heuristic rules to determine the final E-HowNet representation and reach 89% accuracy. The current version did not exhaustively collect all determiners and measures. The system, however, can be improved by gradual extension of the representations of new determiners and measures without retraining. 
In the future, we will use similar methods to handle general compounds and to improve sense disambiguation and semantic relation identification processing. We intend to achieve semantic compositions for phrases and sentences in the future and we had shown the potential in this paper. 
Compounds under the Framework of E-HowNet 
..} 
表面積的，如：公畝、公頃、市畝、營造畝、坪、畝、分、甲、頃、平方 公里、平方公尺、平方公分、平方尺、平方英哩、英畝。def: size={公畝,...} 表重量的，如：公克、公斤、公噸、市斤、台兩、台斤(日斤)、盎司(斯)、 磅、公擔、公衡、公兩、克拉、斤、兩、錢、噸、克、英磅、英兩、公錢、 毫克、毫分、仟克、公毫。def: weight={公克,...} 表容量的，如：公撮、公升(市升)、營造升、台升(日升)、盎司、品脫(pint)、 加侖(gallon)、蒲式耳(bushel)、公斗、公石、公秉、公合、公勺、斗、毫 升、夸、夸特、夸爾、立方米、立方厘米、立方公分、立方公寸、立方公 尺、立分公里、立方英尺、石、斛、西西。def: volume={公撮,公升,...} 表時間的，如：微秒、釐秒、刻、刻鐘、點、點鐘、更、旬、紀(輪, 12 年) 、世紀、季 def:time={微秒,釐秒,…}；秒、秒鐘 def:time={second|秒}； 分、分鐘 def:time={minute|分鐘}；時、小時 def:time={hour|時}；夜、晚、 宿 def:time={night|夜}；天(日) def:time={day|日}；星期(禮拜、週、周) def:time={week|周}；月、月份 def:time={month|月}；年、載、歲、年份 def:time={year|年}；週年、周歲 def:duration={年} 表錢幣的，如：元(圓)、塊、兩 def:role={money|貨幣}；分、角(毛)、先令、 盧比、法郎(朗)、辨士、馬克、鎊、盧布、美元、美金、便士、里拉、日 元、日圓、台幣、港幣、人民幣。def: role={分, …,盧布…} 其他：刀、打(dozen)、令、綸(十條)、蘿(gross)、大籮(great gross)、焦耳、 千卡、仟卡、燭光、千瓦、仟瓦、伏特、馬力、爾格(erg)、瓦特、瓦、卡 路里、卡、仟赫、位元、莫耳、毫巴、千赫、歐姆、達因、兆赫、法拉第、 牛頓、赫、安培、周波、赫茲、分貝、毫安培、居里、微居里、毫居里 def: quantity={刀,打,…,焦耳,...} Nfh->準量詞— 指行政方面，如：部、司、課、院、科、系、級、股、室、廳。def: location={部, 
Automatic Sense Derivation for Determinative-Measure 39 Compounds under the Framework of E-HowNet 
司...} 
指時間方面，如：世、輩、輩子、代、學期、學年、年代 def: time={學期, 年代,...} 會、會兒、陣(子) 、下(子) def: duration={TimeShort|短時間} 指方向的，如：面(兒)、方面、邊(兒)、方 def: direction={EndPosition|端}； 頭(兒) def: direction={aspect|側} 指音樂的，如：拍、小節。def: quantity={拍,板...} 
"
"© The Association for Computational Linguistics and Chinese Language Processing 
Abstract 
This paper presents a Chinese named entity recognizer (NER): Mencius. It aims to address Chinese NER problems by combining the advantages of rule-based and machine learning (ML) based NER systems. Rule-based NER systems can explicitly encode human comprehension and can be tuned conveniently, while ML-based systems are robust, portable and inexpensive to develop. Our hybrid system incorporates a rule-based knowledge representation and template-matching tool, called InfoMap 
Introduction
Information Extraction (IE) is the task of extracting information of interest from unconstrained text. IE involves two main tasks: the recognition of named entities, and the recognition of the relationships among these named entities. Named Entity Recognition (NER) involves the identification of proper names in text and classification of them into different types of named entities (e.g., persons, organizations, locations). NER is important not only in IE 
Maximum Entropy-based Hybrid Model 
that of an English one; thus, the complexity of a Chinese NER system is greater. 
Previous works 
Borthwick 
Maximum Entropy-Based NER Framework
For our purpose, we regard each character as a token. Consider a test corpus and a set of n named entity categories. Since a named entity can have more than one token, we associate the following two tags with each category x: x_begin and x_continue. In addition, we use the tag unknown to indicate that a token is not part of a named entity. The NER problem can then be rephrased as the problem of assigning one of 2n + 1 tags to each token. In Mencius, there are 3 named entity categories and 7 tags: person_begin, person_continue, location_begin, location_continue, organization_begin, organization_continue and unknown. For example, the phrase [李 遠 哲 在 高 雄 市] (Lee, Yuan Tseh in Kaohsiung City) could be tagged as _begin, 
Maximum Entropy
ME is a flexible statistical model which assigns an outcome for each token based on its history and features. Outcome space is comprised of the seven Mencius tags for an ME formulation of NER. ME computes the probability p(o|h) for any o from the space of all possible outcomes O, and for every h from the space of all possible histories H. A history is composed of all the conditioning data that enable one to assign probabilities to the space of outcomes. In NER, history can be viewed as consisting of the all information derivable from the test corpus relavant to the current token. 
The computation of p(o|h) in ME depends on a set of binary-valued features, which are helpful in making a prediction about the outcome. For instance, one of our features is as follows: when the current character is a known surname, it is likely to be the leading character of a person name. More formally, we can represent this feature as 
(1) ⎩ ⎨ ⎧ = = = else : 0 _ and true Surname(h) - Char - Current if : 1 ) , ( begin person o o h f 
Here, Current-Char-Surname(h) is a binary function that returns the value true if the current character of the history h is in the surname list. 
Given a set of features and a training corpus, the ME estimation process produces a model in which every feature f i has a weight α i . This allows us to compute the conditional probability as follows 
∏ = i o h f i i h Z h o p ) , ( 
) ( 
1 ) | ( α . (2) 
Intuitively, the probability is the multiplication of the weights of active features (i.e., those f i (h,o) = 1). The weight α i is estimated by means of a procedure called Generalized Iterative Scaling (GIS) 
As Borthwick 
Decoding
After an ME model has been trained and the proper weight α i has been assigned to each feature f i , decoding (i.e., marking up) a new piece of text becomes a simple task. First, Mencius tokenizes the text and preprocesses the testing sentence. Then for each token, it checks which 
Maximum Entropy-based Hybrid Model 
features are active and combines theα i of the active features according to equation 2. Finally, a Viterbi search is run to find the highest probability path through the lattice of conditional probabilities that does not produce any invalid tag sequences (for instance, the sequence 
Features
We divide features that can be used to recognize named entities into four categories according to whether they are external or not and whether they are category dependent or not. McDonald defined internal and external features in 
InfoMap – Our Knowledge Representation System
To the calculate values of location features and organization features, Mencius uses InfoMap. InfoMap is our knowledge representation and template matching tool, which represents location or organization names as templates. An input string (sentence) is first matched to one or more location or organization templates by InfoMap and then passed to Mencius; there, it is assigned feature values which further distinguish which named entity category it falls into. 
Knowledge Representation Scheme in InfoMap
InfoMap is a hierarchical knowledge representation scheme, consisting of several domains, each with a tree-like taxonomy. The basic units of information in InfoMap are called generic nodes, which represent concepts, and function nodes, which represent the relationships among the generic nodes of one specific domain. In addition, generic nodes can also contain cross references to other nodes to avoid needless repetition. In Mencius, we apply the geographical taxonomy of InfoMap called GeoMap. Our location and organization templates refer to generic nodes in Geomap. As shown in 
InfoMap Templates
In InfoMap, text templates are stored in generic nodes. Templates can consist of character strings, wildcards (see $$ in 
Maximum Entropy-based Hybrid Model 
Category-Dependent Internal Features
Recall that category-dependent features are used to distinguish among different named entity categories. 
Features for Recognizing Person Names
Mencius only deals with a surname plus a first name (usually composed of two characters), for example, 陳水扁 (Chen Shui-bian). There are various other ways to identify a person in a sentence, such as 陳先生 (Mr. Chen) and老陳 (Old Chen), which have not been incorporated into the current system. Furthermore, we do not target transliterated names, such as 布希 (Bush), since they do not follow Chinese name composition rules. We use a table of frequently occurring names to process our candidate test data. If a character and its context (history) correspond to a feature condition, the value of the current character for that feature will be set to 1. Feature conditions, examples and explanations for each feature are shown in 
Features for Recognizing Location Names
In general, locations are divided into four types: administrative division, public area (park, airport, or port), landmark (road, road section, cross section or address), and landform (mountain, river, sea, or ocean). An administrative division name usually contains one or more 
Maximum Entropy-based Hybrid Model 
location names in a hierarchical order, such as 安大略省多倫多市 (Toronto, Ontario). A public area name is composed of a Region-Name and a Place-Name. However, the Region-Name is usually omitted from news content if it was previously mentioned. For example, 倫敦海德公園 (Hyde Park, London) contains the Region-Name 倫敦 (London) and the Place-Name 海德公園 (Hyde Park). But "" Hyde Park, London "" is usually abbreviated as "" Hyde Park "" within a report. The same rule can be applied to landmark names. A landmark name includes a Region-Name and a Position-Name. In a news article, the Region-Name can be omitted if the Place-Name has been mentioned previously. For example, 溫 哥 華 市 羅 伯 遜 街 五 號 (No. 5, Robson St., Vancouver City) will be stated as 羅伯遜街五號 (No. 5, Robson St.) later in the report. In Mencius, we build templates to recognize three types of location names. Our administrative division templates contain more than one set of location names in a hierarchical order. For example, the template, 
(3) e x o o h f (4) 
When recognizing a location name in a sentence, we test if any location templates match the sentence. If several matched templates overlap, we select the longest matched one. As mentioned above, the feature Current-Character-InfoMap-Location-Begin of the first character of the matched string is set to 1 while the feature Current-Character-InfoMap-Location-Continue of the remaining characters of the matched string is set to 1. 
Features for Recognizing Organization Names
Organizations include named corporate, governmental, or other organizational entities. The difficulty in recognizing an organization name is that it usually begins with a location name, such as 台北市地檢署 (Taipei District Public Prosecutors Office). Therefore, traditional machine learning NER systems can only identify the location part rather than the full organization name. For example, the system only extracts 台北市 (Taipei City) from 台北市 SOGO 百貨週年慶 (Taipei SOGO Department Store Anniversary) rather than 台北市 SOGO 百貨 (Taipei SOGO Department Store). According to our analysis of the structure of Chinese organization names, they mostly end with a specific keyword or begin with a location name. Therefore, we use those keywords and location names as the boundary markers of organization names. Based on our observation, we categorize organization names into four types according to their boundary markers. 
Type I: With left and right boundary markers 
The organization names in this category begin with by one or more geographical names and 
Maximum Entropy-based Hybrid Model 
ended by an organization keyword. For example, 台北市 (Taipei City) is the left boundary marker of 台北市捷運公司 (Taipei City Rapid Transit Corporation), while an organization keyword, 公司 (Corporation), is the right boundary marker. 
Type II: With a left boundary marker 
The organization names in this category begin with by one or more than one geographical names, but the organization keyword (e.g., 公司 (Corporation)) is omitted. For example, 台灣捷安特 (Giant Taiwan) only contains the left boundary 台灣 (Taiwan). 
Type III: With a right boundary marker 
The organization names in this category end with an organization keyword. For example, 捷安 特公司 (Giant Corporation) only contains the right boundary 公司 (Corporation). 
"" 公 "" 司, 公 "" 司 "" 
Probably part of an organization keyword 
Type IV: No boundary marker 
In this category, both left and right boundaries as above mentioned are omitted, for example, 捷 安特 (Giant). The organization names in this category are usually in abbreviated form. In Mencius, we build templates for recognizing Type I organization names. Each organization template begins with a location name in GeoMap and ends with an organization keyword. For example, we can build 
 When a string matches an organization template, the feature Current-Character-InfoMap-Organization-Start of the first character is set to 1. In addition, the feature Current-Character-InfoMap-Organization-Continue of the remaining characters is set to 1. The necessary conditions for each organization feature and examples of matched data are shown in 
Experiments
Data Sets
For Chinese NER, the most famous corpus is MET-2 
Maximum Entropy-based Hybrid Model 
Experimental Results
To understand how word segmentation might influence Chinese NER and the differences between a pure template-based method and our hybrid method, we configure Mencius using the following four settings: (1) Template-based with Char-based Tokenization (TC), (2) Template-based with Word-based Tokenization (TW), (3) Hybrid with Char-based Tokenization (HC), and (4) Hybrid with Word-based Tokenization (HW). Following the standard 10-fold cross-validation method, we tested Mencius with each configuration using the data set mentioned in section 4.1. The following subsections provide details about each configuration and the results obtained. 
Template-based with Char-based Tokenization (TC)
In this experiment, we regarded each character as a token, and used a person name list and InfoMap templates to recognize all named entities. The number of lexicons in the person name lists and gazetteers was 32000. As shown in 
Table 6. Performance of the Template-based System with Char-based Tokenization. 
NE 
Template-based with Word-based Tokenization (TW)
In this experiment, we used a word segmentation module based on the 100,000-word CKIP Traditional Chinese dictionary to split sentences into tokens. This module combines forward and backward longest matching algorithms in the following way: if the segmentation results of the two algorithms agree in certain substrings, this module outputs tokens in those substrings. While in the part which the segmentation results of the two algorithms differ, this module skips word tokens and only outputs character tokens. In the previous test, 98% of the word tokens were valid words. Then, we used person name lists and InfoMap templates to recognize all the named entities. The number of lexicons in the person name lists and gazetteers was 32,000. As shown in 
Table 7. Performance of the Template-based System with Word-based Tokenization. 
NE 
Hybrid with Char-based Tokenization (HC)
In this experiment, we regarded each character as a token without performing any word segmentation. We then integrated person name lists, location templates, and organization templates into a Maximum-Entropy-Based framework. As shown in 
Hybrid System with Word-based Tokenization (HW)
In this experiment, we used the same word segmentation module described in section 4.2.2 to split sentences into tokens. Then, we integrated person name lists, location templates, and organization templates into a Maximum-Entropy-Based framework. As shown in 
Comparisons TC versus TW
We observed that TW achieved much higher precision than TC in PER. When word segmentation is not performed, some trigrams and quadgrams may falsely appear to be person names. Take the sentence "" 新古典主義 "" for example. TC would extract "" 古典主 "" as a person 
Maximum Entropy-based Hybrid Model 
name since "" 古典主 "" matches our family-name trigram template. However, in TW, thanks to word segmentation, "" 古典 "" and "" 主義 "" would be marked as tokens first and would not match the family-name trigram template. 
HC versus HW 
We observed that HW achieved similar precision to that of HC in all three NE categories. HW also achieved recall rates similar to those achieved by HC with PER and ORG NEs. In the case of PER NEs, this is because the length of person names is 2 to 4 characters. Therefore, a five-character long window (-2 to +2) is sufficient to recognize a person name. As far as recognizing LOC NEs is concerned, HW's recall rate was worse than HC's. This is because the word segmentation module marks occupational titles as tokens, for example: "" 台北市長 "" . HW cannot extract the LOC NE "" 台北市 "" from "" 台北市長 "" because it has already been defined as a token. To recognize LOC and ORG NEs, we need higher-level features and more external features. Since Mencius lacks these kinds of features, HW doesn't achieve significantly better performance than HC. 
TC versus HC 
We observed that in PER, HC achieved much higher precision than TC, while in LOC and ORG, HC performed slightly better than TC. This is because most of the key features for identifying a person name are close to the person name, or inside the personal name. Take the sentence "" 立 即連絡海鷗直升機 "" as an example; when we wish to determine whether "" 連絡海 "" is a person name, we can see that "" 立即 "" seldom appears before a person name, and that "" 鷗 "" seldom appears after a person name. In HC, ME can use this information to determine that "" 連絡海 "" is not a person name, but to recognize a location name and an organization name, we need wider context and features, such as sentence analysis or shallow parsing. Take "" 如馬公、七美、望安、 蘭嶼、綠島、馬祖和金門等離島為管制航線 "" as an example; the two preceding characters are "" 美 "" and "" 、 "" , and the two following characters are "" 、 "" and "" 蘭 "" . ME cannot use this information to identify a location name. 
TW versus HW 
We observed that HW achieved better precision than TW in identifying personal names. This is because in HW, ME can use context information to filter some trigrams and 4 grams, which are not personal names. Take "" 王 金 平 和 其 他 委 員 "" as an example; it matches the double-family-name quadgram template because "" 王 "" and "" 金 "" are both family names. However, "" 王金平 "" is the correct person name. In HW, ME can use the information that "" 王金 平 "" has appeared in the training corpus and been tagged as a PER NE to identify the person name "" 王金平 "" in a sentence. We also observed that HW achieved better recall than TW in identifying person names. This is because in HW, ME can use the information that bigram personal names are tagged as PER NEs from the training data, but TW cannot because we don't have bigram-person-name templates. In addition, some person names are in the dictionary, so some tokens are person names. Take "" 陳建仁 的 作為 "" as an example. Although the token "" 陳 建仁 "" cannot match any person name template, in HW, ME can use context information and training data to recognize "" 陳建仁 "" . To identify location names, ME needs a wider context to detect location names, so HW's recall is worse than TW's. However, ME can filter out some unreasonable trigrams, such as "" 黃榮村 "" , because it matches a location name template $$(2..3): 村 , which represents a village in Taiwan. Therefore, ME achieves bigger precision in identifying location names. 
Conclusions
In this paper, we have presented a Chinese NER system, called Mencius. We configured Mencius according to the following settings of to analyze the effects of using a Maximum Entropy-based Framework and a word segmentation module: (1) Template-based with Char-based Tokenization (TC), (2) Template-based with Word-based Tokenization (TW), (3) Hybrid with Char-based Tokenization (HC), and (4) Hybrid with Word-based Tokenization (HW). The experimental results showed that whether a character or a word was taken as a token, the hybrid NER System always performed better in identifying person names. However,this had little effect on the identification of location and organization names. This is because the context information around a location name or an organization name is more complex than that around a person name. In addition, using a word segmentation module improved the performance of the pure Template-based NER System. However, it had little effect with the hybrid NER systems. The current version of Mencius lacks sentence parsing templates and shallow parsing tools to handle such complex information. We will add these functions in the future. 
"
"Introduction
With the evolution of human lives and the accelerated spread of information, new words are created quickly as new things emerge every day. It is then necessary for natural language processing systems to identify and learn new words to progress with time. Chinese word segmentation systems, for example, typically utilize large dictionaries collected over a long period of time. No matter the size of the vocabulary for the dictionaries, it is hardly possible for them to include all of the words or phrases that have been invented so far in the extensive knowledge domains, not to mention to predict in advance new terms to appear in the future. Therefore, it is more practical for Chinese word segmentation systems to use dynamic dictionaries that can be updated quickly and frequently with the new words found in the corpora of the desired domains. Hence, unknown word extraction is actually essential for quite a few natural language processing systems. It is also useful for exploring hot or new terms for desired knowledge domains or internet communities. 
The approaches to unknown word extraction can be roughly divided into two categories, rule-based approaches and statistical approaches. For rule-based approaches, semantic rules for specific types of words, such as the names of humans, places, and organizations, normally are specially designed (
Chinese Unknown Word Extraction 
approaches to combine the statistical features. Histogram equalization for statistical features was further introduced to compensate for the mismatch between the training and testing corpora that might come from the difference in corpus size or the change of the domain. It is then unnecessary to retrain the model parameters, and the extraction approach becomes more general for new domains. This scheme was first evaluated on SIGHAN2 corpora for traditional Chinese provided by Chinese Knowledge Information Processing Group (CKIP) and City University of Hong Kong (CUHK). When combing four heterogeneous statistical features, DLG, AV, Link, and PreC, and applying histogram equalization for DLG, the F-measures of 68.43% and 71.40% for within-domain CKIP corpus and cross-domain CUHK corpus, respectively, can be achieved. This scheme was finally used to explore unknown words in a novice domain of a news event. When compared with the words extracted by two word segmentation systems provided by CKIP and Institute of Computing Technology Chinese Academy of Science (ICTCAS), it was found that this approach is complementary with the other two. Such terms as "" 海角七號＂(Cape No. 7, the name of a film), "" 蠟筆小 新 ＂ (Crayon Shinchan, the name of a figure in a cartoon), "" 金 融 海 嘯 ＂ (Financial Tsunami), and so on, with prominent statistical characteristics but less structure in semantics, can be extracted successfully by the proposed approach only. These terms are hard to identify using rule-based approaches because it is difficult to draw semantic rules from such terms. Without using semantic rules, however, this extraction approach seems less robust for extracting the names of humans, places, or organizations with prominent structure. This, however, could be overcome by integrating the proposed scheme with the rule-based approaches. 
Statistical Features
Every sentence in a Chinese corpus contains a sequence of characters. If every combination of adjacent characters in a sentence must be considered as a word candidate, there would be huge number of word candidates where a large portion would be redundant. Therefore, every combination of adjacent characters, denoted as "" character group "" in this paper, needs to be screened first so the total number of word candidates can be reduced to a manageable size and the statistics could be computed. The occurrence count for each character group, i.e. the character n-gram, is computed and used as one of the screening criteria. Those character groups with length less than eight and with occurrence count more than or equal to five are accepted as word candidates. For each word candidate, the statistical features are computed as below. 
Logarithm of Character N-Gram (LogC)
( ) ( ) ( ) i i LogC T log C T = (1) 
T i : the word candidate with index i. C(T i ) : the occurrence count for the word candidate T i . 
Since words tend to appear repeatedly in the corpora, those word candidates with high occurrence count are more probable to be words. Nevertheless, there are often quite a few false alarms when occurrence count is the only decision feature. Description length gain was proposed by Kit et al. to measure the amount of information for every word candidate according to the degree of data compression (
Description Length Gain (DLG)
( ) ( ) ( [@ ]) i i DLG T L X L X T = − → (2) 2 ( ) ( ) ( ) x V L X X p xlog p x ∈ = − ∑ X 
(T i ) 
indicates the entropy reduction due to the elimination of the word candidate T i in the corpus, or equivalently the information gain of the corpus contributed by including the word candidate T i . The more information a word candidate contributes, the higher the probability that it is a word. Access variety was proposed by 
Accessor
Chinese Unknown Word Extraction 
group is often used together with specific characters, and thus tends to be a part of a word instead of being a word itself. Hence, the larger the access variety is, the more probable the character group is a word. 
Logarithm of Total Links (Link)
The feature LogC defined in Eq. 1 considers the occurrence count of a word candidate but does not take its internal structure into account. Since the occurrence counts of partial character sequences for a word candidate (denoted as links here) might also provide some evidence in support of this candidate being a word, a novel feature for estimating such links is proposed as follows. 
( ) 1 ( ) ( ( ; , )) i i k Link T log C S T k l ≤ = ∑ (4) 
S(T i ;k,l): a partial character sequence of the word candidate T i from position k through position l. 
The word candidate "" 行政院長 "" (meaning executive director), for example, has the partial character sequences "" 行政, "" "" 行政院, "" "" 行政院長, "" "" 政院, "" "" 政院長, "" and "" 院長, "" in which the first three and the last one are also known words. The occurrence counts of these internal links can be accumulated, and the logarithm of the summation can be taken to obtain this feature. 
Independence of Prefix Character(PreC)
In the Chinese language, some characters are frequently used and co-occur with other words as prefixes. The preposition "" 在 "" (meaning at), for example, might co-occur with the words "" 台北 "" (Taipei), "" 拍攝 "" (take a photo) or "" 學校 "" (school), and so on. Since such prefix characters are of high frequency, their combinations with other words (e.g. "" 在台北 "" , "" 在拍 攝 "" or "" 在學校 "" ) might also be of high frequency. This induces quite a few false alarms when only occurrence count is used for word extraction. To alleviate such problems, a novel feature is proposed here to measure the independence of the prefix character for a word candidate, which is defined as the average of the occurrence counts for all the character groups with the same prefix character. 1 
( ) ( ) ( ) L x S F C F C x ∈ = ∑ 
(5) 1 : the partial sequence of a character group x after eliminating its prefix character F. 
( ) 2 ( ) ( ) ( ) i i i C F 
For the prefix character "" 在, "" the independence is computed according to the occurrence counts of those character groups whose first character is "" 在, "" such as "" 在台北, "" "" 在學校, "" and "" 在拍攝 "" . If the average of these occurrence counts is high, it means this prefix character has high variety of context and should be separated from the other characters in a word candidate. In such a case, every word candidate with this prefix character is less probable to be a word. In other words, the higher the independence of the prefix character, the less probable that the candidate is a word. 
Normalization
As the statistical features defined above are computed from the corpus, the dynamic range of the features for the training and the testing corpora might be different when the corpus is obtained from different domains and has a different size. Therefore, the statistical features need to be normalized before being used as the inputs of the classifier. In this paper, the following formula is utilized to normalize the features onto the range of 0 to 1. 
( ) ( ) ( ) ( ) v Min y F v Max y Min y − = − (6) 
v: the input value of the feature. : the type of the feature. 
Word Extraction Method
Distribution of Statistical Features
Since the statistical features in this paper are obtained from the corpora, both the dynamic range and the distribution for the features might change. Although a normalization formula is introduced in Section 2.6 to deal with the problem, it is probably not sufficient for compensating for the mismatch of the feature distributions between the training and testing corpora, which often leads to performance degradation when the statistical approach is applied to new domains. In this section, we analyze how the histograms for the statistical features 
Chinese Unknown Word Extraction 
might differ between various domains. The SIGHAN2 corpora, provided by CKIP and CUHK, respectively, are used for analysis here. First, the CKIP corpus was randomly and equally divided into two sets, named as the CKIP_Train set for training and the CKIP_Test set for testing, respectively. CKIP_Test can be regarded as the within-domain test set. The corpus provided by CUHK is used as the cross-domain test set, and named as CUHK_Test set. The histograms of DLG feature for the CKIP_Train and CKIP_Test sets are depicted in 
Advanced Normalization Schemes
When the mismatch between the training set and the testing set is significant, the classifier generally fails to classify the testing data reliably. Since we hope to use the classifier for word extraction to explore novice domains, such a problem is inevitable. To handle this problem, a typical normalization scheme, mean standard deviation weight (denoted as MSW here) was often used, as defined below. Note that the source domain denotes the testing domain, while the destination domain denotes the training domain. This is because the classifier was trained with the training corpus, so the features for the testing corpus should be transformed back to the training domain to match the distribution of the training data as much as possible. MSW is a linear normalization scheme according to the distance between the feature value and the mean measured with the standard deviation. When the shapes of the distributions differ largely between the source and the destination domains, such a mismatch cannot be compensated for simply by linear shift or scaling, and MSW might not be effective enough. Another normalization scheme, histogram equalization, denoted as HEQ here, was first introduced in image processing community and used for enhancing the contrast of an image (
s s d d d s X M X M σ σ ⎛ ⎞ − = + ⎜ ⎟ ⎝ ⎠ (7) 
X d ＝ P(X s )．( X MAX – X MIN ) + X MIN (8) 
X s : the input feature from source domain. 
Chinese Unknown Word Extraction 
Figure 2 illustrates how histogram equalization is performed. P(X) is the cumulative distribution function (CDF) of feature X in the source domain, as denoted by the solid curve, while P EQ (X) is the equalized cumulative distribution function in the destination domain, as denoted by the dashed line. The transfer function between the input feature X s and the output feature X d has to make the equality, P(X s ) = P EQ (X d ), hold, which leads to Equation 8. Since the heuristic cumulative distribution function of the output feature, P EQ (X d ), is desired to be linear, the corresponding probability density function, i.e. the histogram, needs to be uniform (equalized). Both HEQ and MSW have monotonic transfer functions, but the transfer function for HEQ could be nonlinear, and its output features in the destination domain will fall into the same dynamic range from X MIN to X MAX as the input features in the source domain. 
When applying HEQ to word extraction, the cumulative distribution functions of the features for the training and testing domains need to be computed first, and are here denoted as P TRAIN (X) and P TEST (X), respectively. In the training phase, the features obtained from the training domain (with CDF P TRAIN (X)) need to be transformed to the equalized domain according to Equation 8 so as to obtain the features for training. That is, the classifier is trained with the equalized features. In the classification phase, the features obtained from the testing domain (with CDF P TEST (X)) also need to be transformed to the equalized domain, and the classifier then performs classification for equalized features. It should be noted that, for either MSW or HEQ, such statistics as mean, standard deviation, and CDF need to be computed first so the transformation of the features can be performed accordingly. This imposes an extra limitation to batch mode for the statistical approaches since the testing corpus for computing statistics needs to be collected beforehand. 
Classifier Based on Multilayer Perceptrons
The structures and rules for word formation in the Chinese language are so sophisticated that it is quite difficult to perform word identification based on a single feature. Occurrence count, 
X MAX 
Figure 2. Histogram equalization. 
X d X s ── P(X) ‐ ‐ ‐ ‐ P EQ (X) P EQ (X d ) P(X s ) X MIN 1 X 
for example, is not a reliable enough feature because quite a few fragments (e.g. "" 是非常, "" meaning is very) occur frequently, but should not be regarded as words. If multiple decision features with complementary characteristics could be combined appropriately, better performance could be obtained in general. In this paper, a classifier based on multilayer perceptrons (MLP) is used for word verification. MLP is a machine learning approach based on nonlinear regression. In order to minimize the square errors, the gradient descent algorithm is applied, and the connection weights in the network are updated iteratively according to the errors propagated backwards till the estimation error converges. 
x y Threshold Test HEQ/MSW Selection F(．) F(．) F(．) F(．) F(．) 
DLG Word Non-word 
Chinese Unknown Word Extraction 
Experiments and Analysis
In this section, the corpora CKIP_Train, CKIP_Test, and CUHK_Test, as described in Section 3.1, were used for experiments. They contain 361,691, 363,382, and 54,511 sentences and contain 222,446, 224,929, and 149,160 word candidates, respectively. In addition, the numbers of words for them are 33,429, 33,661, and 22,913, respectively, according to the sentences with word segmentation in every corpus. The segmentation results were used to label the ground truths for word verification, i.e., whether a candidate can be a word or not in general without considering its usage context. That is, no information regarding to the local context of a candidate is tracked or used in word verification. The details of the experimental corpora are depicted in First, the basic experiments of word verification were conducted using at least four types of features based on the architecture in 
Chinese Unknown Word Extraction 
Figure 4. Performance improvements by MSW/HEQ for within-domain test. 
Note here that only the performance of HEQs for DLG is depicted because the other features are not helpful when integrated with HEQ in our auxiliary experiments. For Link and AV, the histograms for the training and testing domains almost coincide such that it can hardly get benefits from equalization, while the extra nonlinear transformation of the feature might degrade the performance. This is similar to the robustness issue, where the robustness approaches for compensating for the mismatch between two environments usually incur the side effect of degrading the performance if the mismatch does not exist. In addition, for PreC, the histograms are very sparse and jerky with many zeros in bins, since many word candidates might share the independence of a prefix character. Therefore, it is not easy to model the cumulative distribution functions smoothly so HEQ can be well applied, and the results are not shown here. Further, the same experiments were conducted for cross-domain testing data. That is, the CKIP_Train corpus was used for training while the CUHK_Test corpus was used for testing. The experimental results are shown in 
Performance improvements by MSW/HEQ for cross-domain test.
The above results show that HEQ can compensate for the mismatch of the DLG feature between the training and testing domains effectively, and the improvements achieved for cross-domain test and within-domain test are compatible. Besides, HEQ can improve the performance more significantly than MSW. This is because, the nonlinear equalization of HEQ works effectively even if the shapes of the distributions between different domains are quite different, but the equalization of MSW by shifting or scaling can work well only when the distributions have close shapes. The above word verification scheme is further applied to the statistical classifier based on Gaussian mixture models (GMM) for comparison. That is, the MLP-based classifier in 
Unknown Word Extraction for Novice Domain
In this section, the word extraction scheme was applied to a novice domain. First, the corpus of the novice domain was collected from a news website in 2009 using the keyword of a news event, "" 八八水災 "" (the flood on August 8 th ). This corpus was used for test and denoted as UKW_Test. From 32,207 sentences in the corpus, 81,447 word candidates were extracted in accordance with the screening criteria in Section 2. Statistical features for these candidates were then computed and used as the input for the classifier trained with the corpus CKIP_Train. The system architecture is similar to that in 
Labeling the Ground Truths
The main problem in applying word extraction in a novice domain is that there is no consensus about the definition of words for the Chinese language; therefore, it is difficult to decide the ground truth for every candidate. The CKIP corpus and CUHK corpus, for example, have different criteria for word defined by the organizations. 
Analysis of Unknown Word Extraction
Since the words extracted by our approach are different from those extracted from the two word-segmentation systems, the unknown words extracted by all of these approaches are compared in this section. Note that the unknown words are defined as those words unseen in 
Figure 7. Comparison for differences of unknown words. 
Although the proposed approach is distinguished in extracting some hot or novice words, it is more vulnerable than the other two in extracting those terms whose patterns are more static with specific structure, such as "" 蘇縣長 "" (Su, the head of the county), "" 經發局 "" (the bureau of economic development) or "" 光林村 "" (Kuang-lin village), as can be seen in 
Chinese Unknown Word Extraction 
Conclusion
This paper proposes a more reliable word extraction scheme by combining multiple statistical features based on machine learning approaches. Since the formation and the structure for Chinese words are sophisticated, it is generally not robust enough to extract words simply according to single feature. This paper combined four features of the word candidates with diverse statistical characteristics to achieve the optimal performance, including the DLG that conveys the information for entropy gain with respect to the corpus, the AV for the usage context, the Link for the evidences of the internal structure, and the PreC for the independence of the prefix character. This scheme was initially verified on the CKIP corpus announced in SIGHAN2, and the performance of F-measure at 60.03% was achieved for within-domain test. This scheme was further applied to the study of statistical mismatch problem between the training the testing domains. The difference of corpus size and the change of domain might lead to difference of dynamic range or distribution for the features, which inevitably degrades the verification performance. Histogram equalization proposed in this paper can compensate for the mismatch of DLG features effectively; thus, it is unnecessary to rebuild the training data for every desired testing domain or to worry about the incompatibility of the feature distributions due to different sizes of corpora. When this scheme of word extraction was evaluated on the within-domain test corpus provided by CKIP and cross-domain test corpus by CUHK, the F-measures can be improved from 60.03% and 63.03% to 68.43% and 71.40%, respectively, by equalization. Finally, this scheme was used to explore a novice domain for a news event of the flood in Taiwan on Aug. 8 th 2009. We proposed a strategy of labeling the ground truths for novice domains according to the consensus between two word segmentation systems. Experimental results show that this scheme can successfully identify some pronouns with prominent statistical tendency but without apparent semantic structure, which cannot be reliably identified with rule-based approaches. This scheme, however, is less robust for extracting those terms whose patterns are static with prominent semantic structure, since it is based on the statistical features instead of the semantic rules. Due to the functional complementation, it is promising to integrate this scheme with other rule-based approaches. 
"
"Introduction
Sequence comparison can be viewed as the string editing problem, i.e., computing the distance between two strings. 
The edit distance is one of the most widely used notions of similarity: it is the least-cost operation set of deletions , insertions and substitutions required to transform one string into another. Sequence comparison is widely used in many engineering systems, such as fuzzy keyword search 
 @BULLET Our scheme satisfies the security requirement of input/output privacy against the malicious servers. 
In multi-server model, one trivial method to realize the verifiability is: the user picks N different servers and asks each of those to execute his programme and return the output. Now, the user takes the plurality value of those answers to be the correct answer. As long as there is a majority of honest servers, the user gets the correct answer. The main drawback of this approach is the need for an honest majority of servers. To get better performance and abate the assumption of a plurality of honest servers, we propose a new approach which is called refereed computation delegation of sequence comparison based on Canetti's computation delegation scheme 
Preliminaries and Tools
Edit Distance
We now precisely give the definition of edit distance 
i.e., of transforming λ 1 , λ 2 , · · · , λ i into µ 1 , µ 2 , · · · , µ j . Then M (0, 0) = 0, M (0, j) = j k=0 I(µ k ) for 1 ≤ j ≤ m, and M (i, 0) = i k=0 D(λ k 
), for 0 ≤ i ≤ n. For positive i and j, we have 
M (i, j) = min    M (i − 1, j − 1) + S(λ i , µ j ) M (i − 1, j) + D(λ i ) M (i, j − 1) + I(µ j ) 
for all 1 ≤ i ≤ n, 1 ≤ j ≤ m. Hence, M (i, j) can be evaluated row by row or column by column in O(mn). Observe that, of all the entries of the M -matrix, only the three entries M (i − 1, j − 1), M (i − 1, j) and M (i, j − 1) are involved in the computation of the final value M (i, j). 
Merkle Hash Tree
Merkel Hash Tree (MHT) 
System Model and Security Definitions
System Model
A refereed computation delegation RCD 
S * 1 , · · · , S * i−1 , S * i+1 , · · · , S * N , 
the output of R is f (x) with probability at least 1 − ε, where ε is negligible. An optional security requirement of RCD is I/O privacy. In the following subsection, we will give a formal definitions of the security requirements. Firstly, we retrospect the traditional verifiable computation model. In detail, the traditional verifiable computation scheme consists of four algorithms defined below (KeyGen,ProbGen, Compute, Verify): @BULLET (pk, sk)← KeyGen(f,λ): Based on the security parameter λ, the randomized key generation algorithm generates a public key pk that encodes the target function f (·), which is used by the cloud server to compute f (·). It also computes a matching secret key sk, which is kept secret by the user U . 
@BULLET (σ x , τ x ) ← ProbGen sk (x): 
 The problem generation algorithm uses the secret key sk to encode the input x as a public value σ x , which is given to the cloud server to compute with, and a secret value τ x , which is kept private by the user U . @BULLET σ y ← Compute pk (σ x ): Using the user's public key pk and the encoded input σ x , the server computes and outputs an encoded version of the result y = f (x). @BULLET y ∨ ⊥ ← Verify sk (σ y , τ x ): Using the secret key sk and the secret "" decoding "" value τ x , the verification algorithm converts the cloud server's encoded output into the output of y = f (x) or ⊥ indicating that σ y does not represent the valid output of f (·) on x. Now we give a formal description of refereed computation delegation. In detail, a refereed computation delegation scheme RCD=(KeyGen, ProbGen, Compute, Verify) consists of four algorithms as defined below, which is the same as that in the verifiable computation 
@BULLET (σ x , τ x ) ← ProbGen sk (x): 
 The problem generation algorithm uses the secret key sk to encode the input x as a public value σ x , which is given to all the cloud servers to compute with, and a secret value τ x , which is kept private by the user U . @BULLET σ y ← Compute P K (σ x ): Using the user's public key pk and the encoded input σ x , the servers compute and output an encoded version of the result y = f (x), respectively. @BULLET y ∪⊥ ← Verify sk (σ y ): Using the secret key sk and the secret "" decoding "" value τ x , the verification algorithm converts the cloud servers' encoded output y * . Note that y * = y if the server is dishonest. Then U verifies the correctness of y * and obtains the correct result as long as there is at least one honest server. 
The basic efficiency requirement of a RCD scheme is that the time to encode the input and verify the output must be smaller than the time to compute the function from scratch, and the complexity of the servers is polynomial in the complexity of evaluating f . Formally, A RCD can be outsourced if it permits efficient problem generation and verification. This implies that for any input x and output σ y , the time required for ProbGen sk (x) plus the time required for Verify sk (τ x , σ y ) is smaller than T , where T is the time to compute the function f (x) from scratch. 
Security Requirements
A refereed computation delegation scheme should be both correct and secure. Intuitively, a RCD scheme is correct if the user always outputs the correct result f (x) as long as there is at least one honest cloud server. In the following experiments, A denotes the set of malicious cloud servers who are allowed to collude with each other, of which the size is at most N − 1. Note that in the refereed delegation of computation, all the servers receive the same input, therefore, the oracle answer for an adversary set A just contains one answer. And the members of the adversary set A will output the same result in order to improve the probability of successful attack. Thus, the adversary set A can be viewed as one party. 
Experiment Exp C A [RCD, f, κ] 
(pk, sk) ← KeyGen(f, κ); 
For i = 1, · · · , l = poly(κ) x i ← A(x 1 , σ x1 , β 1 , · · · , x i−1 , σ xi−1 , β i−1 ); (σ xi , τ xi ) ← ProbGen sk (x i ); σ yi ← A(pk, x 1 , σ x1 , β 1 , · · · , x i−1 , σ xi−1 , β xi−1 , σ xi ); β i = Verify sk (τ xi , σ yi ); x ← A(pk, x 1 , σ x1 , β 1 , · · · , x l , σ x l , β l ) (σ x , τ x ) ← ProbGen sk (x); σ y ← A(pk, x 1 , σ x1 , β 1 , · · · , x l , σ x l , β l , σ x ); ˆ y ← Verify sk (τ x , σ y ); ˆ y = f (x)
, output 1, else 0; In the above experiment, the malicious servers are given oracle access to generate the encoding of multiple problem instances, and also oracle access to the result of the verification algorithm on arbitrary strings on those instances. The adversary succeeds if they convince the user to output wrong result for a given input value. Our goal is to make the adversary succeed only with negligible probability. Correctness. For a refereed computation delegation scheme RCD, we define the advantage of a set of adversaries A in the experiment above as: 
Adv A (RCD, f, κ) = P r[Exp C A [RCD, f, κ] = 1] 
A refereed computation delegation scheme RCD is correct if for any function f ∈ F, and for any adversary set A whose members run in probabilistic polynomial time, 
Adv A (RCD, f, κ) ≤ neg(κ) 
where neg(·) is a negligible function of its input. While the basic security requirements of correctness protects the integrity of RCD, it is also desirable to protect the input and output of the refereed computation delegation scheme against the malicious servers. Below, we define the input/output privacy based on a typical indistinguisability argument that guarantees that no information about the inputs/outputs is leaked. Intuitively, a refereed verifiable computation scheme is input private when the public outputs of the problem generation algorithm ProbGen over two different inputs are indistinguishable. In the following experiment, the adversaries are allowed to request the encoding of any input they desires. The PubProbGen returns the public parameter σ x to the adversary. 
Experiment Exp IP riv A [RCD, f, κ] (pk, sk) ←KeyGen(f, κ) (x 0 , x 1 ) ← A PubProbGen(·) (pk) (σ 0 , τ 0 ) ← ProbGen sk (x 0 ) (σ 1 , τ 1 ) ← ProbGen sk (x 1 ) b ← R {0, 1}ˆb 1}ˆ1}ˆb ← A PubProbGen sk (·) (pk, x 0 , x 1 , σ b ) IfˆbIfˆIfˆb = b
, output1, else 0. Input Privacy. For a refereed delegation of computation scheme, we define the advantage of an adversary set A in the experiment above as : 
Adv IP riv A (RCD, f, κ) = |P r[Exp IP riv A [RCD, f, κ] = 1]− 1 2 |. 
 A refereed computation delegation scheme is input private for a function f , if for any adversary set A whose members run in probabilistic polynomial time, 
Adv IP riv A (RCD, f, κ) ≤ neg(κ) 
where neg() is a negligible function of its input. A similar definition can be made for output privacy. We consider the following the experiment. During the process, the adversaries are allowed to request the output of the algorithm y ∪ ⊥ ← Verify. In the challenged phase, the adversary has to submit two encoded outputs σ 0 and σ 1 . After the adversaries have chosen the challenged output , they can continue the query process. The experiment ideally simulates the execution process of the refereed computation delegation scheme. The oracle OVerify returns the output of the algorithm y ∪ ⊥ ← Verify to the adversaries. 
Experiment Exp OP riv A [RCD, f, λ] (pk, sk) ←KeyGen(f, λ) (σ y0 , σ y1 ) ← A OVerify PubProbGen (pk, f, λ) y 0 ← Verify(σ y0 , f, λ) y 1 ← Verify(σ y1 , f, λ) 
b ← {0, 1}ˆb 1}ˆ1}ˆb ← A OVerify PubProbGen (f, pk, y b ) ifˆbifˆifˆb = b, output 1, else 0. Output Privacy. For a refereed computation delegation scheme RCD, we define the advantage of an adversary set A in the experiment above as: 
Adv Opriv A (RCD, f, κ) = |P r[Exp Opriv A [RCD, f, κ] = 1− 1 2 | 
A refereed delegation of computation scheme is output private if for the legal encoded output set σ, and for any adversary set A whose members run in probabilistic polynomial time, 
Adv Opriv A (RCD, f, κ) ≤ neg(κ) 
where neg() is a negligible function of its input. 
Construction
Main Idea. In the following sections, we only discuss the case where there are two pairs of servers (W 11 , W 12 ) and (W 21 , W 22 ), of which one pair of servers is honest. In the following subsections, we will explain how to extend it to N server model. Suppose the user has two sequences λ and µ over some finite alphabet Σ = {0, · · · , σ − 1} and he wants to delegate the sequence comparison of λ and µ to the cloud servers. We assume each pair of cloud servers cooperatively execute the protocol, but they do not collude with each other. The reason why we use a pair of cloud servers as a server unit will be explained in the following sections. As stated in 
Σ = {0, 1, · · · , σ − 1} and λ i = λ i + λ i for all 1 ≤ i ≤ n
. To split λ, the user can first generate a random sequence λ over the alphabet Σ of length n, and then set λ 
i = λ i − λ i mod σ
, for all 1 ≤ i ≤ n. Similarly, the user splits µ into µ and µ such that 
µ j = µ j + µ j , 
for all 1 ≤ j ≤ m. Then, λ , µ are sent to W 11 and λ , µ are sent to W 12 . The servers W 11 , W 12 collaboratively compute the the edit distance M (n, m) of λ and µ in an additively split fashion. That is, W 11 and W 12 each maintain a matrix M (1) and M (1) such that M (i, 
j) = M (1) (i, j) + M (1) (i, j) for 1 ≤ i ≤ n, 1 ≤ j ≤ m. 
matrixes M (1) and M (1) , and return the result M (1) (n, m), M (1) (n, m) together with the value M H root (M (1 ) (n, m)), M H root (M (1 ) (n, m))
, which is the root value of the Merkle Hash Tree for their respective result. The second pair of servers (W 21 , W 22 ) do the same as described above. Thereafter, the user runs the consistency proof with (
W 11 , W 12 ), (W 21 , W 22 )
, and outputs the correct result if at least one pair of servers are honest. As described in Section 2, a refereed computation delegation of sequence comparison scheme consists of four algorithms (KeyGen, ProbGen, Compute, Verify), which will be formally defined below. The framework of our scheme is presented in 
a = (a 1 , · · · , a n ) and b = (b 1 , · · · , b m ). Then U computes two vectors c = (c 1 , · · · , c n ) and d = (d 1 , · · · , d m ) where c i = i k=1 D(λ k ) − a i , for 1 ≤ i ≤ n, and d j = j k=1 I(µ k ) − b j , for 1 ≤ j ≤ m. 
The outputs of the algorithm is set to be (σ x , τ x 
)=((λ , λ , µ , µ , a, b, c, d), (λ, µ)). Then U sends λ , µ , b, c to W k1 , and sends λ , µ , a, d, to W k2 , k = 1, 2. @BULLET σ y ← Compute(σ x ): As described in Figure 2. (W k1 , W k2 ), k = 1, 2, collaboratively compute M (i, j) for all 1 ≤ i ≤ n, 1 ≤ j ≤ m. Note that W k1 owns M (k) (i, j) and W k2 owns M (k) (i, j) , such that M (i, j) = M (k) (i, j) + M (k) (i, j) . 
The protocol specifications are described as follows: 
 1) W k1 and W k2 first initialize their respective matrix, and then cooperatively compute 
(u (k) , v (k) , w (k) ) and (u (k) , v (k) , w (k) 
). 
2) After the minimum finding protocol 
W k1 gets M (k) (i, j) and W k2 gets M (k) (i, j). 
3) Then W k1 and W k2 each construct the Merkel Hash Tree for the result matrices M (k) and M (k) respectively and return the result and root value to the user. Thus, the output 
σ y =(M (k) (n, m), M H root (M (k) ),M (k) (n, m),M H root (M (k) )), k = 1, 2. 
 Note that γ and γ are computed through the following split-S protocol. 
1) W k1 generates a σ ×σ tablê S withˆSwithˆ withˆS(r, l) equals to E pk k1 (S(r + λ i mod σ, l)) and sends it to W k2 . 2) For 1 ≤ j ≤ m, W k2 extracts the λ i -th row ofˆSofˆ ofˆS as a vector v, 
v l = E pk k1 (S(λ i + λ i mod σ, l)) = E pk k1 (S(λ i , l)
 ). Then W k2 circularly left-shift the vector v µ j positions and updates v with a random number γ , as a 
sult v l = E pk k1 (S(λ i , µ j + l) * E pk k1 (−γ )) = E pk k1 (S(λ i , µ j + l) − γ ). 3) W k1 uses 1
 -out-of-m oblivious transfer proto- col 
γ = S(λ i , µ j + µ j ) − γ = S(λ i , µ j ) − γ . @BULLET y ∪ ⊥ ← Verify sk (σ y ): U first verifies whether M (1) (n, m) + M (1) (n, m) = M (2) (n, m) + M (2) (
n, m) or not. In case the equation holds, the answer is correct. Else, the user continues to a binary search as described as follows. 
Remark. Assume that the split edit distance matrix is reordered in row-major principle as a vectorˆmvectorˆ vectorˆm when the cloud servers construct the Merkle Hash Tree for them. We use variable n g to denote the good positions which means thatˆmthatˆ thatˆm 
(1) (n g )+ ˆ m (1) (n g )= ˆ m (2) (n g )+ ˆ m (2) (n g )
 , and use variable n b to denote the bad positions . When the binary search ends, U ask W 11 and W 12 for the consistency proof at the positions n g and n b . If the verification process returns true, U outputs M (n, 
m) = M (1) (n, m) + M (1) (n, m) . Otherwise, he outputs M (n, m) = M (2) (n, m) +M (2) (n, m) . 
 The specifications are presented as follows: 
1) U initializes position variables as n g = 1, n b = mn and asks W k1 and W k2 , k = 1, 2, for the mid-th value For k = 1, 2 Initialization: 
W k1 sets M (k) (0, j) = b j , for 1 ≤ j ≤ m, and sets M (k) (i, 0) = c i , for 1 ≤ i ≤ n. W k2 sets M (k) (0, j) = d j , for 1 ≤ j ≤ m, and sets M (k) (i, 0) = a i , or 1 ≤ i ≤ n. 
For 1 ≤ i ≤ n, 1 ≤ j ≤ m (1). W k1 computes u (k) = M (k) (i − 1, j) + M (k) (i, 0) − M (k) (i − 1, 0) = M (k) (i − 1, j) + D(λ i ) − a i + a i−1 . W k2 computes u (k) = M (k) (i − 1, j) + M (k) (i, 0) − M (k) (i − 1, 0) = M (k) (i − 1, j) + a i − a i−1 . u (k) + u (k) = M (k) (i − 1, j) + M (k) (i − 1, j) = M (i − 1, j) + D(λ i ). 
(2). W k1 computes v (k) = M (k) (i, j − 1) + M (k) (0, j) − M (k) (0, j − 1) = M (k) (i, j − 1) + b j − b j−1 . W k2 computes v (k) = M (k) (i, j − 1) + M (k) (0, j) − M (k) (0, j − 1) = M (k) (i, j − 1) − b j + b j−1 + I(µ j ). v (k) + v (k) = M (k) (i, j − 1) + M (k) (i, j − 1) = M (i, j − 1) + I(µ j ). 
(3). W k1 sets w (k) = M (k) (i − 1, j − 1) + γ . W k2 sets w (k) = M (k) (i − 1, j − 1) + γ . w (k) + w (k) = M (i − 1, j − 1) + S(λ i , µ j ) 
(4). After the implementation of minimum finding protocol, W k1 and W k2 get 
M (k) (i, j), M (k) (i,j) respectively, such that M (k) (i, j) + M (k) (i, j) = min   M (i − 1, j − 1) + S(λ i , µ j ) M (i − 1, j) + D(λ i ) M (i, j − 1) + I(µ i )   
= (n g + n b )/2. IfˆmIfˆ Ifˆm (1) (mid) + ˆ m (1) (mid)= ˆ m (2) (mid) + ˆ m (2) (mid)
, he sets n g = mid, otherwise, he sets n b = mid. U continues the binary search in that way till he gets n b = n g + 1. 
2) W 11 returns the consistency proof 
p 1g = M H poof ( ˆ m (1) , n g ), p 1b = M H poof ( ˆ m (1) , n b ), 
W 12 returns p 2g = M H poof ( ˆ m (1) , n g ), p 2b = M H poof ( ˆ m (1) , n b ). 3) U runs V erM HP (M H root ( ˆ m (1) ), n g , ˆ m (1) (n g ), p 1g ) V erM HP (M H root ( ˆ m (1) ), n b , ˆ m (1) (n b ), p 1b ) V erM HP (M H root ( ˆ m (1) ), n g , ˆ m (1) (n g ), p 2g ) V erM HP (M H root ( ˆ m (1) ), n b , ˆ m (1) (n b ), p 2b ) 
 to verify the consistency proof. If either proof is invalid , the cloud servers W k1 and W k2 are marked as dishonest. Otherwise, U proceeds as follows. 
4) Suppose thatˆmthatˆ thatˆm 
(1) (n g ) is equivalent to M (1) (α, β), thenˆmthenˆ thenˆm (1) (n g − m) andˆmandˆ andˆm (1) (n g − m + 1) are equivalent to M (1) (α − 1, β), M (1) (α − 1, β + 1
 ). As described above, U asks W 11 and W 12 for consistency proof at the position n g − m and n g − m + 1, and then computes 
M (α − 1, β) = M (1) (α − 1, β) + M (1) (α − 1, β) M (α−1, β+1) = M (1) (α−1, β+1)+M (1) (α−1, β+1). 
Now that U owns M (α, β), M (α − 1, β), and M (α − 1, β + 1), he can compute M (α, β + 1) by himself using the dynamic programming algorithm. Then U verifies whether M (α, 
β + 1) = M (1) (α, β + 1) + M (1) (α, β + 1) holds (Remember that M (1) (α, β + 1) = ˆ m (1) (n b ), M (1) (α, β + 1) = ˆ m (1) (n b 
). If the equation holds, U outputs the value of M (n, m) =is also the probability that the user can get the correct result in our scheme. 
 Theorem 2. Suppose that any pair of servers do not collude with each other, the additively homomorphic encryption scheme is semantically secure, 1-out-of-n OT is receiver secure, and the minimum finding protocol is secure in two-party computation model against malicious adversaries . Then, our scheme satisfies the security requirement of input an output privacy. Proof. Firstly, recall that the original sequence λ and µ are splitted into λ ,λ and µ , µ , such that 
λ i = λ i + λ i mod σ, µ j = µ j + µ j mod σ. λ i , λ i , µ 
j and µ j are uniformly distributed over the alphabet set . Therefore , in the experiment Exp IP riv A 
j) = M (k) (i, j) + M (k) (i, j
). Therefore, in the challenge phrase, if any adversary is able to corrupt the output privacy our scheme, he can successfully attack the underlying minimum finding protocol with the same probability, which contradicts our assumption. 
Conclusion
 In this paper, we propose the refereed computation delegation of sequence comparison for the first time. Compared with previous computation delegation schemes, our scheme is qualitatively more practical. Our scheme works in the multi-server model, and the user can get the correct result of the computation as long as there is at least one honest server. For the sequences of length m and n, respectively, our scheme reduce the user computation complexity from O(mn) to O(log 2 (mn)). The security analysis shows that our scheme satisfies the security requirements of I/O privacy and correctness. 
"
"Introduction
Boolean functions play an important role in the design of symmetric key cryptographic algorithms. In case of block ciphers like AES and DES, nonlinearity is provided by the substitution boxes (S-box) solely. S-boxes are the most important and complex part of many block ciphers . Implementation of S-box in software or hardware requires either significant amount of memory or silicon area. Therefore, S-boxes make the block ciphers unsuitable for light weight cryptography 
Preliminaries
Some basic definitions and notations are discussed in this section. 
ξ(x 1 , x 2 , ..., x n ) = p 0 ⊕ p 1 x 1 ⊕ p 2 x 2 ⊕ p n x n ⊕ p 12 x 1 x 2 ⊕ ... ⊕ p 12...n x 1 x 2 ...x n , where p 0 , p 1 
, ..., p 12...n ∈ {0, 1}. 
Definition 6. An n variables Boolean function ξ(x 1 , x 2 , ..., x n ) 
is said to be an affine function if the ANF of ξ is of the 
form ξ(x 1 , x 2 , ..., x n ) = p 0 ⊕p 1 x 1 ⊕p 2 x 2 ⊕...⊕p n x n , where p 0 , p 1 , ...p n ∈ {1
, 0}. If p 0 is 0 then the function is said to be linear. 
Definition 7. Nonlinearity of an n variable 
Proposed Nonlinear Function
In this section, a new nonlinear, reversible and balanced vectorial Boolean function denoted as 'Rain' and its inverse function denoted as 'I-Rain' are introduced. 
y i = x i ⊕ k i ⊕ c i−1 ; c i = i ⊕ j=0 x j · k i−j (1) 
where  It is noted that c i−1 is a Boolean function of 2i variables and it is in the form of bent function and obviously it is not balanced. Whereas, y i can be considered as a combination of two functions: one linear function x i ⊕ k i and a bent function c i−1 . Therefore, y i is balanced. 
Definition 12. Inverse function takes two n-bit inputs 
Y = (y n−1 y n−2 . . . y 0 ) and K = (k n−1 k n−2 . . . k 0 ) and 
 produces an n−bit output X = (x n−1 x n−2 . . . x 0 ). Inverse function I-Rain is defined as 
X = F −1 (Y, K), where x i = y i ⊕ k i ⊕ d i−1 ; d i = i ⊕ j=0 x j · k i−j (2) 
where ⊕ is modulo-2 sum, · represents AND operation, 
0 ≤ i ≤ n − 1, d −1 = 
Analogy Between Rain and Addition
Following four properties show the similarity of Rain with integer addition. 
Property 1: F(A, A) ̸ = 0 if any a k ̸ = 0, where 0 ≤ k ≤ ⌊ n−2 2 ⌋; F −1 (A, A) = 0. Analogy: A + A = 2A ̸ = 0 ∀ A ̸ = 0; A − A = 0. 
Proof. Assume Y = F(A, A), where A and Y are two n-bit variables. Then from the definition of F we can write 
y i = a i ⊕ a i ⊕ i−1 ⊕ j=0 a j a i−1−j = i−1 ⊕ j=0 a j a i−1−j . 
It is observed that y 2k+1 = a k and y 2k = 0, where 0 ≤ k ≤ ⌊ n−2 2 ⌋ i.e. even output bits are zero. Hence F(A, A) ̸ = 0 if any a k ̸ = 0, where 0 ≤ k ≤ ⌊ n−2 2 ⌋ Assume X = F −1 (A, A), where A and X are two n-bit variables. Then from the definition of F −1 , we can write 
x i = a i ⊕ a i ⊕ i−1 ⊕ j=0 x j a i−1−j = i−1 ⊕ j=0 x j a i−1−j . 
From definition of F −1 , x 0 = a 0 ⊕ a 0 = 0. Since x 0 = 0 therefore x 1 = 0. Now x 2 = 0, because x 1 = 0 and x 0 = 0. Similarly, it can be shown that output bit x i will be zero if all x j s are zero, where 0 ≤ j ≤ (i − 1). Therefore, F −1 (A, A) = 0. 
Property 2: F(A, 0) = A ; F −1 (A, 0) = A. Analogy: A + 0 = A; A − 0 = A. 
Proof. Let Y = F(A, 0) and X = F −1 (A, 0), where A, X and Y are three n-bit variables and A ̸ = 0. From the definition of F, it is noted that y i = a i , where 0 ≤ i ≤ n − 1. Therefore, F(A, 0) = A. From the definition of F −1 , it is observed that 
x i = a i , for 0 ≤ i ≤ n − 1. Hence F −1 (A, 0) = A. 
Property 3: F −1 is the inverse of F i.e. 
F −1 (F(A, K), K) = F(F −1 (A, K), K) = A. Analogy: (A + K) − K = (A − K) + K = A. Proof. Assume R = F (A, K) and S = F −1 (F (A, K), K) = F −1 (R, K), 
where A, K, R and S are four n-bit variables. From the definition of F, we can write 
r i = a i ⊕ k i ⊕ i−1 ⊕ j=0 a j · k i−1−j . 
Since S = F −1 (R, K), therefore s i can be written as 
s i = r i ⊕ k i ⊕ i−1 ⊕ j=0 s j · k i−1−j = a i ⊕ i−1 ⊕ j=0 (a j ⊕ s j ) · k i−1−j . Let u i = ⊕ i−1 j=0 (a j ⊕ s j ) · k i−1−j , hence s i = r i ⊕ u i , 
where 0 ≤ i ≤ n − 1. It is noted that s i will be equal 
to a i if u i = 0 i.e. for k l ̸ = 0 if a m = s m , where 0 ≤ l, m ≤ i − 1
. Now from the definition of F and F −1 , we can write 
r 0 = a 0 ⊕ k 0 or a 0 = r 0 ⊕ k 0 = s 0 and r 1 = a 1 ⊕ k 1 ⊕ a 0 k 0 or a 1 = r 1 ⊕ k 1 ⊕ a 0 k 0 = s 1 . Since a 0 = s 0 and a 1 = s 1 , therefore u 2 = 0 i.e. s 2 = a 2 . Now s 3 = a 3 because s 2 = a 2 , s 1 = 
a 1 and s 0 = a 0 . Similarly, it can be shown that s i = a i for 0 ≤ i ≤ n − 1. Therefore, F −1 (F(A, K), K) = A i.e. F −1 is the inverse function of F. 
Let P = F −1 (A, K) and Q = F(F −1 (A, K), K) = F(P, K), 
where A, K, P and Q are four n-bit variables. From the definition of F −1 , we can write 
p i = a i ⊕ k i ⊕ i−1 ⊕ j=0 p j · k i−1−j . 
Since Q = F(P, K), so from the definition of F we can write 
q i = p i ⊕ k i ⊕ i−1 ⊕ j=0 p j · k i−1−j or q i = a i ⊕ k i ⊕ k i ⊕ i−1 ⊕ j=0 p j · k i−1−j ⊕ i−1 ⊕ j=0 p j · k i−1−j = a i . 
Therefore, q i = a i for all 0 ≤ i ≤ n − 1 i.e. F −1 is the inverse function of F. Hence it is proved that 
F −1 (F(A, K), K) = F(F −1 (A, K), K) = A. 
 Property 4: F satisfies but F −1 does not satisfy commutative law i.e. F(X, 
K) = F(K, X); F −1 (Y, K) ̸ = F −1 (K, Y ). Analogy: X + K = K + X; X − K ̸ = K − X. 
Proof. Assume P = F(X, K) and Q = F(K, X), where X, K, P and Q are four n-bit variables. From the definition of F, p i can be expressed as 
p i = x i ⊕ k i ⊕ i−1 ⊕ j=0 x j · k i−1−j = k i ⊕ x i ⊕ i−1 ⊕ j=0 k j · x i−1−j = q i . 
Because ⊕ i−1 j=0 x j · k i−1−j = ⊕ i−1 j=0 k j · x i−1−j . Therefore, F satisfies commutative law. Let R = F −1 (Y, K) and S = F −1 (K, Y 
), where Y , K, R and S are four n-bit variables. From the definition of F −1 , it can be shown that 
r i = y i ⊕ k i ⊕ i−1 ⊕ j=0 r j · k i−1−j ̸ = k i ⊕ y i ⊕ i−1 ⊕ j=0 r j · y i−1−j = s i . 
Hence, F −1 does not satisfy commutative law. 
Performance of Rain Against Linear Cryptanalysis
In this section, performance of the proposed function against linear cryptanalysis(LC) is discussed. The approach in LC is to determine linear expressions of the form which have a low or high probability of occurrence. Bias of best linear approximation for the output bit y i and their linear combinations are computed in this section. 
Theorem 2. The bias of best linear approximation for y i of 
Rain is 2 −(i+1) , where 0 ≤ i < n. 
Proof. From 
 the definition of F, it is evident that the output y i = x i ⊕ k i ⊕ c i−1 , where c i−1 is the carry input into the i-th bit position and it is the only nonlinear term in y i . From the definition of F, c i−1 can be expressed as 
c i−1 = x 0 k i−1 ⊕ x 1 k i−2 ⊕ . . . x i−1 k 0 . 
(3) 
Expression shows that c i−1 is a function of 2i variables and it is in the form of bent function 
N i = 2 2i−1 − 2 i−1 . 
(4) 
It is noted that y i is a combination of two functions: one linear function (x i ⊕ k i ) of two variables and a bent function c i−1 of 2i variables and of nonlinearity N i . Therefore , nonlinearity of y i is 2 2 .
N i = 2 2 (2 2i−1 − 2 i−1 ) = 2 2i+1 − 2 i+1 [using Theorem 1]
. Since y i is a function of 2i + 2 independent variables, hence the number of matches in the best linear approximation of y i is N m = 2 2i+2 − 2 2i+1 + 2 i+1 . Hence, the probability of matches 
p i = Nm 2 2i+2 = 1 − 1 2 + 1 2 i+1 = 1 2 + 1 2 i+1 and 
the bias of best linear approximation is 1 2 i+1 . It is noted that the bias of best linear approximation of y i decreases exponentially with the bit position i. A comparison of linear probability bias of addition, Slash and Rain for the first six output bits is shown in 
y i ⊕ y m of Rain is 2 −(m+1) , where 0 ≤ i, m < n and m > i. 
Proof. The bias of best linear approximation of y i ⊕ y m is derived here, where 0 ≤ i, m < n and i ̸ = m. From the definition of F we can write 
y i ⊕ y m = (x m ⊕ k m ) ⊕ k i ⊕ x i ⊕ c i−1 ⊕ c m−1 . 
(5) 
From the definition of F, x i ⊕ k i ⊕ c i−1 ⊕ c m−1 can be expressed as 
x i ⊕ k i ⊕ c i−1 ⊕ c m−1 = i−1 ⊕ j=0 x j (k i−1−j ⊕ k m−1−j ) ⊕ x i (k m−1−i ⊕ 1) ⊕k i (x m−1−i ⊕ 1) ⊕ m−1 ⊕ j=i+1;j̸ =m−1−i x j k m−1−j = m−1 ⊕ j=0 z j s m−1−j , 
(6) 
where s m−1−j = k i−1−j ⊕ k m−1−j for 0 ≤ j ≤ i − 1, s m−1−j = k m−1−j ⊕ 1 for j = i, s m−1−j = k m−1−j for i + 1 ≤ j ≤ m − 1, z p = x p ⊕ 1, for p = m − 1 − i, otherwise z p = x p . 
Also it may be observed from the substitution that the bits s l for 0 ≤ l ≤ m − 1 are statistically independent if the bits k l for 0 ≤ l ≤ m − 1 are statistically independent and are uniformly chosen. Therefore, x i ⊕ k i ⊕ c i−1 ⊕ c m−1 is also a bent function of 2m variables. So, nonlinearity of  Hence, applying this observation repeatedly we conclude that if more than two bit positions are included in the linear combination, the resultant function will have the highest nonlinearity corresponding to the greatest bit position. This shows that linearly combining with other bit positions at least does not reduce the nonlinearity of the resultant function. A corollary of theorem 2 is as fol- lows. Corollary: The bias of best linear approximation for the non-zero linear combination of the output bits is 2 −(m+1) , where m is the largest bit position involved in the linear combination. These results show that the strength of Rain against linear cryptanalysis is high. The bias of best linear approximation for y i ⊕ y i+1 is obtained by substituting m = i + 1 in the corollary and the bias value is 1 2 i+2 . 
x i ⊕ k i ⊕ c i−1 ⊕ c m−1 is 2 2m−1 − 2 m−1 . Hence, nonlinearity of y i ⊕ y m is 2 2 (2 2m−1 − 2 m−1 ) = 2 2m+1 − 2 m+1 [
n − 1 O(n) O(n 2 ) Rain n(n+1) 2 n(n−1) 2 O(1) O(n 2 ) XOR n O(1) O(n) Reverse Subtraction 2n − 1 n − 2 2(n − 2) n − 2 O(n) O(n 2 ) I-Slash 3(n − 1) 2n − 3 2(n − 1) O(n) O(n 2 ) I-Rain n(n+1) 2 n(n−1) 2 O(n) O(n 3 ) 
Hardware and Time Complexity
 In this section we discuss the hardware and time complexity of the proposed mixing function for n-bit block size. It is found that y i contains i number of AND terms and two linear terms. Therefore to implement y i , i numbers of two input AND gates and i+1 number of two input XOR gates are required. So implementation of an n-bit function requires n(n−1) 2 
two input AND gates and n(n+1) 
2 two input XOR gates. 
Application of Rain
In this section, we first give a brief description of stream cipher NLS 
 6.1 Brief Description of NLS Stream Ci- pher 
NLS has two components: NFSR and NLF whose work is synchronized by a clock. In NLS, key stream generator uses NFSR whose outputs are fed to the nonlinear filter NLF that produces output key stream bits. Detailed about NLS may be found in 
1) r t+1 [i] = r t [i + 1] for i = 0, ..., 15; 2) r t+1 [16] = f ((r t [0] <<< 19) + (r t [15] <<< 9) + Konst) ⊕ r t [4]; 3) r t [0] is abandoned; 4) if t = 0 (modulo f 16), r t+1 [2] = r t+1 [2] + t. 
Here f 16 is 65537 and + is the addition modulo 2 32 . The Konst value is a 32-bit key dependent constant. The function f : {0, 1} 32 → {0, 1} 32 is constructed using an S-box with 8-bit input and 32-bit output and defined as f (a) = S-box(a H ) ⊕ a where a H is the most significant 8 bits of 32-bit word a. Each output key stream word ν t of NLF is computed by 
ν t = N LF (σ t ) = (r t [0] + r t [16]) ⊕ (r t [1] + r t [13]) ⊕ (r t [6] + Konst). 
(7) 
 6.2 Overview of Crossword Puzzle At- tack 
In CPA 
1) Find a linear approximation of the non-linear state transition function used by NFSR: 
l 1 (σ i ) = σ i+1 with bias of ϵ 1 ; 
 2) Find a linear approximation of the non-linear function applied by NLF: 
l 2 (σ j ) ⊕ l 3 (ν j 
) = 0 with bias of 
ϵ 2 ; 
3) Obtain two sets of clock I and J such that ∑ 
i∈I (l 1 (σ i ) ⊕ σ i+1 ) = ∑ j∈J l 2 (σ j ); 4) Build a distinguisher by computing ∑ i∈I (l 1 (σ i ) ⊕ σ i+1 ) ⊕ ∑ j∈J (l 2 (σ j ) ⊕ l 3 (ν j )) = l 3 (ν j ) = 0, which has bias of ϵ |I| .ϵ |J| . 
Improved CPA Against NLS
 An improved version of CPA on NLS by exploiting the internal dependencies between NFSR and NLF is presented by MacDoland and Hawkes in 
Existing Countermeasure Against CPA Using Slash
In 
(r[x] ⊘ r[y]) (0) = r[x] (0) ⊕ r[y] (0) . 
(8) 
 But for all i > 0 the linear combination of i-th and (i−1)- th output bits can be expressed as 
(r[x] ⊘ r[y]) (i) ⊕ (r[x] ⊘ r[y]) (i−1) = r[x] (i) ⊕ r[y] (i) ⊕ r[x] (i−1) ⊕ r[y] (i−1) ⊕r[x] (i−1) .r[y] (i−1) . 
(9) 
where '⊘' is the Slash operator. The bias of best linear approximation is 2 −2 and hence Slash function is equivalent to modulo addition which is used in the original NLS. Similarly from the definition of Slash function 
(r[x] ⊘ r[y]) (i) ⊕ (r[x] ⊘ r[y]) (i−1) ⊕(r[x] ⊘ r[y]) (i−2) ⊕ (r[x] ⊘ r[y]) (i−3) = r[x] (i) ⊕ r[y] (i) ⊕ r[x] (i−1) ⊕ r[y] (i−1) ⊕ r[x] (i−2) ⊕r[y] (i−2) ⊕ r[x] (i−3) ⊕ r[y] (i−3) ⊕r[x] (i−1) .r[y] (i−1) ⊕ r[x] (i−3) .r[y] (i−3) . 
(10) 
The bias of the best linear approximation of equation (10) is 2 −3 . Therefore, Slash function has exactly same bias value for linear approximation as that of addition 
Countermeasure Against CPA Using Rain
 In this work, addition modulo 2 n in NLF of NLS is replaced by Rain while NFSR remains unchanged. Hence each modified key stream word ν ′ t of NLF is obtained as 
ν ′ t = N LF (σ t ) = (r t [0] † r t [16]) ⊕ (r t [1] † r t [13]) ⊕ (r t [6] † Konst) 
(11) 
where † is Rain operator. 
Analysis of modified NLS: 
In this section, we show how CPA can be thwarted using Rain. In the modified version of NLS, the bias of the distinguisher decreases to such a low value that any practical attack using this linear distinguisher is impossible. Since, we have not changed the NFSR, therefore the analysis of NFSR reported in 
α t,(0) ⊕ r t 
(12) 
where α t is the 32-bit output of the S-box that defines the transition function f , α t,(i) and x i are the i-th bits of the 32-bit words α t and x respectively. From 
(13) 
Next we determine the bias of linear approximation for modified NLF. We assume initially Konst is zero to make our analysis simpler. Substituting Konst = 0 in Equation (11), we get 
ν ′ t = (r t [0] † r t 
(14) 
From the definition of Rain, we know that the relation between LSBs are linear so the equation (r
ν ′ t,(0) = (r t [0] (0) ⊕ r t [16] (0) ) ⊕ (r t [1] (0) ⊕r t [13] (0) ) ⊕ r t [6] (0) . 
(15) 
But for i > 0, all bits of Rain ( †) are nonlinear. Consider the function (r
= r
(16) 
which has the bias of 2 −(i+1) (from Theorem 3). Using the above approximation, we can determine the linear approximation of 
ν ′ t,(i) ⊕ ν ′ t,(i−1) as follows ν ′ t,(i) ⊕ ν ′ t,(i−1) = r t [0] (i) ⊕ r t [16] (i) ⊕ r t [0] (i−1) ⊕ r t [16] (i−1) ⊕r t [1] (i) ⊕ r t [13] (i) ⊕ r t [1] (i−1) ⊕r t [13] (i−1) ⊕ r t [6] (i) ⊕ r t [6] (i−1) , 
(17) 
with the bias of 2.
(2 −(i+1) ) 2 = 2 −(2i+1) . 
In case of Rain, applying approximation (17), for i > 2 the following expression holds 
ν ′ t,(i) ⊕ ν ′ t,(i−1) ⊕ ν ′ t,(i−2) ⊕ ν ′ t,(i−3) = r t [0] (i) ⊕ r t [0] (i−1) ⊕ r t [0] (i−2) ⊕ r t [0] (i−3) ⊕r t [16] (i) ⊕ r t [16] (i−1) ⊕ r t [16] (i−2) ⊕ r t [16] (i−3) ⊕r t [1] (i) ⊕ r t [1] (i−1) ⊕ r t [1] (i−2) ⊕ r t [1] (i−3) 
⊕r t 
⊕r t [6] (i) ⊕ r t [6] (i−1) ⊕ r t [6] (i−2) ⊕ r t [6] (i−3) , (18) 
with bias 2 −(2i+1) (from corollary), when Konst = 0. Next we compute the complexity of CPA on modified NLS. The case for Konst = 0 has been studied at first and then the attack has been extended to Konst ̸ = 0. 
α t,(0) = r t [0] (12) ⊕ r t [15] (22) , 
(19) 
which has been reported in 
and 
(19), we have the following approximation 
r t [0] (12) ⊕ r t [15] (22) ⊕ r t [0] (13) ⊕ r t [15] (23) ⊕r t [4] (0) ⊕ r t+1 [16] (0) , 
(20) 
which has same bias as Equation (19) because remaining terms are linear. Approximation (20) has been divided into two parts: the least significant bits and the other bits as 
l 1 (r t ) = r t [4] (0) ⊕ r t+1 [16] (0) l 2 (r t ) = r t [0] (12) ⊕ r t [0] 
(13) ⊕ r t 
(21) 
Since, l 1 (r t ) contains only least significant bit variables so approximation is true with probability one. From the expression of l 1 (r t ), we obtain the following system of equations. 
l 1 (r t ) = r t [4] (0) ⊕ r t+1 [16] (0) = r t+4 [0] (0) ⊕ r t+17 [0] (0) l 1 (r t+1 ) = r t+1 [4] (0) ⊕ r t+2 [16] (0) = r t+4 [1] (0) ⊕ r t+17 [1] (0) l 1 (r t+6 ) = r t+6 [4] (0) ⊕ r t+7 [16] (0) = r t+4 [6] (0) ⊕ r t+17 [6] (0) (22) l 1 (r t+13 ) = r t+13 [4] (0) ⊕ r t+14 [16] (0) = r t+4 [13] (0) ⊕ r t+17 [13] (0) l 1 (r t+16 ) = r t+16 [4] (0) ⊕ r t+17 [16] (0) = r t+4 [16] (0) ⊕ r t+17 [16] (0) . 
 Adding all the approximations of Equation (22) and applying Equation (15), we get 
l 1 (r t ) ⊕ l 1 (r t+1 ) ⊕ l 1 (r t+6 ) ⊕ l 1 (r t+13 ) ⊕ l 1 (r t+16 ) = ν ′ t+4,(0) ⊕ ν ′ t+17,(0) . 
(23) 
Substituting t = t, t+1, t+6, t+13, t+16 in the expression of 
l 2 (r t 
) and after simplifying we get the following system of equations: l2(rt) = rt
 Combining the set of equations in (24) with Equation (17), we get 
l 2 (r t ) ⊕ l 2 (r t+1 ) ⊕ l 2 (r t+6 ) ⊕ l 2 (r t+13 ) ⊕ l 2 (r t+16 ) = ν ′ t,(12) ⊕ ν ′ t,(13) ⊕ ν ′ t+15,(22) ⊕ ν ′ t+15,(23) . 
(25) 
Combining Equations (23) and 
(25) we get 
l 1 (r t ) ⊕ l 1 (r t+1 ) ⊕ l 1 (r t+6 ) ⊕ l 1 (r t+13 ) ⊕ l 1 (r t+16 ) ⊕ l 2 (r t ) ⊕ l 2 (r t+1 ) ⊕ l 2 (r t+6 ) ⊕ l 2 (r t+13 ) ⊕ l 2 (r t+16 ) = ν ′ t,(12) ⊕ ν ′ t,(13) ⊕ ν ′ t+15,(22) ⊕ ν ′ t+15,(23) 
⊕ν ′ t+4,(0) ⊕ ν ′ t+17,(0) 
= 0. 
(26) 
The bias can be computed using the piling-up lemma. As we use the approximation (20) five times and the approximation (17) twice, therefore the bias of the ap- proximation Therefore, complexity of the attack for the modified NLS is 2 190.6 . Since, the specification of the NLS cipher allows the adversary to observe up to 2 80 keystream words per key/nonce pair 
Case for Konst ̸ = 0 Biases of linear approximations for α t,(0) and NLF decreases with non-zero Konst and it has been explored in 
). Here we have explored only the case where Konst (H) ̸ = 0 and Konst (L) = 0. It has been reported in 
(2 −6.19 ) 5 .(2 −27 
).(2 −47 ) = 2 −98.95 , which is low enough to thwart any linear distinguishing attack. Therefore, It is possible to thwart the crossword puzzle attack 
for y i ⊕ y m is 2 −(m+1) , where 0 ≤ i, m ≤ (n − 1) and m > i. 
Conclusion
In this paper, a new nonlinear, balanced and reversible vectorial Boolean function called 'Rain' has been proposed . It has been shown that the nonlinear property of the Rain improves the resistance against linear cryptanalysis . Resistance of stream cipher NLS against crossword puzzle attack has been improved by replacing the modulo addition by Rain in NLF of NLS. It is shown that modified NLS can thwart the CPA. Also, the proposed function has low bias of best linear approximation for the non-zero linear combination of the output bits. Therefore, improved CPA unlikely to succeed against modified NLS. 
"
"Introduction
Anomaly-based detection system is designed to uncover abnormal patterns of behaviors, in which anything that widely deviates from normal usage patterns will be considered as an intrusion 
Related Works
IDS alarm classification has been an active research area for the past few years, recent researchers have focused on managing the generated alarms to identify real threats from false alarms and to classify the alarms into distinct classes. Several methods have been proposed to analyse the reported alarms based on different classification algorithms and network traffic features 
 3 Feature Selection Based on Attack Scenarios 
 Feature selection is an important step in building intrusion detection and constructing alarm classification modules . During feature selection phase, a set of network traffic attributes or features deemed to be the most effective attributes is extracted in order to construct suitable classification module 
Resource Depletion (DOS)
 An attacker depletes a resource to the point that the target's functionality is affected. The result of a successful resource depletion attack is usually the denial of one or more services offered by the target 
Network Reconnaissance (Probe)
 An attacker engages in network reconnaissance operations to gather information about a target network or its hosts. Network Reconnaissance techniques can range from stealthy to noisy and utilize different tools and methods depending upon the scope of the reconnais- sance 
Spoofing
An attacker interacts with the target in such a way as to convince the target that it is interacting with some other principal and as such take actions based on the level of trust that exists between the target and the other prin- cipal 
 2) Total number of sequences sent during the observation period to different IP's; 
3 13) Number of confirming connection termination re- ceived. 
Exploitation of Authentication
An attacker actively targets exploitation of weaknesses, limitations and assumptions in the mechanisms a target utilizes to manage identity and authentication. Such exploitation can lead to the complete subversion of any trust the target system may have in the identity of any entity with which it interacts. The exploitation of authentication attacks are detectable from the payload data by looking for specific patterns. Some of the attacks are though also detectable from the network traffic by looking for malformed packets that are oversized, fragmented or using, for example, abnormal TCP flag options 
 1) Total number of sequences received during the observation period from different IP's; 
2
Exploitation of Privilege/Trust
An attacker actively targets exploitation of weaknesses, limitations and assumptions in the mechanisms a target utilizes to manage access to its resources or authorize utilization of its functionality. Such exploitation can lead to the complete subversion of any control the target has over its data or functionality enabling almost any desired action on the part of the attacker. Similarly to exploitation of authentication attacks, this type of attacks detectable from the payload data by looking for specific patterns. However, some of the attacks are though also detectable from the network traffic. Therefore, the features that should be monitored for such attacks are as follows 
 2) Total number of sequences sent during the observation period to different IP's; 
3
Network Anomalies Classifier (NAC)
This section presents an A-IDS alarm classification method which relies on machine learning algorithm and attack examples learnt from S-IDS during the training process. The proposed method monitors the network communication pattern and actively extracts the required network traffic features. The proposed system analyse IDS alarms and attempt to classify them based on prelearnt classification model. The classification model is constructed based on attack examples supplied during training phase, during the training phase Snort have been used to provide alarm class definitions of the activities detected by the anomaly detection system. The proposed system is represented by the Network Anomalies Classifier (NAC) module depicted in 
Packet Features Extractor (PFE)
Network traffic contains features that are redundant or their contribution to the classification process is little. Therefore, it is essential to choose among the data what is relevant to consider and what is not 
Packet Features Selection
After analysing the features from the attack scenarios point of view and what have been utilized in the literature , it seemed that the features used by 
Anomaly Classifier Engine (ACE)
 The anomaly classifier engine is responsible for automatically classify the detected activity and determine the attack class, based on predefined set of patterns of known attack mechanisms that are defined in the CAPEC and CVE databases. The PFE monitors network traffic flow and extracts traffic flow features to generate alarm metainformation as a vector representing symptoms vector. The symptoms vector is then passed to the anomaly classifier engine that automatically determines the attack class. The development of ACE goes through two stages. First, the ACE is trained with several types of attack symptoms vectors. Then, when the training is completed, the ACE is ready to classify new incoming alarms auto- matically. During training phase, a signature-based IDS is deployed next to the A-IDS such that the two systems monitor the exact network traffic as illustrated in 
Machine Learning Algorithm Selection
The choice of which specific learning algorithm should be used is a critical step. The classifier's evaluation is most often based on classification accuracy (the percentage of correct classifications divided by the total number of events in the data set). There are various techniques available used to calculate a classifier's accuracy. One technique is to split the training set by using two-thirds for training and the other third for estimating mance. In another technique, known as cross-validation, the training set is divided into mutually exclusive and equal-sized subsets and for each subset the classifier is trained on the union of all the other subsets. The average of the error rate of each subset is therefore an estimate of the error rate of the classifier. If the error rate evaluation is unsatisfactory, the selected features must be re-examined. 
Since the attack class and the related meta-information can be obtained, only supervised machine learning 
 It is required that, the machine learning algorithm is able to develop the classification model in a small number of data set, to decrease the amount of alarms required. 5 Having an explicit underlying probability model 
The machine learning algorithm should be based on statistical approaches, which provides a probability that an instance belongs in each class, rather than simply a classifica- tion. 6 Developed for academic re- searches 
Because machine learning is beyond the scope of this work. the variability information in the data. Thus, number of axis rotations takes place to form the new features for a base classifier. The proposed rotation forest ensemble have been evaluated on a selection of 33 benchmark data sets from the UCI repository and compared it with Bagging , AdaBoost, and Random Forest. The classification accuracy was more accurate than in AdaBoost and Random Forest, and more diverse than these in Bagging as well. 
PART 
Evaluation Results of NAC
 This section presents the evaluation results of the proposed network anomaly classifier. First, it describes the dataset employed and then the evaluation results are pre- sented. 
Evaluation Dataset
 The selected machine learning algorithms have been evaluated against five different datasets. The evaluation was based on the classification accuracy using the defined network traffic features. The datasets contain network traffic features representing network state during alarms identified by security analyst or raised by signature-based IDS (attack only dataset); each dataset contains number of instances representing network traffic audit records during a detected malicious activity as shown in 
Dataset B: contains 1350 instances, having majority of dos attacks and some other attacks randomly selected , this dataset represents a scenario when an attacker uses probe and remote to user attacks to cause network resource unavailable to its intended users, which is common in real scenarios. 
Dataset C: include 2790 instances represents a scenario when an attacker uses probe and remote to user attacks with dos to gain root privileges. Therefore, the dataset have a majority of user to root attacks. The occupancy ratio of denial of service attacks and remote to local attacks is nearly 1:2.  Dataset E: This dataset contains 5571 instances randomly collected, having a majority of u2r attacks. The occupancy ratio of denial of service attacks and remote to local attacks is nearly 2:1. 
Evaluation Result
Three performance metrics have been used for machine learning comparison, classification accuracy, Precision and F-Measure. The performance of the selected machine learning algorithms have been conducted by training and testing with above five datasets to show its performance in different scenarios. However, there are four factors which influence the classification accuracy; the number of samples (alarms) processed during training phase, the frequency distribution of the alarms, the machine learning used and the network traffic features used. 
Conclusions and Future Works
 In order to estimate the risk of unknown attacks, a solution to automate the classification of anomaly-based alarms is required. However, So far no effective and efficient automatic or semi-automatic approach is currently available able to classify anomaly-based alarms at time. Thus, any anomaly-based alarm must be manually processed to identify its class; this may increase the workload of security analyst, and will effectively increase time required; as well as, the dependence on security analysts. This paper presents Network Anomalies Classifier (NAC) that uses machine learning technologies to automatically classify activities detected by a packet headerbased anomaly detection system. The concept behind the proposed methodology is that, attacks those share some common network traffic flow behaviors are usually in the same class. Based on the available information from the literature it seems that an efficient attack classification can be done by using the network traffic flow information. Recent researches showed that network traffic flows could improve the accuracy of attack classification. Therefore, the network traffic flow method has been used in this work to monitor network behaviour. Thus by extracting traffic flow sequences triggered by certain attack, it is possible to compare those sequences to previously collected data using machine learning algorithm, then to infer the attack class from the matching sequences. Two approaches were used to select the relevant features from the network traffic. Initially, an analysis of what information the field of literature holds on this topic; then an evaluation of different attack scenarios and how they affect the network traffic behaviour have been prepared . The most suitable traffic flow features are selected by handpicking from the feature spectrum based on the prior knowledge about the environment that the IDS is monitoring and the analysis of known attack types. In this work, five machine learning algorithms have been considered as follows; Random Committee, Rotation Forest, PART, Random Forest and Random Tree. Evaluation experiments showed that machine learning algorithms are capable of classifying malicious activities in an effective and efficient means. However, a too low number of samples could generate an inaccurate classification. Therefore, as the number of training samples increases, accuracy increases. Based on the evaluation experiments results, it can conduct that Random Committee and Random Tree perform better than other algorithms and their detection accuracy almost identical, but the precision of Random Committee is higher than Random Tree. Therefore , as future works random committee algorithm will be used to classify the detected activities to estimate the security risk level. architecture, programming language, communication and distributed, formal method. 
Mohd Juzaiddin Ab Aziz received his PhD degrees 
in Computer Science from University Putra Malaysia (UPM). Currently, he is the deputy dean of undergraduate studies in faculty science and information technology, UKM. His specialization in programming language technology and natural language processing. 
"
"Introduction
 In Cloud computing, resources may be simultaneously accessed by multiple members. Here the resources may be considered as database, CPU, storage, applications. The members can be overlapped to access the above resources. Other example HDTV, users can subscribe to various layers such as base layer, medium layer and enhanced layer channel. Users which subscribed medium layer can watch HDTV base layer as well as medium layer channel . User which subscribed enhanced layer can watch base layer, medium layer as well as enhanced layer channel. In existing group key management, single separate key tree is built to form group key even if members are overlapped to access the resources. Member has to maintain keys for each key tree. The solution to this is to form the combined key trees for resources which containing common members. To form the group key, TGDH protocol is used 
2) Algorithms: Single, batch join, single, batch leave; 
3) Formulation of computational cost; 4) Computation cost analysis of resource key formation for separate key trees and combined key tree in terms of number of modulation exponentiation operations and sequential operations. 
The paper is organized as follows. In Section 2, we present about resource key tree. Section 3 presents combining key trees algorithm, Section 4 covers Results and Analysis, Section 5 presents conclusion. 
Resource
Initializations
 Let Resource group R = {R 1 , R 2 , R 3 , R 4 , · · · , R n }. Consider two resources R1 and R2. R1 = {m 1 , m 2 , m 3 , m 4 , · · · , m n } 
 be the members accessing resource R1. 
R2 = {n 1 , n 2 , n 3 , n 4 , · · · , n n } 
be the members accessing resource R2. 
 2.2 Resource Tree For Group Key For- mation 
@BULLET Each member forms the public key called as blinded key. In this case, g is generator, p is prime number. @BULLET Member blinded keys are g α1 mod p, g α2 mod p, g α3 mod p, g α4 mod p. @BULLET Each member with its key and sibling blinded key forms intermediate key. For that path from leaf node to root node is traversed. @BULLET Resource group key is formed as below. g α1α2α3α4 mod p. Number of modular exponential operations required when four members are equal to 12; In general, when number of members are N , the modular exponential operations are equal to N + N log 2 N = N (1 + log 2 N ). 
Resource Membership Matrix
Every member that accesses the resource, entry is made in resource membership matrix also for any member that joins/leaves in single or batch makes. Resource matrix contains the following entries. Rows represents members {m 1 , m 2 , 
m 3 , · · · , m n } and columns represents resources {R 1 , R 2 , R 3 , · · · , R n }     1 0 · · · · · · 1 1 · · · · · · 1 0 · · · · · · 1 · · · · · · · · ·     
It shows that there are R1, R2, · · · , Rn resources. Member m 1 accesses resource R1 while member m 2 accesses resource R1 and R2, i.e. overlapped to access the resources R1 and R2. 
Combining Resource Key Trees Algorithm
In existing key management algorithm [5, 8, 9, 10, 11, 14, 15] seperate key tree is built for each resource, even if same members are accessing multiple resources. Thus we can combine multiple resource key trees. Algorithm 1 illustrates combining resource key trees algorithm. Computation cost analysis is given in Section 4. 
Algorithm 1 Combining resource key trees algorithm 1: Begin 2: Let R be the set of resources 
R 1 , R 2 , R 3 , R 4 , · · · , R n ; 3: Let M = {m 1 , m 2 , m 3 , · · · , m n } 
be set of members; 4: Each member keep the track of members through resource access matrix; Rows represents members {m 1 , m 2 , 
m 3 , · · · , m n } and columns represents resources {R 1 , R 2 , R 3 , · · · , R n }:     1 0 · · · · · · 1 1 · · · · · · 1 0 · · · · · · 1 · · · · · · · · ·     
5: Identify the members which are overlapped to access multiple resources; 6: Build the key tree of overlapped members. Maintain the following entries in 
Computational Cost for Group Key Formation
 There can be multiple members overlapping to any resources . Let MEO denotes Modular Exponential Operations ; RMM denotes Resource Membership Matrix. MEO after combining key trees is equal to MEO for separate key trees minus MEO due to overlapping members: M EO f or separate key trees 
= k i=1 N i (1 + log 2 N i ) 
and where N is equal to number of members per key tree; k is equal to total number of key trees; T index is equal to number of entries in 
RT count is equal to total resource count per entry; CM is equal to number of members overlapped per entry. It is observed that computation cost in terms of number of modular exponential for separate key tree is O(N ) while for key trees combined is O(N − RCm). Thus we can observe that number of modular exponential operations required in separate key trees is more, i.e. RCm compared to the combined key trees. 
1) Best case complexity in terms of MEO is when all members of resource groups are overlapped to access the resources. 
2) Worst case complexity in terms of MEO is when members of resource groups are not overlapped to access the resources. 
Single Member Join
Algorithm 2 explains algorithm for a member single join. In single member join algorithm, it requires two mes- sages: 
1) Message from member for accessing the resource. 
2) Message from sponsor to send the blinded key to form the group key.  1) Joining member which is not going to be overlapped , it is added as per TGDH. 
2) Joining member which is already accessing other resource (overlapping) becomes the sponsor. 6: Joining member which is overlapped forms key graph of resources. 7: Non overlapping member maintains its own key tree. 8: Sponsor computes the blinded key and broadcasts it. 9: Group key establishment as in Section 2.2. 10: End 
Batch Join
There can be members which simultaneously access the resources 
 2) Some members in group may access the multiple resources at particular instant of time. 
Finding Suitable Position for Common Members in Tree
After building subtree of the overlapping members,it is inserted at the root of the tree to minimize the height of the tree. 
Sponsor Selection for Batch Join
 Sponsor is overlapped member which accessing the resources . Otherwise it is selected as TGDH approach 
Single Leave
Algorithm 4 explains algorithm for a member single leave.  In single member leave algorithm, it requires two mes- sages: 1) Message from member for non-access the resource/s. 
2) Message from sponsor to send the blinded key to form the group key. 
Batch Leave
There can be members which simultaneously access the resources and completes access of resources. In these members, 1) Member may finishes access of single resource; 
 2) Members may finishes access of the multiple re- sources. 
Algorithm 5 explains algorithm for members Batch leave. 
Algorithm 5 Batch leave 1: Begin 2: Leaving members broadcasts that it is leaving from particular resource as overlapping members in resource groups or as per TGDH if not overlapping ex- ists. 3: If the leaving member is itself sponsor, sponsor selec- tion. 4: Each member of resource/s including sponsor notices it and makes these entries in resource membership matrix. 5: Build the key graph as per Algorithm 1. 6: Sponsor computes the blinded key and broadcasts it. 7: Group key establishment as in Section 2.2. 8: End 
Sponsor Selection in Batch Leave
If the sponsor is leaving member, sponsor is selected as one of the overlapping member. If no overlapping member exists, sponsor is selected as per TGDH 
MEO 2 0 2 (N − n)(1 + log 2 (N − n)) 
Results and Analysis
Analysis is done by taking resources, varying members size. From 
Conclusions
In Cloud computing, resources such as CPU, VM, database, storage and applications are simultaneously accessed by multiple members. The members can be overlapped to access the above resources. Group key is formed by contributing share of each members with TGDH approach . In existing group key management, single separate key tree is built to form group key even if members are overlapped to access the resources. From the result and analysis, it is observed that there is reduction of computation cost more than 22% when resource key trees are combined. 
"
"Introduction
 Denial of service (DoS ) or Distributed DoS (DDoS ) attacks have become one of the most severe network attacks today. Though relatively easy to be executed 
Existing Algebraic Marking Scheme
Dean, Franklin and Stubblefield proposed an algebraic marking scheme 
f (x) = a n + a n−1 x + a n−2 x 2 + ... + a 0 x n = a n + (a n−1 + (a n−2 + ... + (a 1 + a 0 x)x...)x)x. 
They used Fullpath and x to denote the two values. In general, an attack packet will pass through a number of routers before reaching the victim. The first router that decides to make a marking determines a value for x and let Fullpath be the value of its IP address represented by a 0 . Then the next router computes its Fullpath value by multiplying the Fullpath value (from the packet) by x, and adding its IP address (represented by a 1 ). The following routers mark the packet in a manner similar to what the second one did. When the packet arrives at the victim, it records a Fullpath value related to a path formed by a number of routers. In fact, it is the value of the above polynomial with the IP addresses of the routers represented by a i 's and the highest exponent (i.e. n) of the unknown x. Note that there is no way for a router to know whether it is the "" first "" participating router on a particular path; so it has to adopt a coin-flipping methodrandom full (or partial ) path encoding to solve this problem . The router flips a coin and if it comes up tails the router will assume it is not the first router and simply follows the algorithm as presented above; otherwise the router will select an x for marking this packet and do the marking in the capacity as the first router. With this packet marking method, each marked packet received by the victim represents a polynomial. Each polynomial represents one suffix of the whole path. Because the selection of the first marking router is random, the degree of the polynomial is not fixed. They pointed out that with the recent advances in coding theory such mixed data problem could be solved to identify the paths if there are enough marked packets. However, their approach is not powerful enough for dealing with distributed DoS attacks because at present there is no effective means to find out those packets which have traversed to the victim via the same path; it also requires a huge number of packets to reconstruct the multiple paths. 
Proposed Marking Scheme
 In this section, we introduce our improved algebraic marking scheme in detail. Unlike the algebraic marking scheme of Dean, et. al., our proposal does not require the use of sophisticated mathematical techniques for paths reconstruction , because we have improved the underlying packets marking procedure. We exploit the idea of probabilistic packets marking (i.e. to mark the packets with a low probability) scheme 
Definitions and Assumptions
An upstream routers map describes the topology of the upstream routers of a single host. We assume the upstream routers map captures the IP addresses of the routers. 
A 1 R 6 R 3 R 2 R 1 )
 , the distance between router R 6 and the victim is 3. Some routers might be compromised by the attacker and they would mark fake information in the packets. Therefore, we limit the traceback problem to finding a candidate attack path that contains a suffix of the real attack path, and such a suffix is called valid suffix of that path. For example, the path (R 3 R 2 R 1 ) is a valid suffix of the real attack path (A 1 R 6 R 3 R 2 R 1 ). We say a traceback technique is robust if the attackers cannot prevent the victim from finding the candidate paths containing the valid suffixes of the attack paths. We say that a router is a false positive if it is in the reconstructed attack path but not in the real attack path.  For practical considerations, we make the following assumptions , some of them being similar to those outlined in 
R 8 A 1 R 9 R 10 V R 5 R 6 A 2 R 7 R 3 R 4 R 2 R 1 
2) Multiple attackers may coordinate their attack. 
3) Packets may be reordered or lost. 
4) The routes between the attack sources and the victim are fairly stable. 
5) The routers have limited CPU and memory resources and cannot do too much processing per packet. 6) Attackers might be aware that they are being traced. 
7) The markings in a packet may be modified by the attacker. 
8) The source address of a packet may be forged. 9) Routers are not compromised widely and the routers adjacent to the victim should not be compromised. 
10) The packet size should not grow as a result of tracing. 
 Assumptions 1 to 8 reflect the ability of the DoS attackers and the weakness of the current network infrastructure . Sophisticated attackers could detect that they are being traced and might send fake packets to confuse the victim. So any IP traceback algorithm designer should be aware of such a potential ability of the attackers . Similar to the probabilistic marking scheme proposed in 
Improved Algebraic Marking Scheme
 Our proposed marking scheme is presented below. Before introducing the packets marking algorithm, and the attack paths reconstruction algorithm, we first introduce the underlying basic mathematical theory. 
    1 x 1 x 2 1 · · · x n−1 1 1 x 2 x 2 2 · · · x n−1 2 · · · · · · · · · · · · · · · 1 x n x 2 n · · · x n−1 n         A 1 A 2 · · · A n     =     F ullpath 1 F ullpath 2 · · · F ullpath n     
Basic Mathematical Theory
The above is a matrix equation (or system of equations) with Vandermonde matrix coefficients. In linear algebra, there is a theorem stating that the above matrix equation, with A i 's unknown, has a unique solution if and only if the x i 's are distinct 
    1 x 1 x 2 1 · · · x 7 1 1 x 2 x 2 2 · · · x 7 2 · · · · · · · · · · · · · · · 1 x 8 x 2 8 · · · x 7 8         A 1,1 A 1,2 · · · A 2,4     =     F ullpath 1 F ullpath 2 · · · F ullpath 8     
The matrix equation now represents 8 polynomials which encode an edge formed by two adjacent routers, referred to as the first (or start) router and second (or end) router; where A 1,1 ...A 1,4 and A 2,1 ...A 2,4 represent the four IP address fragments of the first router and the four IP address fragments of the second router respectively ; x 1 ...x 8 represent 8 distinct random integers, one for each marked packet. split a router R i 's IP address into c identical fragments, and use A i,j (j = 1, 2,...,c) to denote the value of each fragment. For example, if router R 1 's IP address is 137.189.89.101 and we split it into 4 fragments, then A 1,1 = 137, A 1,2 = 189, A 1,3 = 89, and A 1,4 = 101. Using c equal to 4 is an eclectic choice while considering the bits needed to store the Fullpath value, and the reconstruction time. The idea behind the proposed packet marking is similar to edge sampling. Consider a packet being marked respectively by any two consecutive routers R i and R j ; that is, R i and R j would become the start router and end router of the edge respectively in the marking. Router R i may compute the Fullpath as follows: 
F ullpath = (A i,1 + A i,2 x + A i,3 x 2 + A i,4 x 3 ) mod p. 
Then router R j may compute the Fullpath for the edge as follows: 
F ullpath = (F ullpath + A j,1 x 4 + A j,2 x 5 + A j,3 x 6 + A j,4 x 7 ) = (A i,1 + A i,2 x + A i,3 x 2 + A i,4 x 3 + A j,1 x 4 +A j,2 x 5 + A j,3 x 6 + A j,4 x 7 ) mod p, 
 where p is the smallest prime number larger than 255(2 8 - 1), i.e. 257. If R i is adjacent to the victim, the last 4 terms of Fullpath for R j would be omitted. The aim of mod p in the above formulae is to reduce the value of Fullpath so that it would occupy fewer bits in the IP header. 
Packet marking procedure for each packet P { generate a random number u [0, 1) ; if (u q ) { // start router R' // q is the marking probability of each router P.distance = 0; select an integer x in the range 0..7; P.x = x ; // each packet P is assigned one value of Figure 3: Packet marking illustration at start or end router.F and d denote Fullpath and distance respectively, 
x Fullpath = (A 1,1 + A 1,2 x + A 1,3 x 2 + A 1,4 x 3 ) mod p; } else { if (P.distance == 0) {// end router R Fullpath = (Fullpath + A 2,1 x 4 + A 2,2 x 5 + A 2,3 x 6 + A 2
v = A 1,1 + A 1,2 x + A 1,3 x 2 + A 1,4 x 3 , v = A 2,1 + A 2,2 x + A 2,3 x 2 + A 2
,4 x 3 ; R is an upstream router of R the value of A 1,1 + A 1,2 x + A 1,3 x 2 + A 1,4 x 3 for router R , where A 1,i 's( i = 1, 2, 3, 4) are the 4 fragments of the IP address of R . When router R receives a packet from its upstream router R , it first generates a random number u and performs packet marking depending on the value of u, and the distance d from the packet. As an example, let the IP address of router R be 192.168.10.5 and the values of (F, d, x) from the packet being marked are (133, d, 2). Router R would first generate a random number u. Then the marking algorithm would produce one of the 3 possible outcomes: @BULLET Case 1 (u ≤ q): Suppose the randomly selected x is 3. 
Then, @BULLET Case 2 (u > q&d = 0): Assume d from packet is 0. When 8 (or 4) packets with distinct x s arrive at the victim, the victim can solve the relevant matrix equation in Section 3.2.1 to obtain the IP addresses (or address) of two adjacent routers (or the nearest router to the victim ) in the attack path. Therefore, we use a set of 8 distinct x s (0-7) to do the marking. The distance field in the packet indicates the number of routers the packet has traversed from the router which first marked the packet (without being re-marked afterwards) to the victim. An attacker could insert fake marks which may remain unchanged in the packets. Using such a scheme, any packet written by the attacker will have distance field greater than or equal to the length of the real attack path because of the mandatory increment of the distance field. The distance field allows a victim to check if the distance value corresponds to the actual distance of the particular edge, represented by the mark, to the victim. So, with the help of an upstream internet map, and the distance field, the victim can easily check if the edge, represented by the packet mark, in the map and the distance value are compatible during the hop by hop reconstruction process. If they are not compatible, it implies that the marking is a fake one. Thus the inclusion of the distance field helps to improve the robustness of the proposed method. 
Attacks Paths Reconstruction
There is no simple means to group the packets coming from the same path. It will involve a high computation overhead if we check all possible combinations of the marked packets similar to the probabilistic marking scheme 
path = (A 1,1 + A 1,2 x + A 1,3 x 2 + A 1,4 x 3 ) mod p. 
Then search for a packet from P 0,x with Fullpath equal to the computed path value. If there are 4 packet subsets each having at least one packet with Fullpath equal to the path value, we can conclude that the selected router is on one of the attack paths and insert it in the reconstructed attack graph. 
path = (A 1,1 + A 1,2 x + A 1,3 x 2 + A 1,4 x 3 + A 2,1 x 4 + A 2,2 x 5 + A 2,3 x 6 + A 2,4 x 7 ) mod p. 
If path is equal to Fullpath from any packet in P i,x , we move to another packet subset P i,x+1 . If there is no single packet in P i,x having a Fullpath value equal to path, we can declare that the selected router is not on the attack paths involving routers in this layer (it could be on the 
Reconstruction algorithm 
/* Let M denote the upstream routers map; Let G denote the reconstructed attack graph and be initialized with one node V for the victim; Let P d denote a set of packets with distance d (0 d maxd) and P d,k denote a subset of P d with x = k; maxd is the distance from the furthest attack source to the victim; */ for each direct upstream router R of V in M { count = 0; k = 0; while (count <4 && k < 8) { x = k; path = (A 1,1 + A 1,2 x + A 1,3 x 2 + A 1,4 x 3 ) mod p // A 1,j (j = 1, 2, 3, 4) form the IP address of R // x and Fullpath are from the packet for each packet in P 0,k { if (path ==Fullpath){ count=count+1; quit this loop; } k=k+1; } if (count == 4) insert R into G next to V; } for d = 1 to maxd for each router R inserted into G in the last loop { for each upstream router R' of R in M{ k = 0; while (k < 8){ x = k; found = false; 
path = (A 1,1 + A 1,2 x + A 1,3 x 2 + A 1,4 x 3 + A 2,1 x 4 + A 2,2 x 5 + A 2,3 x 6 + A 2,4 x 7 ) mod p // A 1,j (j = 1
, 2, 3, 4) form the IP address of R' // A 2,j (j = 1, 2, 3, 4) form the IP address of R for each packet in P d,k { if (path == Fullpath ) {k = k + 1; found = true; quit the present for loop} } if not found {quit while loop}; if (k == 8) insert R' into G next to R; } }} Output the reconstructed attack paths from graph G 
P d,x d F x 0 F 01 0 … … … P 0,0 0 F 0a 0 0 F 11 1 … … … P 0,1 0 F 1b 1 … … … … 0 F 71 7 … … … P 0,7 0 F 7h 7 R 2 R 1 V (A 1,1 , A 1,2 , A 1,3 , A 1,4 ) 
Figure 5: Reconstruction Illustration 1. F and d denote Fullpath and distance respectively. R 1 , R 2 are upstream routers of V paths involving other layers). If each of the 8 packet subsets has at least one packet with its Fullpath value equal to path, we can conclude that the selected router is on one of the attack paths and insert it into the reconstructed Figure 6: Reconstruction illustration 2. F and d denote Fullpath and distance respectively. R k is an upstream router of R j attack graph. With the proposed reconstruction algorithm, we can reconstruct multiple attack paths by examining the routers on the victim's upstream routers map, starting from the routers adjacent to the victim, and adding routers to the reconstructed attack graph hop by hop until the ends of the paths have been reached. Note that to identify each router nearest to the victim on an attack path, four packets are used; whereas to identify two adjacent routers, eight packets are used. 
R k P d,x d F 
Analysis
The evaluation of a marking scheme for IP traceback is normally based on a number of parameters, including number of false positives, minimum number of packets needed to reconstruct each path, marking and reconstruction overheads, backward compatibility, etc. In the following sub-sections, we analyze our proposed IP traceback method based on the above-mentioned parameters. 
Number of Positives
The most prominent strength of our marking scheme is that no false positives are generated by the attack paths reconstruction algorithm. Any two routers with distinct IP addresses cannot yield the same Fullpath value for their packets having the same set of values for x s; in addition, any two edges formed by a router R and any two of its immediate upstream routers R 1 and R 2 will not have same Fullpath value in their packets. Therefore , the reconstruction algorithm will never include any irrelevant router in an attack path. Moreover, the unique paths traced by the proposed method can be proved mathematically because a Vandermonde matrix equation has a unique solution as long as distinct values of x s are used in solving the equation (Section 3.2.1). Many other marking schemes produce a certain amount of false positives; for instance, some of them employ hash functions for encoding purpose, which could have a collision problem; that is, they could have the same hash value for two different IP addresses. The algebraic marking schemes proposed by Dean et al. could also have some amount of false positives (refer to 
Minimum Number of Packets
 As the minimum number of packets required to reconstruct an attack path is path independent, it can be analyzed based on a single attack path. Suppose we split an IP address into c identical chunks and the distance from the attacker to the victim is d. As mentioned above, we need c packets to identify each router adjacent to the victim and 2c packets to identify each upstream edge formed a pair of routers. For each edge, the victim should receive at least 2c packets with markings of the edge for attack path reconstruction. If the marking probability is q, we need at least 2c/(q(1 − q) d−1 ) packets. For example, with c, d, and q equal to 4, 20, and 0.01 respectively, the minimum number of packets needed would be 968. We can also evaluate an upper bound for the expected number of packets for path reconstruction. The probability that a router receives a packet having a marking with a distance d is q(1 − q) d−1 . Suppose the attack path length is D. We can conservatively estimate the probability of a packet marked with a distance d < D to be q(1 − q) D−1 . Since the victim needs at least 2c packets marked with distinct values of x and distance from 0 to D − 1 for reconstructing the entire path, based on the well-known coupon collector problem 
E(N ) < 2cln(2cD) q(1−q) D−1 
where E(N ) denotes the expectation of the number of packets needed for attack path reconstruction. For example, with c = 4, D = 20, q = 0.01, the upper bound expectation of the number of packets needed for path reconstruction would be 4242. The experimental results presented in Section 5 show that, for this case (c=4, D =20, q=0.01), the number of packets needed for path reconstruction, with a success probability of 95%, is around 3500, which is smaller than the expectation. It is obvious that a larger value for q(1 − q) D−1 implies a smaller value for E(N ). In addition, it can be shown that when q is 1/D, E(N ) reaches a minimum; and as long as q is smaller than 1/D, the value of E(N ) differs by only a small amount, and q should not be smaller than 1%. In our implementation, the x values are selected by each router in such a way that it can mark packets with markings using the 8 different values (0..7) of x relatively quickly. In this way, fewer marked packets would be required for attack paths reconstruction. Since neither any concrete packet marking schemes and attack paths reconstruction algorithms nor any implementation details were provided by Dean et al. 
Multiple Attacks
 A distributed DoS attack normally involves a huge number of packets being sent from multiple attack sources under the control of the attacker. The proposed packets marking algorithm performs packets marking in such a way that the attack paths reconstruction algorithm does not need to discern the packets by the paths through which they traversed to the victim. With the help of the victim's upstream routers map, it can uniquely identify any upstream edge formed by two adjacent routers on each path during attack paths reconstruction. Therefore , the proposed marking scheme is effective for tracing multiple attacks. Dean et al. did not mention in 
 4.4 Marking and Reconstruction Over- heads 
The packet marking algorithm as shown in 
Backward Compatibility
Backward compatibility is an important issue concerning whether the proposed method can be put into practice. As our marking scheme involves writing some information to the IP header of a packet, we should find out the maximum number of bits available in an IP header that can be used to store the markings. The total number of bits b needed to store the markings can be estimated by: log 2 (p)+ log 2 (d)+ log 2 (n); the three terms estimate the bits to store Fullpath (a value less than p), distance, and x respectively. In practice, we can set c, d, p, and n as follows: c = 4, d = 32, p = 257, n = 2c = 8. Then the total number of bits b would be 17. The reason for setting n equal to 2c is that each Fullpath value is related to 2c fragments of two IP addresses. As long as there are 2c packets with distinct values of x, the next hop router can be identified. Therefore, 3 bits have been used to represent 8 distinct values of x. There is a tradeoff between the number of packets needed for paths reconstruction and the number of bits for the markings, which depends partly on the number of IP address fragments, c. A smaller c implies: i) fewer packets and a shorter time would be required for attack paths reconstruction; ii) more bits would be needed since the value of each IP address fragment would be larger. Though the range of distinct values for x would be smaller, the total number of bits needed would be larger. As the number of bits available in the IP header that can be used to store the markings is very limited, we eclectically choose c equal to 4 in our implementation. Since almost any packet can reach its destination through no more than 32 hops 
Simulation Results
 We have performed a good number of simulation experiments to examine the feasibility and to assess the performance of our marking scheme. The primary objective of the experiments is to examine the following parameters related to the performance of the marking scheme: the number of false positives, the minimum number of packets needed for reconstruction, the reconstruction time, etc. We prepare for the simulation experiments an upstream routers map with over 2000 routers. The routers are assigned some real IP addresses obtained from the Internet by using the traceroute technique. The attack paths are randomly chosen from the paths in the map; and different numbers of packets are generated and transmitted along each of these paths respectively. Each router simulates marking any packets it receives, according to our packet marking algorithm. After collecting sufficient number of marked packets, the victim simulates reconstructing the attack paths according to our proposed reconstruction algorithm . The experiment results show that the proposed marking scheme is feasible and the performance is satis- factory. Figures 8 and 9 present two plots showing the minimum number of packets, required for reconstruction, sent by the attacker along any single path for two different marking probabilities 4% and 1% respectively, assuming the reconstruction success probability being 95%. As expected , with a smaller marking probability q, more packets would be needed for attack paths reconstruction. Each data point in each of the plots corresponds to an average of the data values obtained from over 300 independent experiments for a certain path length. The experiment results on the minimum number of packets needed for paths reconstruction have been compared with those presented in FMS 
Conclusion
 The algebraic marking scheme proposed in this paper simplifies and improves on the algebraic marking scheme proposed by Dean et al. 
"
"Introduction
Digital watermarking has been widely used to protect the copyright of digital images. In order to strengthen the intellectual property right of a digital image, a trademark of the owner could be selected as a watermark and embedded into the protected image. The image that embedded the watermark is called a watermarked image. Then the watermarked image could be published, and the owner can prove the ownership of a suspected image by retrieving the watermark from the watermarked image. According to the retrieved results, we can determine the ownership of the suspected image. Generally, a practical and useful watermarking scheme has to meet the following require- ments 
2) Imperceptibility: A watermark can be embedded into an image as either visible or invisible. The visible watermark is perceptible and is just like noise. It mostly can be removed by a noise removing process. In order to decrease the risk of cracking, most of the proposed watermarking schemes are invisible. On the other hand, the quality of the watermarked image is also important. If the watermark embedding process seriously affects the quality of the watermarked image, the watermarked image will draw the attention of attackers or even lose its value. Therefore, the quality between the original image and the watermarked image should not be seriously degraded. The property is called imperceptibility. 
3) Readily Embedding and Retrieving: The watermark must be able to be easily and securely embedded and retrieved by the owner. Therefore, the overheads of embedding process and retrieving In recent years a special kind of digital watermarking is discussed widely, called reversible watermarking. It not only provides the protection of the copyright by embedding the assigned watermark into the original image but also can recover the original image from the suspected image . The retrieved watermark can be used to determine the ownership by comparing the retrieved watermark with the assigned one. Similar to conventional watermarking schemes, reversible watermarking schemes have to be robust against the intentional or the unintentional attacks, and should be imperceptible to avoid the attraction of attacks and value lost. Therefore, the reversible watermarking also has to satisfy all requirements of the conventional watermarking such as robustness, imperceptibility, and readily embedding and retrieving. Except for these requirements, reversible watermarking has to grantify the following two additional requirements. 1) Blind: Some of the conventional watermarking schemes require the help of an original image to retrieve the embedded watermark. However, the reversible watermarking can recover the original image from the watermarked image directly. Therefore, the reversible watermarking is blind, which means the retrieval process does not need the original image. 
2) Higher Embedding Capacity: The capable size of embedding information is defined as the embedding capacity. Due to the reversible watermarking schemes having to embed the recovery information and watermark information into the original image, the required embedding capacity of the reversible watermarking schemes is much more than the conventional watermarking schemes. The embedding capacity should not be extremely low to affect the accuracy of the retrieved watermark and the recovered image. 
 The procedure of conventional and reversible watermarking schemes can be illustrated by using the flowcharts in 
1) The schemes by applying data compression, such as 
2) The schemes by using difference expansion, such as 
3) The schemes by using histogram bin exchanging, such as 
 The detailed descriptions of the three types are introduced in Sections 2, 3, and 4, respectively. The features and comparisons are analyzed in Section 5. Some research issues are discussed in Section 6, and the conclusions are in Section 7. 
Reversible Watermarking Schemes by Applying Data Compression
 In order to recover the original image from the watermarked image, an intuitional strategy is to embed the recovery information into the original image. Except for the recovery information, we also have to embed the data of watermark into the original image. Therefore, the capacity of the embedded information is much more than the conventional watermarking schemes. In order to embed more data into the original image, a straight solution is to compress the embedding data. There are several watermarking schemes 
 1) Every pixel is quantified by using the following Llevel scalar quantization, and the corresponding remainders are generated: 
Q L (x) = L × x L . 
For example, allow the 4 × 4 block of original image be 
H =     
 3) To concatenate the L-ary converted watermark information after the compressed remainders. 
For the same example, watermark W is converted from {10 0010 1011} 2 to {4 2 1 0} 5 , and becomes {x 0 , x 1 , . . . , x 11 , 4, 2, 1, 0}. 
4) The watermarked image is generated by adding the compressed data and the watermark to the quantified image. Finally, we gain the watermarked image In the retrieving and recovering process, the first step of embedding process is applied again in the suspected image. The first twelve digits of the remainders can be decompressed to the 16 original remainders and the last four bits is the hidden watermark. Therefore, the watermark can be retrieved to verify the ownership and the original image can be recovered. The processes of this scheme is shown in 
Reversible Watermarking Schemes by Using Difference Expansion
The second type of reversible schemes is using difference expansion to embed information. So far, several schemes 
l = (x + y) 2 , (1) 
where x and y are two adjacent pixels. The inverse integer transformation can be represented as 
x = l + (h + 1) 2 , and (2) y = l − h 2 , (3) 
where 
h = x − y. 
(4) 
The processes of Tian's scheme are described as follows.  1) Calculating l i and h i for the i-th pair of pixels, denoted as {x i , y i }, by using Equations 1 and 4. Using the same example in the above. 
If w 0 = {0} 2 , then h 0 = 6 × 2 + 0 = 12. If w 0 = {1} 2 , then h 0 = 6 × 2 + 1 = 13. 
3) The parameters l i and h i are used to substitute for the parameter l i and h i in Equations 2 and 3 to get 
x i and y i . 
Continuity, if the embedded bit of watermark is 1, 
then x 0 = l 0 + (h 0 +1) 2 = 103 + (13+1) 2 = 110 and y 0 = l 0 − h 0 2 = 103 − 13 2 = 97
. Similarly, if the embedded bit of watermark is 0, 
then x 0 = l 0 + (h 0 +1) 2 = 103+ (12+1) 2 = 109 and y 0 = l 0 − h 0 2 = 103 − 12 2 = 97. 
 4) Repeat the former steps until all pixel pairs are pro- cessed. 
5) The watermarked image can be constructed by using each x i and y i . 
In Tian's scheme, h represents the generated feature of the original image and it is expanded to embed information . The flowcharts of Tian's scheme is illustrated in 
(x 0 +y 0 ) 2 = (110+97) 2 = 103 and h 0 = x 0 − y 0 = 
Reversible Watermarking Schemes by Using Histogram Bin Shifting
The former two types of reversible watermarking are not robust under image processing and distortions. In order to enhance the robustness of the reversible watermarking , the embedding target is replaced by the histogram of a block. There are several schemes 
1) For each block B, we randomly separate B into two zones Z a and Z b , and calculate the two corresponding histograms of pixel values, H a and H b . For example, 
 2) If the corresponding bit of watermark is 1, we downgrade every bin in H b except in shifting the lowest bin to the highest one. If the corresponding bit of watermark is 0, we upgrade the bins except in shifting the highest to the lowest one. The embedded results are shown in 
3) Repeat the former steps until all blocks are processed. From the characteristics of the images, the neighboring pixels are usually similar to each other. Therefore, H a and H b should also be similar for most of the image blocks. In normal cases, it is assumed that the peak bin of Z a and Z b are the same. In cases in which the highest and lowest bins are shifted to the other side may cause a huge distortion. For example , the white background may become black. Therefore, Vleeschouwer et al. 
Discussions
In this section, we discuss the properties of the three types of reversible watermarking schemes. Usually, higher embedding capacity usually comes along with a higher distortion . Therefore, based on the watermarked images with the similar image quality, we compare the embedding capacity with the three types by using the three standard test images, "" F-16 "" , "" Mandrill "" , and "" Lena "" , as shown in 
Research Issues
It is clear from the comparisons in Section 5 that the embedding capacity and the robustness are the two major challenges of reversible watermarking. Furthermore, many of the conventional watermarking schemes are embedding watermarks in frequency domains. In general, the advantages of embedding watermarks in frequency domains are naturally resisting some attacks, immuned to several destruction, or others. But the existing reversible watermarking schemes did not fully utilize the advantages. Therefore, we provide the following three research issues. 1) Providing a higher embedding capacity. Although capacity is indeed not the necessary requirement of watermarking schemes, but higher capacity is one of the ultimate goals. There's still room for reversible watermarking schemes to improve this terminology. 
2) Providing a higher robustness. The reversible watermarking schemes using histogram bin shifting can resist several attacks, but 
(a) F-16 (b) Mandrill (c) Lena 3) Use the advantage of frequency domains. We thought it could be an opportunity to raise the practicability by this direction. 
Conclusions
In this paper, the reversible watermarking schemes are defined and introduced. In order to work out the features of reversible watermarking schemes, we particularly classified the existing reversible watermarking schemes into three types. They are the schemes which apply data compression , by using difference expansion, and the schemes by using histogram bin shifting. The three types are analyzed and compared to introduce the current status of reversible watermarks. 
In the schemes by applying data compression, a proper design of compression method is important. The existing schemes belong to this type lacking robustness because the applied compression methods can not resist the distortions . The formula to produce the difference and the techniques to handle location map are still under development in reversible watermarking schemes by using difference expansion. The schemes belong to this type are also weak in robustness because the destroyed location map will cause mismatching. The type of histogram bin shifting is different from the previous two types with robustness . Reversible watermarking schemes are still in development and have dramatically potential possibilities. From this paper, we hope to provide an overall introduction of reversible watermarking, and give a proper cause to commence the research in this fascinating area. Yen-Ping Chu is a Professor in the Computer Science and Information Engineering and the library director at Tung Hai University, Taichung, Taiwan. His research interests include high-speed networks, operating system , neural network, and computer assistant learning. 
"
"Introduction
Database security has been provided by physical security and operating system security. As far as we know, neither of these methods sufficiently provides a secure support on storing and processing the sensitive data. Cryptographic support is another important dimension of database security. It is complementary to access control and both of them should be used to guide the storage and access of confidential data in a database system. In 
2) Encryption mechanism can verify the authentic origin of a data item. 
3) Encryption mechanism also prevents from leaking information in a database when storage mediums, such as disks, CD-ROM, and tapes, are lost. However, how to query efficiently the encrypted database becomes a challenge. This usually implies that the system has to sacrifice the performance to obtain the security. When data is stored in the form of cipher, we have to decrypt all the encrypted data before querying them. It is impractical because the cost of decryption over all the encrypted data is very expensive 
Related Work
In 
AES (Rijndael): OverviewSubsection
The Advanced Encryption Standard (AES) 
The Proposed Encryption Algorithm (REA)
We recommend the new encryption algorithm, "" Reverse Encryption Algorithm (REA) "" , because of its simplicity and efficiency. It can outperform competing algorithms. REA algorithm is limiting the added time cost for encryption and decryption to so as to not degrade the performance of a database system. In this section we provide a comprehensive yet concise algorithm. We also give a general analysis of the functioning of these structures. 
Our new algorithm (REA) is a symmetric stream cipher that can be effectively used for encryption and safeguarding of data. It takes a variable-length key, making it ideal for securing data. The REA algorithm encipherment and decipherment consists of the same operations, only the two operations are different: 1) added the keys to the text in the encipherment and removed the keys from the text in the decipherment. 2) Executed divide operation on the text by 4 in the encipherment and executed multiple operation on the text by 4 in the decipherment. We execute divide operation by 4 on the text to narrow the range domain of the ASCII code table at converting the text. The details and working of the proposed algorithm REA are given below. 
Encryption
Text
The first example on which we applied our new encryption algorithm REA is on the text, the explanation has been provided below. The second example on which we applied our new encryption algorithm REA is on database Microsoft SQL Server 2005 is called ""Northwind_Plaintext"". The programming tasks were built by Microsoft Visual Studio 2005 .net. In the previous our paper, we encrypted/decrypted some fields from the database "" Northwind_Plaintext "" by the proposed encryption algorithm REA and compares with the most common encryption algorithms namely: DES, 3DES, RC2, AES and Blowfish. A comparison has been presented for those encryption algorithms at encryption and decryption time. The experiment results show the superiority of REA algorithm over other algorithms in terms of the encryption and decryption time. We also will use these experiments in this current paper to encrypt some fields of the database "" Northwind_REA "" with the proposed encryption algorithm REA (see 
4) Reverse the previous binary data: 
Simulation Results
A typical case study is studied in this section, to give the query processing performance evaluation over encrypted databases with the proposed algorithm (REA) and with the most common encryption algorithm AES. The performance measure of query processing will be conducted in terms of query execution time. In the experiments, we use three databases from the database "" Northwind "" are: 1) Northwind_Plaintext has not any encrypted fields. 
2) Northwind_AES has encrypted fields with AES encryption algorithm (see example in 
3) Northwind_REA has the same encrypted fields in "" Northwind_AES "" . But, with using our new encryption algorithm REA (see example in 
Conclusions and Future Work
There is a lot of very important data in the database, which need to be protected from attack. Cryptographic support is an important mechanism of securing them. People, however, must tradeoff performance to ensure the security because the operation of encryption and decryption greatly degrades query performance. For the query types that require extra query processing over encrypted database, the cost differentials of query processing between nonencrypted and encrypted database increase linearly in the size of relations. To solve such a problem, the proposed encryption algorithm REA can implement SQL query over the encrypted database. 
In this paper, we will introduce a new encryption algorithm, which we call "" Reverse Encryption Algorithm (REA) "" , restating its benefits and functions over other similar encryption algorithms. REA algorithm limits the added time cost for encryption and decryption so as to not degrade the performance of a database system. We also provide a thorough description of the proposed algorithm and its processes. This paper examines a method for evaluating query processing performance over encrypted database with our new encryption algorithm (REA) and with the most common encryption algorithm AES. The performance measure of query processing will be conducted in terms of query execution time. The results of a set of experiments show the superiority of the proposed encryption algorithm REA over other encryption algorithm AES with regards to the query execution time. Our new encryption algorithm REA can reduce the cost time of the encryption/decryption operations and improve the performance. 
In the future work, we are interested in extending the proposed encryption algorithm REA in order to apply it to other kind of databases such as distributed DBMSs and object oriented DBMSs of the query processing. 
"
"Introduction
The primitive of oblivious transfer (OT) introduced by Rabin 
1) It sets the secret key as (a, b), where a is used only for decryption and b is used only for signing, separately . But we know it is usual that a single secret key a can be used simultaneously for both signing and decryption. 
2) For random r, s, t ∈ Z p , it expresses the ciphertext as 
g r , (g j 1 h) r , M · e(g 1 , g 2 ) r , g t , (u r v s d) b (g j 3 h) t , u r , s where p, g, e(·, ·), g 1 , g 2 , g 3 , u
 , v, d, h are included in public parameters. The session key s is directly exposed . That means the corresponding parameter v might be removed reasonably. 
 In this paper, we present an improvement of Green- Hohenberger adaptive OT scheme and show its security under 3DDH assumption. We also correct some typos in the original scheme. The analysis and optimizing skills presented in the paper is novel. We think they are helpful for optimizing other cryptographic schemes. 
Related Works
In past decades, there were many works on the research of OT n k , such as Bellare and Micali 
 1.2 The Definition of Adaptive k-out-of- N Oblivious Transfer 
The definition can be found in 
Real Experiment. In the experiment of Reaî 
S, ˆ R (N, k, M 1 , · · · , M N , Σ), the possibly cheating senderˆS senderˆ senderˆS is given messages (M 1 , · · · , M N ) as input and interacts with the possibly cheating receiverˆRreceiverˆ receiverˆR(Σ), where Σ is a selection algorithm that on input the full collection of messages thus far received, outputs the index σ i of the next message to be queried. At the beginning of the experiment, bothˆSbothˆ bothˆS andˆRandˆ andˆR output initial states (S 0 , R 0 ). In the transfer phase, for 1 ≤ i ≤ k the sender computes S i ← ˆ S(S i−1 ), and the receiver computes (R i , M i ) ← ˆ R(R i−1 ), where M i may or may not be equal to M i . At the end of the k-th transfer the output of the experiment is (S k , R k ). 
Ideal Experiment. In the experiment of Ideaî 
S , ˆ R (N, k, M 1 , · · · , M N , Σ) 
the possibly cheating sender algorithmˆSalgorithmˆ algorithmˆS generates messages (M * 1 , · · · , M * N ) and transmits them to a trusted party T. In the i-th roundˆSroundˆ roundˆS sends a bit b i to T; the possibly cheating receiverˆRreceiverˆ receiverˆR 
(Σ) transmits σ * i to T. If b i = 1 and σ * i ∈ {1, · · · , N } then T hands M * σ * i tô R 
. If b i = 0 then T hands ⊥ tô R . After the k-th transfer the output of the experiment is (S k , R k ). Sender Security. An OT N k×1 provides Sender security if for every real-world p.p.t. receiverˆRreceiverˆ receiverˆR there exists a p.p.t. ideal-world receiverˆRreceiverˆ receiverˆR 
such that ∀N = (κ), k ∈ [1, N ], (M 1 , · · · , M N )
 , Σ, and every p.p.t. dis- tinguisher: 
Real S, ˆ R (N, k, M 1 , · · · , M N , Σ) c ≈ Ideal S , ˆ R (N, k, M 1 , · · · , M N , Σ), 
where (·) is a polynomially-bounded function. 
Receiver Security. An OT N k×1 provides Receiver security if for every real-world p.p.t. senderˆSsenderˆ senderˆS there exists a p.p.t. ideal-world senderˆSsenderˆ senderˆS 
such that ∀N = (κ), k ∈ [1, N ], (M 1 , · · · , M N )
 , Σ, and every p.p.t. dis- tinguisher: 
Reaî S,R (N, k, M 1 , · · · , M N , Σ) c ≈ Ideaî S ,R (N, k, M 1 , · · · , M N , Σ). 
A Simple Security Assumption
 Let BMsetup be an algorithm that, on input 1 κ , outputs the parameters for a bilinear mapping as γ = (p, g, G, G T , e), where g generates G, the groups G and G T have prime order p, and e : G × G → G T . It is both: 
(bilinear ) for all g ∈ G and a, b ∈ Z p , 
e(g a , g b ) = e(g, g) ab ; 
(non-degenerate) if g generates G, then e(g, g) = 1. 
Assumption 1. (Decisional 3-Party Diffie-Hellman (3DDH))
Pr [g, z 0 ← G; a, b, c ← Z p ; z 1 ← g abc ; d ← {0, 1}; d ← A(g, g a , g b , g c , z d ) : d = d ]. 
We use the notation of Camenisch and Stadler 
 3 Review and Analysis of Green- Hohenberger Adaptive OT 
Review
This protocol follows the assisted (or blind) decryption paradigm 
C = g r , (g j 1 h) r , M · e(g 1 , g 2 ) r , g t , (u r v s d) b (g j 3 h) t , u r , s . 
 Given only pk, j, the VerifyCiphertext function validates that the ciphertext has this structure. VerifyCiphertext (pk, C, j). Parse C as (c 1 , · · · , c 7 ) and pk to obtain g, g 1 , h, g 3 , g 4 , u, v, d. This routine outputs 1 if and only if the following equalities hold: 
e(g j 1 h, c 1 ) = e(g, c 2 ) ∧ e(g, c 6 ) = e(c 1 , u) ∧ e(g, c 5 ) = e(g 4 , c 6 v c7 d)e(c 4 , g j 3 h). 
Drawbacks
The encryption used in the scheme is a combination of the Boneh-Boyen IBE scheme 
1) It sets the secret key as (a, b), where a is used only for decryption and b is used only for signing, separately. 
 But it is usual that a single secret key a can be simultaneously used for both signing and decryption. We will set b = a and show that the setting does not endanger its security. That means the generator g 4 could be removed. 
2) For random r, s, t ∈ Z p , it expresses the ciphertext as 
g r , (g j 1 h) r , M · e(g 1 , g 2 ) r , g t , (u r v s d) b (g j 3 h) t , u r , s 
Notice that the session key s is directly exposed. That means the generator v could be removed, too. The redundant setting is due to that the authors follow the Hohenberger-Waters signature based on RSA assumption (see Section 3 in 
 3) The generator g 2 is used only for the blind decryption and the generator g 3 is used only for the VerifyCiphertext . For simplicity, we could explicitly set that g 3 = g 2 . That is to say, the generator g 3 might be redundant. By the way, the generator d is required necessarily for the Hohenberger-Waters signature based on CDH assumption 
C = (c 1 , · · · , c 7 ) = (g r , (g j 1 h) r , M · e(g 1 , g 2 ) r , g t , 
(u r v s ) b (g j 3 h) t , u r , s). 
(1) 
An adversary can take a random θ ∈ Z p and compute 
ˆ C = (ˆ c 1 , · · · , ˆ c 7 ) = (g rθ , (g j 1 h) rθ , M θ · e(g 1 , g 2 ) rθ , g tθ , 
(u r v s ) b (g j 3 h) t θ , u rθ , sθ). 
(2) 
The ciphertextˆCciphertextˆ ciphertextˆC is valid because 
e(g j 1 h, ˆ c 1 ) = e(g, ˆ c 2 ) ∧ e(g, ˆ c 6 ) = e(ˆ c 1 , u) ∧ e(g, ˆ c 5 ) = e(g 4 , ˆ c 6 v ˆ c7 )e(ˆ c 4 , g j 3 h). 
Remark. The random y ∈ Z p chosen by the receiver is not used at all. This is a typo. 
S I (M 1 , · · · , M N ) R I () 1. Select γ = (p, g, G, G T , e) ← BMsetup (1 κ ), a, b ← Z p , g 2 , g 3 , h, u, v, d ← G and set g 1 ← g a , g 4 ← g b . pk ← (γ, g 1 , g 2 , g 3 , g 4 , h, u
, v, d), sk ← (a, b). 2. For j = 1 to N , select r j , s j , t j ← Z p 5. Verify pk and the proof. and set: 
C j ← [g rj , (g j 1 h) rj , M j e(g 1 , g 2 ) rj , Check for j = 1 to N : g tj , (u rj v sj d) b (g j 3 h) tj , u rj , s j ] VerifyCiphertext (pk, C j , j)=1. 3. Send (pk, C 1 , · · · , C N ) 
to Receiver. If any check fails, output ⊥. 4. Conduct ZKP oK{(a) : 
g 1 = g a }. Output S 0 = (pk, sk). Output R 0 = (pk, C 1 , · · · , C N ). S T (S i−1 ) R T (R i−1 , σ i ) 1. Parse C σi as (c 1 , · · · , c 7 ), select x, y ← Z p and compute v 1 = g x c 1 . 2. Send v 1 to Sender, and conduct: 3. Set R = e(v 1 , g a 2 ). W IP oK{(σ i , x, c 2 , c 4 , c 5 , c 6 , c 7 ) : 
4. Send R to Receiver and conduct: 
e(v 1 /g x , (g σi 1 h)) = e(c 2 , g)∧ ZKP oK{(a) : R = e(v 1 , g a 2 ) ∧ g 1 = g a }. e(c 6 , g) = e(v 1 /g x , u)∧ e(c 5 , g) = e(c 6 v c7 d, g 4 )e(c 4 , g σi 3 h)} 
5. If the proof does not verify, output ⊥. 
Else output M σi = c3·e(g1,g2) x R . Output S i = S i−1 . Output R i = (R i−1 , M σi ) 
 4 An Improvement of Green- Hohenberger OT Scheme and Its Security Proof 
The Improvement
The improvement is obtained by removing the redundant generators g 3 , g 4 , v. See the 
g r , (g j 1 h) r , M · e(g 1 , g 2 ) r , g t , (u r d) a (g j 2 h) t , u r . 
 Given only pk, j, the VerifyCiphertext function validates that the ciphertext has this structure. VerifyCiphertext (pk, C, j). Parse C as (c 1 , · · · , c 6 ) and pk to obtain g, g 1 , g 2 , h, u, d. This routine outputs 1 if and only if the following equalities hold: 
e(g j 1 h, c 1 ) = e(g, c 2 ) ∧ e(g, c 6 ) = e(c 1 , u) ∧ e(g, c 5 ) = e(g 1 , c 6 d)e(c 4 , g j 2 h). Correctness. e(g j 1 h, c 1 ) = e(g j 1 h, g rj ) = e((g j 1 h) rj , g) = e(g, c 2 ) e(g, c 6 ) = e(g, u rj ) = e(g rj , u) = e(c 1 , u) e(g, c 5 ) = e g, (u rj d) a (g j 2 h) tj = e (g, (u rj d) a ) e g, (g j 2 h) tj = e(g 1 , c 6 d)e(c 4 , g j 2 h) c 3 · e(g 1 , g 2 ) x R = M j e(g 1 , g 2 ) rj · e(g 1 , g 2 ) x e(g x c 1 , g a 2 ) = M j e(g 1 , g 2 ) rj · e(g 1 , g 2 ) x e(g x , g a 2 )e(g rj , g a 2 ) = M j 
Security Proof
The improvement is sender-secure and receiver-secure in the full simulation model under 3DDH assumption. The security proof is somewhat like that of the original scheme 
S I (M 1 , · · · , M N ) R I () 1. Select γ = (p, g, G, G T , e) ← BMsetup (1 κ ), a ← Z p , choose g 2 , h, u, d ← G and set g 1 ← g a . pk ← (γ, g 1 , g 2 , h, u
, d), sk ← a. 5. Verify pk and the proof. 2. For j = 1 to N , select r j , t j ← Z p Check for j = 1 to N : and set: 
C j ← [g rj , (g j 1 h) rj , M j e(g 1 , g 2 ) rj , VerifyCiphertext (pk, C j , j)=1. g tj , (u rj d) a (g j 2 h) tj , u rj ] 
If any check fails, output ⊥. 
3. Send (pk, C 1 , · · · , C N ) to Receiver. 4. Conduct ZKP oK{(a) : g 1 = g a }. 
Output S 0 = (pk, sk). Output R 0 = (pk, C 1 , · · · , C N ). S T (S i−1 ) R T (R i−1 , σ i ) 1. Parse C σi as (c 1 , · · · , c 6 ), select x ← Z p and compute v 1 = g x c 1 . 2. Send v 1 to Sender, and conduct: 3. Set R = e(v 1 , g a 2 ). W IP oK{(σ i , x, c 2 , c 4 , c 5 , c 6 ) : 4. Send R to Receiver and conduct: e(v 1 /g x , (g σi 1 h)) = e(c 2 , g)∧ ZKP oK{(a) : R = e(v 1 , g a 2 ) ∧ g 1 = g a }. e(c 6 , g) = e(v 1 /g x , u)∧ e(c 5 , g) = e(c 6 d, g 1 )e(c 4 , g σi 2 h)} 5. If 
the proof does not verify, output ⊥. 
Else output M σi = c3·e(g1,g2) x R . Output S i = S i−1 . Output R i = (R i−1 , M σi ) 
from that ofˆRofˆ ofˆR interacting with the honest ideal-world sender S (Ideal S , ˆ R 
). 
1) To begin, ˆ R selects a random collection of messages 
¯ M 1 , · · · , ¯ M N ← G T and 
follows the S I algorithm with these as input up to the point where it obtains (pk, 
C 1 , · · · , C N ). 2) It sends (pk, C 1 , · · · , C N ) tô R and then simulates the interactive proof ZKP oK{(a) : g 1 = g a }. 
(Even thoughˆRthoughˆ thoughˆR knows sk = a, it ignores this value and simulate this proof step.
) 
3) For each of k transfers initiated byˆRbyˆ byˆR, 
a. ˆ R 
verifies the received WIPoK and uses the knowledge extractor E 2 to obtain the values 
σ i , x, c 1 , c 2 , c 3 , c 4 from it. ˆ R aborts and outputs error when E 2 fails. b. When σ i ∈ [1, N ], ˆ R queries the trusted party T to obtain M σi , parses C σi as (c 1 , · · · , c 6 ) and responds with R = c 3 e(g 1 , g 2 ) x M σi (if T returns ⊥, ˆ R aborts the transfer). When σ i / ∈ [1, N ], ˆ R 
follows the normal protocol. In both cases, ˆ R simulates ZKP oK{(a) : 
R = e(v 1 , g a 2 ) ∧ g 1 = g a }. 
4) ˆ R usesˆRusesˆ usesˆR's output as its own. 
Theorem 1. Let ZK be the maximum advantage with which any p.p.t. algorithm distinguishes a simulated ZKPoK, and Ext be the maximum probability that the extractor E 2 fails (with ZK and Ext both negligible in κ). If all p.p.t. algorithms have negligible advantage ≤ at solving the 3DDH problem, then: 
Pr D(Real S, ˆ R (N, k, M 1 , · · · , M N , Σ)) = 1 − Pr D(Ideal S , ˆ R (N, k, M 1 , · · · , M N , Σ)) = 1 ≤ (k + 1) ZK + k Ext + N 1 + p p − 1 . 
Proof. We first define the following games: 
Game 0. The real-world experiment conducted between S andˆRandˆ andˆR (Real S, ˆ R ). Game 1. This game modifies Game 0 as follows: (1) each of S's ZKPoK executions is replaced with a simulated proof of the same statement, and (2) the knowledge extractor E 2 is used to obtain the 
ues (σ i , x, ¯ c 4 , ¯ c 5 , ¯ c 6 ) 
from each ofˆRofˆ ofˆR's transfer queries. Whenever the extractor fails, S terminates the experiment and outputs the distinguished symbol error. 
(There is a typo in the original argument. It says that "" the knowledge extractor E 2 is used to obtain the values 
(σ i , x, y, z, ¯ c 4 , ¯ c 5 , ¯ c 6 , ¯ c 7 ) 
from each ofˆRofˆ ofˆR's transfer queries "" . We stress that both the values y, z are not used at all.) Game 2. This game modifies Game 1 such that, whenever the extracted value σ i ∈ 
C σi = (c 1 , · · · , c 6 ) and set R = c 3 e(g 1 , g 2 ) x M σi . When σ i / ∈ [1, N ]
, the response is computed using the normal protocol. 
Game 3. This game modifies Game 2 by replacing 
the input to S I with a dummy vector of random 
sages ¯ M 1 , · · · , ¯ M N ∈ G T . 
However when S computes a response value using the technique of Game 2, the response is based on the original message vector By the following Lemmas, we then obtain Adv
M 1 , · · · , M N . 
≤ (k + 1) ZK + k Ext + N (1 + p p − 1 ). 
Lemma 1. If all p.p.t. algorithms D distinguish a simulated ZKPoK with advantage at most ZK and the extractor E 2 fails with probability at most Ext , then Adv
≤ (k + 1) ZK + k Ext . 
Proof. See the proof of Lemma A.1 in 
≤ N p p − 1 · 
Proof. For every query where σ i / ∈ 
g r = v 1 /g x 
for some r ∈ Z p . Express the σ i -th ciphertext in the database as 
C σi = (c 1 , · · · , c 6 ). If g r = c 1 
 then the computed response R will have the same distribution as in the normal protocol. To show this, let c 1 = g rσ i for some 
r σi ∈ Z p and c 3 /M σi = e(g 1 , g 2 ) rσ i . 
We can now write the normal calculation of R as: 
R = e(c 1 g x , g a 2 ) = e(g rσ i g x , g a 2 ) = e(g 1 , g 2 ) rσ i e(g 1 , g 2 ) x = c 3 e(g 1 , g 2 ) x M σi . 
It remains only to consider the case where g r = c 1 . We will refer to this as a forged query and argue thatˆRthatˆ thatˆR cannot issue such a query except with negligible probability under the 3DDH assumption in G. Specifically, ifˆRifˆ ifˆR submits a forged query with non-negligible probability, then we can construct a solver B for 3DDH that succeeds with nonnegligible advantage. We now describe the solver B. B takes as input a 3DDH tuple (g, g τ , g ψ , g ω , Z), where Z = g τ ψω or is random, and each value τ, ϕ, ω was chosen at random from Z p . It will simulate S's interaction withˆRwithˆ withˆR via the following simulation. 
Simulation Setup. B first picks j * ← [1, N ] and y d , x d , x h , x z ← Z p . It sets u = g ψ , d = g −ψx d g y d , h = g −ψj * g x h , g 2 = g ψ g xz , g 1 = g τ . 
 Thus, we implicitly have a = τ . The remaining components of pk are chosen as in the real protocol. (There is a typo in the original argument. It says that "" B first picks 
j * ← [1, N ] and a, y v , y d , x v , x d , x h , x z , r j , t j ← Z p "" . 
Clearly, the secret key a for decryption is not known to the solver B. Besides, it is not necessary for B to pick r j , t j in the Setup because they are not used at all in the phase.) For j = 1 to N , B generates each correctly-distributed ciphertext 
C j = (c 1 , · · · , c 6 ) as follows: 
The simulation for j = j * . Pick t j ← Z p and set the ciphertext as: 
(c 1 , · · · , c 6 ) = g x d , (g j 1 h) x d , M · e(g 1 , g 2 ) x d , g tj , (g τ ) y d (g j 2 h) tj , u x d . 
The ciphertext is well-formed because: 
e(g j 1 h, c 1 ) = e(g j 1 h, g x d ) = e((g j 1 h) x d , g) = e(g, c 2 ) e(g, c 6 ) = e(g, u x d ) = e(g x d , u) = e(c 1 , u) e(g, c 5 ) = e g, (g τ ) y d (g j 2 h) tj = e (g, (u x d d) τ ) e g, (g j 2 h) tj = e(g 1 , c 6 d)e(c 4 , g j 2 h). 
The simulation for j = j * . Pick r j , t j ← Z p . Set Y = g t j /(g τ ) (rj −x d )/(j−j * ) 
and the ciphertext as: 
(c 1 , · · · , c 6 ) = g rj , (g j 1 h) rj , M · e(g 1 , g 2 ) rj , Y, 
(g τ ) y d · Y xzj+x h · (g ψ ) t j (j−j * ) , u rj . 
Let us define Y = g tj and thus implicitly set 
t j = t j − τ (r j − x d )/(j − j * ), 
which is randomly distributed in Z p . Just by inspection, it's clear that all of the elements except c 5 are correctly distributed. Thus it remains to show that: 
(g τ ) y d · Y xzj+x h · (g ψ ) t j (j−j * ) = (u rj d) τ (g j 2 h) tj 
In fact, we have: 
c 5 = (g τ ) y d · Y xzj+x h · (g ψ ) t j (j−j * ) = (g τ ) y d · (g tj ) xzj+x h · (g ψ ) t j (j−j * ) = (g τ ψ ) rj −x d (g τ ) y d · (g tj ) xzj+x h ·(g ψ ) t j (j−j * ) (g −τ ψ ) rj −x d = (g ψ(rj −x d ) ) τ (g y d ) τ · (g xzj+x h ) tj ·(g ψ ) t j (j−j * ) (g −τ ψ ) rj −x d = ((g ψrj )(g −ψx d +y d )) τ · (g xzj+x h ) tj ·(g ψ ) t j (j−j * ) (g −τ ψ ) rj −x d = (u rj d) τ · (g xzj+x h ) tj · (g ψ ) t j (j−j * ) (g −τ ψ ) rj −x d = (u rj d) τ · (g xzj+x h ) tj · (g ψ(j−j * ) ) t j −τ (rj −x d )/(j−j * ) = (u rj d) τ · (g xzj+x h ) tj · (g ψ(j−j * ) ) tj = (u rj d) τ · ((g ψ+xz ) j g −ψj * +x h ) tj = (u rj d) τ · (g j 2 h) tj . 
Answering Queries. Upon receiving a query fromˆR fromˆ fromˆR, B verifies the accompanying WIPoK and extracts (σ i , x, ¯ c 4 , ¯ c 5 , ¯ c 6 ) and the value v 1 . Note thatˆRthatˆ thatˆR must issue at least one forged query where v 1 /g x is not equal to the first element of C σi . When this occurs, if σ i = j * then B aborts and outputs a random bit. Otherwise let us consider the distribution ofˆRofˆ ofˆR's query. For some t, r ∈ Z p the soundness of the WIPoK ensures that 
(v 1 /g x , ¯ c 6 ) = (g r , u r ) and (¯ c 4 , ¯ c 5 ) = (g t , (u r d) a (g σi 2 h) t ). 
By substitution we obtain: 
¯ c 5 = (g ψr g −ψx d +y d ) ) τ (g (ψ+xz)j * g −ψj * g x h ) t = g τ ψ(r −x d ) g τ y d g t(xzj * +x h ) . 
Let us implicitly define the value 
h = (v 1 /g x )g −x d = g r −x d . 
B can obtain h τ ψ by computing 
¯ c 5 /(g τ y d ¯ c xzj * +x h 4 ). 
Provided that h = 1, B can now compute a solution to the 3DDH problem by comparing 
e(h τ ψ , g ω ) ? = e(Z, h ). 
If h = 1 then B aborts and outputs a random bit. Probability of abort. There are two conditions in which B aborts: (1) whenˆRwhenˆ whenˆR does not issue a forgery for σ i = j * , 
and (2) when σ i = j * but (v 1 /g x )g −x d = 1. Since j * , x d are outside ofˆRofˆ ofˆR's view and our base assumption is thatˆR thatˆ thatˆR that makes at least one request on σ i ∈ 
Conclusion
 In this paper, we present a review on the Green- Hohenberger adaptive OT scheme and put forth a concrete improvement which is based on 3DDH assumption in bilinear groups. We show that in the original scheme there are some redundancies. Using the modified simulation which needs more less parameters than the simulation presented in the original paper, we prove that the improvement keeps secure under 3DDH assumption. This is a more simple assumption than q-power DDH assumption and q-strong DH assumption for 
"
"Introduction
A remote user authentication scheme allows a server to check the authenticity of a remote user through an insecure channel. In 1981, Lamport 
Review of Yoon et al.'s Scheme
This section reviews a smart card based remote user authentication scheme proposed by Yoon et al. 
Registration phase 
This phase is invoked when a user U i registers to S . It comprises the following steps: 
1) U i submits his identity ID i and password P W i to S through a secure channel. 
2) S computes V P W i = g xs mod p, where x s is S's longterm secret, p is a large prime number of bit size 1024-2048, q is a prime divisor of p − 1 of bit size 160, and g is an element of order q in the finite field GF (p). 
3) S computes R i = h(ID i , x s ) and X i = R i ⊕ h(ID i , P W i )
, where ⊕ denotes the bitwise exclusive OR operation, h(·) denotes a one-way hash function. The bit size of the output of h(·) is |q|, which denotes bit size of q. 
4) S personalizes the smart card with the following information: 
{ID i , V P W i , R i , X i , h(·)
, p, q, g} and sends the smart card to the user in a secure way. 
Login phase 
This phase is invoked when U i logins to S . U i attaches his smart card to the card reader and keys in his password P W * i . The smart card then performs the following operations: 1) Generate a random number 
r ∈ R Z * q . 2) Compute k = (V P W i ) r mod p. 
3) Compute t = h(k, T ), where T is the current date and time of the input device. 
4) Compute V i = X i ⊕ h(ID i , P W * i ). 5) Compute s = r − V i t mod q. 
6) Send to S the login request C 1 = {ID i , t, s, T }. 
Authentication phase 
Upon receiving the login request C 1 = {ID i , t, s, T }, the server S and the user's smart card perform the following steps for mutual authentication between the user U i and the server S. 
1) The server checks the format of ID i . If the format is incorrect, the login request is rejected. 
2) The server verifies the freshness of T . If T −T ≥ T , where T is the server's current time and T is the expected valid time interval for a transmission delay, the server rejects the login request. 
3) The server computes 
V i = h(ID i , x s ). 
4) The server computes k = g (s+V i t)xs mod p. 
5) The server compares t and h(k , T ). If they are equal, the server accepts the login request and proceeds to the next step, otherwise it rejects the login request. 
 6) The server acquires the current time T and computes C 2 = h(k , V i , T ). The server sends back the message {C 2 , T } 7) Upon receiving the message {C 2 , T }, the user U i 's smart card verifies the validity of the time interval between T and its current time, then computes 
C 2 = h(k, V i , T ) and compares C 2 and C 2 . 
If they are equal, then the user accepts the authenticity of the server, otherwise the user interrupts the connec- tion. 8) After mutual authentication is completed, the user and the server use k = k = g xsr mod p as the session key. 
Password change phase 
This phase is invoked when a user U i wants to change his password from P W i to P W i . In this phase, the user attaches his smart card to the card reader and keys in his password P W * i , then the smart card performs the following operations: 
1) Compute R i = X i ⊕ h(ID i , P W * i ). 2) Compare R i with R i . 
If they are equal, then the smart card concludes that P W * i = P W i , R i = R i and lets the user select a new password P W i , otherwise it rejects the password change request. 
3) Compute X i = R i ⊕ h(ID i , P W i ). 
4) Store X i in smart card in place of X i . 
Forgery Attacks on Yoon et al.'s Scheme and an Amendment
In this section, we show that in Yoon et al.'s scheme, if an attacker steals a user's smart card and extracts the values stored in it through some means 
Masquerade as a user 
We note that, in step 4 of the login phase of Yoon et al.'s scheme, V i should be equal to R i in the smart card if P W * i = P W i . This means that an attacker needs not to know P W i to calculate V i if he had known R i from the smart card. Now the attacker can easily go through the steps in the login phase to forge a valid login request. 
Masquerade as the server 
Suppose an attacker intercepts a valid login request C 1 = {ID i , t, s, T } from a user U i . Since V i = V i = R i , from step 5 of the login phase, the attacker can compute r = s + V i t mod q = s + R i t mod q and k = k = (V P W i ) r mod p. The attacker then gets the current time T and computes C 2 = h(k , V i , T ). The message {C 2 , T } is obviously a valid reply message. The objective of the mutual authentication is now defeated and the session key k is exposed to the attacker. 
An amendment 
We note that in Yoon et al.'s scheme, R i is stored in the smart card in order to check the validity of the user's password in the password change phase. However, to serve for that purpose, it is unnecessary to store R i directly. We propose to store h(R i ) instead. The step 2 of the password change phase should accordingly be modified to "" Compare h(R i ) with the stored value of h(R i ) in smart card "" . Due to the one-way property of h(·) , an attacker cannot reverse h(R i ) to get R i . Our fix forces the attacker who has extracted the values stored in the smart card to guess the password in order to obtain the value of R i , which requires the attacker to launch offline dictionary attack against the password. That is, besides the values in the smart card, the attacker also needs to know the user's password for launching any of the attacks. Therefore , two-factor security is ensured. 
 4 Two New Remote User Authentication Schemes 
Yoon et al.'s scheme is based on generalized ElGamal signature scheme and uses expensive exponential operations which could be time-consuming for a small resourceconstrained device such as a smart card. In the following we propose two new smart card based remote user authentication schemes which use only cryptographic hash functions. They are more efficient and secure than Yoon et al.'s scheme while preserving all of its merits. 
The First Scheme
The first scheme uses the timestamp mechanism, so it needs the users and the server to share a standard time, such as the Greenwich Mean Time. The scheme also has four phases: registration phase, login phase, authentication phase and password change phase. 
Registration phase 
1) U i submits his identity ID i and password P W i to S through a secure channel. We require that the entropy of U i 's password must be large enough to thwart the offline password guessing attack. 
 2) The server chooses four distinct cryptographic oneway hash functions h(·), h 1 
(·), h 2 (·), and h 3 (·). 3) The server computes R i = h(ID i , x s ), H i = h(R i ), and X i = R i ⊕ h(ID i , P W i )
, where ⊕ denotes the bitwise exclusive OR operation. 
4) The server personalizes the smart card with 
{ID i , H i , X i , h(·), h 1 (·), h 2 (·), h 3 (·)
} and sends it to the user in a secure way. 
Login phase 
In this phase, U i attaches his smart card to the card reader and keys in his password P W * i . Then the smart card performs the following operations: 
1) Compute R i = X i ⊕ h(ID i , P W * i ) and H i = h(R i ). 2) Compare H i with H i . 
If they are equal, then the smart card concludes that P W 
* i = P W i , R i = R i and 
proceeds to the next step, otherwise it denies access from the user. 
3) Acquire the current time T and compute 
C 1 = h 1 (S, ID i , R i , T ). 4) Send to S the login request {ID i , T, C 1 }. 
Authentication phase Upon receiving the login request {ID i , T, C 1 }, the server S and the user U i perform the following steps for mutual authentication: 1) S checks the validity of ID i . 
2) S checks the freshness of T . 
3) S computes R i = h(ID i , x s ) and checks whether C 1 = h 1 (S, ID i , R i , T ). 
If the check passes, S deems U i authentic and proceeds to the next step, otherwise it rejects the request. 4) S acquires the current time T and computes C 2 = h 2 (ID i , S, R i , T ). S sends back to user {T , C 2 }. S and U i use different hash functions in order to prevent the parallel session attack 
C 2 = h 2 (ID i , S, R i , T 
). If the check passes, the user accepts the authenticity of the server, otherwise it interrupts the connection. 6) After mutual authentication is completed, the user and the server use h 3 (ID i , S, R i , T, T ) as the session key. 
Password change phase 
This phase is invoked when a user U i wants to change his password from P W i to P W i . In this phase, the user attaches his smart card to the card reader and keys in his password P W * i , then the smart card performs the following operations: 
1) Compute R i = X i ⊕ h(ID i , P W * i ) and H i = h(R i 
). 2) Compare H i with H i . If they are equal, then the smart card concludes that P W * i = P W i , R i = R i and lets the user select a new password P W i , otherwise it rejects the password change request. 
3) Compute X i = R i ⊕ h(ID i , P W i ) 4
) Store X i in smart card in place of X i . 
The Second Scheme
The second scheme uses a nonce based challenge-response mechanism, so it avoids the time synchronization problem in the first scheme. This scheme also has four phases: registration phase, login phase, authentication phase and password change phase. The registration phase and password change phase of the second scheme are the same as that of the first scheme and are omitted. We only elaborate the login phase and authentication phase below. 
Login phase 
In this phase, U i attaches his smart card to the card reader and keys in his password P W * i . Then the smart card performs the following operations: 
1) Compute R i = X i ⊕ h(ID i , P W * i ) and H i = h(R i 
). 2) Compare H i with H i . If they are equal, then the smart card concludes that P W 
* i = P W i , R i = R i and 
proceeds to the next step, otherwise it denies access from the user. 
3) Send to S the login request {ID i , N i } , where N i is the nonce selected by U i . 
Authentication phase Upon receiving the login request {ID i , N i }, the server S and the user U i perform the following steps for mutual authentication: 
1) S checks the validity of ID i . 
2) S chooses a nonce N s , computes 
R i = h(ID i , x s ),C 1 = h 1 (S, ID i , R i , N i , N s ) and sends to U i : {C 1 , N s }. 3) Upon receiving the message {C 1 , N s }, U i checks whether C 1 = h 1 (S, ID i , R i , N i , N s )
. He deems S authentic if the check passes, otherwise he interrupts the connection. 
4) U i computes C 2 = h 2 (ID i , S, R i , N s , N i ) and sends it to S . 5) Upon receiving C 2 , S checks whether C 2 = h 2 (ID i , S, R i , N s , N i ). It deems U i authentic 
if the check passes, otherwise it interrupts the connection. 6) After mutual authentication is completed, the user and the server use h 3 (ID i , S, R i , N i , N s ) as the session key. 
Security Analysis
In the following, we assume that all hash functions used in our schemes behave like random oracles 
h 1 (· · · , R i , · · · ) and h 2 (· · · , R i , · · · ) 
 where "" · · · "" denotes some other parameters , due to the randomness of h 1 (·) and h 2 (·), he cannot get R i in this way. Alternatively, he may try to steal the user's smart card or his password. Obviously , if he steals both of them, then he must succeed in masquerading as the user. So we only consider the situation that he only obtains either the user's smart card or his password, but not both. First, if he obtains the user's password but doesn't get his smart card, then he cannot get R i because R i can only be deduced from R i = X i ⊕ h(ID i , P W i ) which requires both the user's password P W i and the secret value X i stored in the user's smart card. On the other hand, if the attacker obtains the user's smart card and extracts the secret values 
"
"Introduction
During the trust negotiation, the negotiators can establish the trust between each other via the exchange of credentials . In the trust-management systems, credential is the identity of the legitimate user. A credential shows the membership of a person in a security domain or an organization . In Globus Toolkit (GT), a user will be denied to access unless he holds a valid credential issued by CA. Here, we give an instance to illustrate the importance of credentials. A medical organization called MO provides online service for doctors and patients. Every user, including the clerks and guests, can access the information according to his role in the organization. Assume Tom to be a doctor, and Jerry, a patient. Now, Tom is examining Jerry via real-time video. In order to know more information about Jerry's illness records, Tom contacts to MO, submits his certified credential and gets the relative data smoothly. However, as the credentials bring us a lot of favors, they are compromised by many aspects. First, since credentials are transferred via insecure channels like Internet, they are easy to be attacked. Second, the storage of credentials is still a key problem under way. Distributed credentials' discovery poses a great overhead. Third, when the credentials are stolen, opponents can use them to access the system or perform some tasks. In this paper we present a new method to hide credentials , which adopts the technique of digital watermarking in the image process. In total, the contributions of this paper are as follows: 
@BULLET It is the first time to introduce digital watermark hiding techniques to the trust negotiation domain on protecting credentials. Since watermark is often used to protect copyrights, it can be applied to embed the credentials into a meaningful and representative image. 
@BULLET We present a new model to hide credentials. In our model, we treat credentials as a series of information or copyright of a legitimate user. Meanwhile, we provide several alternative algorithms on how to insert the credentials into the mediator. @BULLET It can make the authentication more secure. Our work aims at enhancing the security level of current trust management systems. Exchange of credentials can provide a secure trust negotiation. When the watermark is implemented, the identification of the image can be another security method. @BULLET The model here is flexible and can be easily extended. During the design, we carefully consider the practicability and the complexity of realization so that the model is effective and efficient. In the model, we provide flexible interfaces and some algorithms can be replaced. For example, we can use DCT to substitute DFT as the insertion algorithm. 
Note that this paper deals only with hiding credentials as to protect sensitive information, mainly for authenti-cation during the trust negotiation. And the subsequence is based on such a scenario: HUST library issues imagecredential and provide online services. Surely there is much related research on how to protect sensitive information , such as hidden credentials 
Related Work
 Since credential is introduced to computer system, it is always used to stand up for a user's identity. Winslett analyzed the pitfall of current paper-credentials and proposed to use digital credentials in trust negotiation systems 
@BULLET Hidden credentials focus on non-showing the contents of credentials during the authentication. Our work aims at passing credentials through a carrier, which contains the credential information. 
 @BULLET Hidden credentials are built on identity-based encryption (IBE), attribute values are incorporated into the identity, and the credential issuer's public key is the PKG public key. Our work provides a method to hide credentials. Surely, we can build the model into a system. 
Digital watermark is mainly used to protect copyrights of publications. In order to realize the protection, there are many methods to handle this, and the mainly work focuses on the operations(insert, distill, distort, divide etc) of an image. The common methods are based on mathematic , for instance, DCT makes use of the following two formulas: 
F = (u, v) = 2 N c(u)c(v) · N −1 x=0 y=0 f (x, y)cos (2x + 1)uπ 2N sin (2y + 1)vπ 2N , and f (x, y) = 2 N N −1 u=0 N −1 v=0 c(u)c(v)F (u, v) cos (2x + 1)uπ 2N sin 2y + 1 v π2N , 
where f (x, y) is the value of the image in position (x, y), x, y = 0, 1, · · · , N − 1; F (u, v) is the DCT value of the image in position (u, v), u, v = 0, 1, · · · , N − 1. Yusuk et al. presented a framework for web based image authentication using invisible fragile watermark in 
Hiding Credentials Model
In this section, we will describe the hiding credentials model in detail. The model takes the features of authentication into consideration, and integrates with the digital watermark techniques. And it can be implemented in trust negotiation. 
Main Idea of the Model
To simplify the problem, we give an example to illustrate such a scenario. HUST library plans to provide online services for all the users who hold library credentials issued by LIB. The library stores all the pictures for every user. Now in order to meet the requirements of the online services, the library needs to store all the credentials, which will inevitably bring a great burden. To address such a dilemma, we give a feasible resolution. The resolution can be described as four steps: (1) every user submits his digital picture to library for registering; (2) the library checks the pictures one by one and generates the relative permission credentials; (3) the library embeds the credential information into the user's picture so as to generate image-credential; (4) the library distributes the image-credentials to every user. The users can download their image-credentials at the specific URL or get it in his email based on his own choice. 
The Detailed Model
 The purpose of hiding credentials is to implement imagecredentials in trust management for trust negotiation, credential storage etc. The total model consists of two parts: (1) the generation of image-credential and (2) the implementation of image-credential. 
Image-credential's Generation
 In the part, we describe how an image-credential generates . The components are shown in 
@BULLET Credential Generator (CG): Filter user information and prepare basic information for credential. Commonly in the manual way, after a user submits his basic individual information for registering, the administrator will have a routine to check the information. Later, an effective credential will be generated for the user if the examination passes. Since different system has various certificate form, how to generate the credential, and the required registry information are distinguishable. For instance, in GT (Globus Toolkit) systems, credentials are automated built based on the available functions such as grid-ca-sign, CA.pl -newcert, and the required information is {name, state, zip code, city, organization, unit, email, PEM phrase}. @BULLET Credential Separator (CS): Generate a credential for user and a digital watermark so as to be embedded into an image. After CG, the output (credential information) includes a user's country, city, organization, role, email, PEM pass phrase etc. The CS then produces (1) a complete credential for user to download and (2) a watermark . The credential is the common one, which can be stored as the form of "" .pem "" . Alternatively, the watermark can be the hash of credential information so that it occupies a little storage space.  parts: (1) a watermark: the information to be embedded into an image; (2) an image: the mediator, to carry watermark; (3) parameter-set: information about algorithm type (such as DCT, DFT, DWT), insertion point. Each user provides his own picture as the input image. For the embedding algorithm is variable, ICG is flexible. 
Image-credential's Implementation
 In the practical applications, an image-credential will experience three examinations, shown in 
@BULLET Artificial Verification (AV): Before authentication, the other part can have an artificial verification towards the image through the comparing the image-credential to his portrait. It can ensure that only the user himself can use his image-credential. Of course, AV will have a big cost in human resource so that this examination is op- tional. 
@BULLET Watermark Detector (WD): AV cannot find whether a picture is just an image or an image-credential, while WD can do so. WD will detect the watermark hidden in the image-credential. Of course, there is a relative algorithm to detect watermark according to the insertion algorithm. 
@BULLET Credential Distill (CD): CD will separate credential information from image- credential. It needs administrator to input parameter-set about embedding information (algorithm , insertion point). Note that, in order to avoid the provision of original image, here we use the blind-watermark insertion/distillion techniques, which need not the original image when distilling the watermark. 
 3.3 How to Build Image-Credential Sys- tem 
 Just like hidden credential system, we use a series of functions to realize image-credential system. The functions are listed as follows: 
 @BULLET Credential create(country, organization, PEM password , email, etc.) 
 This function is used to create a credential. Generally , a standard credential at least contains (1) country name, (2) city name, (3) zip code, (4) organization name, (5) PEM pass phrase, (6) user information , (7) email and so on. Since the implement is various, the specific parameters are decided by the practical requirements, here we mainly give a basic framework to illustrate the issue. @BULLET Watermark setup(credential info) To produce watermark using credential information . In order to embed credential information into an image, we select some meaningful parts, such as user's role, dependency between trust participants , email. We use another hash function such as mhash(mhash md5,credential info ) to encode the data. @BULLET Imagecredential create(image, watermark, insertion algorithm, etc.) To insert watermark into a specified image with the selective algorithm. According to the specific application , watermark algorithms are classified as blind and non-blind. The former needs the original image to distill the watermark, while the latter does not. Usually, we use blind watermark algorithm so that will not need the original image to distill the credential information.. @BULLET Watermark detect(image) The function is to detect whether an image is a picture only or an image-credential. During the process of authentication, if a user submits a useless image, it will lead the authentication to failure. Then ,we adapt a detection function to filter the meaningless images as to enhance the success rate to authentication or trust negotiation. @BULLET Watermark distill (image-credential, distill algo- rithm) This function is to separate the credential from image-credential. Just like the function of Im agecredential create(), we need to specify algorithm to distill watermark. 
Features of the Model
 Based on the description of the model above, we can conclude the features of the model as follows: 
1) More secure than common credential In our model, the watermarked credential provides the second security line of defense. The image is a user's personal picture, the participant can judge whether the other is the accessor himself by examining the picture. Meanwhile, the approach can prevent image-credential from being used by others. Generally, when a user logins on a trust-management system, he is required to input his id and password as well as credential. If one user lost his imagecredential , others could not use his due to the consistency verification between the login id and image- credential. 
2) Simple credential form easy to manage In the model, we use an image as the current credential . Surely, the image-credential is easy to store and manage. Meanwhile, an image can scratch the attacker's head over for no one would doubt that an image is in fact a credential. However, if the imagecredential is stolen, one must submit his image again to get a new one. 
3) Flexible framework easy to extend As mentioned above, some components are designed so flexible that they are easy to extend. For example , the component of Image-credential Generator has three input parameters. It does not specify concrete algorithm so that the administrator can select different algorithms for different applications. At the same time, it provides interfaces for new techniques. 
Related Issues
 In this section, we will discuss some much-concerned questions . Since watermark has some important features, the image-credential should meet the requirements, otherwise , anything is of no value. 
How to Ensure the Quality
 Generally, a successful watermark has at least four char- acteristics 
How to Embed the Credentials into an Image
There are too many existing techniques to answer such a question. In the field of image process, DCT, DFT, litterwave etc are used to embed watermark into an image. Surely, we first need to turn the credential data into watermark . Then, we choose an algorithm according to the practical application and requirement. In the subsequent section, we will give a simple experiment to illustrate the total process. 
How Much Data Can Be Embedded into an Image
 Since the storage space of an image very limited, we cannot embed too much data into an image. Pierre Moulin et al. 
A Simple Experiment
In this part, we give a simple experiment to show how image-credential works. Assume Tom to be a professor of HUST, he submits his picture (JPEG, 2160 1440) to the online library, since we need to hide a little more data, so the image is bigger than the standard picture of lena(51251224b). The library accepts his image (if the image is valid and acceptable) and embeds some relative data into it. Suppose credential information follows the form of "" subject, country, city, zip code, organization, unit,role, email "" , and we use md5 32() as the hash function . Here, we give the relative data as follows: 
@BULLET Credential data: Tom, CH, wuhan, 430074, HUST, CS, professor, Tom@hust.edu.cn. @BULLET Analysis on quality of image-credential: the three basic performance parameters are SNR, PSNR, NC. The relative values are described at 
Conclusions and Future Work
 Watermark is a kind of technique in image process. We introduce it to trust management and implement it in trust negotiation, which can bring much convenience and security . In this paper, we consider both features of credential and watermark, and present a new approach to hide credential by embedding the credential data into an image. We describe the model and discuss the relative issues. At the same time, we give a simple experiment to show how it works. Our future work is to design a practical system and implement image-credential to the system. 
m×n×255 2 m j=1 n i=1 (Xi,j − ˜ Xi,j ) 2 ) 49.4600 NC N C = m j=1 n i=1 W (i,j)W (i,j) m j=1 n i=1 W (i,j) 2 
"
"Introduction
 The emerging peer-to-peer model has recently gained significant attention due to its high potential of sharing huge amount of resources among millions of networked users, where each peer acts as both a resource provider and a consumer. A dilemma in P2P computing area is that when every participating peer tries to maximize its own utility, the overall utility of the collaboration might drop. In the worst case scenario, P2P resources are easily depleted due to selfish users take free rides without offering any sharing resource. Unfortunately, such "" tragedy of the commons "" phenomenon also happens in a number of existing peer-to-peer systems where cooperated scientific research systems emphasize on sharing resource voluntarily. Apparently, certain resource management scheme has to be implemented on P2P systems to ensure them working properly and growing healthily. To encourage resource sharing, several previous works adopt soft incentive schemes 
 1) By adopting a continual exchange process and matching user requests with available services, we propose the TIM model which can maximize aggregate resource utilization in an economically and computationally efficient manner. In this model, users can get more only if they are willing to share more and as the same time have higher degree of trust. As a result, it promotes collaborators to share more valuable services and avoid malicious waste. 
 2) In a P2P system, price fluctuates when service supply and demand changes. We separate the role of providers and consumers and apply different strategies to each role. Providers simply mark their price based on supply and demand while consumers offer their bids upon deadline and budget constraints. Because the service dynamics are included in our management model, the workload of providers is balanced and the efficiency of P2P system is improved. 
3) Peers may join or leave a P2P system randomly. To prevent a system from being attacked by malicious peers, we employ a weighted voting scheme in TIM, such that a secure and balanced environment is con- structed. 4) We have successfully implemented TIM in the key project of our lab, CROWN system. Our implementation experiences and experimental results are valuable to research peers. The rest of this paper is organized as follows. Section 2 introduces related work. Section 3 illustrates the service management architecture and dynamic service management model in CROWN system. We present our TIM framework in Section 4, including price strategy, allocation mechanism, and dynamic peers management scheme. Section 5 presents performance evaluations. We conclude this work in Section 6. 
Related Work
The objective of resource incentive is to promote users to share more resources. Therefore, "" tragedy of the commons "" can be avoided. Generally speaking, there are two incentive schemes: Soft Incentive and Hard Incentive. Soft incentive 
System Model
The key project in our lab, CROWN (China R&D Environment Over Wide-area Networks) 
Child Peer: According to the area partition, a child peer connects to the nearest and most trustworthy CDSR, and reports its service status periodically to its designated CDSR. A child peer also can send service request to a CDSR and receives responses. For example, in 
Definition 1. (CHILD Model) Chlid i j is defined as 
(A c i j , directtrust i j , revenue i j ): A c i j 
 to denote availability quantity of the service provided by Chlid i j ; a direct trust value directtrust i j ∈ 
Definition 3. (BIDS Model) Bids is defined as (cid, pid, et, Q, directtrust, C, V ): cid ∈ {1, . . . , p}, pid ∈ N to denote the bid coming from Child pid cid ; a estimated task execution time et; a desired quantity Q ≥ 1 of services; a direct trust value directtrust ∈ 
Then we get the following trust inference. 1) Direct trust computing. After peers in Club i use the services of Child k i to execute tasks, the peers will report positive or negative experiences to the CDSR in Club i . A direct trust relationship will set up only if all experiences with that the CDSR in Club i knows about are positive experiences. Let q be the number of positive experiences, then the direct trust value from the CDSR to Child k i is directtrust k i = 1 − λ q , where λ is the probability of reliability with a single task; 
2) Recommendation trust computing. After peers in Club j have used the services of peers in Club i to execute tasks, the peers in Club j will report positive or negative experiences to the CDSR in Club j . Given numbers of positive and negative experiences p and n, the recommendation trust value from the CDSR of Club j to the CDSR of Club i is: 
rectrust ji (p, n) = 1 − λ p−n if p > n 0 else; 
3) The trust value from Club j to Child k i is given by: 
trust k i = 1 − (1 − directtrust k i ) rectrustji . 
For example, in 
Design of TIM
In this section, we present our TIM framework to manage shared services in CROWN.  The prices of services are set by providers who continuously adjust the prices based on supply and demand. When consumers bid for the services, they offer bids that can maximize their surplus and meet the deadline and the budget constraints. Both BID model and TRUST model are making contributions to evaluate peers and are therefore influencing how the service is allocated and managed. And also, TIM is working in a continually exchanged manner , which guarantees the dynamics of the supply and demand are reflected in service allocation and management. Our work successfully integrates the trust mechanism, incentive mechanism, allocation mechanism, and dynamic management of shared services. First, the peer who supplies valuable services can accumulate more virtual currency and higher degree of trust, thus it can have precedence in bidding for other services. Second, since there is no centralized trust server and peers may join or leave dynamically , collaborative peers need to vote to determine whether to accept the new peers or not. Trust and price are the two major factors that are involved in this decision making process to keep a secured P2P environment. Finally , to ensure the balance of supply and demand in P2P market, prices setting strategy are considered in service allocation. Because it is possible that multiple peers bid for multiple sets of services simultaneously, we assume that the services in one club has the same sale price. Each CDSR sets the sale price according to supply and demand and periodically publishes its local service information. After getting the sale price, each consumer will then offer a bidding price, which can meet its deadline and budget constraints. 
CDSR 
Club 
Price Mechanism
In P2P computing, the evaluation is the key to allocate and manage shared service. It is rather difficult to find a fixed analytic formula to calculate the service price due to geographical distribution, heterogeneity, indeterminably, and large scale in the P2P environment. But with different preconditions and history information, an approximate price model can be constructed. In TIM, we propose the provider price strategy based on supply and demand, and the consumer price strategy upon deadline and budget constraints. 
Provider Price Strategy
 In TIM, we proposed a distributed service quantity estimation strategy based on weighted average calculations, and then estimate the service price based on service quantity . The weights for computing the estimates are based on the iteration indices until the current iteration. First, for estimating the available service quantity at a club, each Club j needs to keep track of the actual service quantity A j from its previous iterations. Note that for implementation purposes it is sufficient to keep a cumulative value for the weighted service quantity. In any iteration q, each club shall estimate the service quantity that will be available for the next iteration (q + 1) as 
ˆ A q+1 j = q k=1 k × A k j q k=1 k 
Second, we can estimate the service price for the next iteration (q + 1) by the formulâ 
P q+1 j = max{, P q j + λ(A q j − ˆ A q+1 j 
), where λ > 0 is a small step size parameter, > 0 is a sufficiently small constant preventing price to approach zero. Finally, the CDSR of each club will periodically publish the available service quantity and price to other clubs. 
Consumer Price Strategy
Each P2P user generates the service request for its tasks according to their requirements, and submits the request to a selected CDSR that the user will bid for. The goal of each P2P user is to maximize its own surplus upon deadline and budget constraints. Given the price P j of service in the Club j and the completion time constraint T , the utility function of each P2P user can be expressed as follows: 
U (V ) = K(T − LP j V ) − V 
in which L is the length of the task, V is the payment value of the user, LPj V is the estimated task execution time, and K is a constant coefficient defined by the user. Thus, the utility optimization problem above can be written as: 
M ax U (V ) s.t. g(V ) ≥ 0 (g(V ) = T − LP j V ). 
 By using the multiplicator method, we obtain the approximative optimal solution V * = 3LPj 2T −K as the bidding price, which is also the value V in BIDS model. 
Trust-incentive Compatible Service Allocation Algorithm
After collecting the consumers' bids, the CDSR of each club uses the trust-incentive compatible mechanism to allocate services. The allocation mechanism in TIM has the following property: where V i j , Q i j , et i j denote bidding price, service number, and estimated execution time, respectively. Then we scale the Bid i j by the formula bid i 
j = Bid i j −Bid min j Bid max j −Bid min j . 
Step 2: We calculate the evaluation value of each bid by the formula: 
evlbid i j = α × bid i j + (1 − α) × trust pid cid , 
where trust pid cid is the trust value from Club j to Child pid cid , and α ∈ 
Step 3: We sort all bids in descending order according to evlbid i j . 
Step 4: We go through the sorted bid list and schedule each single bid. We will allocate the services to each bid 
i with evlbid i j ×totalserviceq uantity n k=1 evlbid k j . 
Dynamic Management for Peers
Peers may join and leave the collaboration dynamically, or transfer from one club to another club. Thus, some CDSRes maybe have the trust records for a child peer. In our TIM approach, the more services a peer provides, the higher degree of trust a peer has. Obviously, every peer is willing to cooperate with a peer with higher trust value. As illustrated in 
vote i =    1 trust > τ 0 no trust value record −1 trust < τ 
where τ ∈ 
... 
Implementation
Implementation Environment
 We have implemented the TIM approach in CROWN system with Java. The cooperation facility among peers is provided by CROWN, a fully decentralized P2P middleware infrastructure. As illustrated in 
Efficiency of Allocation Mechanism
We first conduct an experiment with a set of five peers with varying currency and trust value, bidding for some In this experiment, we assume that the total request quantity of each peer is 100 units. There are five curves in Figures 4 and 5, where x-axis represents the bids times, and y-axis represents the allocation ratio. We first set the risk degree α = 1 in 
Impact of TIM Price Strategy
 This experiment is to study characteristics of price setting strategy with Round-Robin strategy in terms of task completion time, which is measured from accessing the requested P2P services till task is accomplished. Three clubs are the service providers, with each having 200 unit services. Service requests are generated by the child peers and the bid is generated at an interval of 350 time units. We change the system load from 0.1 to 0.9 with a step of 0.1, where system load is defined as a ratio of aggregate bids load to aggregated capability of providers. The initial value of the service price is 50G$, and each CDSR re-publishes the service price with an interval of 500 time units. From Figures 6 and 7, we can see that TIM price setting strategy has better efficiency and spends less time to complete tasks compared with the Round-Robin strategy, especially at higher bids. In 
Evaluation of Trust Model
Malicious peers may exist in P2P environments to disturb service exchange or even destroy services of other peers. We consider the security problem from both sides, including malicious consumers and malicious providers. On one hand, when bidding for services, a malicious consumer can either set a higher bit value arbitrarily or does not give the corresponding payment. Such behaviors adversely 
Conclusion
 Proving trust and incentive in P2P computing environments are of great importance. This paper presents a Trust-Incentive Compatible Dynamic Service Management , TIM, on the basis of economy model and trust model. By introducing the price strategy, trustincentive compatible service allocation mechanism and the weighted voting scheme, TIM encourages peers to share more services, ensures the balance of supply and demand , enhances the aggregated resource utilization, and maintains the secure environment of a P2P computing system. TIM scheme has been successfully implemented in our key project, CROWN environment. We evaluate our proposed approach by comprehensive experiments and achieved much improved results in service allocation, system completion time and aggregated resource utilization. In the future, we will widely deploy our TIM approach in the CROWN to construct a more secured and balanced collaborative P2P environment. 
"
"Introduction
Deniable authentication protocol is a new authentication mechanism compared to traditional authentication protocols . It mainly has the following two properties: First, it enables an intended receiver to identify the source of a given message. Second, the intended receiver cannot prove the source of the message to a third party. Just due to these two properties, deniable authentication protocols are used to provide freedom from coercion in electronic voting systems and to support secure negotiation over the Internet 
In 2004, to resolve the above issue, Shao proposed an efficient non-interactive deniable authenticated protocol based on the generalized ElGamal signature scheme 
 By taking a closer look at these deniable authentication protocols mentioned above, we can see all of them are in manner of person-to-person, which may not meet some special group communication requirements. Therefore , in this paper, we would like to extend the general deniable authentication protocol to group oriented deniable authentication protocol. In the group oriented deniable authentication protocol, the sender is no longer a single person but a sender group. Only all senders in the sender group can collectively send a deniable authentication message to an intended receiver. In what follows, we will present a group oriented Identity-based deniable authentication protocol based on the bilinear pairings. The rest of this paper is organized as follows: In Section 2, we first review the concepts of the bilinear pairings. Then, we present our new group oriented Identity-based deniable authentication protocol in Section 3 and analyze its security in Section 4. Finally, we draw our conclusion in Section 5. 
The Bilinear Pairings
 Let G 1 be a cyclic additive group and G 2 be a cyclic multiplicative group of the same prime order q. We assume that the discrete logarithm problems in both G 1 and G 2 are hard. A bilinear pairing is a map e : G 1 × G 1 → G 2 which satisfies the following properties: 
Bilinear For any P, Q ∈ G 1 and a, b ∈ Z * q , we have e(aP, bQ) = e(P, Q) ab . Non-degenerate There exists P ∈ G 1 and Q ∈ G 1 such that e(P, Q) = 1. 
Computable 
There is an efficient algorithm to compute e(P, Q) for all P, Q ∈ G 1 . From the literature 
y 2 = x 3 + 1 over F p . The group of rational points E(F p ) = {(x, y) ∈ F p × F p : (x, y
) ∈ E} forms a cyclic group of order p + 1. Because the prime q satisfies the condition 6q = p + 1, the group of points order q in E(F p ) also form a cyclic subgroup, namely G 1 . Let P be the generator of G 1 and G 2 be the subgroup of Decisional Diffie-Hellman Problem (DDHP): For a, b, c ∈ Z * q , given P, aP, bP, cP , decide whether c = ab mod q. The DDHP is easy in G 1 as it can be solved in polynomial time by verifying e(aP, bP ) = e(P, cP ). This is the well known MOV reduction 
For a, b ∈ Z * q , given P, aP, bP , compute abP ∈ G 1 . 
Bilinear Diffie-Hellman Problem (BDHP): For a, b, c ∈ Z * q , given P, aP, bP, cP , compute 
e(P, P ) abc ∈ G 2 . 
We have the relationship of the BDHP and CDHP that the BDHP in (G 1 , G 2 , e) is no harder than the CDHP in G 1 or G 2 
Our Proposed Protocol
In this section, we present our new group oriented Identity-based deniable authentication protocol from the bilinear pairings. Let S = {S 1 , S 2 , · · · , S n } be the sender group of n members and R be the intended receiver. Only all senders S 1 , S 2 , · · · , S n ∈ S agree to generate a deniable authentication code for a message m, can the deniable authentication message m be regarded as valid in eye of the intended receiver R. Our proposed protocol, which consists of four algorithms: Setup, Extract, Authenticate and Verify, is described in detail as follows. 
Setup: Let G 1 be a cyclic additive group of prime order q, G 2 be a cyclic multiplicative group of the same order q. A bilinear paring is a map e : G 1 ×G 1 → G 2 . Define two secure hash functions H and H 1 , where H : {0, 1} * → G 1 and H 1 : {0, 1} * → Z * q . PKG choose a random number s ∈ Z * q and sets P pub = sP . Then, the public parameters of the systems are params = {G 1 , G 2 , e, q, P, P pub , H, H 1 }, and the master-key s is kept secretly by PKG. Extract: When the sender group S submits their identity information ID S = {ID S1 , ID S2 , . . . , ID Sn } and authenticates themselves to PKG, PKG runs the following steps to generate the secret keys for the sender group S. 
Step 1: PKG first chooses n random numbers x 1 , x 2 , . . . , x n ∈ Z * q such that 
s = x 1 + x 2 + · · · + x n mod q. 
Step 2: For i = 1, 2, . . . , n, PKG computes X i = x i H(ID S ) and Y i = x i P . Then, PKG sends X i to S i ∈ S via a secure channel and broadcasts Y i among S. 
Step 3: Each S i ∈ S can verify the validity of all Y 1 , Y 2 , · · · , Y n by checking the equality 
P pub = Y 1 + Y 2 + · · · + Y n . 
Then, he can verify the validity of the secret key X i by checking the equality 
e(X i , P ) = e(H(ID S ), Y i ). 
If it holds, the secret key can be accepted, otherwise rejected. Since the DDHP is easy in G 1 , the correctness follows. 
 When the intended receiver R submits his identity information ID R and authenticates himself to PKG. PKG uses the master-key s to compute X R = sH(ID R ), then sends X R as the secret key to R via a secure channel. When R receives X R , he can easily verify its validity by checking the equality e(P pub , H(ID R )) = e(P, X R ). Authenticate: For sending a deniable authentication message m to the intended receiver R, each S i ∈ S performs the following steps: 
Step 1: Each S i ∈ S chooses a random number k i ∈ Z * q , computes K i = k i P and broadcasts K i to all other senders in S. For simplicity, we denote the sum of n random numbers k 1 , k 2 , . . . , k n is k = k 1 + k 2 + · · · + k n mod q in below. 
Step 2: After receiving all K j (j = 1, 2, . . ., n and j = i) from other senders, S i ∈ S computes parameters K and h with the following equations: 
K = K 1 + K 2 + · · · + K n = (k 1 + k 2 + · · · + k n ), P = kP, h = H 1 (ID S ID R Km), 
where "" "" is the concatenation symbol. 
Step 3: Each S i ∈ S uses his secret key X i computes σ i , where 
σ i = k i P pub + hX i = k i P pub + hx i H(ID S ) 
and sends σ i to the dealer S d . The dealer S d is chosen from the sender group S in advance. 
Step 4: The dealer S d verifies the validity of σ i by checking that 
e(σ i , P ) = e(P pub , K i )e (H(ID S ), Y i ) h . 
If it holds, σ i can be accepted, since 
e(σ i , P ) = e (k i P pub + hx i H(ID S ), P ) = e(P pub , k i P )e(hx i H(ID S ), P ) = e(P pub , K i )e(H(ID S ), x i P ) h = e(P pub , K i )e(H(ID S ), Y i ) h . 
Step 5: The dealer S d computes all collected σ i (i = 1, 2, . . . , n) as 
σ = n i=1 σ i = n i=1 (k i P pub + hx i H(ID S )) = kP pub + hsH(ID S ). 
In the end, the dealer S d computes α, β, where 
α = e(H(ID R ), σ), β = H 1 (αm), 
and sends (K, β) with m to the intended receiver R. Verify: Upon receiving (K, β) and m from S, R will run the following steps to verify it. 
Step 1: R first computes h = H 1 (ID S ID R Km) and α as 
α = e(X R , K + hH(ID S )). 
Step 2: R then checks whether H 1 (α m) = β. If it holds, the intended receiver R accepts it; otherwise, R rejects it. 
Security Analysis
In this section, we discuss the security of our proposed protocol. Fundamentally, the security of the proposed schemes is based on the BDHP and the one-way hash function assumptions. Statement 1 (Completeness). If both the sender group S and the intended receiver R follow the protocol, the intended receiver R is always able to identity the source of the message. Proof. Because the deniable authentication code α and α are identical by computing the following equality 
α = e(X R , K + hH(ID S )) = e(sH(ID R ), K + hH(ID S )) = e(H(ID R ), sK + shH(ID S )) = e(H(ID R ), skP + shH(ID S )) = e(H(ID R ), kP pub + hsH(ID S )) = e(H(ID R ), σ) = α. So H 1 (α m) = H 1 (αm). 
Hence if both the sender group S and the intended receiver R follow the protocol, the intended receiver R is always able to identity the source of the message. Statement 2. The dealer S d can authenticate each (K i , σ i ) provided by S i ∈ S, but cannot obtain each S i ∈ S's secret key or the sender group S's secret key. Proof. To prove this statement, we first briefly show that (K i , σ i ) provided by S i ∈ S is secure against existential forgery. Suppose that there is an adversary A who can output an existential forgery of (K i , σ i ) with a nonnegligible probability. Then, by the forking lemmas due to Pointcheval and Stern 
(K i , σ i ) and (K i , σ i ) for h = h . We will have σ i = k i P pub + hx i H(ID S ) σ i = k i P pub + h x i H(ID S ). 
Then, the secret key X i of S i ∈ S can be recovered by the following 
X i = x i H(ID S ) = 1 h − h (σ i − σ i ), 
which also means that given H(ID S ), Y i = x i P , there exists an adversary A who can solve the CDHP instance 
X i = x i H(ID S 
). Therefore, we can conclude that forging (K i , σ i ) is as hard as solving the CDHP in G 1 . According to the result above, we can be sure that the dealer S d can authenticate (K i , σ i ), but can't derive the secret key X i = x i H(ID S ) from (K i , σ i ). At the same time, since the sender group S's secret key is 
sH(ID S ) = X 1 + X 2 + · · · + X n . 
Without knowing all X i , the dealer S d also can't obtain the sender group S's secret key. Therefore, the statement follows. 
 Statement 3. Only the intended receiver R can authenticate the source of message m. Proof. Since the deniable authentication message (K, β) and m are transmitted over an insecure channel, anyone can obtain it. However, only the intended receiver R, with his secret key X R , can compute the implied deniable authentication code α from e(X R , K +hH(ID S )). On the other hand, the deniable authentication code 
α = e(X R , K + hH(ID S )) = e(H(ID R ), P ) ks · e(H(ID R ), H(ID S )) sh , 
includes the static shared secret key e(H(ID R ), H(ID S )) s , which is only shared by S and R, R therefore can authenticate the source of message m, after he computes the deniable authentication code α. We also notice that, even though the deniable authentication code α has leaked, our proposed protocol is still secure. Since the random number k is unknown to all, nobody, except the intended receiver R, can derive the static shared secret key e
(H(ID R ), H(ID S )) s from α = e(H(ID R ), P ) ks · e(H(ID R ), H(ID S )) sh . In 
tion, the deniable authentication code α in our proposed protocol is binding with the message m, the adversary cannot use it to forge other deniable authentication messages . Therefore, from this view of point, our proposed protocol seems to be more secure than other protocols 
α = e(X R , K + hH(ID S )) = e(H(ID R ), P ) ks · e(H(ID R ), H(ID S )) sh , 
 can be computed by both the sender group S and the intended receiver R, R can construct another authenticated message m , which is different from m. R can compute K , h , α , β such that 
K = k P h = H 1 (ID S ID R Km ) α = e(X R , K + h H(ID S )) = e(H(ID R ), P ) k s · e(H(ID R ), H(ID S )) sh β = H 1 (α m ). 
 Obviously, (K , β ) is indistinguishable from the actual message computed by S. Therefore, it follows that our proposed protocol achieves the property of deniabil- ity. 
Based upon the analysis in Statements (1)-(4), we can conclude that: 
Theorem 1. Our proposed group oriented Identity-based deniable authentication protocol is secure and can work correctly.
"
"Introduction
Currently, digital signatures have been adapted in many industrial applications such as electronic payment system, electronic voting system and so on. Some of the applications require multiple signatures to be verified faster than individual verification of the signatures. For instance, in electronic payment system, typically customers interact with a banking server, and then the banking server must verify a large number of signatures. Concerning verification of multiple signatures there are mainly two questions: given signature/message pairs, without checking the validity of individual signatures, (1) determine efficiently whether an instance contains invalid signatures; (2) identify efficiently invalid signatures, if any, in an instance. A batch verification of signature scheme provides a solution of the first question. A batch verification algorithm (or a batch verifier) of signatures is defined as follows. A batch verifier of signatures is a probabilistic algorithm that takes as input a security parameter ℓ and a batch instance (signature/message pairs), satisfying (1) if all the members of an instance are valid then it returns true, (2) if there are invalid signatures then the probability that it returns true is at most 2 −ℓ . A batch verification method verifies multiple signatures altogether at once and reduces verification time compared with individual verification . The concept of batch cryptography was introduced by Fiat in 1984 for an RSA-type signature 
Preliminaries
Batch Verification
Batch verification of digital signatures was introduced by Naccache et al. 
Review of Extremely Short Group Signature Scheme
Delerabee and Pointcheval 
Bilinear Pairing
The XSGS scheme uses bilinear pairing on cyclic groups, which can be described briefly as follows: For two additive cyclic groups G 1 and G 2 , and a multiplicative cyclic group G T of order p, we assume that there is an efficiently computable function e : G 1 × G 2 → G T , with following properties: 
@BULLET Bilinear: for all A, B ∈ G and a, b ∈ Z p , e(aA, bB) = e(A, B) ab . 
@BULLET Non-degeneracy: For two non-identity elements A ∈ G 1 and B ∈ G 2 , e(A, B) is a generator of the group G T . 
Delerablee-Pointcheval's Group Signature Scheme (XSGS)
XSGS scheme 
2) Select a generator G 2 ∈ G 2 at random, and set 
G 1 ← ψ(G 2 ). 3) Select K ∈ G 1 and W ∈ G 2 . 
4) Choose γ ∈ Z * p at random, and set ik = γ. @BULLET Opener does the following: 
1) Choose ξ 1 , ξ 2 ∈ Z p at random. 2) Set H = ξ 1 K and G = ξ 2 K. 
The group public key (or public parameters) gpk of the system, the group manager's secret key ik, and the opener's secret key ok are then given by gpk = (G 1 , K, H, G, G 2 , W ) ik = γ which is the group manager's issuing key ok = (ξ 1 , ξ 2 ) which is the opener's opening key. reg: GM manages this registration table, meaning the user is a group member. Ext-Commit(y): extractable commitment ( c is commitment of y). NIZKPEqDL(c,C,H): proof of equality of DL of C in base H with the committed val. in c. NIZKPoKDL(B,D): proof of knowledge of DL of B in base D. gmsk: some secret information used in Ext-Commit, NIZKPEqDL, and NIZKPoKDL. User (upk, usk) GM (ik = γ, gmsk) 
y ∈ Z p , C = yH U = c = Ext-Commit(y) NIZKPEqDL(c, C, H) C,U −→ Verify C ∈ G 1 , Checks U x ∈ Z p , A = ( 1 γ+x )(G 1 + C) B = e(G 1 + C, G 2 )/e(A, W ) D = e(A, G 2 ) V = NIZKPoKDL(B, D) A,V ←− B = e(G 1 + C, G 2 )/e(A, W ) D = e(A, G 2 ) Verifies A ∈ G 1 , Checks V S = Sign usk (A) S −→ Check S w.r.t (upk, A) x ←− Adds (upk, A, x, S) in reg. Checks (x + γ)A ? = G 2 + yH ie., e(A, G 2 ) x · e(A, W ) · e(H, G 2 ) −y ? = e(G 1 , G 2 ) 
Figure 1: A user performs the protocol with the key issuer 
The public key, gpk, also implicitly include λ, κ, m, and a description of (p, G 1 , G 2 , G T , e). Join Protocol. We assume that there is a PKI environment , and the PKI is separated from the group environment . The certification authority will be assumed fully trusted (the only one). We also assume that each user U i , before joining the group, obtains a personal public key upk
1) Compute (T 1 , T 2 , T 3 , T 4 ) as follows: 
a. Choose α, β. b. Set (T 1 , T 2 , T 3 , T 4 ) = (αK, A+αH, βK, A+βG). 
2) Select r α , r β , r x , r z = Z * p at random. 
3) Compute 
R 1 = r α K, R 2 = e(T 2 , G 2 ) rx · e(H, W ) −rα · e(H, G 2 ) −rz , R 3 = r β K, R 4 = r α H − r β G. 4) Compute c = H(M, T 1 , T 2 , T 3 , T 4 , R 1 , R 2 , R 3 , R 4 ). 
5) Compute 
s α = r α + cα mod p, s β = r β + cβ mod p, s x = r x + cx mod p, s z = r z + cz mod p. 
The signature on message M is 
σ = (T 1 , T 2 , T 3 , T 4 , c, s α , s β , s x , s z ). 
Verify(gpk,M,σ). To verify a signature σ = (T 1 , T 2 , 
T 3 , T 4 , c, s α , s β , s x , s z ) 
 signed on message M , one performs the following: 1) Compute 
R 1 = s α K − cT 1 , R 2 = e(T 2 , G 2 ) sx · e(H, W ) −sα ·e(H, G 2 ) −sz · e(G 1 , G 2 ) e(T 2 , W ) −c , R 3 = s β K − cT 3 , 
R 4 = s α H − s β G − c(T 2 − T 4 ). 2) Check c ? = H(M, T 1 , T 2 , T 3 , T 4 , R 1 , R 2 , R 3 , R 4 ). 
Open(gpk,ok, (M, σ)). To trace the actual signer of a given signature σ, the open manager OM does the following: 1) Recover A: using the opening key ok= (ξ 1 , ξ 2 ), 
T 2 − ξ 1 T 1 = (A + αH) − ξ 1 αK = A + αH − αH = A, T 4 − ξ 2 T 3 = (A + βH) − ξ 2 βK = A + βH − βH = A. 
2) Find the actual signer, using the read-access to the registration table reg. 
3) Provide a publicly verifiable proof τ that @BULLET he did well in the step 1 -which is a simple proof of equality of discrete logarithms in 
G 1 . 
@BULLET the designated user has not be framed, using the corresponding S = Sign usk (A) in reg. New-Sign(gpk,gsk
1) Compute (T 1 , T 2 , T 3 , T 4 ) from DELG(A): 
a. Choose α, β. b. Set (T 1 , T 2 , T 3 , T 4 ) = (αK, A+ αH, βK, A+ βG). 
2) Select r α , r β , r x , r z ∈ Z * p at random. 3) Compute R 1 = r α K, R 2 = e(T 2 , G 2 ) rx · e(H, W ) −rα · e(H, G 2 ) −rz , R 3 = r β K, R 4 = r α H − r β G. 4) Compute c = H(M, T 1 , T 2 , T 3 , T 4 , R 1 , R 2 , R 3 , R 4 ). 
5) Compute 
s α = r α + cα mod p, s β = r β + cβ mod p, s x = r x + cx mod p, s z = r z + cz mod p. 
The signature is σ = (T 1 , T 2 , 
T 3 , T 4 , R 1 , R 2 , R 3 , R 4 , s α , s β , s x , s z ). 
New-Verify(gpk,M,σ). To verify a signature σ of a message M with σ=(T 1 , T 2 , 
T 3 , T 4 , R 1 , R 2 , R 3 , R 4 , s α , s β , s x , s z )
, one performs the following: 
1) Compute c = H(M , T 1 , T 2 , T 3 , T 4 , R 1 , R 2 , R 3 , R 4 ). 2) Check that R 1 ? = s α K − cT 1 , R 3 ? = s β K − cT 3 , R 4 ? = s α H − s β G − c(T 2 − T 4 ). 
3) Check e(T 2 , G 2 ) sx · e(H, W ) −sα · e(H, G 2 ) −sz ? = R 2 · e(G 1 , G 2 ) e(T 2 , W ) c . 
As previously mentioned, these modified algorithms are from direct application of Fiat-Shamir transformation on the zero-knowledge proof of knowledge in 
A Batch Verifier of XSGS Scheme
Let gpk = (G 1 , G 2 , G T , e, ψ, G 1 , K, H = ξ 1 K, G = ξ 2 K, G 2 , W = γG 2 ) 
be the group public key, and let σ j be the j'th signature on message M j for each j = 1, . . . , N , 
where σ j = (T j,1 , T j,2 , T j,3 , T j,4 , R j,1 , R j,2 , R j,3 , R j,4 , s j,α , s j,β , s j,x , s j,z 
). Then to batch the signatures, our XSGS Batch does the following: 1) Compute for all j = 1, . . . , N 
c j = H(M, T j,1 , T j,2 , T j,3 , T j,4 , R j,1 , R j,2 , R j,3 , R j,4 ). 
2) Check, for each j = 1, . . . , N , whether the following non-pairing equations are satisfied or not: 
s j,α K ? = R j,1 + c j T j,1 , s j,β K ? = R j,3 + c j T j,3 , s j,α H − s j,β G ? = R j,4 + c j (T j,2 − T j,4 ). 
 If the equations are satisfied then go to next; Otherwise , return 0 and exit. 
3) Choose a random vector (δ 1 , . . . , δ ℓ ) of ℓ b bit elements from Z p . 
4) Check that N j=1 R δj j,2 ? = e   N j=1 δ j (s j,α T j,2 − s j,z H−c j G 1 ) , G 2   · e   N j=1 δ j (−s j,α H + c j T j,2 ) , W   
If this is satisfied then return 1 and exit. Otherwise, return 0 and exit. 
Theorem 1. For security level ℓ b , the above algorithm is a batch verifier for the XSGS group signature scheme, where the probability of accepting an invalid signature is 
2 ℓ b . 
Proof. Since the algorithm XSGS Batch performs the first three non-pairing tests in G T separately, it suffice to consider for the pairing test. Now, suppose that is N valid signatures corresponding messages M j to be batched where 
σ j = (T j,1 , T j,2 , T j,3 , T j,4 , R j,1 , R j,2 , R j,3 , R j,4 , s j,α , s j,β , s j,x , s j,z ). Then e(T j,2 , G 2 ) sj,x · e(H, W ) −sj,α · e(H, G 2 ) −sj,z = R j,2 · e(G 1 , G 2 ) 
e(T j,2 , W ) cj , 
 and so for any random vector (δ 1 , . . . , δ N ) of ℓ b bit elements from Z q , we get 
e(T j,2 , G 2 ) sj,x · e(H, W ) −sj,α · e(H, G 2 ) −sj,z δj = R j,2 · e(G 1 , G 2 ) 
e(T j,2 , W ) cj δj . That is, R δj j,2 = e(T j,2 , G 2 ) sj,x · e(H, W ) −sj,α · e(H, G 2 ) −sj,z δj · e(T j,2 , W ) cjδj · e(G 1 , G 2 ) −cjδj 
Then we combine and simplify as 
e (δ j (s j,α T j,2 − s j,z H−c j G 1 ) , G 2 ) 
·e (δ j (−s j,α H + c j T j,2 ) , W ) = R δj j,2 . 
Multiplying every N equations, we finally have Now, we show that XSGS Batch = 1 implies that there is no invalid signature in the input signatures except with negligible probability. Suppose that σ j = (T j,1 , T j,2 , T j,3 , T j,4 , R j,1 , R j,2 , R j,3 , R j,4 , s j,α , s j,β , s j,x , s j,z ) are N signatures such that XSGS Batch = 1. Then 
N j=1 R δj j,2 = e   N j=1 δ j (s j,α T j,2 − s j,z H−c j G 1 ) , G 2   · e   N j=1 δ j (−s j,α H + c j T j,2 ) , W   
N j=1 R δj j,2 = e   N j=1 δ j (s j,α T j,2 − s j,z H−c j G 1 ) , G 2   · e   N j=1 δ j (−s j,α H + c j T j,2 ) , W   
It can be rewritten as 
N j=1 E 1 · E 2 · R −1 j,2 δj = 1. 
E 1 = e (s j,α T j,2 − s j,z H−c j G 1 , G 2 ) ; E 2 = e (−s j,α H + c j T j,2 , W ) . 
In the above equation, the braced element can be expressed as e(G 1 , G 2 ) βj since it is in G T , and so N j=1 e(G 1 , G 2 ) βj δj = 1. On the other hand, since e(G 1 , G 2 ) is a generator of G T , we have e(G 1 , G 2 ) N j=1 βj δj = 1 and N j=1 β j δ j ≡ 0 (mod p). Now assume that there is at least one invalid signature, say σ 1 . Then, β 1 = 0 because, if not then 
e (s 1,α T 1,2 − s 1,z H−c 1 G 1 , G 2 )·e (−s 1,α H + c 1 T 1,2 , W )·R −1 1,2 , 
and this means 
e(T 2 , G 2 ) s1,x · e(H, W ) −s1,α · e(H, G 2 ) −s1,z = R 1,2 · e(G 1 , G 2 ) 
e(T 2 , W ) c1 . 
 That is, σ 1 is a valid signature, which leads to a con- tradiction. Now since β 1 ≡ 1 (mod p), β 1 γ 1 ≡ 1 (mod p) for some γ 1 . Hence we have an equation 
δ 1 ≡ −γ 1 N j=2 β j δ j (mod p). 
Let E be an event that occurs if New-Verify(σ 1 ) = 0 but XSGS Batch(σ 1 , . . . , σ N ) = 1, and let X 1 be denote the set of all vectors < δ 2 , . . . , δ N > consisted with the last N − 1 values of ∆ in XSGS Batch. Then Pr
Finding Invalid Signatures in XSGS Scheme
In this section, we show how to find invalid signatures in XSGS group signature when batch instance contains bad signatures. The technique we use is due to Law and Matt 
σ j = (T j,1 , T j,2 , T j,3 , T j,4 , R j,1 , R j,2 , R j,3 , R j,4 , s j,α , s j,β , s j,x , s j,z ), j = 1
,. . ., N , and that the instance contains w invalid ones. To find the invalid signatures, we first compute, for each j = 1, . . ., N , 
c j ← H (M j , T j,1 , T j,2 , T j,3 , T j,4 , R j,1 , R j,2 , R j,3 , R j,4 ). 
We then verify the validity of the non-pairing equations: 
s j,α K ? = R j,1 + c j T j,1 , s j,β K ? = R j,3 + c j T j,3 , s j,α H − s j,β G ? = R j,4 + c j (T j,2 − T j,4 ). 
All j for which the equation is not satisfied indicate the positions of invalid signatures. Now, excluding the bad ones, we assume that there is no such index, i.e., for all j = 1, . . . , N the signatures hold the non-pairing equations. Let j 1 , . . . , j w denote the indexes of the invalid signature and I be the set {j t | t = 1, . . . , w}. We will find the set I consisting of bad signatures. For each j = 1, . . . , N , we choose δ j ∈ Z * p and set 
X j ← δ j (s j,α T j,2 − s j,z H−c j G 1 ) , Y j ← δ j (−s j,α H + c j T j,2 ) , Z j ← R δj j,3 . We define A k := N j=1 e(j k X j , U ) · e(j k Y j , V ) · Z j k j for each k = 0, . . ., w. If A 0 = 1 
then this means there is no bad signatures, i.e. w = 0. Otherwise, the instance should have invalid signature(s) and w 1. Also for each 
k = 0, . . . , w, A k = N j=1 (e(X j , U ) · e(Y j , V ) · Z j ) j k = j∈I (e(X j , U ) · e(Y j , V ) · Z j ) j k . By Newton formula [17], A w = A f1 w−1 · A · · · A (−1) w−2 fw−1 1 · A (−1) w−1 fw 0 , (1) where f i = f i (j 1 , . . . , j w ) 
is ith elementary symmetric polynomial over field Z p in w variables j 1 , . . . , j w . Thus, if we compute f 1 , . . . , f w (without knowing the positions of bad ones) for which Equation (1) holds then we have found the positions of invalid signatures. If no match is found, then there are more bad signatures than w in the batch instance. 
Algorithm. (Batch Identifier) 
1) Compute c j ← H(M j , T j,1 , T j,2 , T j,3 , R j,1 , R j,2 , R j,3 , R j,4 
) for each j. 
2) For j = 1 to N do a. Check the following non-pairing equations EQ(j): 
s j,α K, ? = R j,1 + c j T j,1 , s j,β K, ? = R j,3 + c j T j,3 , s j,α H − s j,β G ? = R j,4 + c j (T j,2 − T j,4 ). 
b. If EQT(j) fails then return j. (In this step, we will discard the invalid signatures. So, in what follows, we assume that the input signatures pass this step.) 
3) Choose a random vector (δ 1 , . . . , δ ℓ ) of ℓ b bit elements from Z p . 4) For j = 1 to N do a. Compute X j ← δ j (s j,α T j,2 − s j,z H−c j G 1 ), b. Compute Y j ← δ j (−s j,α H + c j T j,2 ), c. Compute Z j ← R δj j,3 . 5) Compute U ← −G 2 and V ← −W . 6) Compute A 0 ← e( N j=1 X j , U ) · e( N j=1 Y j , V ) · N j=1 Z j . 7) If A 0 = 1 then output "" valid instance "" , and exit. 8) Set k ← 1. 9) While k < N do a. Compute A k = e( N j=1 j k X j , U ) · e( N j=1 j k Y j , V ) · N j=1 Z j k j b. Find j 1 , . . . , j k such that A k = k t=1 A (−1) t−1 ft k−t , 
 where f t is ith elementary symmetric polynomial in j 1 , . . ., j k c. If such j 1 , . . ., j k exist then output j 1 , . . ., j k . as the position of invalid signatures and exit. d. Otherwise, set k ← k + 1. 10) Output "" all the input signatures are invalid "" and exit. 
Efficiency Analysis
Following earlier batch identification methods 
G 1 , and so N j=1 j k X j and N j=1 j k Y j (k = 1
 , . . . , w) can be computed in 2w(N − 1) addition in G 1 . Similarly, we can compute 
N j=1 Z j k j (k = 1, . . . , w) in w(N − 1) 
multiplications in G 2 which we will see in theorem 2 below. Additionally , to compute A k (k = 1, . . . , w) we require 2w pairings and 2w multiplication in G T . Using Shanks' Baby-step Giant-step and Law et al.'s method, the step 9-(b) (for all iteration up to w) can be done during 2 √ N or 8 (w−1)! N w−1 + O(N w−2 ) multiplications in G T according to w = 1 or w 2, respectively. For w 2, we require w − 1 inverses in G T additionally. (The computation 9-(b) is exactly the same with Law et al.'s algorithm). 
Theorem 2. N j=1 Z j k j (k = 1, . . . , w) in w(N − 1) multiplications in G 2 . 
Proof. First, by Stirling formula, we know that for j = 1, . . . , N . 
j k = k i=1 S k,i (j) i = k i=1 (−1) k−i i! S k,i j + i − 1 i 
where S k,i is the Stirling number of the second kind, 
(j) i = j · (j − 1) · · · (j − i + 1)
 , and k 1 is an integer . Thus, we have 
N j=1 Z j k j = N j=1 Z k i=1 (−1) k−i i! S k,i   i + j − 1 i   j = N j=1 k i=1 Z   i + j − 1 i   ((−1) k−i i!S k,i) j = k i=1     N j=1 Z   i + j − 1 i   i     (−1) k−i i!S k,i Letting U i = N j=1 Z   i + j + 1 i   j , we have N j=1 Z j k k = k i=1 U (−1) k−i i! S k,i i 
In order to compute U i , consider 
U i = N j=1 Z ( i+j−1 i ) j = Z ( i i ) 1 · Z ( i+1 i ) 2 · · · Z ( N +i−1 i ) N = Z ( i i ) 1 · Z ( i i−1 ) 2 · Z ( i−1 i−1 ) 2 · . . . . . . Z ( N +i−2 N −1 ) N · Z ( N +i−3 N −1 ) N · Z ( Z+i−4 N −1 ) N · · · Z ( N −1 N −1 ) N = Z ( N −1 N −1 ) N · Z ( N −1 N −1 ) N −1 · Z ( N N −1 ) N · · · Z ( i i ) 1 · Z ( i i−1 ) 2 · · · Z ( N +i−2 N −1 ) N Thus, by defining U 0 = N j=1 Z j 
, we can compute these w values U 1 , . . . , U w as follows. 
1) For j from 1 to N do 
V j ← Z j 
2) For k = 0 to w For j from N − 1 down to 1 do 
V j ← V j · V j+1 j ← j − 1 U k ← V 1 k ← k + 1 
The computations for k = 0 can be ignored because the values were already computed during Step 6 of the Batch Identifier. So the cost of this algorithm is w(N − 1) multiplications in G T . Finally, we compute prod 
k := N j=1 Z j k j for k = 1, · · · , w as follows. 1) prod 1 ← U 1 ; s 0 ← 0; s 1 ← 1; s 2 = · · · s w ← 0 2) For k = 2 to w prod k ← 1 For m = k down to 1 do s m ← s m−1 + ms m prod k ← U (−1) k−m sm m · prod m m m ← m − 1 k ← k + 1 
Note that this second part does not depend on N , but only on the number w of invalid signatures contained in batch. Also note that the algorithm runs with only O(w 2 ) operations. Since, as most batch verifier or batch identifier , we assume that N is large and w is small, we can ignore the cost O(w 2 ) operation. Therefore, we conclude that the cost of computing 
N j=1 Z j k j (k = 1, . . . , w) is approximately w(N − 1) multiplications in G T . 
Thus, we have approximate cost of the algorithm to find w invalid signatures in a batch of N signature - 2w(N − 1) addition in G 1 , 2w pairings, N exponentiations in G T , w − 1 inverses in G T , and N + 1 + 2 √ N multiplications in G T if w = 1 or w(N + 1) + 8 (w−1)! N w−1 multiplications if w 2. 
e(T2, sxG2 − cW ) · e(H, W ) −sα · e(H, G2) −sz · e(G1, G2) −c ? = R2. By cashing the e(H, W ), e(H, G 2 ) and e(G 1 , G 2 )
, one can verify the equation during 1 multi-exponentiation with two exponents in G 2 , 1 multi-exponentiation in G T , 1 pairing, and 1 multiplications in G T . We note that this property is mentioned in the paper 
Conclusion
In group oriented signature schemes, there are a large number of signatures to be verified. For such signature schemes, designing batch verification methods and finding invalid signatures (when batch test fails) are important issues. In this paper, by exploiting Ferrera et al.'s batch verifier of short signature scheme, we have proposed a batch verification scheme of an extremely short dynamic group signature scheme. In addition, by extending a batch identifier of Law et al. designed for special types of signature scheme (not for group signature schemes), we presented an batch identification method of the extremely short group signature scheme. 
"
"Introduction
 Insiders, by virtue of legitimate access to their organizations' information, systems, and networks, pose a significant risk to the organization. The Insider Threat Study 
Motivation and Related Work
 Internal attacks that continue undetected can cause serious harm to an organization. Perhaps most significant, they can expose the personal information of customers or employees. A breach of this kind -whether it is identity theft, inappropriate use of data or the sale of sensitive information -can leave an organization legally liable for associated damages and subject to regulatory fines. In addition, a company's competitive position could suffer if an insider uses intellectual property or trade secrets for unauthorized purposes. Attacks may also be designed to extort money or damage an organization's reputation. If they lead to IT downtime or damaged systems, they can also disrupt business operations and reduce the value of IT investments. With so much at stake, it is becoming increasingly important to address the threat of insider attacks -before they occur. In this section we give an account of related work. Currently , there exist a rich set of formal security models that can translate enterprise security objectives. This includes Discretionary Access Control (DAC) 
The Formal Model Framework
 3.1 Components of Formal Network Security Policy Model 
@BULLET Data Model The data model introduces the basic sets such as network subjects and objects that represent entities of the network security policy. @BULLET Formal State Machine Model The state machine model specifies the secure state of the underlying formal model of security. It comprises of the model entities defined in the data model and the invariant relationships between these entities. 
@BULLET Policy Model 
 The policy model specifies, through definition of network operation, how operations on secure state are constrained in order to satisfy the network security policy. 
 4 Formal Security Policy Model - A Case Study 
Data Model
Basic Sets
1) Network Subject An active entity in the system, which can be a user or an application process operating on behalf of users. The set of all subjects is called NSUB. 
2) Network Object 
This set includes the set of all entities designated as object in enterprise network system. We consider an object to be any resource in the system that can be assigned access rights. The set of all objects is called NOBJ. Formally; a set of objects is associated to a subject through the function NsubRef. 
Cartesian Product Type
In the NAC-PM model Cartesian product type is used to specify different authorization capability list. In the following the example of the Cartesian product types used in the model is presented. 
1) Connection Authorization List 
 The specification of the network connection authorization list as Cartesian product type is as follows. 
N ConnAuthLists : P(N OBJ × PAU T HM ODE). 
2) Access Authorisation List 
The specification of the network access authorization list as Cartesian product type is as follows. 
N AccLists : P(IEOBJ × PACCM ODE). 
Relations
In the NAC-PM model relations are used to represent the association between different network entities. In the following the example of the relations used in the model are presented. 1) Network Subject Reference A set of network objects is associated to a network subject through the relation N SubRef : N SU B −→ PN OBJ. The relation NSubRef can be specified as follows: 
Formal State Model
Abstract State
The formal model we describe here is state machine based model .We shall refer to this model as a Network Access Control-Policy Model (NAC-PM).We consider network system as a collection of entities and values. The set of relationship at any time between entities and values constitutes the state of the system. The state of the system changes whenever any of these relationship changes. Let us denote the set of possible states of the system with S. Some subset of S consists of exactly those states in which the system is authorized to reside. So whenever the system state is in authorized state, the system is secure . In addition, we also need to ensure that the system state is always an element of authorized state. Formally NACPM is specified in the following schema. 
S: PST AT E N op: PN OP Systran: PN OP × S S ——————————————— domSystran ⊆ PN OP × S ranSystran ⊆ S 
The set NOP describes the network operations related to connection control, information manipulation and flow control. The transformation function Systran describes the transition from one state to another state by applying one or a sequence of operations from the set NOP. The following schema captures the abstract state of the NACPM model. 
Initial State
 In this step, the initialization of the NACPM model is illustrated . Initial State is defined in terms of the abstract state and some extra predicates defining the initial conditions of the system. For more realistic initial state where N sub 0 = φ, N obj 0 = φ, we assume that the initial system state s 0 is defined in such a way that it satisfies all the conditions of the secure state. 
Policy Model
Network Operations-State Based View
In this step, list of legal network operation are defined. The fundamental approach used here is to capture the security constraints of the system and express them from two different points of view: The state based and Operation based. Describing two overlapping perspectives means that a certain amount of duplication can arise, but this also gives two natural approaches to validation. With two level of constraint specification, it is easier to be able to cross-check two such views than to work with a single complex view. In this section we focus on the state based view followed by operation based view in next section. Our primary goal of presenting the state based view is to define the secure state for the enterprise network system . For this purpose, firstly we need to identify all the properties of the secure network state. In order to identify these security properties we need to consider the security condition during the different phases of User interaction with enterprise network system. After going through different phases of Network system operations, the security properties of the secure state may be summarized as fol- lows. 
(neobj, a) ∈ N ConnAuth(nsub)∧ ∀(neobj) / ∈ N OD, (N SubSecF unc(nsub)) dominates (N ObjSecF unc(neobj))∧} ∀(neobj) / ∈ N OD, (N ObjSecF unc(neobj)) dominates(N SubSecF unc(nsub)) 
3) Information Access Property The Information Access property with security constraints is statically represented in InfoAccProp schema. 
IEobj ————————————————————— ∀(iebj) : IEobj; (neobj) : N obj|ieobj ∈ N explore(neobj)@BULLET (N ObjSecF unc(neobj)dominates (N ObjSecF unc(ieobj)) 
4) Authorized User Role Property The Authorized User Role property with security constraints is statically represented in UserRoleProp schema. 
————————————————————— ∀u ∈ U ser @BULLET (CurRole(u) ∈ AuthRole(u)) 
After defining the different security properties, we are now in position to define the secure state of the system. 
SecState N ACP M LoginP rop ∧ ConnP rop ∧Inf oAccP rop ∧ U srRoleP rop. 
Secure State
After defining the different security properties, we are now in position to define the secure state of the system. 
A state s is Secure if 
1) s satisfies the User Login Constraint. 
2) s satisfies the Connection Establishment Constraint. 
3) s satisfies the Information Control Constraint. 
4) s satisfies the User Role Constraint. 
Network Operations -Operation Based View
At the outermost level of the specification, the system is considered to be modelled by the initial state followed by an arbitrary sequence of legal operations. Operations on the system will cause a change of state. There are invariants which relate the before and after states for all operations on the system. Here we use the convention of placing the prime symbol ' in front of a state variable to refer to the new state. Unprimed variables refer to the value in the old state. We begin with description of administrative level operation followed by user level operation. Network administrative operation are used to manipulate security attributes of the subjects and objects, addition and deletion of subjects and objects and all other administrative tasks to ensure secure state of network computing environment . User level operations are used by network authorized users for information access and manipulation. The purpose of these user level network operations is to constrain the types of changes that the system user may make. 
 We here give example of with some fundamental operations related to network objects like addition, deletion and manipulation of security attributes. 
1) Addition of Network Object 
The addition operation with security constraints is illustrated in Add nobj schema. 
[Add nobj] SecState N ACP M nsubP : N sub neobjP : N obj aP : Authmode ————————————————————— neobjP / ∈ N obj ⇒ N Obj' = N obj ∪ {neobjP} N Sub' = N sub ∀nsubP ∈ N Sub' @BULLET neobjP, aP / ∈ N ConnAuth(nsub)'? neobjP, aP / ∈ N CurConn(nsub)' ∀nsubP ∈ N Sub'; ∀objP ∈ N obj@BULLET N ConnAuth(nsub)' = N ConnAuth(nsub) 
2) Access Class Assignment The access class assignment operation with security constraints is illustrated in 3) Deletion of Network Object The deletion operation with security constraints is illustrated in Delete nobj schema. This is an operation , when invoked results in the removal of neobj. 
 We now consider operations related to network subjects authorization like addition of new authoriza- tion. 
————————————————————— (CurRole(su)) ∈ (AuthRole(su)) neobjP ∈ N obj ∀nsubP ∈ N sub; neobjP, aP / ∈ N CurConn(nsub) ⇒ N obj' = N obj {neobjP} N Sub' = N sub ∀nsubP ∈ N Sub'; ∀objP ∈ N obj'@BULLET N ConnAuth(nsub)' = N ConnAuth(nsub) 
4) Addition of New Authorization The operation for adding new authorization with security constraints is illustrated in Set Auth schema. This is an operation, when invoked results in the creation of new capability for network subject nsub. 
[
⇒ N ConnAuth(nsub)' = N ConnAuth(nsub)∪ neobjP, aP∧ N sub' = N sub∧ N obj' = N obj ∀xP ∈ N sub'; ∀yP ∈ N obj'; (x, y) = (nsub, neobj) ⇒ N ConnAuth(x)' = N ConnAuth(x) 
5) Deletion of Authorization The operation for deleting new authorization with security constraints is illustrated in Delete Auth schema. 
————————————————————— (CurRole(su)) ∈ (AuthRole(su)) nsubP ∈ N sub; neobjP ∈ N obj a ⊆ AuthM ode ⇒ N CurConn(nsub) ⇒ N ConnAuth(nsub) neobjP, a ∧N Sub' = N sub ∧N ob' = N obj ∀x ∈ N sub'; ∀yP ∈ N Sub'; (x, y) = (nsub, neobj) ⇒ N ConnAuth(x)' = N ConnAuth(x) 
6) Network Connection Request The operation for establishing connection with network object with security constraints is illustrated in 
⇒ rep! = N op Allowed (CurRole(x)) / ∈ (AuthRole(x))∨ nsubP / ∈ N sub ∨ neobjP, aP / ∈ N ConnAuth(nsub) ∨(CurSecF unc(nsub))notdominates (N ObjSecF unc(neobj)) ⇒ rep! = N op Denied 
In the next section we consider the verification of the propose model. 
Model Verification
 The model verification consisted of two parts: the definition of an initial state, and an informal argument that each state transition function could produce a valid, secure final state when applied to a valid, secure start state. The second part of model verification requires critical examination of all those phases of system functionality during which system may undergo a state transition. The three major phases identified for model verification are Login Phase, Connection Phase and Network Operation Phase. Our aim here is to examine the security properties of the network system as it undergoes state transition and verify that the network system satisfies all the required security properties. Before we begin with phase level verification let us formally specify the change of system state. The change of network system state can be specified as follows. 
SecState N ACP M SecStat N ACP M ∧SecStat N ACP M '. 
Sometimes the state of the system is left unaffected by an operation, particularly if an error is detected or it is a status operation: 
ΞSecState N ACP M [SecState N ACP M |θSecState N ACP M ' = θSecState N ACP M ]. 
1) User Login Phase The security conditions that need to be satisfied during this phase are rightly specified by User Login Property. As no other operation is executed during this phase, therefore system starting with initial state satisfying security conditions of User Login Property will never go to a insecure state. We can now formally state this as follows: 
Constraint 1. The system described by network access control model NACPM satisfies the security conditions of User Login Phase if initial state s 0 satisfies the User Login Constraint. 
The network operation performed during this phase can be specified as NOP Login. The change of state on the execution of the network operation can be specified as SecState N ACP M N OP |'SecState N ACP M ∧SecState N ACP M '. 
Initially there are no logged in users in the network system. 
InitState N ACP M [SysState N ACP M |U sers = φ]. 
We assume here the initial state to be secure state. When Users login into network system with Login Property conditions satisfied, the state of the system will remain in secure state. 
SecState N ACP M IniState N ACP M ∧ LoginP rop. 
2) Network Connection Phase During this phase, network user tries to establish a connection with network resources available at remote network entity after successfully logging onto network system. Before the request for network connection is granted, the mandatory connection conditions and discretionary connection condition must be satisfied. These conditions are rightly specified as a part of Connection Property. 
For a system described by NACPM and starting at initial state s 0 , a system is said to be secure if the initial state s 0 satisfies the security condition of Connection Property. On application of sequence of system transition functions, system will undergo transition resulting in a sequence of states {s 0 , s 1 , s 2 }. To maintain the secure state of the system, every state in a sequence {s 0 , s 1 , s 2 } starting from previous secure state s j need to satisfy the security condition of the Connection Property. We can now formally state the model constraint during the connection phase as follows. 3) Network Operation Phase During this phase, the user tries to perform a sequence of network operations involving information transfer from one network entity to another. The first important security condition that is required before executing any network operation is to obtain an authorized network connection. The system may move to insecure state during this phase if the execution of network operations is allowed by NACPM without having an authorized network connection. The second important concern during this phase is the sequence in which network operation are performed. The sequence of network operation may also cause the system to move to insecure state from secure state. 
 We can now formally state the model constraint during the network operation phase as follows. The network operation performed during this phase can be specified as follows 
N OP Login ∨ Connect/ n obj/ r eq ∨ Connect/ n od/ r eq ∨ Read/ i eobj/ r eq ∨ Append/ i eobj/ r eq ∨ Add/ n obj ∨ Set/ A cls/ n obj ∨ Add/ n sub ∨ Set/ A cls/ n sub ∨ Set/ A uth ∨ Delete/ A uth ∨ Delete/ n sub ∨ Delete/ n obj ∨ Set/ n usr/ r ole ∨ Delete/ n usr/ r ole. 
The change of state on the execution of the network operation can be specified as follows. 
SecState N ACP M N OP |'SecState N ACP M ∧SecState N ACP M '. 
The state of the system will remain in secure state if following is satisfied. When there is a request for user level operation, user must have an authorised connection and when there is a request for administrative operation, user must have administrative privilege to perform it. This can be formally specified as follows. 
For User Level Operation 
SecState N ACP M SecState N ACP M ∧ LoginP rop ∧ConnP rop ∧ Inf oAccprop. 
For Administrative Operation 
SecState N ACP M SecState N ACP M ∧U serRoleP rop. 
Constraint 3. The system described by network access control model NACPM satisfy the security conditions of Network operation Phase if Connection Establishment Constraint is satisfied before executing network operations and secondly the network operation security conditions are satisfied before their execution. After defining the constraints for three phases, we are now in a position to state the security theorem to show that a system described by NACPM is secure. The security conditions are as follows. 
@BULLET The initial state is secure and @BULLET Every request for network resource access, information transfer and network administrative task satisfies the constraints stated in the phases discussed above. 
Conclusion
 In this paper, the key components of Network Access Control Policy Model are formalized in order to be sharp, precise and prevent their multiple interpretations. The schema describing the basic system elements was large due to multiple security constraints of network computing environment. In our future work our focus is to use symbolic computational environment to produce an animation of the formal specification to further refine the framework. 
"
"Introduction
The evolution of communication technologies and data transmission has enhanced global access to information. Consequently, the dissemination and sharing of digital data has become easily accessible to users. However, the robustness issue has become an increasing problem since existing protection techniques relying solely on encryption have become insufficient to address the advancing requirements of data protection. The influence of digitalwatermarking is increasing as a solution for countering various forms of illegal manipulation and piracy. Essentially , it consists of introducing an invisible signature in the host data, and then detecting possible manipulations applied on the watermarked data. Several techniques have been proposed in the literature, however, it remains that the invisibility versus robustness compromise under all attack scenarios remains an elusive goal in the research community. In this paper, we focus on image watermarking and highlight the image watermarking approaches based on the hybrid singular value decomposition (SVD) and the discrete cosine transform (DCT) techniques. For this purpose, we expose some important image watermarking works from the literature based on SVD and DCT. Before examining those works, we review some important concepts of the SVD and the DCT transforms. 
Background and State-of-art
The SVD Transform consists of factorizing a matrix M, into three matrices (components) U, S, V such that: 
[M ] = [U ][S][V T ] 
In fact, the inverse transform of the SVD is not entirely reversible, but rather is the product (
M mn = U mm .S mn .V T nn (1) 
where m, n are the image size (m represents the rows and n the columns). And U mm is the left singular vector, S mn are the singular values and V nn is the right singular vector. The DCT transform consists of changing the data from the spatial domain into the frequency domain. The corresponding DCT transform blocks are given by the following Equation (2): 
F (u, v) = 2 N c(u)c(v) N −1 x=0 N −1 y=0 pixel(x, y) cos[ π N u(x + 1 2 ]cos[ π N v(y + 1 2 ] 
(2) 
The DCT inverse is given by the Equation (3): 
f (x, y) = 2 N N −1 x=0 N −1 y=0 c(u)c(v)F (u, v) cos[ π N u(x + 1 2 ]cos[ π N v(y + 1 2 ] 
(3) 
Where c(u), c(v) = (2) −1/2 for u, v = 0, c(u), c(v) = 1 for u, v = 1, 2, · · · , N − 1, pixel(x, y) is the pixel value at position (x,y). The authors in 
Algorithm 1 Embedding algorithm 1: First compute the matrices: U, S, V , corresponding to both the original image i and the watermark w. 2: Apply the DCT embedding on the S i and S w components produced, suggesting that the embedding exists for only the singular-value matrices (e.g. following the DCT operation on the singular-value matrix of the original image i and the DCT operation on the singular-value matrix of the watermark). The embedding formula applied at this stage is given as follows: U iw =U i , DCT (S iw )=(1-α)DCT(S w ) + α DCT(S i ) and V iw =V i 3: Next 4: Calculate the DCT inverse of the DCT(S iw ), and apply the SVD −1 term to obtain the watermarked image i w as a result of the embedding. It is important to mention that the SVD −1 term does not mean the entire inverse transform, but rather, it is the product of the three matrices U, S, V T . This suggests that SVD process is not completely reversible. 
 4 Proposed Watermark Extraction Scheme 
 In our proposed algorithm, the extraction process consists of applying three inputs i w , w and i wa , representing the watermarked image, the watermark, and the attacked watermarked-image respectively. This unique combination of inputs and operations in the extraction process was not found elsewhere in the related literature and was used effectively to obtain promising results through perfect extraction of the watermark as reported in the next section. The extraction process is as illustrated in 
S wi = (1 − α)DCT (S iw ) + αDCT (S w ) (8) 
 Next, the Unmark operation as illustrated in the Equation (9), consists of the reverse embedding process, which follows from: 
W αSV D = 1 α W iSV D − 1 − α α i waSV D (9) 
 where w a , i wa are the extracted watermark and the attacked watermarked-image respectively. We note by Siw the singular value of the watermarked image i w , however the S wi consists to the results of the watermark embedding in the image i w and not the image i (it is simply for differentiating notations). 
Algorithm 2 Extraction algorithm 1: First, compute the matrices: U, S, V , for the watermark w, the watermark-image i w , and the attacked watermarked image i wa . 2: Apply the embedding of DCT( S w ) using the DCT(Si w ) term. The embedding result of this step gives w iSV DDCT . This embedding exists for only the singular-value matrices of the DCTs of S w and S iw respectively (e.g. following from the DCT operation on the singular-values matrix of the watermark w, and the DCT operation on the singular-values matrix of the watermarked image). The embedding formula used at this stage is given by (Equation (10)): 
 5 Tests and Robustness Evalua- tion 
In this work, tests were conducted on a database of 120 text-image samples. The original images i 
As known, attacks such as cropping, rotation and noise are considered as dangerous attacks due to their nature as a non linear transformation. The robustness of the proposed approach can be justified for two reasons: 1) The perfect invisibility of the watermark (in the embedding process); 2) embedding the watermark only in the singular values matrix S. the singular values matrix is known that contain the most important information in the image , which can very helpful to extract the watermark. The robustness evaluation of the proposed approach is based on calculating the most known similarity measures. Hence, our performance evaluation cost-function consists of comparing the similarity degree PSNR and SSIM 
P SN R = log 10 ( M ax w √ M SE ) (12) 
Given that 
M SE = 1 M N m−1 0 n−1 0 w(i, j) − w a (i, j) 2 (13) 
Where m, n are the image size and w(i,j), w a (i,j) are the values of the pixels in the position (i,j). The SSIM calculation is given by Equation (13): 
SSIM = (2µ w µ wa + c 1 )(2σ wwa + c 2 ) (µ 2 w + µ 2 wa + c 1 )(σ 2 w + σ 2 w + σ 2 wa + c 2 ) 
(14) 
Given that: µ w and µ wa are respectively the average of w and w a . σ 2 w and σ 2 w are respectively the variance of w and w a . σ wwa is the covariance of w and w a . L is the dynamic range, of the pixel intensity (typically 2 #bitsperpixel -1). 
c 1 =(k 1 L) 2 , c 2 =(k 2 L) 2 with k 1 =0
.01 and k 2 =0.03. From 
"
"Introduction
In the past two decades with the rapid progress in the Internet based technology, new application areas for computer network have emerged. At the same time, wide spread progress in the Local Area Network (LAN) and Wide Area Network (WAN) application areas in business, financial, industry, security and healthcare sectors made us more dependent on the computer networks. All of these application areas made the network an attractive target for the abuse and a big vulnerability for the community. A fun to do job or a challenge to win action for some people became a nightmare for the others. In many cases malicious acts made this nightmare to become a reality. In addition to the hacking, new entities like worms, Trojans and viruses introduced more panic into the networked society. As the current situation is a relatively new phenomenon, network defenses are weak. However, due to the popularity of the computer networks, their connectivity and our ever growing dependency on them, realization of the threat can have devastating consequences. Securing such an important infrastructure has become the priority one research area for many researchers. Aim of this paper is to review the current trends in Intrusion Detection Systems (IDS) and to analyze some current problems that exist in this research area. In comparison to some mature and well settled research areas, IDS is a young field of research. However, due to its mission critical nature, it has attracted significant attention towards itself. Density of research on this subject is constantly rising and everyday more researchers are engaged in this field of work. The threat of a new wave of cyber or network attacks is not just a probability that should be considered, but it is an accepted fact that can occur at any time. The current trend for the IDS is far from a reliable protective system, but instead the main idea is to make it possible to detect novel network attacks. One of the major concerns is to make sure that in case of an intrusion attempt, the system is able to detect and to report it. Once the detection is reliable, next step would be to protect the network (response). In other words, the IDS system will be upgraded to an Intrusion Detection and Response System (IDRS). However, no part of the IDS is currently at a fully reliable level. Even though researchers are concurrently engaged in working on both detection and respond sides of the system. A major problem in the IDS is the guarantee for the intrusion detection. This is the reason why in many cases IDSs are used together with a human expert. In this way, IDS is actually helping the network security officer and it is not reliable enough to be trusted on its own. The reason is the inability of IDS systems to detect the new or altered attack patterns. Although the latest generation of the detection techniques has significantly improved the detection rate, still there is a long way to go. There are two major approaches for detecting intrusions , signature-based and anomaly-based intrusion detection . In the first approach, attack patterns or the behavior of the intruder is modeled (attack signature is modeled). Here the system will signal the intrusion once a match is detected. However, in the second approach normal behavior of the network is modeled. In this approach, the system will raise the alarm once the behavior of the network does not match with its normal behavior. There is another Intrusion Detection (ID) approach that is called specification-based intrusion detection. In this approach, the normal behavior (expected behavior) of the host is specified and consequently modeled. In this approach, as a direct price for the security, freedom of operation for the host is limited. In this paper, these approaches will be briefly discussed and compared. The idea of having an intruder accessing the system without even being able to notice it is the worst nightmare for any network security officer. Since the current ID technology is not accurate enough to provide a reliable detection, heuristic methodologies can be a way out. As for the last line of defense, and in order to reduce the number of undetected intrusions, heuristic methods such as Honey Pots (HP) can be deployed. HPs can be installed on any system and act as trap or decoy for a resource. Another major problem in this research area is the speed of detection. Computer networks have a dynamic nature in a sense that information and data within them are continuously changing. Therefore, detecting an intrusion accurately and promptly, the system has to operate in real time. Operating in real time is not just to perform the detection in real time, but is to adapt to the new dynamics in the network. Real time operating IDS is an active research area pursued by many researchers. Most of the research works are aimed to introduce the most time efficient methodologies. The goal is to make the implemented methods suitable for the real time im- plementation. From a different perspective, two approaches can be envisaged in implementing an IDS. In this classification, IDS can be either host based or network based. In the host based IDS, system will only protect its own local machine (its host). On the other hand, in the network based IDS, the ID process is somehow distributed along the network . In this approach where the agent based technology is widely implemented, a distributed system will protect the network as a whole. In this architecture IDS might control or monitor network firewalls, network routers or network switches as well as the client machines. The main emphasis of this paper is on the detection part of the intrusion detection and response problem. Researchers have pursued different approaches or a combination of different approaches to solve this problem. Each approach has its own theory and presumptions. This is so because there is no exact behavioral model for the legitimate user, the intruder or the network itself. Rest of this paper is organized as follows: In Section 2, intrusion detection methodology and related theories are explained. Section 3 presents the system modeling approaches. In Section 4, different trends in IDS design are presented. Section 5 describes the feature selection/extraction methods implemented in this area. In Section 6, application of honey pots in the network security will be discussed. Finally, conclusions and future work are given in Section 7 and Section 8. 
Intrusion Detection
 The first step in securing a networked system is to detect the attack. Even if the system cannot prevent the intruder from getting into the system, noticing the intrusion will provide the security officer with valuable information . The Intrusion Detection (ID) can be considered to be the first line of defense for any security system. 
Artificial Intelligence and Intrusion Detection
Application of the artificial intelligence is widely used for the ID purpose. Researchers have proposed several approaches in this regard. Some of the researchers are more interested in applying rule based methods to detect the intrusion. Data mining using the association rule is also one of the approaches used by some researchers to solve the intrusion detection problem. Researchers such as Barbara et al. 
 2.2 Embedded Programming and Intrusion Detection 
 One approach is to preprocess the network information using a preprocessor hardware (front-end processor). In this method some parts of the processing is performed prior to the IDS. This preprocess will significantly reduce the processing load on the IDS and consequently the main CPU. Otey et al. 
Agent Based Intrusion Detection
The second approach is the distributed or the agent based computing. In this approach not only the workload will be divided between the individual processors, but also the IDS will be able to obtain an overall knowledge of the networks working condition. Having an overall view of the network will help the IDS to detect the intrusion more accurately and at the same time it can respond to the threats more effectively. In this approach, servers can communicate with one another and can alarm each other. In order to respond to an attack, sometimes it can be sufficient enough to disconnect a subnet. In this type of system in order to contain a threat, the distributed IDS can order severs, routers or network switches to disconnect a host or a subnet. One of the concerns with this type of system is the extra workload that the IDS will enforce on the network infrastructure. The communication between the different hosts and servers in the network can produce a significant traffic in the network. The distributed approach can increase the workload of the network layers within the hosts or servers and consequently it may slow them down. There are two approaches in implementing an agent based technology. In the first approach, autonomous distributed agents are used to both monitor the system and communicate with other agents in the network. A Multiagent based system will enjoy a better perception of the world surrounding it. Zhang et al. 
Software Engineering and Intrusion Detection
As the complexity of the IDS increases, the problem of developing the IDS becomes more and more difficult. A programming language dedicated to developing IDSs can be useful for the developer community. Such a programming language with its special components will improve the programming standard for the IDS code. IDS developers can enjoy the benefits of a new language dedicated to the IDS development. Such a language will improve both the programming speed and the quality of the final code. In a paper by Vigna et al. 
Some Selected Papers
 This section will describe selected papers in different research areas of the IDS technology. 
Bayesian (Statistical) Approach
As an example for the implementation of the Bayesian method in IDS, Barbara et al. 
f n (t) = λe −λt (λt) n−1 /(n − 1)!, t > 0 (1) 
In comparison to the Gamma distribution, Bilodeau et al. in their book 
There is still one question that remains to be answered and that is: "" Can one be sure that input parameters to an IDS are independent from one another? "" Dependent on the answer that might be yes or no, the method of approach can be different. We are doubtful about taking the parameters as independent (or conditional independent) parameters. This is because they serve the same purpose that is intrusion. However, on the contrary it can not necessarily mean that they are not dependent either, because not all of the activities in the network are intrusions and most of them are random legitimate activities. From our point of view, this subject deserves more study. This is so because during the design stage understanding the statistical nature of these events will help us to build the optimum model of the system. Barbara et al. 
1) The multinomial distribution assumption in the Bayes estimator. 
2) The assumption for the Naive Bayesian is that the parameters are conditional independent. 
 Once the behavior of the anomalies is similar, the proposed classifier will misclassify the attacks as it is evident in the reported results. Nevertheless this paper presents a good research work in intrusion detection. 
Fuzzy Logic Approach
 As an example for the fuzzy logic based approach, Dickerson et al. 
2) Gaining initial access phase. This phase includes the following parameters: Invalid password attempt, user terminal (network address) and user networking hours. 
 3) Gaining full system access. In this phase the following activities will be encountered: Illegal password file access attempt, illegal file/directory access attempt, illegal application access. 4) Performing the hacking attempt. In this phase intruder is going to use system facilities and information (Intruder's action). 5) Covering hacking tracks. Here the intruder will erase all the track or clues leading to the exposure of his access routes and identity (Audit log access). 6) Modifying utilities to ensure future access. In this phase, the intruder will create a backdoor in the system for himself to use it for his future access (Creating user account). In this paper, it is assumed (authors reason was the lack of data) that the model for the transition from one state to the other is linear. In other words, if anyone fails to access the system out of the regular working hours, then IDS will be 33.3% certain that this was an intrusion attempt. There is a separate membership function assigned to each one of the inputs to the system. Predefined rules together with output from the aforementioned functions are used by the fuzzy inference engine for deriving conclusions. Results reported using only 12 test subjects that looks to be a small number of test cases. 
Data Mining Approach
In the data mining approach, Lee et al. 
support(X) 
is the confidence for the rule. System keeps these rules for a period of time and uses them as the pattern for the event and behavior model for the users. As an example, Lee et al. 
support(X∪Y ) 
is the confidence for the rule and window is the sampling window interval. Analysis: Their idea for tracking users sounds very interesting . As it is explained in the paper, applying proper subintervals, system will reduce the length of the user records. At the same time, system will keep the historical records for the activities in its database (data reduction). Using the user records, system will generate a rule set for the activities within the network. At this stage, system can notice the irregularities and identify them (if they are known). Several test scenarios where presented. Since for the test purposes no standard datasets such as DARPA was used, it is hard to evaluate and compare their results. However, the proposed rule based approach is implemented in a good way. There is an abstraction on the anomaly detection concept in their reported work. In the report 
 1) The reported work is heavily counting on the connectivity or the availability of the network structure for their work. In some occasions, this cannot be expected. This is because in some DOS attacks not the server but the network switches might become saturated, which means that there will be no means by which these distributed systems can communicate with one another. 
2) The feature detection part has to be automated, this is because different attack strategies may have different features and in an adaptive system feature extraction has to be automated. However, authors in their implementation part of the report still report that human inspection is required in their system. 
3) We believe in the Black Box (BB) approach for this type of problems. It is also evident that modeling such a huge and complicated system needs both a great computational power and a large memory space. Nevertheless, one has to accept the fact that some times cost is high! The question is not the cost but on the other hand, it is about the possibility. In many occasions, learning speed in BB modeling is so slow that it is practically impossible to use it in the real world applications. However, if possible to implement , a BB model can never tell you how or why this situation is an attack! It just knows that this is an attack (seems like one or behaves like one)! No numerical results are presented in this report. Just the experimental environment and the experiments were described. 
Different Tends in Data Mining Approach
 In another group of the fuzzy logic and Genetic Algorithms (GA) related papers that are related to IDS concept , the one to start with is a work from Bridges et al. 
As it is evident in this last set of reported works, the fuzzy logic or Bayesian estimator based works can be included under either their own name or under the data mining category name. This is because the data mining work area is a multi-disciplinary area of research. Yoshida 
= Entropy(D) − GiG |G i | |D| Entropy(G i ) (2) 
Where T is the new test dataset and D is the original dataset that is going to be classified. The Entropy can be calculated using the following formula: 
Entropy(D) = n i=1 −p i log 2 p i (3) 
The G i is a subset of D classified by the test T and p i is the probability of class i. Cabrera et al. 
1) Temporal Rules: 
In this category in the detection rule, the antecedent and the consequence will appear in a correct order in distinct time instances (first antecedent followed by the consequence). The time series analysis in this work will deal with the design of the IDS. 
2) Report incoming danger: 
If the antecedent is true then after a certain time delay the attack will commence. Extraction of the temporal rules is an off-line operation that implements data mining methodologies. Extraction of the rules is performed in four stages where a large dataset from the network status evolutions to the history of security violations are analyzed. These stages are as follow: 
Step 1 Extracting the effective parameters/variables at the target side within the dataset. It is very important to know where to look for the clues. 
Step 2 Extracting the key parameters/variables at the intruder side within the dataset. IDS should be able to model the behavior of the intruder and these variables are used to detect the current state of the intrusion process. This information may derive from statistical casualty tests on some candidate variables plus variables from step 1. 
 Step 3 Determining the evolution of the intrusion process using the variables derived from step 2 and comparing them versus normal state of the network. It is clear that this work follows the anomaly detection approach. 
Step 4 In this stage the events extracted in the step 3 are being verified to see if they are consistently followed by the security violations observed in variables extracted in step 1. 
 In their description of this type of attacks, authors depict a timing diagram for a five stage transfer to the final network saturation in DDOS. These steps are: Master initiates installation of slaves (T0), Master completes installation of slaves (T1), Master commands the slave to initiate the attack (T2), Slaves start sending disabling network traffic to the target (T3), Disabling network traffic reaches the Target (T4), The target is shut down (T5). At this time chart, T0 is the start of the attack and T5 is when the network will go down. The time-period between T1-T2 is solely dependent on human factor and on when the master will decide to order the slave to start the attack. Considering this chart and by using the NMS within the IDS, the system might be able to predict or react to the attacks. Authors of the paper have prepared a test rig for the intrusion attack simulation and have carried out few interesting experiments on their test rig. The results are monitored and recorded. In this way, they can investigate the behavior of their IDS and study the results. Their main emphasis is on the data extracted from the MIB variables. They have included few charts from the MIB variables within the test period in their paper and have analyzed them. In intervals of 2 hours and sample rate of 5 seconds, 91 MIB variables corresponding to 5 MIB groups are collected by the NMS. These charts are synchronized so that they can be studied. Charts will provide us with an understanding of the behavior of the system during the normal and under attack periods. Since these charts are synchronized, one can easily relate the sequence of the events from one variable to the other. Later on in their paper, authors explain how to extract rules from this dataset. In their description they have assumed that the sampling interval is constant i.e. samples are taken in equal time intervals. The result is a multivariate time series. Among different definitions in the paper, two of them seem very interesting and they are explained in below: 
Causal rule "" If A and B are two events, define A τ ⇒ B as the rule: If A occurs, then B occurs within time τ . We say that A τ ⇒ B is a causal rule "" 
Precursor rule "" If A and B are two events, define A τ ⇐ B as the rule: If B occurs, then A occurred not earlier than τ time units before B. We say that A τ ⇐ B is a precursor rule "" 
 3 Modeling the Network as a Sys- tem 
The goal of finding a model for the network is to define the normal behavior and consequently anomaly in the behavior of the system. In the current literature, authors have defined the normal behavior of the network with regard to their own view points and no generic definitions are necessarily provided. A generic definition for the normal behavior and anomaly is proposed in below. Generic definition of the normal behavior of the system (network): The most frequent behavior of (events within) the system during a certain time period is called the normal behavior of the system. This behavior is the dominant behavior within the system and is the most frequently repeated one. Generic definition of the anomaly within the system (network): The least frequent behavior of (event within) the system during a certain time period is called anomaly or abnormal behavior. The repeating period for an anomaly event has a very long repeat period and its interval is close to the infinity. The most and the least frequent events will have respectively the lowest and the highest variances among all the other events. Therefore, effective parameters will be: the duration of the time period, the frequency and the variance of the events within that time frame. As it is clear from the literature, researchers have followed different approaches to improve accuracy and performance of their proposed IDS. However, the execution time constraint is always an obstacle or a challenge to overcome. Modeling a dynamic and complex system such as the network is very difficult. Thus, abstraction and partial modeling can be a good solution. This is why some researchers have chosen to separate different parts of the network and model them individually. The whole network can be divided into three different segments: host, user and the network environment. The user itself can be divided into two parts: legitimate user and malicious user (intruder). Different researchers have selected either of these groups. Assigning a behavioral model to either of these groups, one can derive a model of the legitimate or anomaly behavior for them. To model the host, it is required to monitor the system within an intrusion free working environment for a while. Using the collected data, it would be possible to derive a model for the normal behavior of the host. Any deviation from this model can be considered as an anomaly behavior and can be used for the intrusion detection. Usually there is a threshold value that determines the acceptable tolerance for any deviation from this model. Any activity that subjects the system to a deviation larger than a threshold value from its normal behavior model can be considered as an anomaly. Another approach is to monitor the system for a period of time and then assign a baseline to the systems parameters . In this approach, crossing the baseline denotes an anomaly behavior. It is also possible to assign a normal behavior model to a host and to consider any other behavior an anomaly. However, this approach will require applying limitation on the system that might not be desirable . This approach might be suitable for cases where system performs highly repetitive tasks and within a well defined work area. It also requires deep knowledge of the system. The approach is a specification based approach to the ID and Section 4.1 will provide a more detailed discussion on the specification based ID. Sekar et al. 
In another reported work, Botha et al. report a work 
Some Trends in IDS Design
Before getting started with describing trends in the IDS design, it should be noted that IDS has a classifier kernel . The kernel of the IDS is responsible for classifying the acquired features into two groups namely normal and anomaly, where the anomaly pattern is likely to be an attack. Nevertheless, there are occasions where a legitimate use of the network resources may lead to a positive classification result for the anomaly or signature based intrusion detection. As a result of this wrong classification , IDS will wrongly raise the alarm and will signal an attack. This is a common problem with the IDS and is called False Positive (FP). One of the parameters to measure the quality of an IDS is the number of its FP alarms. The smaller is the number of false positives, the better is the IDS. 
Signature Based, Anomaly Based and Specification Based IDS
Signature based intrusion detection (misuse detection) is one of the commonly used and yet accurate methods of intrusion detection. Once a new attack is launched, the attack pattern is carefully studied and a signature is defined for it. The signature can be a name (in characters) within the body of the attack code, the targeted resources during the attack or the way these resources are targeted (attack pattern). Studying the attack pattern, security specialists can design a defense against that attack. Later on, using the proposed defense method, the IDS is updated accordingly to recognize the new attack patterns and to response to them. This approach is very efficient for the known attacks and produces small number of FP alarms. However, as the main short coming of this approach, it is not capable of detecting novel attacks. Once the attack pattern is slightly altered, this approach will not detect the altered versions of the old attacks. Thus, this approach is only efficient in detecting previously known attacks . There is another approach for detecting the novel and unseen attacks that follows. Another widely used ID method is the anomaly detection approach 
Network Based IDS and Host Based IDS
As it was mentioned earlier in the introduction section of this article, network based and host based systems are two categories of the IDSs. The network based IDS is responsible to protect the entire environment of the network from the intrusion. This task asks for full knowledge of the system status and monitoring both the components of the network and the transactions between them. Agent technology plays a key role in this strategy. Network is the infrastructure for a distributed system. Therefore, agents are a natural choice for this approach. Collecting information within the network and processing them, responding to the requests and commands of the kernel of the IDS or working as an individual, all can be accomplished using agent based technology. The network based IDS is capable of accessing the network routers and instructing them to perform tasks. Using this feature, system can ask the router to disconnect a terminal or a subnet that has become a security threat. There are several reported works in this area. In a paper by Foo et al. 
Different Approaches to IDS Design
An active IDS will provide a predefined response to the detected intrusions. The passive IDS is only responsible for monitoring the system and to inform the administrator once an intrusion occurs or to produce an advance warning. The response concept is related to the active IDS. This response can be a reaction to a security breach in the system or a preemptive response to avoid a security breach. One of the main goals of any active IDS is to prevent the security breach and not just to respond to the threat. Continuing their earlier work 
Where to Look for the Features?
 Desired features for the IDS depend on both the methodology and the modeling approach used in building the IDS. These features are usually numerous. Thus considering the volume of data, processing all of them will take quiet awhile. In order to speed-up the process, these features are usually preprocessed to reduce their size, while increasing their information value. There are numerous approaches reported in this area. Most of the reported research is concerned with the header of the packets. However , recently researchers have valued the body or the payload of the packets as well. This part of the packets was usually disregarded due to its large volume and the extensive processing time required for processing them. Researchers such as Lee et al. 
Honey Pot (HP)
 Despite its effectiveness, not until recent years this approach has been taken seriously within the academia. However, recently research in this area has gained some momentum. Maybe this lag was due to the fact that there is no theoretical concept involved in the HP approach, it is just a deception. HP is mainly a heuristic approach and is based on the concept of bait and trap. Nevertheless, industry sector is very attracted to this concept. There are a number of products available that use the HP to trap undetected intrusion attempts. Generally speaking, HP is a deception based approach to detect actions of a deceitful enemy (the intruder). The HP concept has attracted much attention over the internet and there are numerous sites dedicated to this concept 
@BULLET Honeynet 
 1) By selecting correct resources for the HP to emulate as bait, one can distinguish different interests of the attackers. Having the intruder fallen in the trap, HP can start studying the opponent. HP will let the intruder to navigate through the emulated environment and will log its actions. HP or the security officer can gain much information by studying logged actions of the attacker, the targeted resources and attackers information regarding the system such as username, password, etc. 
2) Keeping the attacker busy. HP will buy more time for the response system or the security officer/administrator of the system to come-up with a proper response to the attack attempt. Response latency time is very important since attacks might be too fast for the response system to react to them. At the same time, providing the system administrator with sufficient time might help in finding the root of the connection that the attacker is using. Tracking the attackers can be very difficult and it requires a great deal of experience and time to trace them back to their origin. 
 3) Another benefit gained by HP keeping the attacker busy is to waste attackers time and to prevent him compromising other resources with a fast speed. For example, in a port scanning scenario keeping the attacker waiting for a reply may significantly slowdown the whole process. HP can do this in a different way as well, that is, by emulating a working environment, HP can convince the intruder that he is in a real system and let him try to perform the intrusion. The more convincing is the HP, the more time the intruder will spend in the environment. 
 @BULLET Within an IDS guarded system, HP will detect unnoticed attacks. In this way, HP will increase the reliability of the IDS. The detection offered by the HP is independent of the type of attack that is enforced on the system. @BULLET Another benefit for using a HP in a system is with respect to the overhead on the processor. In the IDS approach every packet transaction in the system has to be monitored and analyzed. Thus, the IDS approach will generate a high computational power demand within the system. The same is true with the data transfer within the system. Due to the size of the packets transferred within the system, data transfer time will consume a large part of the processing time. In addition to the processing time, data transfer will consume different data transfer related resources of the system. Using the HP will reduce the processing and resource consumption overhead on the system. The processing time will be provided for the HP process only when the HP monitored resources within the system are utilized. In this way, the overhead enforced on the host system by the HP during the HPs idle period will be negligible. This scenario is very similar to the differences between the interrupt based and the polling based IO handling methods in the computer hardware design. 
 In order to improve the performance of the HP operation and to increase the hit probability of the HP, usually a large number of HPs are placed in a network. These HPs are close to the important resources in the network and operate both as a guardian and a decoy for them. A group of HPs that are distributed in a system is called a Honey Pot farm. Nowadays, intruders aware of the HP technology try to avoid the HPs or even take advantage of them. To do so, they have implemented tools to detect the HP and once detected, they disengage the HP. At the same time, HPs are usually deployed to protect a resource or a data. Once the HP is detected, it is most probable to find a valuable resource in its neighborhood. Zhang et al. 
Conclusions
 Considering the surveyed literature, it is clear that in order to be able to secure a network against the novel attacks , the anomaly based intrusion detection is the best way out. However, due to its immaturity there are still problems with respect to its reliability. These problems will lead to high false positives in any anomaly-based IDS. In order to solve this problem, usually a hybrid approach is used. In the hybrid approach, the signaturebased approach is used together with the anomaly-based approach. In this way, the second approach is mostly used for the novel tactics while the accuracy of the first approach (signature based approach) will provide a reliable detection for the known attacks. Specification-based approach is only good when system specifications and details are known and applying limitations on the user is acceptable. The generic definition of the normal behavior and the anomaly behavior in the system are presented in this paper. The intension for introducing these generic definitions was to help researchers to converge on the definition of the normal behavior of the network. In network-based IDS, agent based systems play an essential role. In such systems a distributed processing architecture is a must and system has to collect information from different components within the network. Implementing such architecture, one should avoid increasing the network traffic. Large volume of data and non-deterministic normal behavior of the network are two major challenges in IDS design . As the volume of data using the header of the packets is already very large, using information in the payload will make the process even slower. However, there are works reported by some researchers in this area that show good progress in using packets payload for the analysis. The intrusion detection products were analyzed with respect to the software or appliance based production and the benefits of either of the designs were discussed. Building hardware appliances can be more difficult for companies with lower development budget. However, appliance based IDSs are more appreciated in the market. From the consumer point of view, appliance based IDS is easier to install and to maintain. In manufacturers view, appliance based IDS is a more secure design to manufacture but as the same time more expensive to produce. Another aspect of the IDS design is the issue of the missed attacks. If some attacks are not detected by the IDS, there are no means to notice them. This is especially the case with the novel attacks. In addition to all other benefits, HP technology can help to expose these attacks. The accuracy of the HP technology depends on the number of HPs distributed in the system (population of the HP farm). The larger the population of the HPs the more International Journal of Network Security, Vol.1, No.2, PP.84–102, Sep. 2005 (http://isrc.nchu.edu.tw/ijns/) 100 accurate is the detection rate. Increasing the accuracy is not the only benefit for implementing the HP technology, but it can be used for other purposes as well. For example , HP can be used for studying or slowing down the intruder. 
Future Work
As for the future work, intension is to produce an IDS capable of anomaly and signature based intrusion detection . There are two options in front of us, i.e. host based or network based IDS. The host based IDS can be easier to implement, though the network based IDS needs more time and effort for its implementation and design. In return, the network based IDS will provide a more reliable and more accurate IDS. The network IDS needs to have environment awareness. Thus, the network based IDS needs special sensors for its work. Agent based technology is one of the essential blocks in this distributed architecture design methodology. The selected approach for our future work is the network based software product. However, the host based approach will be considered as well. The project timeframe and the budget are main issues with regard to this decision. Nevertheless, accepting the expenses, it is always possible to convert a software based IDS to the appliance version of it. From the theoretical point of view, it is intended to improve the accuracy of the anomaly based intrusion detection . One way to do so is to use the payload of the packets. Therefore, it is necessary to envisage a method either to reduce the size of the data or to process the data more quickly. The main idea is to find a method to handle high volume of data with less information loss. For the same reason, features should be evaluated with respect to their information value. In this way, every feature will be associated with a coefficient of importance that determines its overall effectiveness in comparison to the other features. Efficient algorithms and programs can provide a great help for this purpose. 
"
"Introduction
As the Internet evolves from an academic and research network into a commercial network, more and more organizations and individuals are connecting their internal networks and computers to the insecure Internet. As a result, mass retail electronic commerce in the Internet is born, with more traditional business and services (such as electronic banking, bill payment, gaming) being conducted and offered online over open computer and communications networks. One of the greatest concerns with this phenomenon is the confidentiality and the integrity of data transmitted over the insecure Internet, and hence with being able to provide security guarantees becomes of paramount importance . Many initiatives have been proposed to address this concern; and cryptographic data encryption and authentication constitute the tools to address it. Typically security guarantees are provided by means of protocols that make use of security primitives such as encryption, digital signatures, and hashing. Menezes, van Oorschot, & Vanstone 
 Data integrity guarantees the data has not been tampered with or modified. To achieve this notion, several approaches such as the use of a one-way hash function together with encryption or use of a message authentication code (MAC), have been adopted to detect data manipulation such as insertion, deletion , and substitution. In cryptographic protocols, data integrity ensures that elements such as nonces and identity fields are protected. 
Authentication ensures the identification, which can be of the data (Data Origin Authentication) or the entity (Entity Authentication). Data origin authentication implicitly provides data integrity since the unauthorised alteration of the data implies that the origin of the data is changed, as the origin of data can only be guaranteed if the data integrity has not been compromised in any way. The use of a one-way hash function together with encryption or use of a message authentication code (MAC) can help to achieve data origin authentication. Entity authentication is a communication process by which a principal establishes a live correspondence with a second principal whose identity should be that which is sought by the first principal. In cryptographic protocols, both entity authentication and data origin authentication are essential to establish the key. Non-repudiation ensures that entities cannot deny any previous commitments or actions. Non-repudiation provides data integrity and data origin authentication implying the origin of the data which in turn, implies the integrity of the data. Application of digital signature mechanisms helps to achieve this notion of non-repudiation. In cryptographic protocols, nonrepudiation ensures that entities cannot deny any previous commitments or actions. 
Cryptographic protocols are designed to provide one or more of these security guarantees between communicating agents in a hostile environment, e.g., to achieve confidentiality of data in a session established by some entity, A, with another intended entity, B, one may use a cryptographic algorithm, called symmetric encryption. This cryptographic algorithm produces a ciphertext message, c, when given some plaintext message, m. A then sends B the ciphertext c over the insecure communication channel . Only B who has a pre-established secret information (with A), known as a shared session key, that is fresh and unique for each session, can decrypt c to obtain m (i.e., achieving the notion of data confidentiality). The above security properties are usually meaningful when guaranteed during a complete session of closely related interactions over a communication channel (and in many cases, open and insecure communication channels). In most of these cases, there is a need for some temporary keys (e.g., an encryption key for a shared-key encryption scheme in the above-mentioned scenario). The advantages of using temporary (session) keys relative to using long-term keys directly are four-fold: 
 1) to limit the amount of cryptographic material available to cryptanalytic attacks; 
2) to limit the exposure of messages when keys are lost, 
 3) to create independence between different and unrelated sessions (since in a real world setting, it is normal to assume that a host can establish several concurrent sessions with many different parties. Sessions are specific to both the communicating parties), and 4) to achieve efficiency (e.g., if our long-term keys are based on asymmetric cryptography, using session keys based on (faster) symmetric cryptography can bring a considerable gain in efficiency). 
The many flaws discovered in published protocols for key establishment and authentication over many years, have led to a dichotomy in cryptographic protocol analysis techniques between the computational complexity approach 
@BULLET Since this approach does not account for all possible attacks, the security guarantees are limited and often insufficient. 
@BULLET This approach does not provides a clear framework on a formal description for a "" secure "" protocol and what constitutes an "" attack "" . 
In the provable security approach for protocols, the description of protocols security and the goals provided by the protocols are formally defined (e.g., we will know whether a proposed attack is valid and what it means to be secure). 
Case Study. In 
this work, we advocate the importance of proofs of protocol security and the proposal of any protocol should provide a rigorous proof of security (as we argue that protocols without any computational proofs of security leads one to question the level of trust in the correctness in such protocols). As a case study, we revisit a recent work of Lee, Kim, Kim, & Oh 
The BR93 Model
In this section, an informal overview of the BR93 model is provided primarily for the benefit of the reader who is unfamiliar with the model. For a more comprehensive description, the reader is referred to the original paper 
@BULLET The Send(U 1 , U 2 , i, m) query allows A to send some message m of her choice to either the client Π i U1,U2 at will. Π i U1,U2 , upon receiving the query, will compute what the protocol specification demands and return to A the response message and/or decision. If Π i U1,U2 has either accepted with some session key or terminated , this will be made known to A. @BULLET The Reveal(U 1 , U 2 , i) query allows A to expose an old session key that has been previously accepted. Π i U1,U2 , upon receiving this query and if it has accepted and holds some session key, will send this session key back to A. @BULLET The Corrupt(U 1 , K E ) query allows A to corrupt the principal U 1 at will, and thereby learn the complete internal state of the corrupted principal. The corrupt query also gives A the ability to overwrite the longlived key of the corrupted principal with any value of her choice (i.e. K E ). This query can be used to model the real world scenarios of an insider cooperating with the adversary or an insider who has been completely compromised by the adversary. 
@BULLET The Test(U 1 , U 2 , i) query is the only oracle query that does not correspond to any of A's abilities. If Π i U1,U2 has accepted with some session key and is being asked a Test(U 1 , U 2 , i) query, then depending on a randomly chosen bit b, A is given either the actual session key or a session key drawn randomly from the session key distribution. Note that in the original BR93 model, the Corrupt query is not allowed. However, such a query is important as it captures the notion of unknown key share attack 
Definition of Partnership
Π i A,B Π j B,A 'start' α 1 α 1 β 1 β 1 α 2 α 2 * time τ 0 time τ 1 time τ 2 time τ 3 
Note that the construction of conversation shown in Definition 1 depends on the number of parties and the number of message flows. Informally, both Π i A,B and Π j B,A are said to be BR93 partners if each one responded to a message that was sent unchanged by its partner with the exception of perhaps the first and last message. and a responder oracle Π j B,A who engage in conversations C A and C B respectively. Π i A,B and Π j B,A are said to be partners if they both have matching conversations, where 
C A = (τ 0 , start , α 1 ), (τ 2 , β 1 , α 2 ) C B = (τ 1 , α 1 , β 1 ), (τ 3 , α 2 , * ), for τ 0 < τ 1 < . . . 
The matching conversations play a significant role as they bind together incoming and outgoing messages, and uniquely identify a particular session. 
Definition of Freshness
The notion of freshness is used to identify the session keys about which A ought not to know anything because A has not revealed any oracles that have accepted the key and has not corrupted any principals knowing the key. Definition 2 describes freshness in the BR93 model, which depends on the notion of partnership in Definition 1. Definition 2 (Definition of Freshness). Oracle Π i A,B is fresh (or it holds a fresh session key) at the end of execution, if, and only if, oracle Π i A,B has accepted with or without a partner oracle Π j B,A , both oracle Π i A,B and its partner oracle Π j B,A (if such a partner oracle exists) have not been sent a Reveal query, and the principals A and B of oracles Π i A,B and Π j B,A (if such a partner exists) have not been sent a Corrupt query. 
Definition of Security
Security is defined using the game G, played between a malicious adversary A and a collection of Π i Ux,Uy oracles for players U x , U y ∈ {U 1 , . . . , U Np } and instances i ∈ {1, . . . , N s }. The adversary A runs the game simulation G, whose setting is described in 
Adv A (k) = 2 × Pr[b = b ] − 1. 
 We require the definition of a negligible function, as described in Definition 3. 
Definition 3 ( 
Definition 4 describes the BR93 security definition. 
Definition 4 (BR93 Definition of Security 
2) Adv A (k) is negligible. 
If both requirements of Definition 4 are satisfied, then Π i A,B and Π j B,A will also have the same session key. 
Additional Notions
In order to help the descriptions later we here introduce another property which is often ignored. 
Definition 5 (Key Integrity 
@BULLET For a key transport protocol, key integrity means that if the key is accepted by any principal it must be the same key as chosen by the key originator. 
A 
Stage 1: A is able to send any Send, Reveal, and Corrupt oracle queries at will. 
Stage 2: At some point during G, A will choose a fresh session on which to be tested and send a Test query to the fresh oracle associated with the test session. Note that the test session chosen must be fresh. Depending on a randomly chosen bit b, A is given either the actual session key or a session key drawn randomly from the session key distribution. 
Stage 3: A continues interacting with the protocol by making any Send, Reveal, and Corrupt oracle queries of its choice. 
Stage 4: Eventually, A terminates the game simulation and outputs a bit b , which is its guess of the value of b. @BULLET For a key agreement protocol, key integrity means that if a key is accepted by any principal it must be a known function of only the inputs of the protocol principals. 
The key replicating attack defined by Krawczyk 
Definition 6 (Key Replicating Attack 
Case Study
In this section, we present the necessary mathematical preliminaries. We then revisit the two-party ID-based key agreement protocol due to 
Mathematical Preliminaries
Using the notation of Boneh & Franklin 
Bilinearity. For Q, W, Z ∈ G 1 , bothêbothˆbothê(Q, W + Z) = ˆ e(Q, W )·ê(Q, Z) andêandˆandê(Q+W, Z) = ˆ e(Q, Z)·ê(W, Z). 
Non-Degeneracy. For some elements P, Q ∈ G 1 , we havê e(P, Q) = 1 G2 . 
Computability. For some elements P, Q ∈ G 1 , we have an efficient algorithm to computê 
e(P, Q). 
A bilinear map, ˆ e, is said to be an admissible bilinear map if it satisfies all three properties. Sincê e is bilinear, the mapêmapˆmapê is also symmetric. 
Lee–Kim–Kim–Oh ID-Based Key Agreement Protocol
@BULLET (P 1 P ub = s 1 P 1 , s 1 ) and (P 2 P ub = s 2 P 2 , s 2 ) 
denote the public/private key pairs of P KG 1 and P KG 2 respec- tively, 
@BULLET (Q 1 A , S 1 A = s 1 Q 1 A ) 
denotes the public/private key pair of A acquired from P KG 1 , 
@BULLET (Q 2 A , S 2 A = s 2 Q 2 A ) 
denotes the public/private key pair of A acquired from P KG 2 , and @BULLET a ∈ R Z * q1 and b ∈ R Z * q2 denote the ephemeral private keys of A and B respectively 
2 A B a ∈ R Z * q1 T AB = aP 2 W A = aP 1 P ub T AB , W A − → b ∈ R Z * q2 T BA = bP 1 T BA , W B ← − W B = bP 2 P ub 
Computation of partial session keys 
K 1 AB = ˆ e 1 (aS 1 A , T BA ) K 1 BA = ˆ e 1 (bQ 1 A , W A ) K 2 AB = ˆ e 2 (aQ 2 B , W B ) K 2 BA = ˆ e 2 (bS 2 A , T AB ) 
Computation of actual session keys For completeness, we will replicate the computation of the partial session keys in order to demonstrate that the actual session key agreed in the absence of a malicious adversary, are indeed the same. 
SK AB = H(H 1 2 (K 1 AB ), H 2 2 (K 2 AB )) SK BA = H(H 1 2 (K 1 BA ), H 2 2 (K 2 BA )) 
K 1 AB = ˆ e 1 (aS 1 A , T BA ) = ˆ e 1 (as 1 Q 1 A , bP 1 ) = ˆ e 1 (Q 1 A , P 1 ) abs 1 = ˆ e 1 (bQ 1 A , as 1 P 1 ) = ˆ e 1 (bQ 1 A , aP 1 P ub ) = ˆ e 1 (bQ 1 A , W A ) = K 1 BA K 2 AB = ˆ e 2 (aQ 2 B , W B ) = ˆ e 2 (aQ 2 B , bP 2 P ub ) = ˆ e 2 (Q 2 B , P 2 ) abs 2 = ˆ e 2 (bs 2 Q 2 B , aP 2 ) = ˆ e 2 (bS 2 B , T AB ) = K 2 BA Since K 1 AB = K 1 BA and K 2 AB = K 2 BA , SK AB = H(H 1 2 (K 1 AB ), H 2 2 (K 2 AB )) = SK BA . 
Security Attributes
The protocol described in 
 Known (Session) Key Security. It is often reasonable to assume that the adversary will be able to obtain session keys from any session different from the one under attack. A protocol has known-key security if it is secure under this assumption. This is generally regarded as a standard requirement for key establishment protocols. As Blake-Wilson, Johnson & Menezes 
As discussed by Boyd & Mathuria 
 Protocols proven secure in the BR93 model that allow the Corrupt query are also proven secure against the unknown-key share attack: that is if a key is to be shared between some parties, U 1 and U 2 , the corruption of some other (non-related) player in the protocol , say U 3 , should not expose the session key shared between U 1 and U 2 as described by Choo, Boyd, & Hitchcock 
(Joint) Key Control. In a key agreement protocol, it is usually desired that no involving entity is able to choose or influence the value of the shared (session) key. This prevents any involving entity from forcing the use of an old key and the non-uniform distribution of the session key. 
A Key Replicating Attack
 Figure 4 describes the execution of the two-party IDbased key agreement of Lee, Kim, Kim, & Oh 
K 1 AB = ˆ e 1 (aS 1 A , T BA · c) = ˆ e 1 (as 1 Q 1 A , cbP 1 ) = ˆ e 1 (Q 1 A , P 1 ) abcs 1 = ˆ e 1 (bQ 1 A , acs 1 P 1 ) = ˆ e 1 (bQ 1 A , acP 1 P ub ) = ˆ e 1 (bQ 1 A , W A · c) = K 1 BA K 2 AB = ˆ e 2 (aQ 2 B , W B · c) = ˆ e 2 (aQ 2 B , bcP 2 P ub ) = ˆ e 2 (Q 2 B , P 2 ) abcs 2 = ˆ e 2 (bs 2 Q 2 B , acP 2 ) = ˆ e 2 (bS 2 B , T AB · c) = K 2 BA SK AB = H(H 1 2 (K 1 AB ), H 2A A B a ∈ R Z * q1 c ∈ R Z * q1 , Z * q2 T AB = aP 2 W A = aP 1 P ub T AB , W A − → T AB · c, W A · c − → b ∈ R Z * q2 T BA = bP 1 T BA · c, W B · c ← − T BA , W B ← − W B = bP 2 P ub 
Computation of partial session keys 
K 1 AB = ˆ e 1 (aS 1 A , T BA · c) K 1 BA = ˆ e 1 (bQ 1 A , W A · c) K 2 AB = ˆ e 2 (aQ 2 B , W B · c) K 2 BA = ˆ e 2 (bS 2 A , T AB · c) 
Computation of actual session keys 
SK AB = H(H 1 2 (K 1 AB ), H 2 2 (K 2 AB )) SK BA = H(H 1 2 (K 1 BA ), H 2 2 (K 2 BA )) 
Conclusion
 Through a detailed study of the two-party ID-based authenticated key agreement protocol of Lee, Kim, Kim, & Oh 
We conclude that designing and analysis correct and secure key agreement protocols remains a hard problem. One (hard) way of obtaining a correct and secure protocol is to provide a complete and detailed proof specification (considering all possible scenarios), rather than providing an informal security analysis or sketchy proofs. We may speculate that the flaws in these three protocols could have been discovered by the protocol designers if complete proof specifications had been constructed. 
"
"Introduction
 In many applications there are contexts in which it is necessary to check and verify the operations performed by some entity (possibly an administrator) on another entity (possibly a computer system). For example, in industrial environments some jobs are left in outsourcing to external companies; the operations performed by the external personnel should be controlled in some way, and at the same time, the privacy of the workers must be guaranteed, with the ability, in case of need, to verify and link the operations performed with the person who made them. The system proposed in the present paper is related to the previously discussed context, considering system and network administrators, administered systems and networks , with the objective of giving a secure and reliable auditing system. The main characteristics of this system are the ability of logging all the operations that occur in a complex environment, linking these operations to the entities involved (namely, the administrator of the system and the system itself), with the guarantees that: @BULLET The log does not immediately disclose its content (for privacy reasons), i.e. it is encrypted; @BULLET The log's content may be examined only by the entities having the rights to perform this operation (i.e. only the authorized people, administrators or third parties, may decrypt the log entries content, with some defined modes); 
@BULLET Log entries cannot be directly related to the entities that are involved in the activity described in the log entry itself; 
@BULLET The log cannot be modified without detection (i.e., if the log is modified this can be discovered by the auditors that will eventually check the content). 
The ideas presented in this paper have many fields of application, where the main problem to be solved is the logging of some activity for a subsequent control. The paper is organized as follows: Section 2 presents some works related to the argument we deal with, Section 3 presents the terms of the problem and foresees some solutions, deeply discussed in Section 4. Section 5 shows some characteristics of the solution we propose, whilst Section 6 deals with the specific problem of permitting a set of users to access a log line. Finally, Section 7 presents some conclusions. 
State of the Art
The commonly accepted definition of log is given in 
Preliminary Considerations
In our approach we consider a log file as a journal in which information coming from various activities is stored in a set of lines; each line refers to a particular event of interest in each activity. We do not consider the physical implementation, and refer to the definition given in 
Possible Approaches
As previously seen, for privacy reasons the data section of the log line is encrypted with an randomly generated key. To record this random key for later use in auditing, we consider two possible approaches: A. Each auditor has his own symmetric secret key; the system encrypts the random key for each auditor with his secret key. 
 B. Each auditor has his own pair of asymmetric public/private keys; the system encrypts the random key for each auditor with his public key. The auditor will use his private key to decrypt the random key and access the log line data. 
The approach of directly encrypting the log line data with the auditor's public keys was taken into consideration , but discarded due to the computational complexity of the asymmetric encryption and the amount of data that were required to be encrypted and stored. Let's see the structure of the log lines in the two cases A and B. The index i runs over the log lines; k is an index that runs over the identifiers of the entities involved in the logged transaction, and j indexes the various auditors. (In this example of line structure, auditors from 0 to j − 1 have access to the line content, auditors from j to n have not. This will become clearer throughout the rest of the pa- per.) Case A: 
L i = {T S i , U k , l i , E(A i /D i ), E(K 0 /A i ), · · · , E(K j−1 /A i ), E(K j /H(A i )), · · · , E(K n /H(A i )), HC i , S i } 
Case B: 
L i = {T S i , U k , l i , E(A i /D i ), α(K + 0 /(A i , R 0 )), · · · , α(K + j−1 /(A i , R j−1 )), α(K + j /R j ), · · · , α(K + n /R n ), HC i , S i }. 
 Note, in the preceding lines, the parts that are underlined , whose meaning will be discussed deeply. In the following the meaning of the various parts is presented. @BULLET T S i : It is the timestamp issued by a Time Stamping Authority 1 or it is a timestamp assigned by the system . If a Time Stamping Authority comes into play, then TSi is calculated on the result of a hash function (e.g. like SHA-1 
It may express the time of logging or the time of the reception of the line. Both are possible approaches. Even if the data contained in the log line already contains a timestamp, T S i may be useful for some cross checks on the data. @BULLET U k : It is a set of data related to the log entry; in our environment it represents the identifier of the user (administrator) that generated the data in the log line, along with an identifier of the administered system . For the responses from the systems, this may be an identifier of the system and of the user whose this response is sent. In order to enforce the secrecy of this field the method proposed in 
@BULLET K 0 , · · · , K n 
 are the auditor's secret keys, used in Approach A that uses symmetric encryption of A i . 
@BULLET K + 0 , · · · , K + n 
are the auditor's public keys, used in Approach B that uses asymmetric encryption of A i . @BULLET R 0 , · · · , R n are random values used to preserve the elusion property we will discuss in a following section. @BULLET E(x/y) represents a symmetric encryption function that uses the key x to encrypt data y; it returns the encrypted data. A good candidate function could be AES 
HC i = H(HC i−1 , T S i , U k , l i , E(A i /D i ), E(K 0 /A i ), · · · , E(K j−1 /A i ), E(K j /H(A i )), · · · , E(K n /H(A i )). 
Proposal B: 
HC i = H(HC i−1 , T S i , U k , l i , E(A i /D i ), α(K + 0 /(A i , R 0 )), · · · , α(K + j−1 /(A i , R j−1 )), α(K + j /R j ), · · · , α(K + n /R n )). 
The first element of the hash chain, namely HC 1 , is computed using as previous element a fixed and known value for HC 0 which may be recorded, without encryption , in the beginning of the log file. When a line is verified , the hash of the previous line should be trusted, thus a verification of the signature of the previous line should be performed. 
Encryption and Exclu- sion/Elusion
The objective of this section is to introduce the intrinsic security of the log file when it is stored on any device. In fact, it has to be taken into account that the security of the log file should not change even if it is saved and copied for backup purposes. That is, the log file content should not be alterable (by anyone) and should not be visible by non-authorized people. To avoid the disclosure of the content to nonauthorized people we already introduced the idea of encrypting the data with a random key A i (that changes for every line): thus, this key is used to encipher the data; afterwards, the key A i is made available to the various auditors encrypting it with the personal key of every auditor that should have access to that data. When the key has been encrypted, then it is destroyed, and only the authorized auditors will be able to reconstruct the original data. If there are auditors that should not have access to a particular log line, then A i is not encrypted for them; instead: @BULLET H(A i ): A one-way hash of the key, is encrypted in case of Approach A. @BULLET a random value R r 3 (different for every auditor) is encrypted with the public key in case of Approach B. This is done for the following two reasons: Exclusion: It is easy to exclude one or more auditors from accessing the log line data, simply giving them a fake key: a random number in Approach B or obtained from the right key, but through a noninvertible function, in Approach A. In the literature are presented many one-way hash functions easy to compute. The use of a different A i for every line allows for a fine granularity in giving access to every log line only to a subset of auditors. Thus the exclusion is local to every log line. We used a one-way hash function in Approach A because we considered it as an efficient source of randomness. Due to the elusion property discussed below, we had to use a different random number for every auditor in Approach B. 
Elusion: It is easy to see that simply looking at the log file it is not possible to understand which auditors have access to which log lines. This is due to the fact that we encrypt the key A i for every auditor. At this point we distinguish the two Approaches A and B. previously introduced. A. Access to a line depends on the possession of A i , useful to decrypt the line, or H(A i ), that does not allow access to the line. But, for the properties of symmetric encryption, it is impossible to deduce which case (if A i or H(A i )) has been encrypted for an auditor. Note that it is important to use H(A i ) that changes for every line. In fact, suppose to use a constant value for those auditors that should not have access to a line. Then, encrypting a constant value using a fixed key (the secret key of the auditor) will disclose the lines that are not accessible to an auditor, simply by inspection of the log file looking for a repeated value for an auditor. Note also that the use of H(A i ) has the only objective to create a random number in an efficient manner; that is, instead it could be used a random number 
B i different from A i . 
B. From the properties of asymmetric encryption it is impossible to deduce which auditor is able to decrypt the key A i . Note the use of the random values R r to ensure the elusion property. For those auditors having rights to access to the log line, then the key A i is encrypted along with a random number (different for every auditor) to ensure that an auditor decrypting the key A i is not able, through asymmetric encryption using the other auditors' public key, to deduce which of them has access to the log line. At the same time, for auditors that do not have access to a line, a random value (also in this case, different for every auditor) is encrypted with the public key of each auditor, thus the resulting value is undistinguishable from the encryption of the correct key A i and a random value. 
Group Auditing
One of the objectives of the system is to give different access modes to different auditors. We have already presented a method for allowing or not the access to a line. In this section we present how to give access to a single line to a group of auditors. For example, some log lines should be decrypted only when a set of at least three auditors out of five agree on looking at its content; with this approach it is possible to access the data even if not all the auditors belonging to one group are available. 
State of the Art on Secret Sharing
We give a brief introduction on some proposals for secret sharing found in the literature. 
Group Access to a Log Line
In our application, we use the method from 
@BULLET Each auditor should be able to access the content of a log line both alone (if he has the rights) or with the cooperation of other auditors (if he belongs to a group of auditors that should have access to the line); 
 @BULLET When a group of auditors has used a secret to disclose the content of a line, then this secret must be useless if used to disclose the content of other lines; the reason lies in the fact that when a group of auditors agree in looking at the content of a line, then some of them may not agree in disclosing the content of other lines to the members of the same group; @BULLET Each auditor may belong to any number of groups (also none). We obtain the previous results by distributing to the auditors that need a group access to a log line, a share to determine the secret A i . That is, instead of encrypting the secret A i for an auditor, we encrypt a part that allows the reconstruction of the complete secret A i . This implies that to decrypt a line there may be: @BULLET Users that have access to the line as alone entities, i.e. they have E(
K j /A i ) or α(K + j /(A i , R j )) 4 ; 
 @BULLET Users that do not have access to the line as alone entities , i.e. they have E(
K j /H(A i )) or α(K + j /(R j )) 4 ; 
@BULLET Users that have access to the line only with the collaboration of at least k users, i.e. they have 
E(K j / Ai ) or α(K + j /SA i ) 4 , where 
Ai is the share of a secret that allows disclosing A i with the collaboration of other k − 1 users. Users may belong to many groups, thus having many shares of the secret (obviously, the various shares will be related to different polynomials). Note that the three sets of users may be not disjoint (the first two are obviously disjoint). Thus, our system allows for users that may access a log line by themselves, or in collaboration with other users also, or only when other group members agree in disclosing the content of a line. Let's see which data is saved for every auditor that potentially has access to a line: 
E(Kj/[H(Ai), ID group , A i , ID group , A i , · · · ]) α(K + j /[Rj , ID group , A i , ID group , A i , · · · ]) 4 
or, for some embodiments : 
α(K + j /[H(Ai), Rj , ID group , A i , ID group , A i , · · · ]Rj ) 4 . 
 In this example the j-th auditor has not access as individual , but only as belonging to some groups. If a user does not belong to a group (or a group does not have access to the line) then may be left as a set of zeroes of the right size (using a proper encryption function, all this data will preserve the elusion property). To add an auditor to the group of auditors, it is sufficient to give him a new share based on the polynomial, encrypting this share with the auditor's key. To exclude an auditor from a group it is sufficient not to give him his share anymore. To modify the minimum number of auditors necessary to disclose a log line, a different polynomial should be used, according to 
Observations on Multiple Groups
A question that may arise on the security of the method applied on multiple groups is: what happens if shares of different groups on the secret A i are joined together? Do these parts allow the determination of A i ? That is, let's suppose the worst case. Imagine m − 1 auditors of a group (requiring m auditors to compute A i ) colluding with m −1 auditors of another group (requiring m auditors to compute A i ). Moreover, note that the two groups may overlap. Let's write the two polynomials we want to determine: 
y = α m −1 x m −1 + α m −2 x m −2 + · · · + α 1 x + A i y = β m "" −1 x m −1 + β m −2 x m −2 + · · · + β 1 x + A i . 
The target is to determine the α values, the β values and A i ; that is, in all m + m − 1 values. The colluding auditors have m + m − 2 points (possibly not distinct), m − 1 from one polynomial, and m − 1 from the other polynomial. This allows to write a system of m + m − 2 equations in m +m −1 variables. The target may not be reached because the system of equations is undetermined if we make the assumption that a single polynomial of degree m is undetermined if only m − 1 points are available . But, to discover the shared key, it is sufficient to determine A i : we show that this is not possible. We will call the set of equations coming from the first polyno- mial and the set of equations coming from the second polynomial Θ. Given that A i cannot be determined from Π (
c 1 α j + c 2 A i = b 1 . 
For the same reason, reducing Θ will lead to 
c 3 β k + c 4 A i = b 2 , 
where the c m s and b n s are constant values. The system of these two equations does not allow to determine A i because α j and β k are different unknowns (they are coefficients from different polynomials). Thus, to answer the question we posed in the beginning of this section, even if different auditors from different groups collude to determine the shared key, they will not be able to get it unless the required number of auditors in one of the groups is reached.  The same demonstration works also in the case two auditors belonging to different groups own the same share (i.e. the same point in the plane, where two distinct polynomials intersect). 
Conclusions
In this paper we dealt with the problem of keeping the content of a log file both unalterable (without detection) and private. The first objective is obtained through the use of signatures and a hash chain that links all the log lines together. The privacy of the content is obtained by means of encryption. Log lines are encrypted with a key that is successively encrypted with the keys of the entities that should have access to the line; moreover, the key may be distributed (encrypted) among a set of entities, if these entities should have access to the line only together. The solution proposed allows also for privacy in the access to a line: this was obtained with an exclusion/elusion property of the method. That is, the data to be used for accessing a line is encrypted in a way that it is impossible, if not in possession of the decrypting key, to decide if the data is useful or not for disclosing the log line content. The consequence of this is that no one is able to decide whether an auditor has access or not to a line (except the auditor itself). The proposed method is efficient in the sense that uses only one key to encrypt a line, and distributes this key to the auditors with the modes previously discussed. Nonetheless this key changes for each line, leaving a fine granularity in giving or not the possibility to various auditors to access a log line content. The method is especially suitable to scenarios where a log entry size is considerable.  Pier Luigi Zaccone received a degree in Computer Science from the University of Torino, Italy, on June 1994. Since 1997 he has been working in an Information Security Unit at Telecom Italia Research Department. He has worked on Information Security Risk Analysis and System Management Security. He is currently leading a corporate project regarding non-repudiation for system manage- ment. 
Davide 
"
"Introduction
Digital signatures allow a signer who has established a public key to sign a message in such a way that any other party who knows the public key, as well as the origin of the public key, can verify that the message originated from the signer and has not been modified 
Zhang et al.'s Attack on the Zhang-Chen-Susilo-Mu Signature Scheme
 In this section, we briefly review Zhang-Chen-Susilo- Mu short signature scheme (ZCSM for short), and then give some comments on the cryptanalysis introduced by Zhang, Yang, Zhong, Li and Takagi in 
The ZCSM Short Signature Scheme
Setup: Pick a bilinear group and associate parameters (G, G T , ˆ e, g, q) where g is a generator of G and q ≡ 3 mod 4. The message space is M = {1, . . . , (q − 1)/2}. The system parameter set is (G, 
G T , ˆ e, g, q). 
Key Generation: Pick x, y ∈ Z q at random and compute u = g x , v = g y . The private key is (x, y) and the corresponding public key is (u, v). Sign: Given a message m ∈ M, a user with private key (x, y) generates a signature as follows: 
First, randomly picks r ∈ Z q ; If m is a quadratic residue modulo q, computes 
σ = g (x+my+r) 1/2 . 
Otherwise, if m is a quadratic non-residue modulo q, then computes 
σ = g (x−my+r) 1/2 . 
The signature is (σ, r). Verify: Given a public parameters (G, G T , q, g), public key (u, v), a message m ∈ M, and a signature (σ, r) by the following equation: 
ˆ e(σ, σ) = ˆ e(uv m g r , g) orê orˆorê(σ, σ) = ˆ e(uv −m g r , g). 
Remarks on Zhang et al.'s Attack
In 
"" Given a valid signature (σ, r), (σ ′ , r ′ ) = (σ 2 , 2r) is a forgery for the forged public key (u ′ , v ′ ) = (u 2 , v 2 ) "" . 
 We first note that the resulting values (σ ′ , r ′ ) associated to the public key (u ′ , v ′ ) was thought of as a forgery on the (same) message that has been previously signed using public key (u, v), and that, to succeed in forgery attack , the adversary has to replace the public key (u, v) by (u ′ , v ′ ). Such attacks can be considered as a combination of key replacement attack and strong existential forgery attack. On the other hand, traditional digital signature schemes, as well as ZCSM scheme, assume certificate authority implicitly, and those schemes are not designed to provide strong unforgeability security. So, to the best of our knowledge, their claimed attacks might not be practical to ZCSM scheme. Regardless our above discussion, in the following, we point out that their so-called forgery attack under the chosen message and public key attacks does not correctly operate in the sense that the resulting forgery cannot pass the verify algorithm. Recall that the valuesêvaluesˆvaluesê
(σ ′ , σ ′ ) andê andˆandê(u ′ (v ′ ) m g r ′ , g
ˆ e(u ′ (v ′ ) m g r ′ , g) = ˆ e(g 2x (g 2y ) m g 2r , g) = ˆ e(g, g) 2(x+my+r) . 
In order for (σ ′ , r ′ ) to pass the verification algorithm, the two values should be equal. However, since we may assume x + my + r ̸ ≡ 0 (mod q), it is impossible except when q = 2. Note also that q was chosen to be an odd prime with q ≡ 3 mod 4. Therefore, we conclude that their assertion is not true. Now, we present a correct attack in the sense of Zhang et al.'s attack. Indeed, it is easy to mount such an attack as shown below. Suppose that we are given a signature (σ, r) on a message m generated by using a private key (u, v) = (g x , g y ). For simplicity, we assume that the message m was chosen from quadratic residues modulo q, since the case where m is a quadratic non-residue modulo q can be treated similarly. 1) Choose a quadratic residue s modulo q with 
t 2 = s where s, t ∈ Z * q ; 2) Set σ ′ ← σ t and r ′ ← sr. 
Then (σ ′ , r ′ ) 
 is verified as valid signature on the message m with respect to the public key (u ′ , 
v ′ ) = (u s , v s ): ˆ e(σ ′ , σ ′ ) = ˆ e(g s(x+my+r) , g) = ˆ e(g xs g ysm g rs , g) = ˆ e(u ′ (v ′ ) m g r ′ , g). 
 2.2 Zhang-Yang-Zhong-Li- Takagi(ZYZLT) Short Signature Scheme 
Zhang et al. suggested an improvement of ZCSM scheme to resist their attacks, and gave its correctness and security proofs 
Sign: Given a message m ∈ M, a user with secret key (x, y) produces a signatures as follows: 
@BULLET randomly picks a r ∈ Z q ; 
@BULLET If m is a quadratic residue modulo q, compute 
σ = g 
The signature is (σ, r). Verify: Upon receiving public parameters (G, G T , g, q, z), public key (u, v), a message m ∈ M, and a signature (σ, r), anyone can verify the validation of the signature by the following equations: ˆ 
e(σ 2 , ug m v r ) ? = z orêorˆorê(σ 2 , ug −m v r ) ? = z. 
If either equation holds, it outputs valid. Otherwise, it outputs invalid. Suppose that (σ, r) is a signature on message m generated by a user with his private key (u, v) = (g x , g y ). Again, for simplicity, we assume that the message was chosen in the set of quadratic residues. 
Then ˆ e(σ 2 , ug m v r ) = ˆ e(g 2 √ x+my+r , g x g m g r ) = ˆ e(g 2 √ x+my+r , g x+my+r ) = ˆ e(g, g) 2(x+my+r) √ x+my+r . 
Note that, in their scheme, x + my + r was assumed to be a quadratic residue modulo q, and thatêthatˆthatê(, ) is a symmetric and non-degenerate bilinear pairing in whichê whichˆwhichê(g, g) has order q. Such pairings are usually constructed from the modified Weil pairing over super-singular curves. Let us denote x + my + r mod q by α. Then β 2 = α for some β ∈ Z * q and the last element of G T in the above equations can be written byêbyˆbyê
(g, g) (2β 2 )/β = ˆ e(g, g) 2β . 
In order for the signature (σ, r) to pass the verification test, the following equality should be satisfied: 
ˆ e(g, g) 2β = ˆ e(g, g). 
That is, 2β ≡ 1 (mod q) and hence β = 2 −1 (mod q). As a result, the verification algorithm outputs "" valid "" only if the square root of x + my + r mod q equals the value 2 −1 mod q. This is impossible since x and y were randomly selected during key set up phase and r is randomly chosen each time to generate signatures. Therefore, we conclude that no one can verify signatures generated by the signing algorithm and hence ZYZLT scheme is not a correct signature scheme. 
Conclusions
We have analyzed Zhang et al.'s forgery attack on ZCSM short signature scheme and point out that the attack is not correct one. We then fix the attack to work well in the sense of Zhang et al.'s attack. Furthermore, we show that their improvement cannot be a secure digital signature scheme even though the authors claimed it is proved in the standard model. 
"
"Introduction
The digital counterpart to a handwritten signature is the digital signature, which is an important primitive element in public key cryptosystems. Generally, a signer signed on a message (document) and then sent receiver (verifier) the digital signatures. The recipient verifies signatures by means of a predefined formula for verification of signatures . The signer may be a host computer, a mobile computer, or a smart card. Usually the latter two entities (called mobile signers) are powered by battery, which implies that they have limited processing capability. Some trapdoor hash functions 
 Before discussing computational cost of the RSA signature described above, we quantify the cost of computation . For a typical public key cryptosystem, the bit length of N is 1024. Therefore, we use MM to denote a modular multiplication of two 1024-bit numbers modulo a 1024-bit modulus. Also let |a| stand for the bit length of string a. Ignoring the computational cost of hash function H(), generation and verification of signature demand 1.5|d| and 1.5|e| MMs (on average) respectively to complete these processes. A typical value for |d| may be |d| = |N | = 1024. Thus constructing a digital signature requires an amount of 1536 MMs. This would consume 63.9 milliseconds. The timing was obtained using NTL library in a 3.2 GHz Pentium 4 running XP with 512 M Bytes RAM. Thank for NTL 
Contributions
Further improvement in the calculation of online phase is possible. The paper will propose a scheme to replace the modular reduction with a conventional multiplication (a 160-bit number multiplied by a 1024-bit number). Usually , the computational cost required to do a modular reduction is more expensive than a multiplication. Thus the online computation is cut down. An implementation shows that our scheme saves about 30% as compared with the scheme in 
Organization
Section 2 reviews the scheme of trapdoor hash function in 
Scheme Review and Definitions
This section reviews the trapdoor hash function, called T H, presented in 
T H HK (m 1 , r 1 ) = g m1||r1 mod n, 
(1) where m 1 ∈ R {0, 1} l , r 1 ∈ R Z λ(n) . 
Note that symbol "" a ∈ R A "" means that an element a is randomly selected from the set A; m||r represents concatenation of strings m and r. Since the values of m 1 and r 1 are drawn randomly from the corresponding domains, the quantity R = T H HK (m 1 , r 1 ) can be computed during idle time (offline). Also, signer applies signing key to generate signature, s = H(R) d mod N . The triple (m 1 , r 1 , s) is stored for later usage. Assume that a signer has determined to sign a target message m 2 . Then the signer chooses a stored triple (m 1 , r 1 , s) and performs the trapdoor operation which is defined as: 
T H T K (m 1 , r 1 , m 2 ) = r 2 = 2 k (m 1 − m 2 ) + r 1 mod λ(n). 
(2) 
In Equation (2), k = |λ(n)| is the bit length of the trapdoor key T K = λ(n). 
An Example
Perhaps an example will help us to describe the usage of the trapdoor hash function. This example assume that a trapdoor hash function and a RSA signature scheme are combined together to improve the efficiency of online computation when generating signature. The hash operation and signing signature are performed in the offline phase. Namely, the signer chooses two random numbers, m 1 and r 1 , and computes the hash value, R = T H HK (m 1 , r 1 ). Then signer signs on the resultant hash value. Therefore all of the heavy computations are calculated in the offline phase. Upon receiving the target message, M , signer initiates the online phase and executes the trapdoor operation. Signer uses the secret trapdoor key to find m 2 and r 2 which shall lead to 
T H HK (m 1 , r 1 ) = T H HK (m 2 , r 2 ). 
Therefore signer needs not to sign again. The details are as follows. 
Offline computation: Signer chooses at random a pair 
(m 1 , r 1 ) ∈ R {0, 1} l × Z λ(n) 
and performs hash operation on this pair, i.e., R = T H HK (m 1 , r 1 ) = g m1||r1 mod n. Then using signer's secret key d, a RSA signature s is generated, s = H(R) d mod N. The signer stores the triple (m 1 , r 1 , s) in storage. 
Online computation: When receiving the target message M , the signer performs the trapdoor operation on (
m 1 , r 1 ) and m 2 = H 1 (M ); namely T H T K (m 1 , r 1 , m 2 ) = r 2 = 2 k (m 1 − m 2 ) + r 1 mod λ(n). 
Then the signed message on M is the tuple (M, r 2 , s). Note that in addition to the message M and signature s, an appended string r 2 is added to the signed message. Verification: The signed message is verified by checking that s e = H(T H HK (m 2 , r 2 )) mod N, where m 2 = H 1 (M ). It is easy to see that T H HK (m 2 , r 2 ) = T H HK (m 1 , r 1 ). Suppose that the original signature is secure against generic chosen message attack, the resultant signature is enhanced to be secure against adaptive chosen message attack as shown by Theorem 1 in 
Secure Trapdoor Hash Function
 A secure and practical trapdoor hash function must possess some properties. As introduced in 
Definition 1. A secure trapdoor hash function has three properties: 1) Efficiency: Given hash key HK and 
(m, r) ∈ {0, 1} l × Z λ(n) 
 , the hash value T H HK (m, r) is computable in polynomial time. 
2) Collision resistant: Given hash key HK, there exists no probabilistic polynomial time algorithm outputs two pairs 
(m 1 , r 1 ) and (m 2 , r 2 ) 
producing the same hash value with non-negligible probability, where 
m 1 = m 2 , (m i , r i ) ∈ {0, 1} l × Z λ(n) , i = 1, 2. 
3) Trapdoor collisions: Given trapdoor key T K and a triple 
(m 1 , m 2 , r 1 ) ∈ {0, 1} l ×{0, 1} l ×Z λ(n) , 
∈ Z λ(n) and satisfies T H HK (m 1 , r 1 ) = T H HK (m 2 , r 2 ). Further, if r 1 
 is uniformly distributed over its domain then r 1 and r 2 have statistically indistinguishable distribution in the same do- main. 
The Proposed Trapdoor Hash Function
If we interchange the concatenation order of r and m in Equation (1) (the Definition of hash operation), the new Definition of hash operation is as follows: 
T H HK (m, r) = g r||m mod n. 
Then the new trapdoor operation would be 
T H T K (m 1 , r 1 , m 2 ) = r 2 = 2 −l ((m 1 − m 2 ) + 2 l r 1 ) = 2 −l (m 1 − m 2 ) + r 1 = x(m 1 − m 2 ) + r 1 , 
where x = 2 −l mod λ(n). Note that the modular reduction in the original trapdoor operation (
T H HK (m 1 , r 1 ) = g r 1 ||m 1 mod n. 
(3) In Equation (3), m 1 ∈ R {0, 1} l , r 1 ∈ R {0, 1} k+l , k = |λ(n)|. 
The trapdoor operation is: 
T H T K (m 1 , r 1 , m 2 ) = r 2 = x(m 1 − m 2 ) + r 1 , 
(4) where (m 1 , m 2 , r 1 ) ∈ R {0, 1} l × {0, 1} l × {0, 1} k+l . 
Performance
The running times obtained are as follows (computing environment is the same as those in Section 2.1): @BULLET Cost of performing T H HK () : 97.9 × 10 −3 seconds, @BULLET Cost of performing T H T K () : 6.5 × 10 −6 seconds. 
The online computational cost has been reduced from 9.5 µs to 6.5 µs, save 31.5%. We know that doing a modular reduction requires more computational power than that of multiplication. Therefore, the result is consistent with expectations, since the modular reduction has been replaced with a conventional multiplication. However, the appended string, i.e. r, has been lengthened from 1024 bits to 1184 (or 1185) bits. Also, the timing of computing hash operation is increased from 74.3 ms to 90.5 ms, spend 21.8%. The lengthened appended string causes this increment. 
Security of the Proposed Scheme
 Section 2.2 requires that a useful trapdoor hash function must satisfy the properties of efficiency, trapdoor collisions and collision resistant. In the followings, we will illustrate that the proposed trapdoor hash function, satisfies all of the three properties. Efficiency: Equation (3) describes how to perform the hash operation. It can be seen that the quantity T H HK (m, r) = g r||m mod n is computed at the cost of 2016 MMs, 2016 = 1.5(|r| + |m|). 
Trapdoor collisions: The trapdoor operation is defined in Equation (4). Using the trapdoor key x, the quantity 
T H T K (m 1 , r 1 , m 2 ) = r 2 = x(m 1 − m 2 ) + r 1 
is calculated at the cost of a conventional multiplication. It is clear that if r 1 is uniformly distributed over {0, 1} k+l , then r 2 is also uniformly distributed over {0, 1} k+l . Before proving the property of collision resistant, a lemma is borrowed from 
where M (|n|) = O(|n|log|n|loglog|n|). 
Collision resistant: This property will be proved by contradiction. Assume that public hash key HK = (g, n) is given and there exists a polynomial time adversary A outputs two pairs (m 1 , r 1 ) and (m 2 , r 2 ) producing the same hash value with non-negligible probability, where 
m 1 = m 2 and (m i , r i ) ∈ {0, 1} l × Z λ(n) , i = 1
, 2. Thus the following equations hold true. 
g r 1 ||m 1 = g r 2 ||m 2 mod n r 1 2 l + m 1 = r 2 2 l + m 2 mod λ(n) L = (m 1 − m 2 ) + 2 l (r 1 − r 2 ) = 0 mod λ(n) 
Since (m 1 − m 2 ) = 0 we conclude that L = 0, which is a multiple of λ(n). Now, the quantities of n and 2L (a multiple of φ(n)) are available. By Lemma 2, we can factor the large composite number n. The result contradicts the assumption that it is infeasible to factor a large composite number, which is a RSA modulus. 
Conclusions
Trapdoor hash functions can aid any signature scheme to generate signatures online efficiently. An efficient T H has been proposed in 
 Science and Information Engineering, Chaoyang University of Technology. He is a member of the Chinese Cryptology and Information Security Association (CCISA). His research interests include computer cryptography, network security, and information security. 
"
"Introduction
RSA is a well-known public key cryptosystem where each user has a public key e for encryption (verification) and a private key d for decryption (signature) 
In this paper, we present a new batch verifying scheme which is especially efficient when there are illegal signatures . When the verifier receives t signatures, it generates a cube of side length n and fills these t signatures in the n × n × n cube, where n is the smallest integer which satisfies n 3 ≥ t. Let's assume there are 25 signatures, the verifier generates a cube of side length 3 and executes 3 + 3 + 3 = 9 exponential computations since 3 is the smallest integer which satisfies 3 3 ≥ 25. Moreover, the verification time would not increase as the number of the illegal signatures increases. The paper is organized as follows: In Section 2, we review two batch verifying schemes including Harn's and Li's scheme. Then we present the proposed scheme and compare its performance with that of previous schemes in Sections 3 and 4, respectively. In Section 5, an extended batch verification scheme is described. Finally, we conclude our paper in Section 6. 
Two Batch Verifying Scheme
We review two batch verifying schemes before presenting the proposed one. 
Harn's Scheme
In this section, we first introduce Harn's batch verifying scheme 
( t−1 i=0 S i ) e ? = t−1 i=0 h(M i ). 
(1) If Equation (1) holds, (S 0 , S 1 , · · · , S t−1 ) 
 are valid signatures of M 0 , M 1 , · · · , M t−1 , respectively. In Harn's scheme, these signatures can be verified simultaneously in one exponential operation time. 
Li et al.'s Scheme
The scheme was proposed by Li et al. recently 
M 1 , S 1 ), (M 2 , S 2 ), · · · , (M t , S t ) 
from the signer, the verifier will generate an m×n matrix (where m × n ≥ t) and t random numbers r i , i = 1, 2, . . . , t, where r i ∈ {1, 2, . . . , t}. He then randomly fills these t messages into the m × n matrix using the following equation (see 
(2) 
 After filling these messages in the m×n matrix, the verifier could batch verify each of the rows and the columns, 
 respectively. The complete batch verifying process is divided into two verifications: row verification and column verification. The details of row and column verification are shown as follows. @BULLET Row verification: 
First row: ( n i=1 S (1,i) ) e ? = n i=1 h(M (1,i) ) mod N , Second row: ( n i=1 S (2,i) ) e ? = n i=1 h(M (2,i) ) mod N , . . . m-th row: ( n i=1 S (m,i) ) e ? = n i=1 h(M (m,i) ) mod N . @BULLET Column verification: First column: ( m i=1 S (i,1) ) e ? = m i=1 h(M (i,1) ) mod N , Second column: ( m i=1 S (i,2) ) e ? = m i=1 h(M (i,2) ) mod N , . . . n-th column: ( m i=1 S (i,n) ) e ? = m i=1 h(M (i,n) ) mod N . 
 If there are some signature-verification faults in the matrix , we could find out where these signature-verification faults are located by finding the matrix positions of row and column overlaps. 
The Proposed Scheme
We now present a batch verifying multiple signatures scheme which is more efficient than the previous ones, especially when the illegal signature occurs. The details of our scheme are described as follows. First, the verifier generates a cube with side length m when he receives some pairs of message and signa- ture 
(M 0 , S 0 ), (M 1 , S 1 ), · · · , (M t−1 , S t−1 ) 
from the signer, where m is the smallest integer which satisfies m 3 ≥ t. Next, the verifier chooses t random numbers r i , where 
r i ∈ {0, 1, · · · , m 3 − 1}, i = 0, 1, · · · , t−1
, and fills these t signatures in the m × m × m cube according to coordinate figure (x, y, z), where 
r i = xm 2 + ym + z, and x, y, z ∈ {0, 1, · · · , m − 1}. (3) 
Finally, the verifier could then batch verify each plane according to the three coordinate axes. The details are shown as follows. @BULLET x-axis plane: 
x = 0: ( m−1 i=0 m−1 j=0 S (0,i,j) ) e ? = m−1 i=0 m−1 j=0 h(M (0,i,j) ), x = 1: ( m−1 i=0 m−1 j=0 S (1,i,j) ) e ? = m−1 i=0 m−1 j=0 h(M (1,i,j) ), . . . x = m − 1: ( m−1 i=0 m−1 j=0 S (m−1,i,j) ) e ? = m−1 i=0 m−1 j=0 h(M (m−1,i,j) 
). @BULLET y-axis plane: 
y = 0: ( m−1 i=0 m−1 j=0 S (i,0,j) ) e ? = m−1 i=0 m−1 j=0 h(M (i,0,j) ), y = 1: ( m−1 i=0 m−1 j=0 S (i,1,j) ) e ? = m−1 i=0 m−1 j=0 h(M (i,1,j) ), . . . y = m − 1: ( m−1 i=0 m−1 j=0 S (i,m−1,j) ) e ? = m−1 i=0 m−1 j=0 h(M (i
,m−1,j) ). @BULLET z-axis plane: 
z = 0: ( m−1 i=0 m−1 j=0 S (i,j,0) ) e ? = m−1 i=0 m−1 j=0 h(M (i,j,0) ), z = 1: ( m−1 i=0 m−1 j=0 S (i,j,1) ) e ? = m−1 i=0 m−1 j=0 h(M (i,j,1) ), . . . z = m − 1: ( m−1 i=0 m−1 j=0 S (i,j,m−1) ) e ? = m−1 i=0 m−1 j=0 h(M (i,j,m−1) ). 
If there are some signature-verification faults in the cube, we could find out where these faults are located by finding the point of intersection of three kinds of plane. As shown in 
x = 0: ( 3 i=0 3 j=0 S (0,i,j) ) e ? = 3 i=0 3 j=0 h(M (0,i,j) ), x = 1: ( 3 i=0 3 j=0 S (1,i,j) ) e ? = 3 i=0 3 j=0 h(M (1,i,j) ), x = 2: ( 3 i=0 3 j=0 S (2,i,j) ) e ? = 3 i=0 3 j=0 h(M (2,i,j) ), x = 3: ( 3 i=0 3 j=0 S (3,i,j) ) e ? = 3 i=0 3 j=0 h(M (3,i,j) ). 
@BULLET y-axis plane: 
y = 0: ( 3 i=0 3 j=0 S (i,0,j) ) e ? = 3 i=0 3 j=0 h(M (i,0,j) ), y = 1: ( 3 i=0 3 j=0 S (i,1,j) ) e ? = 3 i=0 3 j=0 h(M (i,1,j) ), y = 2: ( 3 i=0 3 j=0 S (i,2,j) ) e ? = 3 i=0 3 j=0 h(M (i,2,j) ), y = 3: ( 3 i=0 3 j=0 S (i,3,j) ) e ? = 3 i=0 3 j=0 h(M (i,3,j) ). 
@BULLET z-axis plane: After batch verifying each plane, Bob is now confirmed whether the signature-verification fault is occurring or not. We suppose there was one signature-verification fault in the position (1, 2, 3) of the cube, so Bob could realize there was a signature-verification fault occurring and precisely detects where the signature-verification fault is located. From the method described above, there would occur three verification that fails in the x = 1, y = 2, and z = 3 plane, respectively. According to the point of intersection of three kinds of plane, the signature-verification fault could be precisely detected in the position (1, 2, 3) of the cube. 
z = 0: ( 3 i=0 3 j=0 S (i,j,0) ) e ? = 3 i=0 3 j=0 h(M (i,j,0) ), z = 1: ( 3 i=0 3 j=0 S (i,j,1) ) e ? = 3 i=0 3 j=0 h(M (i,j,1) ), z = 2: ( 3 i=0 3 j=0 S (i,j,2) ) e ? = 3 i=0 3 j=0 h(M (i,j,2) ), z = 3: ( 3 i=0 3 j=0 S (i,j,3) ) e ? = 3 i=0 3 j=0 h(M (i,j,3) ). 
Implementation and Result Analysis
Experimental Results
In this section, we implement RSA, Harn's scheme, Li's scheme, and ours, with the experimental results of four schemes shown in 
 4.2 Analysis of Illegal Signature Detec- tion 
In this section, we now present the size of exponentiation operations in four schemes when illegal signatures occur. As described in Section 1, the performance of illegal signatures detection is regarded as the position it is located in. From 
The Extension of the Scheme
 The proposed scheme is based on a cube, and we can extend it to the condition of n-dimension. First, the verifier generates an n-dimension object with side length m when he receives some pairs of message and signa- ture 
(M 0 , S 0 ), (M 1 , S 1 ), · · · , (M t−1 , S t−1 ) 
from the signer, where m is the smallest integer which satisfies m n ≥ t. Next, the verifier chooses t random numbers r i , where 
r i ∈ {0, 1, · · · , m n − 1}, i = 0, 1, · · · , t − 1
, and fills these t messages in the m n object according to coordinate figure 
(a n−1 , a n−2 , · · · , a 1 , a 0 ), where a n−1 , · · · , a 1 , a 0 ∈ {0, 1, · · · , m − 1} and r i = a n−1 m n−1 + a n−2 m n−2 + · · · + a 1 m + a 0 . 
Finally, the verifier could then batch verify each plane according to n-dimension coordinate axis. The details are described as follows. 
1) a n−1 -axis plane: a. a n−1 = 0: 
( m−1 an−2=0 · · · m−1 a0=0 S (0,an−2,··· ,a0) ) e ? = m−1 an−2=0 · · · m−1 a0=0 h(M (0,an−2,··· ,a0) ). b. a n−1 = 1: ( m−1 an−2=0 · · · m−1 a0=0 S (1,an−2,··· ,a0) ) e ? = m−1 an−2=0 · · · m−1 a0=0 h(M (1,an−2,··· ,a0) ). . . . . . . c. a n−1 = m − 1: ( m−1 an−2=0 · · · m−1 a0=0 S (m−1,an−2,··· ,a0) ) e ? = m−1 an−2=0 · · · m−1 a0=0 h(M (m−1
,an−2,··· ,a0) 
). 
2) a n−2 -axis plane: a. a n−2 = 0: 
( m−1 an−1=0 m−1 an−3=0 · · · m−1 a0=0 S (an−1,0,··· ,a0) ) e ? = m−1 an−1=0 m−1 an−3=0 · · · m−1 a0=0 h(M (an−1,0,··· ,a0) ). b. a n−2 = 1: ( m−1 an−1=0 m−1 an−3=0 · · · m−1 a0=0 S (an−1,1,··· ,a0) ) e ? = m−1 an−1=0 m−1 an−3=0 · · · m−1 a0=0 h(M (an−1,1,··· ,a0) ) . . . . . . c. a n−2 = m − 1: ( m−1 an−1=0 m−1 an−3=0 · · · m−1 a0=0 S (an−1,m−1,··· ,a0) ) e ? = m−1 an−1=0 m−1 an−3=0 · · · m−1 a0=0 h(M (an−1,m−1,··· ,a0) ). . . . 
3) a 0 -axis plane: a. a 0 = 0: 
( m−1 an−1=0 · · · m−1 a1=0 S (an−1,··· ,a1,0) ) e ? = m−1 an−1=0 · · · m−1 a1=0 h(M (an−1,··· ,a1,0) ). b. a 0 = 1: ( m−1 an−1=0 · · · m−1 a1=0 S (an−1,··· ,a1,1) ) e ? = m−1 an−1=0 · · · m−1 a1=0 h(M (an−1,··· ,a1,1) ). . . . . . . c. a 0 = m − 1: ( m−1 an−1=0 · · · m−1 a1=0 S (an−1,··· ,a1,m−1) ) e ? = m−1 an−1=0 · · · m−1 a1=0 h(M (an−1,··· ,a1,m−1) ). 
 Therefore, the total number of exponentiation operations is mn in the extended batch verification scheme. If there are some signature-verification faults in the ndimension object, we could find out where these faults are located by finding the point of intersection of n kinds of plane. For example, there is a signature-verification fault in the position (0, 1, · · · , m − 1) of the n-dimension object, if n verifications failed in the a n−1 = 0 plane, a n−2 = 1 plane, · · · and a 0 = m − 1 plane, respectively. 
Conclusions
 We presented a new batch verification multiple RSA signatures scheme which fills the signatures into a cube. It can detect accurately where the illegal signatures are located without additional re-verify operations. Moreover, the verification time would not increase as the number of the illegal signatures increases in one batch verification. Experiment shows our scheme is more efficient than the previous schemes, especially when the number of the signatures is very large. We then extended this scheme to the condition of n-dimension. 
"
"Introduction
Internet technology is growing more and more quickly, and people can process, store, or share with their data by using its ability. Recently, the cloud has emerged to provide various application services to satisfy users' require- ment 
 1.1 The Criteria of An Ideal Attributebased Encryption Scheme 
According to these schemes, a summary of the criteria, that ideal attribute-based encryption schemes, are listed as follows. 
C1. Data confidentiality 
 Before uploading data to the cloud, the data was encrypted by the data owner. Therefore, unauthorized parties including the cloud cannot know the information about the encrypted data. 
C2. Fine-grained access control 
In the same group, the system granted the different access right to individual user. Users are on the same group, but each user can be granted the different access right to access data. Even for users in the same group, their access rights are not the same. 
C3. Scalability 
When the authorized users increase, the system can work efficiently. So the number of authorized users cannot affect the performance of the system. C4. User accountability 
C5. User revocation 
If the user quits the system, the scheme can revoke his access right from the system directly. The revocable user cannot access any stored data, because his access right was revoked. 
C6. Collusion resistant 
Users cannot combine their attributes to decipher the encrypted data. Since each attribute is related to the polynomial or the random number, different users cannot collude each other. 
Organization
 In this paper, we survey several attribute-based encryption schemes including two varied access structures, which are monotonic and non-monotonic. The organization of the paper is organized as follows. In Section 2, we introduce these attribute-based encryption schemes. In Section 3, we compare these schemes by using the criteria illustrated in Section 1, and in Section 4, our conclusions are given. 
Related Works
In cloud environments, if a data owner wants to share data with users, he will encrypt data and then upload to cloud storage service. Through the encryption step, the cloud cannot know the information of the encrypted data. Besides, to avoid the unauthorized user accessing the encrypted data in the cloud, a data owner uses the encryption scheme for access control of encrypted data. In existing schemes, many encryption schemes can achieve and provide security, assure data confidential , and prevent collusion attack scheme. One of the encryption schemes is attribute-based encryption scheme. 
 The first concept of attribute-based encryption was proposed in 2005. And then many attribute-based encryption schemes were proposed. According to the access policy , two types of these schemes can be classified, the keypolicy and ciphertext-policy attribute-based encryption schemes. The key-policy attribute-based scheme is that the access policy is attached to the user's private key, and a set of descriptive attributes is in the encrypted data. If a set of attributes satisfies the access policy, the user will recover the message. If not, he cannot obtain it. And the ciphertext-policy attribute-based scheme is that the access policy is associated to the encrypted data, and a set of descriptive attributes is in the user's private key. If a set attribute satisfies the access policy, the user can decipher the encrypted data. In this section, we will introduce five attribute-based encryption schemes. And according to the type of access policy, there are monotonic access structure and non-monotonic access structure. Non-monotonic access structure can use the negative word to describe every attribute, but the monotonic access structure cannot. The notations used in this paper are listed in 
Attribute-based Encryption Scheme
Sahai and Waters proposed an attribute based encryption scheme in 2005. There are authority, data owner (also be called sender) and data user (also be called receiver) in this scheme, and authority 's role is to generate keys for data owners and users to encrypt or decrypt data. In this scheme, the authority generates keys according to attributes; and these attributes of public key and master key, which are generated by the authority, should predefine (means that it will list attributes which will be used in the future). If any data user who wants to add to this system, and he owns to attributes don't include predefined attributes. The authority will re-define attributes and generate a public key and master key again. And data owner's role in this scheme is to encrypt data with a public key and a set of descriptive attributes. A data user's role is to decrypt encrypted data with his private key sent from the authority, and then he can obtain the needed data. For decrypting data, attributes in data user's private key will check by matching with the attributes in encrypted data. If the number of "" matching "" is at least a threshold value d, the data user's private key will be permitted to decrypt the encrypted data. For example, for a set of descriptive attributes in the encrypted data, {M IS, T eacher, Student}, the threshold value is 2. If a data user wants to decrypt the encrypted data, his number of attributes in private key will need two or the more than two of attributes in the encrypted data, so that a data user has a private key with attributes, {M IS, Student} to decrypt and obtain the data. In this scheme, there are four algorithms to be executed: Setup, KeyGen, Encrypt, and Decrypt. Let G 1 and G 2 be two bilinear groups of prime order p, and let g be a generator of G 1 . In addition , let e : G 1 × G 1 −→ G 2 denote the bilinear map, and let d be a threshold value. 1) Setup(d): The authority uniformly and randomly chooses t 1 , ..., t n , y from Z q , and publishes the public key, 
P K =(T 1 = g t 1 , ..., T n = g t n , Y = e(g, g) y ). 
And the master key is M K = (t 1 , ..., t n , y). 
2) KeyGen(A U , P K, M K): 
The authority executes and generates a private key for the data user U . Choose a d − 1 degree polynomial q randomly such that q(0) = y. The data user's private key D is 
{D i = g q(i) t i } ∀i∈A U . 
 3) Encrypt(A CT , P K, M ): Data owner encrypts message M ∈ G 2 with a set of attributes A CT . Choose a random number s ∈ Z q , and the encrypted data is published as 
CT = (A CT , E = M Y s = e(g, g) ys , {E i = g tis } ∀i∈A U ). 4) Decrypt(CT , P K, D
 ): Data user decrypts the encrypted data CT with the private key D. Choose 
d attributes from i ∈ A U A CT to compute e(E i , D i ) = e(g, g) q(i)s if |A U A CT | ≥ d. And compute Y s = e(g, g) q(0)s = e(g, g) ys with 
the Lagrange coefficient, and the message M = E/Y s can be ob- tained. 
 In KenGen() algorithm, the user's private key is generated with secret sharing 
 2.2 Key-Policy Attribute-based Encryption Scheme 
In 2006, Goyal proposed an key-policy attribute-based (KP-ABE) scheme. This scheme uses a set of attributes to describe the encrypted data and builds a access policy in user's private key. If attributes of the encrypted data can satisfy the access structure in user's private key  D, an user can obtain the message through decrypt algorithm . In addition, the KeyGen() algorithm is different from the attribute-based encryption which is introduced at subsection one in this section. The user's private key is according to the access structure to generate. In this algorithm, it adopts secret sharing and chooses a polynomial q x such that q x (0) = q parent(x) (index(x)), (Note that parent(x) is x's parent node, and index(x) is the number associated with node x that is given by x's parent node.) in a top-down manner which is to start from the root node r for each node x in the access structure. So q r (0) is equal to the master key y, and the master key y is distributed among the user's private key component D i which is corresponding to the leaf node (Note that the leaf node represents attribute). Since the KeyGen() algorithm is different, the Decrypt( ) algorithm also be different. It use attributes of encrypted data to run decryptnode function in the decryption algorithm. And it can input encrypted data, user's private key, and nodes of the access structure in user's private key; it adopts bottom-up manner in the access structure and recursive manner to decrypt the encrypted data. Beside, this scheme divides nodes of the access structure into the equal the leaf nodes. Finally, it will get a bilinear formula and use polynomial interpolation to get the message. For example, the encrypted data with attributes are {M IS Student}, and user's private key with access structure is {M IS 
(T eacher Student)}. 
The encrypted data with attributes satisfies the access structure of an user's private key, and then user can get the message. In this scheme, there are four algorithms to be executed: Setup, KeyGen, Encrypt, and Decrypt. And the parameters described in this scheme and parameters of the ABE scheme are the same. It will be depicted as follows. 
1) Setup(d): The authority chooses several uniform and random numbers t 1 , ..., t n , y from Z q , and makes public the public key, 
P K =(T 1 = g t1 , ..., T n = g t n , Y = e(g, g) y 
). And keeps the master key, M K = (t 1 , ..., t n , y) be secret. 
2) KeyGen(A U −KP , P K, M K): 
 The authority generates private key components for each leaf node x in the access structure. The private key components are 
D x = g qx(0) 
t i , where i is equal to a leaf node in the access structure. These components will be merged into the user's private key, and be sent to an user. 
 3) Encrypt(M, A CT , P K): Data owner chooses a random number s from Z q and encrypts a message M ∈ G 2 with a set of attributes A CT , and then he generates the encrypted data as 
CT = (A CT , E = M Y s = e(g, g) ys , {E i = g t i s } ∀i∈A CT ). 
4) Decrypt (CT, D): This algorithm can be executed by a recursive algorithm, It inputs the encrypted data, user's private key, and nodes of the access structure in user's private key. If i is equal to the leaf node, and i is in the access structure of user's private key, it will call the decryptnode function, 
e(D x , E i ) = e(g, g) s·qx(0) 
. If i is not in the access structure of an user's private key, it will call the decryptnode function; and it outputs invalid. If i is not equal to the leaf node, it will call decryptnode function and input all children nodes of node x, z, and use lagrange coefficient to compute to obtain e(g, g) s·qx(0) . Finally, the decryption algorithm call the decryptnode function on the root of the access structure and compute e(g, g) ys = Y s , if and only if the encrypted data satisfies the access structure of private key. And the message M = E Y s can be ob- tained. 
In this scheme, the user's private key is associated with access structure, and the encrypted data with a set of descriptive attributes can be used to be corresponding to the access structure of the user's private key. Since access control is built in user's private key, the attributes of the encrypted data satisfies access structure so to let a data user decrypt the encrypted data. However, the access control right is owned by user's private key, and some people just obtaining this key can let him decrypt data. It means the user's private key can choose the encrypted data which is satisfied, but the encrypted data cannot choose who can decrypt this data. For a scheme, the ciphertext policy attribute-based encryption is proposed. It builds the access policy in the ciphertext and uses a set of attributes to describe the user's private key. 
Ciphertext-Policy Attribute-based Encryption Scheme
In 2007, Bethencourt et al. proposed a ciphertext policy attribute-based scheme, and the access policy in the encrypted data (ciphertext). The access control method of this scheme is similar to the key policy attribute-based encryption. In key policy attribute-based encryption, the access policy is in user's private key, but the access policy is switched to the encrypted data in ciphertext policy attribute-based encryption. And a set of descriptive attributes are associated with the user's private key, and the access policy is built in the encrypted data. The access structure of the encrypted data is corresponding to the user's private key with a set of descriptive attributes. If a set of attributes in user's private key satisfies the access structure of the encrypted data, the data user can decrypt the encrypted data; if it cannot, the data user cannot obtain the message. For example, the access structure in the encrypted data is {M IS (T eacher Student)}. If a set of attributes in user's private key is {M IS T eacher}, the user can recover the data. 
In the access structure of this scheme, it adopts the same method which was depicted in KP-ABE to build. And the access structure built in the encrypted data can let the encrypted data choose which key can recover the data, it means the user's key with attributes just satisfies the access structure of the encrypted data. And the concept of this scheme is very close to the traditional access control scheme. There are five algorithms in this scheme, Setup(), KeyGen(), Encrypt(), Delegate(), Decrypt( ). The Delegate algorithm is in addition more than above schemes, and it can input user's private key and regenerate the new one with another attributes which are in a set of attributes of the original user's private key. And this key is equal to the key generated from the authority. 
The parameters in this scheme and in KP-ABE are the same. This scheme will be described as follows. 
1) Setup: The authority chooses two random numbers α, β from Z q as exponents, and generates the public key, 
P K = (G 0 , g, h = g β , f = g 1 β , e(g, g) α ). 
The master key is M K = (β, g α ). 
2) KeyGen(M K, A U ): The authority chooses a random number s from Z q , and random s j for each attribute j in a set of attributes in user's private key. The user's private key, D = (DK = g 
(α+s) β , ∀j ∈ A U : D j = g s · H(j) s j , D * j = g 
C = M e(g, g) αy ), C = h y , ∀i : C i = g qi(0) , C * i = H(att(i)) qi(0)) . 4) Delegate(D, A U ): 
 This algorithm takes the user's private key D and a set of attributes whose each attribute is in A U to create a new user's private key D. 
5
 ) Decrypt(CT, D): When data user receives the encrypted data, he can execute this algorithm. The user's private key D and the encrypted data are input in this algorithm and the recursive function, and decryptnode is called. If the node x is a leaf node and let k = att(x), where k ∈ A U , the decryptnode can be called , then it computes decryptnode(CT, D,  The CP-ABE builds the access structure in the encrypted data to choose the corresponding user's private key to decipher data. It improves the disadvantage of KP-ABE that the encrypted data cannot choose who can decrypt. It can support the access control in the real environment. In addition, the user's private key is in this scheme, a combination of a set of attributes, so an user only use this set of attributes to satisfy the access structure in the encrypted data. Moreover, the CP-ABE scheme is applied in the proxy re-encryption field to increase security of this field. The CP-ABE scheme can be applied in the scheme which can achieves proxy reencryption in cloud environments 
x) = e(D k ,C x ) e(D * k ,C * x ) = e(g
Attribute-based Encryption Scheme with Non-Monotonic Access Struc- tures
In 2007, Ostrovsky et al. proposed an attribute-based encryption with non-monotonic access structure. The access formula of access structure in user's private key can represent any type through attributes such as negative ones. It is different from the previous attributebased encryption scheme. The previous schemes are like KP-ABE scheme, and the access structure in user's private key has monotonic access formula. No negative attributes exist in it. Apart from this, the access structure of this scheme is the same as the access structure of KP-ABE scheme. There is a Boolean formula such as And, OR, and threshold gates in these access structures, but there is a boolean formula, NOT in access structure of this scheme. However, other schemes do not include it. There is an example for this scheme. If a teacher in department of information management wants to share the data with students, he will set a set of attributes in the encrypted data. And there is an access structure , {M IS Student} in students' private key. But the teacher doesn't want graduates to access this data, he adds N OT graduate to the access structure. So the access structure is {M IS Student N OT graduate}. It can let data not be accessed by graduates. This scheme proposes the first method that can add negative constraints to describe attributes. And it is flexible to use access policy for a data owner. This scheme contains four algorithms: Setup(), KeyGen(), Encrypt(), and Decrypt(), and they will be introduced as follows. 
1) Setup(d): A parameter d is decided that how many attributes the encrypted data has. And let G 1 be a bilinear group of prime order p, let g be a generator of G 1 , and let e : 
G 1 × G 1 −→ G 2 
denote the bilinear map. After that, choose two random numbers α, β from Z q , and denote 
g 1 = g α , g 2 = g β . Let h(x), q(x) 
be two polynomials of degree d and constraint that q(0) = β. Generate the public key, 
P K = (g, g 1 ; g 2 = g q(0) , g q(1) , g q(2) , ..., g q(d) ; g h(0) , g h(1) , g h(2) , ..., g h(d) 
), and the master key, M K = α. In addition , denote and publish two publicly computable function, T, V : 
Z q −→ G 1 such as T (x) −→ g x d 2 · g h(x) , V (x) −→ g q(x) . 2) KeyGen( A U , P K, M K): 
 The authority would execute this algorithm to output a key for users, and let them decrypt the encrypted data only if the attributes of the encrypted data that satisfies the nonmonotonic access structure A U in user's private key. Let A U be a non-monotonic access structure, and choose a random number s i ∈ Z q for each attribute x i . Moreover, denote a polynomial p(x) randomly such that p(0) = α. For x i is a non-negated attribute, the component of user's private key is 
D i = (D (1) i = g p(xi) 2 · T (x i ) s i , D (2) i = g si ). For x i 
is a negated attribute, the component of user's private key is 
D i = (D (3) i = g p(x i )+s i 2 , D (4) i = V (x i ) s i , D (5) i = g s i ). 
 And then every user's private key contains each component of private key D i . 
3) Encrypt(M, A CT , P K): when a data owner wants to encrypt a message M ∈ G 2 under a set of attributes A CT ⊂ Z * q , he would comply this algorithm. First, the random number s is chosen from Z q , and is used to compute the encrypted data. And then output the encrypted data as 
CT = (A CT , E (1) = M · e(g 1 , g 2 ) s , E (2) = g s , {E (3) x = T (x) s } x∈A CT , {E (4) x = V (x) s } x∈A CT ). 4) Decrypt(CT, D): 
Input the encrypted data CT and private key D, and this algorithm is exe- cuted. First, a data user checks if A CT ∈ A U . If not, its output is invalid. If A CT ∈ A U , it will compute 
e(D (1) i ,E (2) ) e(D (2) i ,E (3) i ) = e(g 2 , g) s·p(xi) 
for a non-negated attribute x i , and compute 
e(D (3) i ,E (2) ) e(D (5) i ,Π x∈A CT (E (4) i ) σ x )·e(D (4) i ,E (2) ) σ x i = e(g 2 , g) s·p(xi) , 
where 
e(g 2 ,g) s·α = M . 
This scheme is undesirable for two reasons. First, there are many negative attributes in the encrypted data, but they don't relate to the encrypted data. It means that each attribute adds a negative word to describe it, but these are useless for decrypting the encrypted data. It can cause the encrypted data overhead becoming huge. In addition, the new attributes may be used after the encrypted data is created, and a data owner can't know all the attributes which may be used to encrypt in the future. In addition, the negative attributes are used to let the setting of access structure be more flexible. Data owner can add a negative word in front of an attribute, and this action can let the person who possesses this attribute be unable to decrypt the data. 
 2.5 Hierarchical Attribute-based Encryption Scheme 
In 2011, Wang et al. proposed a hierarchical attribute-based encryption scheme composed of a hierarchical identity-based encryption scheme (HIBE) and a ciphertext-policy attribute-based encryption scheme. This scheme used the property of hierarchical generation of keys in HIBE scheme to generate keys. Moreover, it used disjunctive normal form (DNF) to express the access control policy, and the same domain authority in this scheme administered all attributes in one conjunctive clause. There are five roles in this scheme: the cloud storage service, data owner, the root authority, the domain authority, and data users. The role of cloud storage service is that let a data owner can store data and share data with users. The role of data owner is encrypting data and sharing data with users. The role of the root authority is generating system parameters and domain keys, to distribute them. The role of domain authority is managing the domain authority at next level and all users in its domain, to delegate keys for them. Besides, it can distribute secret keys for users. And users can use their secret keys to decrypt the encrypted data and obtain the message. The key generation in this scheme adopts a hierarchical method. The root authority generates a root master key for domain authority at the first level. The system public key and the master key of the domain authority at first level are used to create the master keys for the domain authorities at the next level by the root authority or the domain authority at the first level. In addition, the domain authority generates the user identity secret key and the user attribute secret key for the authorized user. The processes of this scheme will be introduced as follows. 
1) Setup(K): The security parameter K is input, mk 0 is chosen from Z q , two bilinear groups G 1 , G 2 of order p, and a bilinear map e : 
G 1 × G 1 −→ G 2 
are chosen by the root authority. And then three random oracle H 1 , H 2 , H A , and a generator P 0 ∈ G 1 are picked. The system public key is 
P K = (p, G 1 , G 2 , e, P 0 , Q 0 , H 1 , H 2 , H A )
 , and the system master key is M K 0 = mk 0 which will be kept secret. 
2) CreateDM(P K, M K i , P K i+1 
 ): The root authority or the domain authority generates mater keys 
M K i+1 = (mk i+1 , Hmk i+1 , D i+1 , Q − tuple i+1 
) for domain authorities DM i+1 by using system public key P K, the public key of domain authorities DM i+1 , P K i+1 , and its master key M K i . Where mk i+1 is the index of the random oracle H mki+1 , 
D i+1 = D i + mk i P i+1 , P i+1 = H 1 (P K i+1 ) ∈ G 1 , Q − tuple i+1 = (Q − tuple i , Q i+1 ), and Q i+1 = mk i+1 P 0 ∈ G 1 . Assume that D 0 is an identity element of G 1 , and Q − tuple 0 = (Q 0 ). 3) CreateUser(P K, M K i , P K u , P K a ): 
 The domain authority first checks whether the user u is authorized for attribute a which is monitored by itself, when a user sends a request to domain authority for the user identity secret key D i,u and the user attribute secret key on a, D i,u,a . If so, it creates the user identity secret key 
D i,u = (Q − tuple i−1 , mk i · mk u P 0 ), and the user attribute secret key D i,u,a = D i + mk i · mk u P a ∈ G 1 by computing mk u = H A (P K u ) ∈ Z q and P a = H mk i (P K a )P 0 ∈ G 1 . If not, it outputs "" NULL "" . 4) Encrypt(P K, M, A CT −HA , {P K a | a ∈ A CT −HA }): 
The data owner encrypts data M with a DNF access control policy A CT 
−HA = N i=1 (CC i ) = N i=1 ( ni j=1 a ij ) 
and public keys of all attributes in access control policy 
{P K a | a ∈ A CT −HA }. The encrypted data is CT = [A CT −HA , C f = (U 0 , U 12 , ..., U 1t1 , U 1 , ..., U N 2 , ..., U N t N , U N , V )]. 
 Where U 0 = rP 0 , r is a random number which is 
lected from Z q , U 0 = rP 0 , U 12 = rP 12 , U 1t 1 = rP 1t 1 , U 1 = r n 1 j=1 P a 1j , U N 2 = rP N 2 , U N t N = rP N t N , U N = r n N j=1 , P ij = H 1 (P K ij ) ∈ G 1 for 1 ≤ i ≤ N and 1 ≤ j ≤ t i , P a ij = H mk it i (P K a ij )P 0 ∈ G 1 for 1 ≤ i ≤ N and 1 ≤ j ≤ n i , and V = M ⊕ H 2 (e(Q 0 , rn A P 1 )
). Note that n A is the lowest common multiple of n 1 , ..., n N . 
5) Decrypt(P K, CT, D i,u , {D i,u,a | a ∈ CC j }): 
The user want to obtain the message M , and his attributes satisfies the access policy in the encrypted data A CT −HA . He recovers the message M by com- puting 
V ⊕ H 2 ( e(U 0 , n A n i n i j=1 D it i ,u,a ij ) e(mkumkit i P0, n A n i Ui) t i j=2 e(Uij ,n A Q i(j−1) ) ) = M . 
 This scheme can satisfy the property of fine-grained access control on the cloud by combining HIBE scheme and CP-ABE scheme , and full delegation to cloud computation . It can share data for users in the cloud in an enterprise environment. Furthermore, it can apply to achieve proxy re-encryption 
U | + L G 1 |A U |L G 1 (2|A U | + 1)L G 1 6|A U |L G 1 |A U |L G 1 Ciphertext |A CT |L G 1 + LG 2 |A CT |L G 1 + L G 2 (2|A CT | + 1)L G 1 + L G 2 (2|A CT | + 1)L G 1 + L G 2 |A CT |L G 1 + LG 2 
Conclusions
 In this paper, we survey five different attribute-based encryption schemes: ABE, KP-ABE, CP-ABE, ABE with non-monotonic access structure, and HABE, and illustrate their schemes and compare them. These schemes can be classified according to their access policy. The access policy in the user's private key is KP-ABE, and the access policy in the encrypted data is CP-ABE. Besides, we can find these schemes that are hard to satisfy user accountability . Moreover, the access structure is pre-defined in these schemes; if a new user wants to access data and his attributes are not in the access structure, these encrypted data will be re-generated. 
Thus, based on the discussion above, these existing attribute-based encryption schemes have properties: (1) These schemes are encrypted with attributes, so a data owner just needs to predefine these attributes that he would use, he doesn't need to care about the number of users in the system; (2) Each attribute has public key, secret key, and a random polynomial, so different users cannot combine their attributes to recover the data, and different users cannot carry out collusion attacks; (3) Only the user who possesses the authorized attributes can satisfy the access policy to decrypt data; (4) The access policy contains a boolean formula such as AND, OR et al. which can let the access structure be flexible to control users' access. However, almost all schemes exist that the authority is used to generate keys. Since these schemes contain the authority that just suits the private cloud environments, the authority should be removed in the fu-ture. Furthermore, ABE schemes (like KP-ABE or CP- ABE scheme) are generally applied in the field of proxy re-encryption. 
"
"Introduction
Traditionally, a forward secure cryptography assumes that one to mitigate the damage caused by exposure of secret keys. In a forward-secure public-key system, private keys are updated at regular time periods; an exposure of the private key SK i corresponding to a given time period i does not enable an adversary to compromise the security of the scheme for any time period prior to i. For the case of signature, forward security guarantees that past signatures are protected even if the secret key of the current time period exposed. There are a number of known forward-secure signature schemes 
Preliminaries
For a positive integer T , 
Composite Order Bilinear Groups
We will use composite order bilinear groups introduced in 
Hardness Assumptions
We define the following three assumptions in composite order bilinear groups. Assumption 1. The challenger runs G(1 λ ) and gives to the adversary A the tuple 
D 1 = (N, G, G T , e, g 1 , g 3 ). 
Then the challenger flips a random coin ν ∈ R {0, 1} and picks random (z, ξ) 
random exponents (α, s, ξ, µ, ) $ ← − Z p1 × Z p1 × Z p2 × Z p2 × Z 
p2 . He gives to A the tuple D 3 = (N, G, G T , e, g 1 , g 3 , g α 1 g ξ 2 , g s 1 g µ 2 , g 2 ). Then he flips a random coin ν ← {0, 1} and picks a random w Assumption 4. The advantage of any PPT adversary A in Assumption i where i ∈ {1, 2, 3} is Adv i G,A = 1 2 (P r[A(D i , T i 
0 ) = 0] − P r[A(D i , T i 1 ) = 0]
). We say that G satisfies Assumption i for all PPT algorithms A, Adv i G,A ≤ negl(n). 
Binary Tree Encryption Scheme
Binary tree encryption (BTE) was introduced by Canetti et al. 
 Der(w, sk w ) → (sk w0 , sk w1 ). The key derivation algorithm takes as input the name of a node w ∈ {0, 1} ≤ and its associated secret key sk w . It outputs secret keys sk w0 , sk w1 for the two children of w. Enc(w, pk, M ) → CT . It takes as input pk, the name of a node w ∈ {0, 1} ≤ , and a message M , and returns a ciphertext CT . Dec(w, sk w , CT ) → M . The deterministic algorithm takes as input w ∈ {0, 1} ≤ , its associated secret key sk w , and a ciphertext CT . It returns a message M or the distinguished symbol ⊥. 
We require that for all (pk, sk w ) output by Gen, any w ∈ {0, 1} ≤ and any correctly-generated secret key sk w for this node, any message M , and all CT output by Enc pk (w, M ) we have Dec skw (w, Enc pk (w, M )) = M . Definition 2. A binary tree encryption scheme BTE is secure against selective-node, chosen-plaintext attacks (SN-CPA) if for all polynomially-bounded functions (·) the advantage of any PPT adversary A in the following game cpa-bte is negligible in the security parameter k. Setup Phase. A(k, (k)) outputs a node label w * ∈ {0, 1} ≤(k) . Algorithm Gen(1 λ , 1 ) outputs (pk, sk ε ). Phase 1. In addition, algorithm Der() is run to generate the secret keys of all the nodes on the path from the root to w * (we denote this path by P ). The adversary is given pk and the secret keys sk w for all nodes w of the following form: 
@BULLET w = w b,where w b is a prefix of w * and b ∈ {0, 1}(w is a sibling of some node in P ); 
@BULLET w = w * 0 and w = w * 1
, if |w * | < (w is a child of w * ). Note that it allows the adversary to compute sk w for any node w ∈ {0, 1} ≤ that is not a prefix of w * . 
Challenge Phase. The adversary generates a request challenge (M 0 , M 1 ). A random bit b is selected and the adversary is given 
C * = Enc(pk, w * , M b ). 
Guess Phase. Eventually, A outputs a guess b $ ←− − {0, 1}. 
Symmetric Encryption
A symmetric encryption (SE) scheme is a tuple of PPT algorithms Π = (Gen, Enc, Dec) such that: 
1) The key-generation algorithm Gen is a randomized algorithm. Takes as input the security parameter 1 n and outputs a key k; we write this as k ← Gen(1 n ). Without loss of generality, we assume that any key k output by Gen(1 n ) satisfies n ≤ |k|. 
2) The encryption algorithm Enc may be randomized. Takes as input a key k and a plaintext message m ∈ {0, 1} * , and outputs a ciphertext c, write by c ← Enc k (m). 
3) The decryption algorithm Dec is deterministic. Takes as input a key k, a ciphertext c and outputs a message m. That is, m := Dec k (c). It is required that for every n, every key k output by Gen(1 n ), and every m ∈ {0, 1} * , it holds that Deck(Enck(m)) = m. We present the formal security definition against chosen-ciphertext attacks where the adversary has access to a decryption oracle and the encryption oracle. Consider the following game c/a for any private-key encryption scheme Π = (Gen, Enc, Dec), adversary A and the security parameter n. 
1) A random key k is generated by running Gen(1 n ). 
2) The adversary A is given input 1 n and oracle to Enc k (·) and Dec k (·). It outputs a pair of messages m 0 , m 1 of the same length. 
 3) A random bit b ← {0, 1} is chosen, and then a ciphertext c ← Enc k (m b ) is computed and given to A. We call c the challenge ciphertext. 
4) The adversary A continues to have oracle access to Enc k (·) and Dec k (·), but is not allowed to query the latter on the challenge ciphertext itself. Eventually, A outputs a bit b ← {0, 1}. 
5) The adversary wins the game if output of the game is 1, that is, b = b. 
Our Model
Our model extends the forward security model of the work 
 3.1 Forward-Secure Public-Key Encryption Scheme with Untrusted Update 
 We give the formal definition of Forward-Secure Public- Key encryption scheme with untrusted update (uuf- sPKE). 
 Definition 4. A uufsPKE scheme consists of four algorithms , each of which is described in the following. 
 KeyGen uu (λ, N ). The key setup algorithm is a probabilistic algorithm that takes as input a security parameter λ and time period N , outputs decryption key DecK, initial encrypted secret key EncSK 0 and public parameters P K. U pdate uu (EncSK i−1 , i). The untrusted update algorithm that takes as input the encrypted secret key EncSK i−1 for time period i − 1, generates a new encrypted secret key EncSK i . Then deletes the old key EncSK i−1 . Note that this algorithm does not require the decryption key. Encrypt uu (P K, i, M ). The encryption algorithm is a probabilistic algorithm that takes as input public parameters , the current time period i and a message M , outputs the ciphertext CT for time period i. Decrypt uu (P K, i, EncSK i , DecK, CT ). The decryption algorithm is a deterministic algorithm that takes as input public parameters P K, the current time period i, the current encrypted secret key EncSK i , the decryption key DecK and a ciphertext CT , outputs the message M . Decyption Consistency Requirements. For any message M , the public key P K, the decryption key DecK, the secret key SK i for time period i and the output of Encrypt uu (P K, i, M ), Decrypt(P K, SK w , Encrypt uu (P K, i, M )) = M always holds. 
Security Definitions for uufsPKE
 Now we give the formal security definition for Forwardsecure Public-Key encryption scheme with untrusted update in terms of two games. 
Forward Security
Formally, for a uufsPKE scheme, its semantic security against an adaptive chosen ciphertext attack under an adaptive break-in attack can be defined via the following game f sc between an attacker A and a challenger B. Setup Phase. The challenger B runs algorithm KeyGen uu (λ, N ) and gives A the resulting public parameters P K, keeping the secret key (EncSK 0 , DecK) to itself. Here, a handle counter i is set to 0. 
Phase 1. A adaptively issues the following three queries: @BULLET update(i) queries. B runs algorithm Update uu and updates the handle counter to i ← i + 1. @BULLET breakin(i ) queries. At any time i , B firstly checks if i ≤ N − 1. If this is true, it responds with the corresponding private-key share EncSK i for current time period i . @BULLET decryption(j, CT ) queries. At any time j, A picks a ciphertext CT and sends to B. The challenger makes a call to Decrypt uu (P K, j, EncSK j , DecK, CT ) using the corresponding private-key and forwards the result to A. We refer to the above game as an IND-fs-CCA2 game. We let Adv f sc Π,A denote the advantage of an attacker A in this game fsc. Definition 5. An uufsPKE scheme Π is IND-fs-CCA2 secure if for every PPT adversary A, we have Adv f sc Π,A ≤ negl(λ) in the above IND-fs-CCA2 game. 
Update Security
Formally, we define update security for a uufsPKE scheme via the following game uuc between A and B. 
Setup Phase. The challenger B runs algorithm 
KeyGen uu (λ, N ) and gives (P K, EncSK 0 ) to A, keeping the secret key DecK for itself. Also, a handle counter i is set to 0. Guess Phase. Finally, A outputs a guess c $ ←− − {0, 1}. 
We refer to the above adversary A as a IND-uu-CCA2 adversary. We let Adv uuc Π,A denote the advantage of an attacker A in this game uuc. Definition 6. An uufsPKE scheme Π is IND-uu-CCA2 secure if for any PPT adversary A, we have Adv uuc Π,A ≤ negl(λ) in the above IND-uu-CCA2 game. 
The General Transformation
In this section, we present the generic construction of uufsPKE from any binary tree encryption. We apply a symmetric encryption scheme to implement update security. And we can use any chosen ciphertext secure symmetric encryption. Formally, a BTE consists of PPT algorithms E 1 = (Gen, Der, Enc, Dec) and a SE scheme E 2 = (Gen , Enc , Dec ). Our scheme can be described by four algorithms, denoted by Π = (KeyGen uu , Update uu , Encrypt uu , Decrypt uu ). KeyGen uu (k, N ). It runs algorithm Gen(k, ) → (sk ε , pk) that takes as input a security parameter k ∈ N and , the smallest integer satisfying N ≤ 2 . Firstly, sets DecK ← Gen (1 k ) Then, symmetric encryption algorithm generates esk ε = Enc DecK (sk ε ) and uses esk ε generating the initial encrypted secret key of our generic construction by calling algorithm Der(., .). Denoted by EncSK 0 = (esk 0 , {esk 1 , esk 01 , esk 001 , ..., esk 0 −1 1 }). Finally, it returns public key (pk, N ) and the secret key (EncSK 0 , DecK). U pdate uu (EncSK i , i + 1). The encrypted secret key EncSK i be organized as a stack of node keys where the secret key esk i on top. Here, i = i 1 i 2 ...i is the binary expression for the i th time period. Firstly, pops the top off the stack. If i = 0, search the only path from node i to root, denoted by P i . And generate another set R i = ({esk i1...i l } l∈{1,...,,}s.t.i l =0 ). Push the node secret key onto the stack from R i according the relation between the leaf i and the sibling of the node in R i . The closer two nodes, then first-in stack. Otherwise , let h ∈ {1, ..., } denote the largest index such that i h = 0. It have w = i 1 i 2 ...i h−1 1 ∈ {0, 1} h and recursively use Der(w, esk w ) to generate node keys esk w1 , esk w01 , ..., esk w0 −h−1 , esk w0 −l . Push all these node secret keys onto the stack by the reverse order. The new top of the stack is esk w0 −l that is i + 1 = w0 −l . In both cases, it erase the leaf node key esk i and return the new stack. 
Encrypt uu (i, pk, M ). In period i, to encrypt a message M ∈ G T , the sender parses i as i 1 i 2 ...i . Then, it computes CT ← Enc(pk, i, M ). Decrypt uu (i, DecK, EncSK i , pk, CT ). To decrypt ciphertext CT , it regenerates sk i ← Dec DecK (esk i ). Then, it computes M ← Dec(sk i , i , CT ). 
Simulation Theorems
 In this section, we give the proof on how to reduce the security of SN-CPA for BTE scheme and SE scheme against chosen ciphertext attack to forward security and update security for a uufsPKE scheme. We only consider the strongest security, that is, chosen ciphertext security. We will prove our general transformation's forward security and update security against an adaptive chosen ciphertext attack in the following two theorems. Theorem 1. Suppose E 1 is a SN-CPA(resp.SN-CCA) secure BTE and E 2 is a symmetric encryption scheme with chosen ciphertext security. Then the uufsPKE scheme Π described above is IND-fs-CPA(resp.IND-fs-CCA2) secur- tiy. Proof. Suppose there is an adversary A has non-negligible advantage in attacking forward security of the above scheme. That is, Adv f sc A,Π >ε, ε is a negligible parameter . We build an algorithm B that breaks BTE scheme E 1 with advantage Adv f sc A,Π /N (k) where N (k) is polynomial in the security parameter k. Algorithm B uses A to interact with a BTE challenger as follows: 
 Initialization. Firstly, A outputs the challenge time period i * and the corresponding to the node label i * of the binary tree. Secondly, B runs Gen (k) → DecK. 
It outputs (i * , DecK). 
Setup. The BTE challenger runs Gen(k, ) → (sk ε , P K) and gives to B. And then B forwards (P K, DecK) to A. Phase 1. A adaptively issues the following queries. @BULLET update(i) queries: B runs algorithm Der(.,.) and updates the handle counter to i ← i + 1 at the same time. @BULLET breakin(i ) queries: At any time i , B firstly checks if i ≤ N − 1. If this is true and i ≤ i * , then B outputs a random bit and halts. Otherwise , if i > i * , B runs Enc DecK (sk ε ) to obtain esk ε and forwards to the BTE challenger. Using esk ε . The BTE challenger recursively apply algorithm Der(.,.) to obtain node keys and finally EncSK i . Then it responds with the corresponding private-key EncSK i for the current time period i . @BULLET decryption(j, CT ) queries: At any time j, A picks a ciphertext CT and sends to B. If i * ≤ j, B decrypt the ciphertext by himself. Otherwise,B makes a call to Dec DecK (esk j ) and forwards to the BTE challenger for Dec(P K, j, sk j , CT ) using the corresponding private-key. Then returns the result to A. 
Challenge Phase. A outputs two equal length messages 
M 0 , M 1 . If i ≤ i * , B forwards M 0 , M 1 to 
the BTE challenger. It runs Enc(P K, i * , M b ) to obtain CT * for a random b ∈ {0, 1} and gives A the challenge ciphertext CT * . Otherwise, B outputs a random bit and halts. 
Phase 2. A continues to issue queries as Phase 1 where decryption queries for (j, CT ) with 
j = i * . 
Guess Phase. Finally, A outputs its guess b ∈ {0, 1} for b. B forwards b to the BTE challenger and wins the game if b = b . This completes the description of algorithm B. Let Adv cpa−bte B,Π be B's advantage in winning the BTE game cpa-bte. Let Adv f sc A,Π be A's advantage in winning the fsc game of uufsPKE scheme. It is straightforward to see that when i * = i the copy of A running within B has exactly the same view as in a real fsc game. Since B guesses i * = i with probability 1/N , we have that A correctly predicts the bit b with advantage Adv f sc A,Π /N (k).  Proof. Suppose A wins uuc game with non-negligible advantage in the above scheme. That is, Adv uuc A,Π >ε where ε is a negligible parameter. Then we show how to construct an algorithm B that breaks SE scheme E 2 . Algorithm B starts by breaking the SE scheme E 2 . Using A, B interacts with a SE challenger as follows: 
Setup. To launch the game, B runs Gen(k, ) → (sk ε , P K). It sends sk ε to the SE challenger and obtains the initial encrypted node key esk ε . Using esk ε , B recursively runs algorithm Der(.,.) and generates all the encrypted keys {EncSK 0 , EncSK 1 , ..., EncSK N }. And then, B sends the set of all the encrypted keys to A. Phase 1. A adaptively issues the following two queries: @BULLET update(i) queries. B updates the handle counter to i ← i + 1. @BULLET decryption(j, CT ) queries. At any time j, A picks a ciphertext CT and sends to B. B request the secret key of time period j for the SE challenger. It runs algorithm Dec'(EncSK j ) and outputs the secret key sk j . B computes Dec(sk j , CT ) = M and sends the result to A. Challenge Phase. A chooses two equal length messages M 0 , M 1 and sends to B. It runs Enc(P K, i * , M b ) to obtain CT * for a random b ∈ {0, 1} and gives A the challenge ciphertext CT * . Phase 2. A continues to issue queries as Phase 1 where decryption queries for (j, CT ) with 
j = i * . 
Guess Phase. Finally, A outputs its guess b ∈ {0, 1} for b. B forwards b to the SE challenger and wins the game if b = b . 
Boilding Blocks: BTE and SE
Firstly, we propose a new binary tree encryption scheme E 1 with SN-CPA based on dual system encryption and prove its security. For simplicity, we do not consider chosen-ciphertext security of binary tree encryption which can be added by simply using CHK transformation 
1) Choose bilinear map groups (G, G T ) of order N = p 1 p 2 p 3 where p i is the order of the subgroup G i in G, with p i > 2 k for each i ∈ {1, 2, 3}. 
2) Let w = w 1 ...w l be an l-bits string representing a node of binary tree and w i ∈ {0, 1} for all i ∈ {1, ..., l}. Define a function H : {0, 
1} ≤ → G 1 as H(w) = h 0 l j=1 h wj j . 3) Outputs the public key pk = (g 1 , g 3 , V = e(g 1 , g 1 ) α , h 0 , h 1 
, ..., h , H) and a root secret key sk ε = α for independent and uniformly random 
α ∈ Z N , (h 0 , h 1 , ..., h ) ∈ G +1 1 , g 1 ∈ G 1 and g 3 is a generator of G 3 . 
Der(pk, w, sk w ) is an algorithm that deduce the secret keys sk w0 , sk w1 where w0, w1 are two children of w. Execute the following steps. 
1) Let w = w 1 ...w l . Parse sk w = (k 0 , k 1 , s l , ..., s ) = (g α 1 · H(w 1 ...w l ) r · g u 3 , g r 1 · g v0 3 , h r l · g v1 3 , ..., h r · g v 3 ), u, v 0 , v 1 , ..., v ∈ Z p3 , r ∈ Z N . 
2) For j ∈ {0, 1}, output 
sk wj = (k 0 · s j l · H(w 1 ...w l j) r j · g u 3 , k 1 · g r j 1 · g v 0 3 , s l+1 · h r j l+1 · g v 1 3 , ..., s ·h r j ·g v 3 ) = (g α 1 ·H(w 1 ...w l j) rj ·g u 3 , g rj 1 · g v0 3 , h rj l · g v1 3 , ..., h rj · g v 3 ) where u , v 0 , v 1 , ..., v ∈ Z p3 , r j ∈ Z N and r j = r j + r, u = u + u , v 0 = v 0 + v 0 , ..., v = v + v . 
Enc(i, pk, M ) does the following: 1) Let i = i 1 ...i ∈ {0, 1} , select random s ∈ Z N . 
2) Compute and output the ciphertext 
CT = (i, M · V s , g s 1 , H(i 1 ...i ) s ). 
Dec(w, sk w , CT ) does the following: 1) et i = i 1 ...i ∈ {0, 1} , parse sk i as (k 0 , k 1 ) and parse CT as (i, 
C 0 , C 1 , C 2 ). 2) Output the message M = C 0 · e(k 1 , C 2 ) e(k 0 , C 1 ) . 
Correctness. Assuming the ciphertext is well-formed, we have 
C 0 · e(k 1 , C 2 ) e(k 0 , C 1 ) = M · V s · e(g r 1 · g v0 3 , H(i 1 ...i ) s ) e(g α 1 · H(w 1 ...w l ) r · g u 3 , g s 1 ) = M. 
Secondly, we give a construction of SE scheme E 2 = (Gen , Enc , Dec ) applying to build uufsPKE scheme. The encryption algorithm Enc skss 
(m) = (c 1 , c 2 , ..., c κ , m · j∈[κ] c µj j 
 ) for independent and uniformly random c j ∈ G. The decryption algorithm Dec skss (c 1 , c 2 , ..., c κ , c 0 ) firstly parses the ciphertext c as (c 1 , c 2 , ..., c κ , c 0 ). And compute the message 
m = c 0 j∈[κ] c µj j . 
A Concrete uufsPKE Scheme
Our concrete uufsPKE scheme consists of the following algorithms where periods are indexed from 0 to T -1 with T = 2 . 
KeyGen uu (k, N ). 1) Run Gen(k, ) → (sk ε , pk) where pk = (g 1 , g 3 , V = e(g 1 , g 1 ) α , h 0 , h 1 , ..., h , H) and sk ε = α. 2) Generate G(r) → DecK, denoted by DecK = (d 1 , d 2 , ..., d t ) where d i ∈ Z N . 
3) Compute the initial encrypted root key esk 
ε = Enc DecK (sk ε ) = (c 1 , c 2 , ..., c t , sk ε i∈[t] c di i ). 
4) The initial encrypted secret key EncSK 0 = (esk 0 , {esk 1 , esk 01 , esk 001 , ..., esk 0 −1 1 }) using esk ε recursively apply algorithm Der. 
U pdate uu (EncSK i , i + 1). 1) Parse i = i 1 ...i ∈ {0, 1} and EncSK i = (esk i , {esk i1...i l } l∈{1,...,,}s.t.i l =0 ). And delete esk i . 2) If i = 0, EncSK i+1 = (esk i1·i −1 , {esk i1·i l−1 } l∈{1,...,,−1}s.t.i l =0 
 ), that is, EncSK i+1 includes the remaining node keys. Otherwise, let l ∈ {0, 1} be the largest index such that i l = 0. Let w = i 1 ...i l −1 1 ∈ {0, 1} l . Recursively run Der for the node key esk w to generate node keys esk w 1 , esk w 01 , · · · , esk w 0 −l −1 1 and esk w 0 −l −1 1 = esk i+1 . Delete esk w and return 
EncSK i+1 = (esk i+1 , {esk i1...i l−1 1 } l∈{1,...,l −1}s.t.i l =0 , {esk w 1 , esk w 01 , · · · , esk w 0 −l −1 1 }). Encrypt uu (i, pk, M ). For i ∈ [1, N ]
 , to encrypt the message M , does the following. 
1) Parse i as i 1 ...i ∈ {0, 1} . 
2) Run Enc(i, pk, M ) and return the ciphertext 
CT = (i, M · e(g 1 , g 1 ) αs , g s 1 , H(i 1 ...i ) s ). Decrypt uu (i, EncSK i , pk, CT ). Given a ciphertext CT = (i, C 0 , C 1 , C 2 ) and a encrypted secret key EncSK i = (esk i , {esk i1...i l } l∈{1,...,,}s.t.i l =0 ) 
 for the current period i. 
1) Parse esk i as (esk 0 i , esk 1 i ). 2) Compute sk i = ( esk 0 i i∈[t] c di i , esk 1 i i∈[t] c di i ) by applying decryption algorithm Dec . 
3) Run the algorithm Dec(sk i , i, CT ) to obtain the message M . 
Security Proof
In the following, we devote to prove SN-CPA for the above BTE scheme E 1 under three static assumptions in the standard model. Our security proof will use semifunctional ciphertext and semi-functional node keys which defined as follows. All the ciphertexts and node keys defined by the following are normal, where by normal we mean that they have no G 2 parts. On the other hand, a semi-functional key or ciphertext has G 2 parts. @BULLET semi-functional ciphertexts is generated from a 
mal ciphertext CT = (C 0 , C 1 , C 2 ) and some g 2 ∈ G p2 , by choosing random x, z c $ ← − Z N and setting CT semi = (C 0 , C 1 · g x·zc 2 , C 2 · g x 2 ). 
 @BULLET semi-functional node key are generated from a normal node key sk 
w = (k 0 , k 1 , s l , ..., s ) by choosing random y, z w $ ← − Z N , and setting sk semi1 w = (k 0 · g y·zw 2 , k 1 · g y 2 , s l · g y 2 , ..., s · g y 2 ). 
We now prove the semantic security of this BTE against selective-node chosen plaintext attacks(SN-CPA). In order to prove security we need a hybrid argument using a sequence of games. For each i, we denote by S i the probability that the challenger returns 1 at the end of Game i . We also define Proof. B first receives (N, G, G T , e, g 1 , g 3 , 
Adv i = |P r[S i − 1/2]| for each i. 
g α 1 g ξ 2 , g s 1 g µ 2 , g 2 ) 
and T , where T = e(g 1 , g 1 ) αs or T = e(g 1 , g 1 ) αw . B chooses random exponents x 0 , x 1 , ..., x $ ← − Z N and sets the public parameters as 
V = e(g 1 , g α 1 g ξ 2 ), h 0 = g x0 1 , h 1 = g x1 1 
, ..., and h = g x 1 . It sends these to A. When A requests a key for time period i, B generates a semi-functional. It does this by setting sk w 
= (g α 1 g ξ 2 · l j=1 (g xj ·wj 1 ) r ·g u 3 , g r 1 ·g v0 3 , h r l ·g v1 3 , ..., h r ·g v 3 
). After providing the appropriate secret keys, B responds to the challenge query from A. Specifically, B chooses a random bit b and returns 
CT = (M b · T, g s 1 g µ 2 , (g s 1 g µ 2 ) Σ l j=1 xj wj ). If T = e(g 1 , g 1 ) αs , 
 then this is a properly distributed semifunctional ciphertext with message M b . On the other hand, if T $ ← − G T , then this is a semi-functional ciphertext with a random message. Therefore, the value of b is information-theoretically hidden and the probability of success of any algorithm A in this game is exactly 1/2, According to Theorem 1 and Theorem 2, we conclude our concrete scheme has forward security and update se- curity. 
Comparison with the Existing Schemes
Now we compare the efficiency of our method with the prevail classic uufsPKE 
Conclusions
In this paper, motivated by the work of Libert 
"
"Introduction
Over the last decade, several value-added services have been proposed for deployment in the Internet. These include IP multicast 
Problem Description
PIM-Join Mechanism
PIM supports two types of join operations: (1) shared tree joins, and (2) source specific joins. Shared tree joins are used to establish a shared tree between the receivers and a pre-selected router, called the Rendezvous Point (RP). Since the RP is a domain-local router, the PIM- Join message and the state created are also local. Hence, state overload attacks using shared tree joins can have a localized effect. In source specific joins, the receivers join directly to the multicast source, S, of a group, (S,G). Since S can be located anywhere in the Internet, attacks via source specific joins are extremely potent, impacting potentially any local or remote victim sites. Therefore, we focus on source specific join attacks in this paper. In PIM when a receiver R desires to join a multicast group (S,G), it creates an IGMP INCLUDE message for the desired group and sends it to its designated router DR(R). Upon receiving the INCLUDE message, DR(R) creates a new Join(S,G) message and forwards it towards the designated router of the source, DR(S). All the routers between DR(R) and DR(S) create forwarding state for the (S,G) group as the PIM-Join message propagates towards S. Routers forward PIM-Join messages on their shortest path interface towards the source. This interface is called the incoming interface (iif ) or reverse path forwarding interface (Int RP F ) for the group. For each iif entry, the router also includes all the interfaces from which it received PIM-Join messages in an outgoing interface list (oif ) for the group. A router needs to create new forwarding state for a PIM-Join for each distinct (S,G) group. In source specific multicast, a single source can support up to 2 24 multicast groups, all of which can be used to generate distinct Join(S,G i ) messages. The forwarding state created is "" soft "" , i.e., it expires if no refreshing PIM-Join messages arrive from downstream. Each state entry is associated with an entry-timer (ET) and each interface in the oif is associated with an oiftimer (OT). If the ET expires and the oif becomes empty, the router sends a PIM-Prune(S,G) message on its iif interface for the group and leaves the multicast tree. When an upstream router receives a PIM-Prune message on an interface, i, in oif, it removes i from oif. Alternatively, if no refreshing PIM-Join message arrives on i during a Join Hold Time period (the default is 260 seconds), OT i expires and i is removed from oif. The protocol is visually presented in 
State Overload Attacks
There are two vulnerabilities in the PIM protocol that are exploited in a state overload attack. The first vulnerability is in the PIM-Join procedure which requires that DR(R) issue a PIM-Join message without verifying whether the source or the group requested in the PIM- Join message exist. The second vulnerability is that the protocol requires the creation of state information in response to any PIM-Join message for its correct operation. The PIM-Join(S,G) message (legitimate or bogus) propagates towards S creating forwarding state at all routers on the R-to-S path. Since the routers do not have any mechanism to verify the validity/existence of the source or the group, they will maintain the (S,G) state as long as R, who could be an attacker, sends refresh messages. Consider an attack scenario that proceeds in rounds. For the first round of 260 seconds (or the local default Join Hold Time), the attacker generates distinct bogus PIM-Join messages to create unwanted state information in the routers. In subsequent rounds, it sends refresh messages to continue to maintain the state at the routers. If there are multiple attackers, the amount of state information maintained at routers could be prohibitively large. For example, in a directed end system attack, if there 1 /* Consider a router R j */ 2 On receiving PIM-Join(S,G) on an interface i 3 IF (S,G) state exists and (i ∈ oif ) THEN 4 oif = oif ∪{i} 5 IF (S,G) state NOT exists THEN 6 Create (S,G) state with iif = Int RP F (S); oif = {i} 7 IF NOT DR(S) THEN 8 Forward PIM-Join(S,G) on Int RP F (S) 9 On receiving Prune(S,G) on an interface i or OT for i expires 10 Remove i from oif of (S,G) 11 IF oif is empty and ET expires THEN 12 Send PIM-Prune(S,G) on Int RP F (S) 13 Remove (S,G) state from forwarding state table 
Figure 1: Source specific joins in PIM are 5000 zombies (attackers), with each zombie issuing 10 separate PIM-Join requests per minute, at the end of the first round, the routers at the target site will need to store more than 200 000 different multicast entries. Similarly, an infrastructure attack can be launched with attackers choosing highly used paths to target routers at the core of the network. In this case, the attacks are less targeted and the state created would potentially be distributed among multiple core routers. Considering the same attack parameters as before, in the worst case, a core router may end up storing up to 200 000 different entries. In both attacks, the number of created entries can be large. The above discussion suggests that a solution to the problem must involve a verification of the validity of the source and the groups being subscribed to in a PIM-Join message. We expect that the routers are suitably provisioned to handle legitimate PIM-Join requests so that joins to legitimate groups will not create an attack. In the rest of this paper, we present two approaches to include such a verification mechanism in the multicast join process . To simplify the initial discussion, we assume that attacks which may involve sources and receivers cooperating with each other to overcome the verification mechanism are not possible. We will address these attacks in more detail in Section 4.3. 
Related Work
One basic idea to defend against state overload attacks is to have DR(R) rate limit joins and prunes originating from end-hosts in its subnet 
Our second solution is an overlay based architecture that builds on the techniques used in MCOP and MAFIA. The main difference lies in the fact our solution is targeted directly at preventing state overload attacks at the inter-domain scale. We also ensure that the infrastruc-ture by itself is protected from DoS attacks. Additionally, subverting nodes in the network to launch attacks is not possible due to the independent decisions made by each domain the join path. Our first solution is a novel approach which aims to eliminate the problem completely. It achieves this by enhancing the PIM protocol to make it more secure and hence prevents these attacks completely. 
Solution 1: Enhanced Multicast Joins
Our first solution is a comprehensive approach which aims to eliminate the vulnerability in the PIM-Join procedure that is exploited in a state overload attack. Our objective is to ensure that every router can verify the validity of any PIM-Join message it receives. Before creating any state, every router in the forwarding path will individually verify the validity of the source and the group being subscribed to in the join. This ensures that bogus PIM-Join messages sent by malicious receivers cannot create unwanted state in the routers. During join forwarding, routers do not create any forwarding state, but instead add the requisite state information to the PIM-Join message before sending it upstream towards the source. Each on-tree router, R j , appends this state information as a nonce, say N j , to the end of a nonce block in a new Join(S,G,N) message. The state information added includes the incoming interface, i j , of the PIM-Join message and a secure hash of all the locally added state, H j (a detailed description of the state added is deferred to Section 4.2). 
 If the source and the group in the Join(S,G,N) message are valid, the accumulated state information is returned by DR(S) in a new JoinACK(S,G,N) message. Each router, R j , in the return path individually verifies the JoinACK(S,G,N) by recomputing the secure hash H j with the relevant state information in the nonce N j . This ensures that the received JoinACK(S,G,N) is a valid acknowledgment of the Join(S,G,N) that R j had previously forwarded upstream. Once the verification is complete, R j creates a forwarding entry for (S,G) with i j as the oif and Int RP F (S) as the iif for the group. Once the JoinACK reaches and is verified by DR(R), the join process is complete (see 
PIM Router Operation Under the Modified Join Procedure
We now present a detailed description of operation of a router after the proposed modifications to the PIM-Join mechanism. The protocol is visually presented in 
DR R DR S R 1 R n N=N+ N R n N=N+ N R 1 N DR R N= Join(S,G,N) Join(S,G,N) Join(S,G,N) N=N-N R 1 N=N-N R 2 N=N-N R n 
Step 3 Multicast Data on (S,G) R S Step 1 JACK(S,G,N) JACK(S,G,N) JACK(S,G,N) Step 2 Step1: Append N and forward Join(S,G,N) toward S Step 2: JoinACK propagates toward R; routers remove N corresponding to themselves Step 3: Multicast data propagates on the established (S,G) path 1 /* At a router R j */ 2 On receiving Join(S,G) on an interface i 3 IF (S,G) state exists at and (i ∈ oif )THEN 4 oif = oif ∪{i} 5 IF (S,G) state NOT exists THEN 6 IF R j = DR(S) THEN 7 IF (S,G) group is valid 8 Create (S,G) state with iif = Int RP F (S); oif = i 9 ELSE 10 Compute N j and Append N j to N 11 Send Join(S,G,N) on interface k = Int RP F (S) toward S 12 On receiving Join(S,G,N) on an interface i 13 IF (S,G) state exists at and (i ∈ oif ) THEN 14 oif = oif ∪{i} AND Send JoinACK(S,G,N) on i 15 IF (S,G) state NOT exist THEN 16 IF R j = DR(S) THEN 17 IF (S,G) group is valid 18 Create (S,G) state with iif = Int RP F (S); oif = i 19 Send JoinACK(S,G,N) on i 20 ELSE 21 Append N j to N 22 Send Join(S,G,N) on interface k = Int RP F (S) 23 On receiving JoinACK(S,G,N) on an interface k for a Join(S,G,N) 24 IF k = Int RP F (S) THEN 25 Recompute and verify N j 26 IF (S,G) state NOT exists 27 Create (S,G) state with oif = i; iif = Int RP F (S) 28 Modify N = N -N j 29 Send JoinACK(S,G,N) on i 30 On receiving Prune(S,G) on an interface i or OT for i expires 31 Remove oif = oif -{i} 32 IF oif is empty THEN 33 Send Prune(S,G) on Int RP F (S) 34 Remove (S,G) state from forwarding state table Send JACK downstream Forward PJN upstream Rcv PR on i or OT for i expires J 
(2-a) R j = DR(S) and receives Join(S,G)/Join(S,G,N): Verify the validity of (S,G) (by checking with S); create (S,G) state; initialize ET and OT timers; in the case of Join(S,G,N), send a JoinACK(S,G,N) on the interface in oif ; and move to J state. 
(2-b) R j = DR(S) and receives Join(S,G)/Join(S,G,N): Add relevant state, N j , to the incoming join message and forward a Join(S,G,N) message toward S on Int RP F (S). J State. In this state there are four events causing router R j to take different actions: @BULLET Receiving Join(S,G)/Join(S,G,N) on interface, i: Refresh its ET for (S,G); set oif = oif ∪{i} and start OT for i. In addition, on Join(S,G,N), send a JoinACK(S,G,N) on i. @BULLET Receiving JoinACK(S,G,N) on interface, k, for a Join(S,G,N) on i: Verify k = Int RP F (S); verify N j and update N = N − N j ; forward JoinACK(S,G,N) on interface i toward R j−1 . Set oif = oif ∪{i} and start OT for i. @BULLET Int RP F (S) change or join timer expiry: Send Join(S,G) on Int RP F (S). Here R j issues a Join(S,G) rather than Join(S,G,N) as the (S,G) is already verified prior to the creation of (S,G) state at R j . In this case, Int RP F (S) could change due to a unicast routing change. A join timer is used to trigger the transmission of periodic refresh messages upstream. @BULLET Receiving Prune(S,G) on interface, i, or OT for i expires: Set oif = oif -{i}. If oif = ∅, send Prune(S,G) on Int RP F (S); remove (S,G) state; and move to NI state. 
Authenticating Joins
In the modified join procedure, every router, R j , adds a nonce, N j , to a Join(S,G,N) message. When the JoinACK(S,G,N) is returned, R j retrieves N j for verification and then creates the forwarding state for (S,G). N j has to carry the requisite information which will allow R j to create the forwarding state for (S,G). It should also carry path information to ensure that the JoinACK(S,G,N) is returned downstream along the same path as the original join upstream. Additionally, the nonce has to be secure against modification, brute force, and replay attacks during its valid duration. Routers can create this nonce by including (1) the 16- bit incoming interface, i, for the incoming join, (2) the lower order 16 bits of the IP address of the downstream Incoming Inerface i Lower order 16 bits of IP k 64-bit Keyed Hash MAC (S, G, , timer) i 
15 16 31
Figure 5: Nonce, N j , added at router, R j router forwarding the join, and (3) a keyed-hash or MAC of the group address, the incoming join interface, i, and an ascending counter T (see 
Discussion
We now briefly comment on some important issues and questions about our proposed solution. One naive solution to state overload attacks may involve the designated multicast router DR(R) at a receiver site R to send a probe towards DR(S) to verify the existence of a remote source S or a group (S,G). This may not be effective all the time. Instead of issuing an IGMP-based group join request, an attacker can establish PIM Neighborhood relation with its DR router and can send a Join(S,G) request to get around this protection mechanism. Also this method cannot prevent an attacker from injecting bogus state information into the channel path. An effective solution in this case requires all the routers to verify the existence of the source S or the group (S,G) by sending probes to the source S. But this by itself creates a flooding attack toward the source site S and hence cannot be used effectively. The modified PIM protocol effectively prevents a receiver from overloading routers with bogus (S,G) states. This feature however assumes that the sender is not maliciously sourcing bogus groups. In this case, malicious senders and receivers can co-operate with each other to launch an infrastructure attack. To protect against malicious sources, the modified protocol can be combined with source authentication mechanisms such as MAFIA 
Partial Deployment Scenario
 Our proposed solution requires all PIM routers to be updated to support the modified join operation. In this section, we consider a method which can provide a temporary solution to ISPs supporting our protocol when the neighboring domains do not support the modifications. In this discussion, we refer to routers with the updated PIM protocol as modified routers and the routers employing the non-updated PIM version as legacy routers. For our protocol to function properly, the modified routers require a valid JoinACK message from their upstream neighbors. Downstream routers can be legacy routers without affecting the protocol which will function normally in the domains with modified routers. If a modified router is a domain edge router having a PIM neighborhood relationship with a legacy router of a neighboring domain, it will not be able to receive JoinACK messages. 
To deal with such cases, we introduce a proxy-based approach for an ISP supporting our proposed protocol. In this approach, the ISP can deploy state boxes at the edges of its domain. The state boxes are high capacity storage devices capable of handling large amounts of data. When an edge router, R e , of the domain detects (as a result of periodic PIM Hello message exchange) that its next hop neighbor in the neighboring domain is a legacy router, it removes and forwards the accumulated nonce information from the join messages to the local state box. This state is maintained for a short duration (e.g., 260 seconds), and is indexed under the appropriate (S,G) value of the incoming join message. The edge router, R e , then forwards an unmodified join message upstream towards the source. 
If the source is valid and is transmitting regularly, its data will flow down the established path to the edge router, R e . R e verifies with the state box if an entry for this (S,G) exists in it. If an entry exists, R e retrieves the state information from the local state box and issues a JoinACK with the stored state information downstream, thereby establishing forwarding state in the routers in its domain. This state caching operation is visually presented in 
Evaluations
In this section, we evaluate the overhead introduced by our modified PIM protocol and its performance under DoS attacks. Processing overhead at a router: We use a Linuxbased router to measure the processing overhead in computing and verifying the nonce in the modified joins. For this measurement, we use the sample implementation of HMAC-MD5 from RFC 2104 
Architecture
The architecture required is shown in 
Operation
 We will now describe the operation of the above mentioned components of our architecture and how they effectively prevent state overload attacks. When a receiver R (see 
Security and DoS Resistance of the Architecture
In this section we consider the security and DoS resistance of the overlay architecture and its individual components. These are important considerations to ensure that the introduction of the new architecture does not create any new security flaws which can be exploited by an attacker to launch new attacks on the architecture and hence the service it seeks to protect. Indirection boxes (IB) are limited in their traffic scope to users from their subnet and the local overlay node. This makes them easy to protect against DoS attacks 
Partial Deployment
Partial deployment is an important concern in any new architectural proposal. In our solution, if the source domain is non-deploying, verification of the validity of the (S,G) pair requires some modifications to the architecture . In this case, to verify the validity of the (S,G), the overlay node in the neighboring domain to the source can issue a join request to the group. If the source and group are valid and regularly transmitting, the node will start receiving multicast data. This can be used as a validity check for the presence of the group. If the source and receiver domains have both deployed the architecture, the solution will provide complete protection to the deploying domains irrespective of the status of other domains. To maintain connectivity, traffic between overlay nodes can be tunnelled across non-deploying domains. Finally, if a transit join request arrives at the edge of a deployed domain, and the (S,G) is not known to be valid, the ingress VB cannot always drop this request in the presence of partial deployment. If the request arrives from a non-deploying domain, the validity of the join message needs to be verified before creating state in the transit domain. For this purpose, the join request can be tunnelled to the exit VB of the deployed domain (without creating state) which forwards the join upstream as usual. If the (S,G) is valid and regularly transmitting, the VB will start receiving multicast data. At this stage ingress VB can be directed to forward the join request as usual. The join request will establish states within the deployed domain and operation can continue as normal. This solution is similar to partial deployment modifications for the first solution described in more detail in Section 4.4. 
Evaluations
In this section we evaluate the overhead introduced due to the overlay based approach and the effectiveness of the protocol in defending against state overload attacks. Latency introduced during join procedure: To evaluate the added latency introduced for the verification process prior to the join, we model the operation of the IB and overlay nodes as applications in NS2 
Conclusion
DoS attacks pose a serious problem to the health and security of value-added services in the Internet. In this paper, we have examined DoS attacks, called state overload attacks, for a specific service, IP multicast. Since the attacks exploit an inherent weakness in the PIM protocol , we proposed two solutions to make it more secure against these attacks. The modifications provide an effective solution against DoS attacks while creating minimal performance loss or latency for the end user. Our solutions can be incrementally deployed in the inter-domain with some modifications and can provide an equally effective defense in the partial deployment case as compared to the full deployment case. Our study of state overload attacks in multicast demonstrate the importance of careful protocol design and the intrinsic difficulty in designing secure protocols. In particular a high level lesson we have taken away is the difficulty of introducing value-added services in the Internet without introducing added overhead and new security vulnerabilities. In our first solution, we have provided a blue-print for building secure protocols for value-added services. In particular we demonstrate that the overhead introduced for the operation of value-added protocols can be effectively controlled without taking away from the protocols functionality and without creating new vulnerabilities that can be exploited to bring down the service. 
Jinu 
"
"Introduction
Smart cards have been widely used in many e-commerce applications and network security protocols due to their low cost, portability, efficiency and the cryptographic properties. Smart card stores some sensitive data corresponding to the user that assist in user authentication. The user (card holder) inserts his smart card into a card reader machine and submits his identity and password. Then smart card and card reader machine perform some cryptographic operations using submitted arguments and the data stored inside the memory of smart card to verify the authenticity of the user. A number of static identity based remote user authentication protocols have been proposed to improve security, efficiency and cost. The user may change his password but can not change his identity in password authentication protocols. During communication, the static identity leaks out partial information about the user's authentication messages to the attacker. Most of the password authentication protocols are based on static identity and the attacker can use this information to trace and identify the different requests belonging to the same user. On the other hand, the dynamic identity based authentication protocols provide multi-factor authentication based on the identity, password, smart card and hence more suitable to e-commerce applications. The aim of this paper is to provide a dynamic identity based secure and computational efficient authentication protocol with user's anonymity using smart cards. 
Literature Review
In 1981, Lamport 
Liu et al.'s Scheme
Main phases of Liu et al.'s scheme
In this section, we examine the remote user authentication scheme proposed by 
Initialization Phase
 Key Information Center (KIC) generates secret parameters corresponding to the user, store them on the smart card and issue the smart card to the user. KIC is also responsible to change the passwords of registered users. It generates two large prime numbers p and q and computes n = p.q. Then it chooses a public key e and finds a corresponding secret key d that satisfies e.d ≡ 1 mod (p − 1).(q − 1). The secret key d is sent to the server S over a secure communication channel. Afterwards , KIC finds an integer g that is a primitive element in GF (p) and GF (q), where g is the public parameter of KIC. Finally, KIC sends the parameters n, e and g to the server S. 
Registration Phase
 A user U i has to submit his password P i to KIC for registration over a secure communication channel. KIC selects an identity ID i corresponding to the user U i . Then KIC computes 
CID i = H(ID i ⊕ d), S i ≡ ID d i mod n and h i ≡ g P i .
 d mod n, where H() is a one-way hash function . Afterwards, KIC issues the smart card containing secret parameters (n, e, g, ID i , CID i , S i , h i ) to the user U i through a secure communication channel. 
Login and Authentication Phase
The user U i inserts his smart card into a card reader to login on to the server S and submits his identity ID * i and password P * i . The smart card compares the identity ID * i 
with the stored value of ID i in its memory to verify the legitimacy of the user. Then the smart card computes 
SID i = H(CID i 
) and sends the login request message 
M 1 = {ID i , SID i } 
to the service provider server S. The service provider server S computes 
CID i = H(ID i ⊕ d) and compares H(CID i ) 
with the received value of SID i . If they are not equal, the server S rejects the login request and terminates this session. Otherwise the server S stores the parameters ID i , SID i and chooses nonce value N S as a challenge to the user U i . The server S computes S N = N S ⊕ CID i and sends the message M 2 = {S N } back to the smart card of the user U i . On receiving the message M 2, the smart card chooses a random nonce value N C and computes 
N S = S N ⊕ CID i , X i ≡ g N c. Pi mod n and Y i ≡ S i .h N i c. N s mod n. 
Then, the smart card sends the message M 3 = {X i , Y i } to the server S. On receiving the message M 3, the server S checks whether the equation Y e i ≡ ID i . X N i s mod n holds. If it holds, the server S accepts the login request and computes Z i ≡ (H(CID i .X i )) d mod n and sends the message M 4 = {Z i } back to the smart card. On receiving the message M 4, the smart card checks whether the equation Z e i ≡ H(CID i .X i ) mod n holds or not. This equivalency authenticates the legitimacy of the service provider server S and the login request is accepted else the connection is interrupted. 
Cryptanalysis of Liu et al.'s Scheme
Liu et al. 
Stolen Smart Card Attack
A user U i may lose his smart card, which is found by an attacker or an attacker steals the user's smart card. An attacker can extract the stored values through some technique like by monitoring their power consumption and reverse engineering techniques as pointed out by Kocher et al. 
1) The attacker can extract the (n, 
e, g, ID i , CID i , S i , h i ) 
parameters from the memory of a smart card. 
2) Now the attacker computes SID i = H(CID i ) and sends the login request message M 1 = {ID i , SID i } to the service provider server S. 
3) The service provider server S computes and verifies the received value of SID i . 4) Then the service provider server S chooses random nonce value N S as a challenge to the smart card of the user U i , computes S N = N S ⊕ CID i and sends the message M 2 = {S N } back to the smart card of the user U i . 
5) Afterwards, the smart card chooses a random nonce 
value N C , computes N S = S N ⊕ CID i , X * i ≡ h N C .e i mod n and Y i ≡ S i .h N C . N S i mod n. X * i ≡ h N C .e i mod n ≡ (g Pi.d mod n) N C .e mod n because h i ≡ g Pi.d mod n ≡ (g Pi.d.N C .e mod n) mod n ≡ (g Pi.N C mod n) mod n because g e.d mod n ≡ 1 ≡ (g N C .Pi mod n) mod n ≡ X i 6
) Then the smart card sends the message 
M 3 = {X i , Y i } to the server S. 
7) The server S checks and verifies that the equation 
Y e i ≡ ID i .X N i s mod n holds. 8) Then the server S computes Z i ≡ (H(CID i .X i )) d mod n 
and sends the message M 4 = {Z i }back to the smart card of the user U i . 9) Now the attacker masquerading as the user U i has authenticated itself to the service provider server S without knowing the password of the user U i . 10) That means once an attacker gets the smart card of the user U i ,he can masquerade as a legitimate user U i by authenticating itself to the server S without knowing the password of the user U i corresponding to his smart card. 
Dynamic Identity Based Smart Card Authentication Protocol
 In this section, we describe a new remote user authentication scheme which resolves the above security flaws of Liu et al.'s 
Registration Phase
A user U i has to submit his identity ID i and password P i to the server S via a secure communication channel to register itself to the server S. 
Step 1: U i → S : ID i , P i 
The server S computes the security parameters 
Z i ≡ g (ID i |P i )+H(P i ) mod n, B i ≡ g (ID i |x|y i )+H(P i ) mod n and C i ≡ g x+yi+Pi mod n
, where n is large prime number and g is a primitive element in GF (n). The server S chooses its secret key x and H() is a oneway hash function. The server S also computes A i ≡ g (ID i |x|y i )+y i mod n for each user and stores y i ⊕ x corresponding to A i in its database. The server S chooses the value of y i corresponding to each user in such a way so that the value of A i must be unique for each user. Then the server S issues the smart card containing security parameters 
(Z i , B i , C i , n, g, H()) to the user U i . 
Step 2: S → U i : Smart card 
Login Phase
A user U i inserts his smart card into a card reader to login on to the server S and submits his identity 
ID * i and password P * i . The smart card computes Z * i ≡ g (ID * i |P * i )+H(P * 
i ) mod n and compares it with the stored value of Z i in its memory to verify the legitimacy of the user U i . 
Step 1: Smart card checks 
Z * i ? = Z i 
After verification, the smart card computes B 
i ≡ B i g −H(P i ) mod n ≡ g (ID i |x|y i ) mod n, C i ≡ C i g −P i mod n ≡ g x+y i mod n, D i ≡ B i . C i mod n ≡ g (ID i |x|y i )+x+y i mod n, E i ≡ g w+H(B i |T ) mod n and M i = H(B i |C i |T ), where smart card chooses w ∈ R Z * n 
and T is current time stamp of the smart card. Then the smart card sends the login request message 
(D i , E i , M i , T ) to the server S. 
Step 2: Smart card → S : D i , E i , M i , T 
 4.3 Verification and Session Key Agreement Phase 
After receiving the login request from the user U i ,the service provider server S checks the validity of timestamp T by checking (T − T ) ≤ δT , where T is current timestamp of the server S and is permissible time interval for a transmission delay. The server S computes A i ≡ D i g −x mod n and compares A i with the stored values of A i in its database. 
Step 1: Server S checks A i ? = A i 
 If no match found, the server S rejects the login request and terminates this session. Otherwise, the server S extracts y i from y i ⊕ x corresponding to A i from its database. Now the server S Step 3: S → Smart card: G i , N i , T On receiving the message (G i , N i , T ), the user U i 's smart card checks the validity of timestamp T by checking (T − T ) ≤ δT , where T is current time stamp of the smart card. Then the smart card extracts g m ≡ G i g −B i mod n and computes N 
computes B i ≡ A i g −yi mod n ≡ g (IDi|x|yi) mod n, C i ≡ g x+y i mod n, g w ≡ E i g −H(B i |T ) mod n, M i = H(B 
i ≡ g H(C i |T ) .g m mod n ≡ g H(C i |T 
 )+m mod n and compares it with the received value of N i to verify the legality of the service provider server S. 
Step 4: Smart card checks N i ? = N i 
This equivalency authenticates the legitimacy of the service provider server S and the login request is accepted else the connection is interrupted. Finally, the user U i and the server S agree on the common session key as 
S k = H(g w |g m |B i |C i |T |T ). 
Password Change Phase
The 
Z * i ≡ g (ID * i |P * i )+H(P * 
 i ) mod n and compares the calculated value of Z * i with the stored value of Z i in its memory to verifies the legitimacy of the user U i . Once the authenticity of the card holder is verified then the user U i can instruct the smart card to change his password. Afterwards, the smart card asks the card holder to resubmit a new password 
Security Analysis
Smart card is a memory card that uses an embedded micro-processor from smart card reader machine to perform required operations specified in the protocol. Kocher et al. 
Z i ≡ g (ID i |P i ) + H(P i ) modn, B i ≡ g (IDi|x|yi)+H(Pi) mod n and C i ≡ g x+yi+Pi mod n 
from the memory of smart card. Even after gathering this information, the attacker has to guess out ID i and P i correctly at the same time. It is not possible to guess out the two parameters correctly at the same time in real polynomial time. Therefore, the proposed protocol is secure against stolen smart card attack. 
2) Man-in-the-middle attack: In this type of attack, the attacker intercepts the messages send between the client and the server and replay these intercepted messages with in the valid time frame window. The attacker can act as client to server or vice-versa with recorded messages. In our proposed protocol, the attacker can intercept the login request message 
(D i , E i , M i , T 
) from the user U i to the server S. Then he starts a new session with the server S by sending a login request by replaying the login request message 
(D i , E i , M i , T ) with 
in the valid time frame window. The attacker can authenticate itself to the server S as well as to the legitimate user U i but can not compute the session key 
S k = H(g w |g m |B i |C i |T |T ) 
because the attacker does not know the value of g w , g m , B i and C i . Therefore , the proposed protocol is secure against man-inthe-middle attack. 
3) Impersonation attack: In this type of attack, the attacker impersonates as the legitimate user and forges the authentication 
(D i , E i , M i , T ) into (D i , E * i , M * i , T * ) 
 so as to succeed in the authentication , where T * is the attacker's current date and time. However, such a modification will fail in Step 2 of the verification and session key agreement phase because the attacker has no way of obtaining the value ID i , P i , x and y i to compute the valid parameters E * i and M * i . Therefore, the proposed protocol is secure against impersonation attack. 4) Malicious user attack: A malicious privileged user U i having his own smart card can gather information like 
Z i ≡ g (IDi|Pi)+H(P i) mod n, B i ≡ g (IDi|x|yi)+H(Pi) mod n and C i ≡ g x+yi+Pi mod n 
from the memory of smart card. This malicious user can not generate smart card specific value of 
B k ≡ g (ID k |x|y k )+H(P k ) mod n and C k ≡ g x+y k +P k mod n 
to masquerade as other legitimate user U k to the service provider server S because the value of B k and C k is smart card specific and depends upon the value of ID k , P k , x and y k . The malicious user does not have any method to calculate the value of ID k , P k , x and y k . Therefore, the proposed protocol is secure against malicious user attack. 5) Offline dictionary attack: In offline dictionary attack, the attacker can record messages and attempts to guess the user's identity ID i and password P i from recorded messages. The attacker first tries to obtain some user or server verification information such as 
D i ≡ B i . C i mod n ≡ g (ID i |x|y i )+x+y i mod n, E i ≡ g w+H(B i |T ) mod n, M i = H(B i |C i |T ), T , G i ≡ g B i +m mod n, N i ≡ g H(C i |T 
)+m mod n, T and then tries to guess the ID i , P i , x and y i by offline guessing. Even after gathering this information, the attacker has to guess ID i , P i , x and y i correctly at the same time. In another option, the attacker requires valid smart card and then has to guess the identity ID i and password P i correctly at the same time. It is not possible to guess out two parameters correctly at same time. Therefore , the proposed protocol is secure against offline dictionary attack. 6) Denial of service attack: In denial of service attack, the attacker updates password verification information from the memory of smart card to some arbitrary value so that the legitimate user can not login successfully in subsequent login request to the server. In the proposed protocol, the smart card checks the validity of user identity ID i and password P i before password update procedure. The attacker inserts the smart card into the smart card reader and has to guess the identity ID i and password P i correctly corresponding to the user U i . Since the smart card computes 
Z * i ≡ g (ID * i |P * i )+H(P * 
i ) mod n and compares it with the stored value of Z i in its memory to verify the legality of the user before the smart card accepts the password update request. It is not possible to guess out identity ID i and password P i correctly at the same time in real polynomial time even after getting the smart card of the user. Therefore, the proposed protocol is secure against denial of service attack. 7) Replay attack: In this type of attack, the attacker first listens to communication between the legitimate user and the server and then tries to imitate user to login on to the server by resending the captured messages transmitted between the legitimate user and the server. Replaying a message of one session into another session is useless because the user U i 's smart card and the server S uses current time stamp values T and T in each new session, which make the values of E i , M i and N i dynamic and valid for small interval of time. Hence replaying old messages is useless and the proposed protocol is secure against message replay attack. 8) Leak of verifier attack: In this type of attack, the attacker may be able to steal verification table from the server. If the attacker steals the verification table from the server, he can use the stolen verifiers to impersonate as a participant of the scheme. In the proposed protocol , the service provider server S knows secret x and stores y i ⊕ x corresponding to the user's A i value in its database. The attacker does not have any way to find out the value of x and hence can not calculate y i from y i ⊕ x. Also the attacker can not calculate ID i , x and y i from A i ≡ g (ID i |x|y i )+y i mod n. In case verifier is stolen by breaking into smart card database, the attacker does not have sufficient information to calculate the user's identity ID i and password Pi. Therefore, the proposed protocol is secure against leak of verifier attack. 9) Server spoofing attack: In server spoofing attack, the attacker can manipulate the sensitive data of legitimate users via setting up fake servers. The proposed protocol provides mutual authentication to withstand the server spoofing attack. Malicious server can not generate the valid value of 
G i ≡ g B i +m mod n and N i ≡ g H(C i |T 
)+m mod n meant for the smart card because malicious server has to know the value of B i ' and C i ' to generate the valid values of G i and N i corresponding to user U i 's smart card. Therefore, the proposed protocol is secure against server spoofing attack. 10) Online dictionary attack: In this type of attack, the attacker pretends to be legitimate client and attempts to login on to the server by guessing different words as password from a dictionary . In the proposed protocol, the attacker has to get the valid smart card and then has to guess the identity ID i and password P i corresponding to user U i . Even after getting the valid smart card by any means, the attacker gets very few chances (maximum 3) to guess the identity ID i and password P i because the smart card gets locked after certain number of unsuccessful attempts. Moreover, it is not possible to guess out identity ID i and password P i correctly at the same time. Therefore, the proposed protocol is secure against online dictionary attack. 11) Parallel session attack: In this type of attack, the attacker first listens to communication between the user and the server. After that, he initiates a parallel session to imitate legitimate user to login on to the server by resending the captured messages transmitted between the client and the server with in the valid time frame window. He can masquerade as the legitimate user U i by replaying a login request message 
(D i , E i , M i , T ) with 
in the valid time frame window but can not compute the agreed session key 
S k = H(g w |g m |B i |C i |T |T ) 
because the attacker does not know the values of g w , g m , B i and C i . Therefore, the proposed protocol is secure against parallel session attack. 
Cost and Functionality Analysis
 An efficient authentication scheme must take communication and computation cost into consideration during user's authentication. The cost comparison of the proposed scheme with the most related smart card based authentication schemes is summarized in 
+ 1T H + 1T X 1T E + 2T H 2T E + 1T H + 1T X 2T E + 1T H + 1T X 2T E E4 6T E + 5T H 3T E + 5T H 3T E + 2T H + 1T X 3T E + 2T H 2T E + 1T H E5 6T E + 4T H + 1T X 3T E + 4T H 2T E + 3T H + 2T X 3T E + 3T H + 1T X 2T E + 1T H 
"
"Introduction
Internet worms can reduplicate themselves and attack computers which have vulnerability and are connected to the Internet without any human intervention. They have addressed a serious threat to confidentiality, integrity, and availability of computer resources on the Internet. Internet worms have reached a horrendous propagation speed because of their increasingly sophisticated spreading mechanisms. The time required for the infection of global targets has shrunk from days to minutes. Moreover , some worms exploiting the non-uniform vulnerablehost distribution may use advanced scanning strategies, e.g., good point set scanning, to infect a large number of hosts in a shorter time. This type of worms is called as self-learning worms. How to combat self-learning worms effectively is an urgent issue confronted by defenders. The concept of predator is addressed by Toyoizumi 
Preliminaries
Distribution of Vulnerable Hosts
The distribution of vulnerable hosts in the Internet is not uniform. The reasons are as follows. Our measure results demonstrate that the vulnerable-host distribution is highly non-uniform by two collected data sets. The first data set is a traffic log of the Witty worm obtained from CAIDA (the Cooperative Association for Internet Data Analysis) 
(1) 
where i = 0, 1, · · ·, 255. The results are shown in 
Good Point Set Scanning
Good point set scanning (GPSS) 
Z n = (z n 1 , z n 2 , · · ·, z n d 
). When using the GPSS to generate next IP address, the detail process is as follows. In the d-dimensional unit cube, we set a good point set with n points. 
P n (i) = {{α 1 × i}, {α 2 × i}, · · ·, {α d × i}}, 
(2) 
where i = 1, 2, · · ·, n, α k = 2 cos(2πk/p), 1 ≤ k ≤ d, p is the smallest prime number which satisfies p ≥ 2d + 3, {ν} denotes the fractional part of ν. Among the n newly generated IP addresses, we assume that the k IP address < B > k is represented as < B 
> k = (b k 1 , b k 2 , · · ·, b k d ), where b k i = {α i × k}, 
b k i = { 1 if {α i × 
k} ≥ 0.5; 0 otherwise. 
(3) 
Using this method, we can obtain n newly generated IP addresses, which can be targeted by the worm. At each infected host, the worm generates newly IP addresses by the use of the good point set scanning and attacks the rest of vulnerable hosts. 
Predator/Prey Models
In Reference 
Worm Interaction Model
The total population N is partitioned into four groups, and any host can potential be in any of these groups at any time tick t: Susceptible (S) -all hosts have not encountered the preys and have no circulating predators; Prey (I A ) -all hosts have encountered the preys in the Internet during the outbreak; Predator (I B ) -all hosts in this group are infected by predators and no longer susceptible to infection; Recovered (R)-all hosts in this group are not infectious and no longer susceptible to infection. We base our model on the following assumptions: (1) We ignore the removal times. (2) The total population N is fixed, and does not vary with time t. (3) Once hosts are recovered or infected by predators, they have gained a certain period of permanent immunity and can no longer be infected by the same prey. This assumption is reasonable , because predators embedded relevant patches can guarantee hosts' security. From the assumptions above, the standard incidence of the total population size can be expressed as 
N = S(t) + I A (t) + I B (t) + R(t). 
(4) 
Infection Rate
A self-learning worm (named as a prey) replication can be significantly slowed down by network delay (ND) 
{ β A = φ A s A ∑ m i=1 pg(i)p * g (i) Ωi , β B = φ B s B ∑ m i=1 pg(i)p * g (i) Ωi , 
(5) 
where p g (i), referred to as the group distribution, is the percentage of live vulnerable hosts in group i (i = 1, 2, · · ·, m), Ω i is the size of address space in group i, 
p * g (i)
, referred to as the group scanning distribution, is the probability that a scan will hit group i. Because self-learning worms (preys) can exploit the vulnerable host distribution, and scan the entire Internet according to this probability distribution, i.e., 
p * g (i) = √ Ωipg(i) ∑ m j=1 √ Ωj pg(j) 
, which is the optimal static strategy. If 
Ω 1 = Ω 2 = · · · = Ω m = Ω/m, p * g (i) = √ pg(i) ∑ m j=1 √ pg(j) 
. The infection rate of prey and predator are 
     β A = sA Ω φ A × m ∑ m i=1 √ p 3 g (i) ∑ m j=1 √ pg(i) , β B = sB Ω φ B × m ∑ m i=1 √ p 3 g (i) ∑ m j=1 √ pg(i) . 
(6) 
Therefore, good point set scanning-based self-learning worms (preys) can increase the infection rate with the 
tor of m ∑ m i=1 √ p 3 g (i) ∑ m j=1 √ pg(i) 
, compared to random-scanning worms (where 
β A = s A Ω φ A ). Let N D A and N D B 
denote the network delay for prey and predator, respectively. We can derive φ A and φ B as follows: 
{ φ A = 1 1+s A N D A , φ B = 1 1+sB N DB . 
(7) 
 The infection rate will be dynamic if the network congestion happens, which is consistent with the practical network. Let l be the number of targeted sub networks. For sub network i, let h Ai and h Bi denote the probability of network i being scanned for prey and predator, g A and g B be the worm replication size for prey and predator, q Ai and q Bi be the average queue length of outgoing links for prey and predator, bw Ai and bw Bi be the average bandwidth of outgoing links for prey and predator, c Ai and c Bi be the average packet drop rate for prey and predator , ld Ai and ld Bi be the average link delays for prey and predator. We can derive N D A and N D B as follows: 
{ N D A = ∑ l i=1 (h Ai (1 − c Ai )(ld Ai + gA(qAi+1) bwAi )), N D B = ∑ l i=1 (h Bi (1 − c Bi )(ld Bi + gB (qBi+1) 
bwBi 
)). 
(8) 
Interaction Model
When there is a prey (A) and a predator (B), and the predator does not infect (or vaccinate) any susceptible host, but terminate any found prey, we consider this as infection-driven interaction. We propose the model represented in 
         dS dt = Π − β A SI A − µS − γ S S, dIA dt = β A SI A − µI A − β B I A I B − γ A I A , dI B dt = β B I A I B − µI B − γ B I B , dR dt = γ S S + γ A I A + γ B I B − µR. 
(9) 
 4 Model Analysis and Basic Prop- erties 
Summing the equations in (9), we obtain that the total population N satisfies the differential equation 
dN dt = Π − µN. 
(10) 
Thus, we assume that the initial value is 
N 0 = S 0 + I A0 + I B0 + R 0 = Π µ 
in order to have a population of constant size (that is, 
S(t) + I A (t) + I B (t) + R(t) ≡ Π µ ). Obviously, the state variables (S(t), I A (t), I B (t), R(t)
 ) remain in the biologically meaningful set 
Φ = {(S, I A , I B , R) ∈ R 4 + |0 ≤ S +I A +I B +R ≤ Π µ } for (S(0), I A (0), I B (0), R(0)) ∈ R 4 + , which is a positively invariant region. Using R(t) = Π/µ − S(t) − I A (t) − I B (t) to eliminate R(t) 
 from the equations in (9) leads to the following reduced three-dimensional model: 
     dS dt = Π − β A SI A − µS − γ S S, dIA dt = β A SI A − µI A − β B I A I B − γ A I A , dI B dt = β B I A I B − µI B − γ B I B . 
(11) 
The dynamical behavior of (9) on Φ is equivalent to that of (11). Thus, in the rest of the paper we will study Model (11) in the feasible region Φ 
1 = {(S, I A , I B ) : S ≥ 0, I A ≥ 0, I B ≥ 0, 0 ≤ S + I A + I B ≤ Π µ } 
 is also a positively invariant region for Model (11), and Model (11) is obviously well-posed in Φ 1 . Firstly, we derive the basic reproduction number of Model (11). It is easy to see that Model (11) always has a worm-free equilibrium, 
P 0 = (S * 0 , I * A0 , I * B0 ) = (Π/
µ, 0, 0). 
Let x = (I A , I B , S) T 
, then Model (11) can be written as 
dx dt = F (x) − Y (x), where F (x) =   β A SI A 0 0   , Y (x) =   µI A + β B I A I B + γ A I A −β B I A I B + µI B + γ B I B −Π + β A SI A + µS + γ S S   . 
Differentiating F (x) and V (x) with respect to I A , I B , S and evaluating at the worm-free equilibrium P 
0 = (S * 0 , I * A0 , I * B0 ) = (Π/
µ, 0, 0), we have 
F (P 0 ) = ( F 2×2 0 0 0 ) , Y (P 0 ) =   Y 2×2 0 0 β A Π/µ 0 µ + γ S   , where F 2×2 = ( β A Π/µ 0 0 0 ) , Y 2×2 = ( µ + γ A 0 0 µ + γ B ) . F (P 0 )Y −1 (P 0 
) is the next generation matrix for Model (11). It then follows that the spectral radius (the largest absolute eigen value) of the matrix F (P 0 )Y −1 (P 0 ). Thus, 
ρ(F (P 0 )Y −1 (P 0 )) = β A Π µ(µ + γ A ) . 
According to Theorem 2 in 
R 0 = β A Π µ(µ + γ A ) . 
(12) 
Let the right-hand side of equalities in Model (11) be zero, then calculating straightforwardly we can obtain I A = 0 or I A > 0. For the case of I A = 0, we have the worm-free 
rium P 0 = (S * 0 , I * A0 , I * B0 ) = (Π/
µ, 0, 0). For the case of I A > 0, we can obtain the endemic 
equilibrium P * (S * , I * A , I * B ), where S * = Π β A I * A + µ + γ S , I * A = µ + γ B β B , I * B = Π − (β A I * A + µ + γ S )(µ + γ A ) β B (β A I * A + µ + γ S ) . 
 We have the following results on the stability of equilibrium P 0 and P * : 
Proof. According to P 
0 = (S * 0 , I * A0 , I * B0 ) = (Π/
µ, 0, 0), the Jacobian matrix at the worm-free equilibrium P 0 is 
J(P 0 ) =   −µ − γ S −β A S * 0 0 0 β A S * 0 − µ − γ A 0 0 0 −µ − γ B   . 
The corresponding eigenvalues of J(P 0 ) are 
     λ 1 = −µ − γ S , λ 2 = β A S * 0 − µ − γ A , λ 3 = −µ − γ B . 
(13) 
All parameters of the model are assumed to be positive. Therefore, for λ 1 , λ 3 to be negative, i.e., for a worm-free equilibrium to be locally asymptotically stable, the following condition has to be required: 
(β A Π−µ 2 −γ A µ)/µ < 0. 
By the stability theory 
S * 0 < µ+γA βA . 
If we substitute S * 0 = Π/µ into the above inequality, we have βAΠ µ(µ+γA) < 1, which is exactly the sufficient condition in the lemma. Further, we can obtain the following theorem. 
Theorem 4.2. The worm-free equilibrium P 0 is globally asymptotically stable if R 0 ≤ 1. 
Proof. Learn from the first equation of Model 
(11) S ′ (t) ≤ Π − (µ + γ S )S(t). Thus S(t) ≤ Π µ + (S(0) − Π µ )exp[−µt], when t → ∞, we obtain S(t) ≤ Π µ . 
 Let us consider the following Lyapunov function defined by 
L(t) = I A (t). 
The time derivative of L(t) along the solution of Model (11) is given by 
L ′ (t) = I ′ A (t) = β A SI A − µI A − γ A I A − β B I A I B ≤ β A SI A − µI A − γ A I A ≤ β A I A Π/µ − (µ + γ A )I A = I A [ βAΠ µ − (µ + γ A )] ≤ 0. 
Thus, we prove that the worm-free equilibrium P 0 is globally stable. This completes the proof. For the case of the endemic equilibrium P * . The Jacobian matrix at P * is 
J(P * ) =   C −β A S * 0 β A I * A D −µ − γ B 0 β B I * B 0   , where C = −µ − γ S − β A I * A , and D = β A S * − µ − γ A − β B I * B . The eigenfunction of J(P * ) is f (λ) = λ 3 +a 1 λ 2 +a 2 λ+ a 3 , where a 1 = β B I * B + γ A + β A I * A + 2µ + γ S > 0, a 2 = γ S β B I * B + µ 2 + µγ S + 2µβ B I * B − γ S β A S * + β A β B I * A I * B + γ S γ A + γ B β B I * B + γ A β B I * A + µγ A + β A I * A µ − β A µS * , a 3 = γ S γ B β B I * B + β A β B γ B I * A I * B + µ 2 β B I * B + β A β B µI * A I * B + µβ B I * B γ B + γ S µβ B I * B > 0. 
 By the Routh-Hurwitz theorem, the Routh-Hurwitz array for P * is as follows: 
    1 a 2 a 1 a 3 (a 1 a 2 − a 3 )/a 1 0 a 3 0     . 
Thus, if we can verify that (a 1 a 2 − a 3 )/a 1 has the same sign with a 2 , then the three eigenvalues all have negative real parts. Obviously, 
a 1 > 0, a 3 > 0. Also, (a 1 a 2 − a 3 )/a 1 > 0 ⇐⇒ a 1 a 2 − a 3 > 0 
 holds by the little algebraic calculation of a 1 , a 2 , a 3 . Thus, the Routh- Hurwitz stability conditions are satisfied, which implies that the endemic equilibrium P * is locally asymptotically stable. From the above discussion, we can summarize the following conclusion. 
if R 0 > 1. 
Proof. It is easy to see that the model has unique positive equilibrium P * if R 0 > 1 holds. Then we consider the following Lyapunov function 
L(t) = ∫ S S * x − S * x dx + ∫ I A I * A x − I * A x dx. 
(14) 
 The time derivative of L(t) along the solution of Equation (11) is given by 
L ′ (t) = ( S−S * S )S ′ + ( IA−I * A IA )I ′ A = (1 − S * S )[Π − β A SI A − µS − γ S S] + (1 − I * A IA )[β A SI A − µI A − β B I A I B − γ A I A ] ≤ (1 − S * S )[Π − β A SI A − µS − γ S S] + (1 − I * A IA )[β A SI A − µI A − γ A I A ] = −Π( S S * )( S * S − 1) 2 ≤ 0. 
Thus, we prove that the endemic equilibrium P * is globally stable. This completes the proof. 
Experiments
Simulation Settings
 Our main goal is to verify the accuracy of our mathematical model and have better understanding of worm infection in a rich set of environments. We choose the Slammer-like self-learning worm as basic behavior of a prey in this experiment. Slammer worm is chosen because , despite its simplicity, it still holds the world record of fastest-spread worm yet 
p g (i) = p e (i). 
(15) 
We simulate prey (A) and predator (B) which may have different scan rates, initial number of infected hosts and the same group distribution information. We assume that the average scan rate of predators is s B = 4000 scans/second, the worm replication size of predators is g B = 404 bytes, and the initial infective of predators is I B (0) = 1. In order to obtain the authentic network delay, we generate a two-level topology with 1000 vulnerable hosts. It can help us test our model with bottleneck network having large number of hops (1-4 hops) with moderate bandwidth (512 kbps, 10Mbps local network) and delay between hosts (1 ms on average). The topology has 10 local networks; each local networks has 100 hosts with one of them acting as a router. One AS (Autonomous System) has one or two local networks. We use BRITE Internet topology generator to generate the links between routers. We obtain the relatively actual network delay (N D A = N D B = 0.011 seconds). Other parameters in these simulations are given as follows: the manual vaccination rate of susceptible hosts is γ S = 6 × 10 −6 ; manual removal rates for prey and predator are γ A = 6 × 10 −4 and γ B = 5 × 10 −4 , respectively. The death rate is µ = 0.00001. The results are based on the average of at least 10 simulation runs. 
Performance Evaluations
The basic reproduction number is R 0 = 0.294 through the calculation by the use of the above parameter values. The prey will gradually disappear from the theory. From 
"
"Introduction
The concept of blind signatures was introduced by Chaum 
Preliminaries
 2.1 Security Properties of Blind Signa- ture 
 A blind signature scheme is a cryptographic primitive involving two entities: an user and a signer. So, we consider the user as an adversary for providing the security proof. In this subsection, we describe the required security properties of a blind signature scheme 
 2.2 Braid Groups and Conjugality Prob- lem 
In this section, we give a brief description of the braid groups and discuss some hard problem related to conjugality search problem. For more information on braid groups, word problem and conjugality problem please refer to 
1) σ i σ j = σ j σ i , Where | i − j |≥ 2. 2) σ i σ j σ i = σ j σ i σ j , Otherwise. 
The integer n is called braid index and each element of B n is called an n-braid. 
In this section, we describe some mathematically hard problems over braid groups. We say that two braids x and y are conjugate (written as x ≈ y) if there exist a braid a such that y = axa −1 . For m < n, B m can be considered as a subgroup of 
B n generated by σ 1 , σ 2 , · · · , σ m−1 . 
(x, x = axa −1 ) ∈ B n × B n and y ∈ B n . Objective: Find a triplet (α, β, γ) ∈ B n × B n × B n such that α ≈ x, β ≈ γ ≈ y, αβ ≈ xy and αγ ≈ x y. 
In 
b = ∆ u π 1 π 2 · · · π l , 
 where ∆ is called the fundamental braid and π's are permutations from Z n to Z n . Hence | B n (l) |≤ (n!) l . Now there is an efficient polynomial time algorithm in 
Proposed Security Analysis
Signature Scheme by Verma
In this section, we are giving blind signature scheme by Verma 
2) Chooses (x = axa −1 , a) ∈ R RSSBG(x, d); 
3) Return pk = (x = axa −1 , x) and sk = a. 
Blind Signing. There are four steps as follows: 
1) signer chooses (α = bxb −1 , b) ∈ RSSBG(x, d) and sends α as a commitment to the user. 
 2) Blinding: User chooses δ ∈ R B n (l) and computes α = δαδ −1 and h = H(mH 1 (α )) and then sends h to the signer. 
3) signer computes β = bhb −1 , γ = ba −1 hab −1 and sends (β, γ) to user. 4) Unblinding: User computes β = δβδ −1 , γ = δγδ −1 and display (α , β , γ 
 ) as a blind signature on message m. 
Verification. Verifier computes h = H(mH 1 (α )) and accepts the signature if and only if α ≈ x, β ≈ h, γ ≈ h, α β ≈ xh, and α γ ≈ x h. 
Analysis of Scheme
In this section, we analyze the security of the above scheme in the random oracle model under chosen message attack. We say that the blind signature scheme BS is secure against one more forgery under chosen message attack or just secure blind signature scheme if there does not exist a polynomial time adversary(PPT) A with non-negligible advantage Adv blind B n (A). 
Definition 5. Let S = (K, S, V
Theorem 1. If MTSP is infeasible in braid group B n then the blind signature scheme in Section 3.1 is unforgeable under one more forgery as defined in Section 2.1. 
 Proof. Let in braid group B n , Conjugality Decision Problem (CDP) is easy and conjugality Search Problem (CSP) is hard. Let A be any polynomial time adversary attacking the blind signature scheme over braid group against one more forgery under chosen message attack. Now we will use A to construct another Probabilistic Polynomial Time (PPT) adversary B that will solve the MTSP with advantage Adv M T SP Bn 
(B) = Adv blind Bn (A) q h . 
Suppose the adversary B is given (x, axa −1 = x ) ∈ B n ×B n and y ∈ B n as challenge and B has to simulate the random hash oracle H : {0, 1} * → B n and blind signing oracle BS for adversary A. Suppose the number of hash oracle queries by A be q h and q s the number of queries to blind signing oracle. Each time A makes a new hash oracle query m i || H 1 
(α i ) for 1 ≤ i ≤ q h , 
that is distinct from the previous hash oracle query. If A makes a hash oracle query that it already made before, B searches in H list and replies with old one. Otherwise it replies in the following way. 
If i = i 0 (for some 1 ≤ i 0 ≤ q h ), then B returns H(m i || H 1 (α i )) = y. Otherwise B chooses a random braid k i ∈ R B n and returns H(m i || H 1 (α i )) = k i and 
adds the answer to its H list . When A makes a blind signing oracle queries on h, then B resends it to blind signing oracle BS and forwards the answers to A. Eventually A halts and output a list of message signature pairs 
(m 1 , σ 1 ), (m 2 , σ 2 ), · · · , (m q s +1 , σ q s +1 ) where each σ i = (α i , β i , γ i ) for 1 ≤ i ≤ q s + 1
. Now A selects a message signature pair (m, (α , β , γ )) and outputs it as forgery on message m. If m = m i0 , then α ≈ x, β ≈ y, γ ≈ y, α β ≈ xy, and α γ ≈ x y where H(mH 1 (α )) = y. Therefore (α , β , γ ) is a solution of MTSP of instance (x, axa −1 = x ) ∈ B n × B n and y ∈ B n and 
Adv M T SP B n (B) = Adv blind B n (A) q h 
. Hence the theorem follows. Otherwise, B reports failure and halt. 
Conclusion
In this paper, we have analyzed a blind signature scheme over braid groups given in 
"
"Introduction
Encryption is the cryptographic primitive which provides confidentiality to the digital communication. The public key encryption provides a powerful mechanism for protecting the confidentiality of stored and transmitted information . When a data provider wants to share some information with a user, the provider must know exactly the one he/she wants to share with. In many applications , the data provider wants to share some information according to the policy based on the receiving of the users' credentials. Attribute-based encryption (ABE) has prominent advantages over the traditional public key encryption which can be exemplified by the fact that the flexible one-tomany encryption will take the place of one-to-one encryption . The ABE scheme provides a powerful method to achieve both data security and fine-grained access control. In CP-ABE scheme, private keys are labeled with sets of descriptive attributes. Only when the set of descriptive attributes satisfies the access structure in the ciphertext can the user get the plaintext. When a data owner wants to provide a document to all the users who have a certain set of attributes, he/she could use ABE to encrypt the document, and the users could decrypt the document when they have satisfied a certain set of attributes. For example, the headmaster wants to encrypt a document to all the professors of 45 years old in the computer science department, the document would be encrypted with access structure { "" professor "" AND "" CS department "" AND "" age 45 "" }, and only the users who hold the private key containing these three attributes can decrypt the document while others cannot get any information from the ciphertext. However, the professors can be classified into different types in reality, such as, full professor and distinguished professor. It is tricky for data provider to encrypt message by using the access structure to achieve both fine-grained access control while reflect importance of attributes. To solve this problem, we introduce hierarchical attributes to CP-ABE. In this paper, the universal attributes in the scheme we proposed are classified into different levels according to their importance defined in the access control system. Every user in the system possesses a set of attributes in hierarchy. The data owner encrypts a data to users in the system who have a certain set of attributes. The ciphertext contains a kind of hierarchical access structure. In order to decrypt the message, users' attributes in hierarchy must satisfy the hierarchical access structure. The notion of CP-HABE can be considered as the generalization of traditional CP-ABE scheme where all attributes are in the same level. 
Related Work
ABE is one of the important applications of fuzzy identity-based encryption 
Our Contributions
The contributions of this paper are listed as following: (1) We formalize the model of CP-HABE and give security model for CP-HABE. We also give specific construction about CP-HABE. (2) We prove our scheme is secure under the standard model by using decisional parallel bilinear Diffie-Hellman exponent assumption. (3) We make analysis of CP-HABE scheme. The analysis shows that our scheme can reach both fine-grained access control and hierarchical attributes. 
Organization
In Section 2, we formalize model for CP-HABE and present the security model for CP-HABE. In Section 3, we review some concepts of the hierarchical access structure , birkhoff interpolation, hierarchical threshold secret sharing schemes, bilinear maps and decisional parallel bilinear Diffie-Hellman exponent assumption. In Section 4, we provide the specific construction about the CP-HABE scheme. In Section 5, we offer security proof under the security model for CP-HABE. In Section 6 we make analysis of CP-HABE scheme and Section 7 is the conclusion of this paper. 
Ciphertext-policy Hierarchical Attribute based Encryption
In this section, we present the definition of CP-HABE scheme and its security model. 
CP-HABE Scheme
 A CP-HABE scheme consist of four fundamental algorithms: Setup, Encrypt, KeyGen and Decrypt. Setup(1 λ , U): The setup algorithm inputs a security parameter 1 λ and hierarchical attribute universe description U. It outputs the public parameters params and master key MSK. Encrypt(params, m, A): The encryption algorithm inputs the public parameters params, hierarchical access structure A and the message m which the sender wants to encrypt. It outputs the ciphertext CT. We assume the ciphertext contains A. KeyGen(MSK, S): The key generation algorithm inputs the master key MSK and a set S of attributes in hierarchy. It outputs a private key SK. Decrypt(CT, SK): The decryption algorithm inputs the ciphertext CT and the private key SK. When a set of attribute satisfies the hierarchical access structure, it can decrypt the ciphertext and return message m. In the CP-ABE scheme, the ciphertext is related with the access structure while the private key is associated with the attribute set. In our security model, the adversary will choose challenge access structure A * and ask for any private key SK containing the hierarchical attribute set S where S does not satisfy A * . We now give the formal security game for CP-HABE as below. 
Security Model for CP-HABE
Init. The adversary declares the access structure A * that he wishes to be challenged upon. 
 Setup. The challenger runs the Setup algorithm and outputs the public parameters param. Challenger gives param to the adversary. 
Phase 1. The adversary makes repeated polynomially private key queries corresponding to the sets of hierarchical attributes S 1 , · · · , S q1 which none of these attribute sets satisfy the challenge hierarchical access structure A * . 
Challenge. The adversary submits two equal length 
A = Pr [β ′ = β] − 1 2 . 
If we allowing for decryption queries in Phase 1 and Phase 2, the model can easily be extended to prevent chosen-ciphertext attacks. 
 Definition 1. An ciphertext-policy hierarchical attribute based encryption scheme is indistinguishable secure against selective chosen plaintext attack if no polynomial time adversaries win the above game with nonnegligible advantage. 
Basic Constructions
 In this section, we introduce the notions related to hierarchical access structure, hierarchical threshold secret sharing schemes, bilinear maps and decisional parallel bilinear Diffie-Hellman exponent assumption. 
Hierarchical Access Structure
Definition 2 (Hierarchical access structure 
Γ = V ⊆ U : V ∩ i j=0 U j ≥ k i , ∀i ∈ {0, 1, · · · , m} 
 3.2 Hierarchical Threshold Secret Sharing Schemes 
In this subsection, we introduce the essential use of the hierarchical threshold secret sharing schemes. We adopt the idea proposed in 
v 1 , · · · , v l0 ∈ U 0 , v l0+1 , · · · , v l1 ∈ U 1 , . . . v lm−1+1 , · · · , v lm ∈ U m , where 0 ≤ l 0 ≤ l 1 ≤ · · · ≤ l m = |V| 
V is authorized subset if and only if l i ≥ k i for all 0 ≤ i ≤ m, where m denote as the number of total levels in this scheme and l i is the number of element in subset V under level i. Let r : F → F k be defined as r(x) = (1, x, x 2 , · · · , x k−1 ) and r (i) (x) denote the i-th derivative of r(x) for i ≥ 0. The share which distributed to participants u ∈ U i is σ(
u) = r (ki−1) (x) · a, where a = (a 0 = s, a 1 , · · · , a k−1 ) 
is the vector of coefficient of p(x). When all participants of V put their shares together , the system can calculate the unknown vector a is M a = σ, where the coefficient matrix is: 
M = (r(v 1 ), · · · , r(v l0 ); r (k0) (v l0+1 ), · · · , r (k0) (v l1 ); · · · ; r (km−1) (v lm−1+1 ), · · · , r (km−1) (v lm )) while σ = (σ(v 1 ), σ(v 2 ), · · · , σ(v lm )) T . For all j = 1, · · · , l 0 , · · · , l m , 
the j ′ -th row of M we let the function ρ defined the party labeling row j as ρ(j). Suppose j = l i−1 + c, then the share r (ki−1) (v li−1+c ) belong to party ρ(j). Suppose that is hierarchical threshold secret sharing scheme for the hierarchical threshold access structure A. The set S ∈ A is defined as any authorized set, and let 
I ⊆ {1, · · · , l 0 , · · · , l m } 
be defined as I = {j : ρ(j) ∈ S}. If {λ j } j∈I are valid shares of secret s according to and authorized set S satisfy Pólya's Condition (the Pólya's Condition can be found in 
Bilinear Maps
Let G and G T be two cyclic groups of prime order p with the multiplication. Let g be a generator of G and e be a bilinear map. Let e : G × G → G T be a bilinear map has the following properties: 1) Bilinearity: for all u, v ∈ G and a, b ∈ Z p , we have e(u a , v b ) = e(u, v) ab . 
2) Non-degeneracy: e(g, g) = 1. 
 3) Computability: There is efficient algorithm to compute bilinear map e : 
G × G → G T . 
Notice that the map e is symmetric since e(u a , v b ) = e(u, v) ab = e(u b , v a ). 
 3.4 Decisional Parallel Bilinear Diffie- Hellman Exponent Assumption 
We use the assumption proposed in 
g, g s , g a , · · · , g (a q ) , g (a q+2 ) , · · · , g (a 2q ) , ∀ 1≤j≤q g s·bj , g a/bj , · · · , g (a q /bj ) , g (a q+2 /bj ) , · · · , g (a 2q /bj ) . . . ∀ 1≤j,k≤q;k =j g a·s·b k /bj , · · · , g a q ·s·b k /bj 
It remain hard to distinguish e(g, g) a q+1 s ∈ G T from a random element in G T . We define the advantage of an adversary A in solving the decisional q-parallel bilinear Diffie-Hellman exponent problem as Adv A = Pr B y, T = e(g, g) 
a q+1 s = 0 − Pr [B (y, T = R) = 0] 
Definition 3. We say that the decision q-parallel DBHE assumption holds if no polytime algorithm has a nonnegligible advantage in solving the decisional q-parallel DBHE problem. 
Our Construction
 In this construction, the attributes are assumed to be divided into m + 1 levels. Different attributes belong to different levels according to their importance in the system and these attributes do not have any relationship for access control. We show how the attributes are categorized and constructed in 
( 1 ) h ( 2 ) h 0 ( ) l h 1 ( 2 ) m l h 1 ( 1 ) m l h ( ) m l h : 0 l e v e l : l e v e l m 
 Setup(U): The setup algorithm inputs the set of universe attributes U in the system. The attributes are in the different hierarchy. The algorithm then chooses a group G of prime order p, a generator g and group element h 1 , · · · , h |U | ∈ G that associated with |U| the attributes in the system. In addition, it chooses random exponents α, a ∈ Z p . The public parameters is published as: 
params = {g, e(g, g) α , g a , h 1 , · · · , h |U | } 
The authority set MSK =g α as the master secret key. 
Encrypt(params, (M V , ρ),m ′ ): 
 The encryption algorithm inputs the public parameters params and a message m ′ to encrypt. In addition, it inputs a HTSSS access structure (M V , ρ). The function ρ associates rows of M V to attributes. The algorithm first chooses a random vector 
a = (a 0 = s, a 1 , · · · , a k−1 ) ∈ Z k p . 
These values will be used to share the encryption exponent s. For all j 
= 1, · · · , l 0 , · · · , l m , it calculates λ j = M j · a
, where M j is corresponding to the j ′ th row of M V . In addition, the algorithm chooses random r l0 , · · · , r lm ∈ Z p . The ciphertext is published as: 
CT = {C = me(g, g) αs , (MV , ρ), C ′ = g s , C1=g aλ 1 h −r l 0 ρ(1) , · · · , 
(Cj=g aλ j h − i=l 0 ,··· ,l k r i ρ(j) , · · · ,C lm =g aλ lm h − i=l 0 ,··· ,lm r i ρ(lm) , D l 0 =g r l 0 , · · · ,D lm =g r lm } where ki−1 ≤ j ≤ ki 
Keygen(MSK,S): The key generation algorithm inputs the master secret key MSK and a set S which has different hierarchical attributes. The algorithm first chooses a random t ∈ Z p . It creates the private key as 
SK = {K = g α g at , L = g t , ∀x ∈ S : K x = h t x }. 
Decrypt(CT, SK): The decryption algorithm inputs a ciphertext CT for access structure M V and private key for set S. Suppose that S satisfy the hierarchical access structure which the number of attributes in level U i must exceed its threshold 
k i . Let I ⊆ {1, · · · , l 0 , · · · , l m } 
be defined as I = {j : ρ(j) ∈ S}. If {λ j } j∈I are valid share according to M and set S satisfy Pólya's Condition, then j∈I ω j λ j = s where {ω j ∈ Z p } be a set of constants. The decryption algorithm first computes 
CW = e(C ′ , K)/ j∈I (e(Cj, L) · e(D l 0 D l 1 · · · D l k i , K ρ(j) )) ω j ) = e(g, g) αs e(g, g) ast /( j∈I e(g, g) taλ j ω j ), (ki−1 ≤ j ≤ ki) = e(g, g) αs . 
Then algorithm divides out CW from C and obtain the message m. 
Proof
Theorem 1. Suppose the q-parallel DBHE assumption holds. Then no polytime adversary can selectively break our system with a challenge matrix of size 
l * × n * , where n * ≤ q. 
 Proof. Suppose we have an adversary A with nonnegligible advantage ǫ = Adv A and it choose a challenge matrix M * of dimension at most q columns in the selective security game against our construction. We show how to build a simulator B to play the q-parallel DBHE problem. Init: The simulator takes in the q-parallel DBHE challenge y, T . The adversary gives the challenge access structure (M * , ρ) to the algorithm, where M * has n * ≤ q columns. Setup: The simulator chooses random α ′ ∈ Z p and sets α = α ′ + a q+1 . Then we have 
e(g, g) α = e(g a , g a q ) · e(g, g) α ′ . 
Here we make analysis of how the simulator programs the parameter h 1 , · · · , h |U | . For each x = 1 to |U|, choose a random value z x ∈ Z p . If there exist an j such that ρ * (j) = x, then let 
h x = g zx g aM * j,1 /bj g a 2 M * j,2 /bj · · · g a n * M * j,n * /bj . Otherwise, let h x = g zx . 
Phase 1: In this phase, the adversary A makes a private key query for a set S, where S does not satisfy M * . Then the simulator answers private key queries. The simulator first chooses a random r ∈ Z p . Because S does not satisfy M * , simulator can finds a vector 
ω = (ω 1 , ω 2 , · · · , ω n * ) ∈ Z n * p such that ω 1 = −1. For all i satisfied ρ * (i) ∈ 
S, we have that ω · M * j = 0. The simulator defining t as: 
t = r + ω 1 a q + ω 2 a q−1 + · · · + ω n * a q−n * +1 . 
Then we have: 
L = g t = g r i=1···n * g a q+1−i ωi . 
Here we calculate K x where ∀x ∈ S. We first consider x ∈ S where no j such that ρ * (j) = x. In this circumstance , we simply let K x = L zx . Next we deal with the task to create keys for attribute x where x is used in the access structure. Because simulator can not simulate the terms of the form g a q+1 , we must make sure private key do not have this terms. Notice that in calculating h t x all terms of this form in the exponent come from M * j,k a k · ω k a q+1−j /b j for some k, where ρ * (j) = x. However, we have that M * j · ω = 0 Therefore, all exponent of a q+1 can cancel when combined. The simulator creates K x as follows 
K x = h t x = (g zx g aM * j,1 /bj g a 2 M * j,2 /bj · · · g a n * M * j,n * /bj ) t = L zx · k=1,··· ,n * (g a k ·M * j,k ·t/bj ) = L zx · k=1,··· ,n * (g a k ·r w=1,··· ,n * k =j (g a q+1+k−w/b j ) ωw ) M * j,k 
 Challenge In this stage we build the challenge ciphertext . The adversary gives two message m 0 and m 1 to the simulator. The simulator flips a coin β, creates 
C = m β e(g, g) αs = m β T · e(g, g) α ′ s and C ′ = g s . 
The more difficult task is to simulate the C i values since it contain g sa i that we can not simulate. However , the simulator can use the secret splitting such that these items cancel out. The simulator first choose random 
y ′ 2 , · · · , y ′ n * ∈ Z p , 
then share the secret using the vector 
v = (s, sa + y ′ 2 , · · · , sa n * + y ′ n * ) ∈ Z n * p . In addition, it chooses random values r ′ 1 , · · · , r ′ lm . For i = 1, · · · , n * , 
the challenge ciphertext components are then generated as Guess: The adversary will eventually output a guess β ′ of β. The simulator outputs 0 to guess that T = e(g, g) a q+1 s if β ′ = β. Otherwise, it and outputs 1 to indicate that it believes T is a random group element in G T . When T is a tuple the simulator B gives a perfect simulation so we have that 
D 1 = g r ′ l 0 g sb1 , · · · , D i = g r ′ l k i g sbj where k i−1 ≤ j ≤ k i . C j = g aλj h − i ri ρ * (j) = g asM * j,1 g (sa 2 +y ′ 2 a)M * j,2 · · · g (sa n * +y ′ n * a)M * j,n * · h − i=l 0,··· ,l k r * i ρ * (j) (g bj s ) −z ρ * (j) · r ′ l k 0 ,··· ,r ′ l k i i =j w=1,··· ,n * (g −a w s·(bi/bj ) ) M * j,w · w=1,··· ,n * (g −a w s ) M * j,w . 
Pr B( y, T = e(g, g) a q+1 s ) = 0 = 1 2 + Adv A 
 When T is a random group element, the message m β is hidden from the adversary and we have Pr [B( y, T = R) = 0] = 1 2 . Therefore, can play the decisional q-parallel DBHE game with non-negligible advan- tage. 
Analysis of the Proposed Scheme
Li et al. 
Conclusion
 In this paper, we propose a scheme called ciphertextpolicy hierarchical attribute based encryption in which the attributes in the system are not always in the same level. We present specific construction of CP-HABE which uses the hierarchical access structure that can be considered as a generalization of traditional ABE. Only when a set of attributes possessed by the user satisfies the hierarchical access structure can he/she decrypt the ciphertext. We also give a security model for CP-HABE. Finally, we prove our scheme under the security model by reducing it to decisional q-parallel bilinear Diffie-Hellman exponent assumption. More importantly, this construction can exhibit significant improvement over the traditional ABE schemes accordant with the practical situa- tion. This work motivates a few interesting problems in this topic: 1) How can we construct more efficient schemes with attributes in hierarchy. 2) In this paper, the size of the ciphertexts is not constant, how to improve CP- HABE scheme with a ciphertext of a constant size. 3) How can we revocation attributes in different levels more efficiently. Therefore, Our future work will be design a CP-HABE scheme to solve these problems. 
"
"Introduction
A Wireless Sensor Network (WSN) consists of wireless sensors, small devices that collect data readings such as light or temperature from an environment. The sensors then send the data to a base station, a central location for the data to congregate 
Related Work
Numerous techniques have been proposed in recent years for estimating battery lifetime. In addition, a variety of strategies have been proposed to exploit battery characteristics for designing more battery friendly systems and communication protocols. Authors in 
.. * A n 
is the Cartesian product of the sets of actions available to each player, and {u i } is the set of utility functions that each player i wishes to maximize, where u i : A → . Our proposed framework enforces cooperation among nodes and provides punishment for non-cooperative behavior . We assume that the rational users optimize their profits over time. The key to solve this problem is when nodes of a network use resources, they have to contribute to the network life in order to be entitled to use resources in the future. The base station keeps track of the behavior of other nodes, and as they contribute to common network operation, their reputation increases. We are interested in solving a game by predicting the strategy of each player, considering the information that the game offers and assuming that the players are rational. Authors in 
Equilibrium
seek to minimize battery usage; the decrease in available battery level must discourage nodes from overloading the network but not to the limit that they do not cooperate with the rest of the network for their selfish act of energy utilization. Therefore we need to design a cooperative security mechanism that enforces cooperation and shows that when no countermeasures are taken against misbehaving nodes, network operation can be heavily jeopardized . Also we capture and describe battery usage behavior at each node, and based on this battery model we present a battery-aware strategy for each node to avoid energy loss but gain better reputation over the course of the game. 
Payoff and Reputation
 Each node i has a von Neumann-Morgenstern utility function defined over the outcomes of the stage game G, as u i : A → , where A is the space of action profiles 
A = F orward packets A 1 Do not f orward packets A 2 
Let G be played several times and let us award each node a payoff which is the sum of the payoffs it received in each period from playing G. Here, 
u t i = αr t i − βc t i 
where r t i is the gain of node i's reputation, c t i is the cost of sending or forwarding a packet for the node as energy loss, and α and β are weight parameters. We assume that measurement data can be included in a single message that we call a packet. Packets all have the same size. The transmission cost for a single packet is a function of the transmission distance 
u t A1 = T * r t+1 i − B * (c s + c r ) 
where r t+1 i is the predicted gain of node i's reputation. For sending a packet, c t i is broken down into two constant values: c s and c r . c s is the voltage cost to send a packet and c r is the voltage cost to receive a packet. B is the weight parameter for cost, and represents the importance of being conservative about sending packets when a node has a low battery leave. At a node's highest battery level, B will be 1. As the node's battery level crosses designated thresholds by decreasing, B will increase. T is the weight parameter for the gain component of the equation and represents the number of units of time since node i has last forwarded a packet. T starts at 1 for each node i and increments every time any node i decides to not forward a packet. When a node sends a packet, T is reset back to 1. If a node has recently sent a packet, it may not be important to send another packet right away, which is why T starts at a low value. But as time passes without forwarding any packets, it is important that a node sends data through the network, which leads T to increase. The utility for not forwarding a packet is calculated as: 
u t A2 = T * 0 − B * c s 
Since there is no gain in reputation when not sending a packet, the gain is 0. However, receiving a packet from another node still costs energy. After calculating the utility for each of these actions, the node will perform the action that yields the greater utility. The strategy for each node i at time t is: 
s i (h t ) = F orward if u t+1 A 1 > u t+1 A 2 
Do not f orward otherwise 
In order to compute the values of a node's gain, we turn our attention to the work proposed in 
r t i = t−1 k=1 ρ i (k) 
where ρ i (k) represents the ratings that the base station has given to node i, and ρ i ∈ 
Configurations
 We use three major network configurations. The first configuration , named case1, consists of a network of wireless sensors which broadcast packets to any nodes within range. Since the nodes in our experiment are located within a small distance of each other, all nodes in the network are capable of broadcasting directly to every other node in the network. Whenever a node receives a packet from another node and forwards the packet, that packet is re-broadcast to every node within range. For networks of a large size, this generates a large amount of traffic. In an attempt to remedy this, another network configuration was developed 
Malicious Node Detection
In our simulations, we introduce malicious nodes into the network to see how they affect the network and if there is a way to detect and neutralize such nodes. Malicious nodes randomly drop packets, reducing the throughput of the network. Malicious nodes also consume additional power when randomly deciding whether or not to drop packets. The base station keeps track of the reputation of each node in the network. Periodically, the base station will decide whether or not a node is acting malicious based on its throughput. The base station takes the current reputation of each node in the network and calculates the average, as well as the standard deviation. If a node's reputation is lower than the average minus the standard deviation, that node is deemed malicious. The base station sends a packet to that node ordering the node to turn its radio off and shut down. 
Performance Evaluation
 In the case1 scenarios, the simulation starts with an initial voltage reading from each sensor. In this work we have used MICAZ sensors 
Conclusion and Future Work
Our results indicate that, under most cases, implementing game theory in a WSN is beneficial by helping reduce the amount of voltage consumption throughout the network. By adding a decision making process of when to send and not to send packets, the sensors conserve energy while maintaining the throughput. Further work includes experimenting with different strategies in order to save power, as well as improving the accuracy of our malicious node detection procedure. Other possible extensions of this project would be to experiment with packets of different priorities and implement coalitions of nodes, also implementing different mechanisms for selection of cluster heads. Afrand Agah obtained her Ph.D. degree in Computer Science from the University Texas at Arlington. She is executive board member for Pennsylvania Association Computer and Information Science educators. In addition to her work on security in wireless sensor networks, Dr. Agahs scholarly interests include security and trust in pervasive computing and security in mobile ad-hoc networks. Dr. Agah has contributed as a committee member for various national and international conferences such as IEEE International workshop on wireless and sensor networks security; the International conference on wireless networks ; IEEE communications society conference on sensor and ad-hoc communications and Networks; the IEEE wireless communications and networking conference; the Asian International mobile computing conference; and the IEEE International conference on high performance computing . She also served as a reviewer for a number of journals such as the journal of mobile communication, computation and information; the International journal of network security. In past she has also contributed as a reviewer for handbook of information security, and the Internet Encyclopedia, by John Wiley and sons. 
"
"Introduction
Wireless sensor networks WSNs are composed of hundreds to thousands of small, low cost, low power and multifunctional sensor nodes, having the possibility to sense environment measures like temperature, pressure and movement to allow environment monitoring 
Routing in Wireless Networks
Several classifications of routing algorithms in wireless ad hoc network exist towards the specificities of wireless networks such as node mobility, devices' constraints as well as the underlying technology: 2) Reactive protocols: The shortcoming of proactive routing is the increasing size of the routing tables since wireless nodes can't keep the whole topology of the network in their routing tables especially in WSNs, the reactive routing protocols propose to find routes on demand. In the way that, whenever a node needs to establish a route to a given destination it diffuses a route request which is propagated over the whole network until it arrives to the destination node which responses by a route reply to establish the final route between the source and the destination nodes. This kind of routing is very suitable for wireless networks however there is an additional overhead during the route discovery due to flooding used for route request broadcasting 
3) Hierarchical routing: Also called hybrid routing, these protocols try to overcome the shortcoming of reactive and proactive protocols by using a combination of the two strategies. The hybrid protocols divide the whole network into regions or clusters and use a proactive technique inside the cluster and a reactive technique outside the cluster, in the way that the network topology is kept for close neighbors and routes to far nodes, are established using a route request launched by the cluster head using a reactive strategy, which minimize considerably the overhead of routing 
Routing Challenges and Objectives
As described above WSNs are usually composed of sensors having reduced computing, radio and battery resources as well as the ad hoc paradigm of wireless sensor networks relying on multi hop to ensure connectivity over the network without any infrastructure or centralized authority any protocol should take into consideration the following characteristics of a WSN 
Swarm Intelligence Routing
Ant Colony Heuristics
An ant colony is composed of millions of ants usually well organized and structured; individually ants are incapable to perform structured tasks, however due to their social nature, ants can achieve complex tasks such as build and protect their nest, carry large items, find and optimize routes between their nest and the source of food. Communication between ants is based on a chemical substance called pheromone. During its movement each ant deposits a certain amount of pheromone on its trail; this pheromone helps it to find its way back to the nest. The Pheromone deposed on its trail is also detected by other ants within the same colony which leads to an implicit way of communication between ants which is used by the whole colony to organize, optimize and structure their movement in a large and unpredictable area. Ants are attracted by the pheromone concentration which always leads to the recent trail and therefore to the shortest path to food or nest 
The principle of finding the shortest path can be explained using the case of 
Ant Routing Algorithm
As devoted in the previous section, ants can find, maintain and optimize their trail between the source of food and the nest using little intelligence and communication capabilities by each individual in the colony. From WSNs perspectives, it seems that the characteristics of such communities are very suitable to ensure routing, since a WSN is very often composed of small sensor nodes with limited capabilities working together to ensure the objectives of the network as well as the connectivity of the network and the continuity of the routing service. 
Inspired from real ants the ant routing algorithm ARA 
            i i N j j i j i j i N j N j p i 0 , , ,   (1) 1 ,    i N j j i p 
Previous Work
Since the first use of Ant colony optimization for routing in ad hoc networks, several algorithms and adaptations exist in literature to adapt this heuristic for wireless networks: 
1) Position Based Ant Colony Routing Algorithm for Mobile Ad-hoc Networks: 
the authors in this algorithm 
2) QoS-aware Ant Routing with Colored Pheromones: 
this protocol is proposed for Wireless Mesh Networks, this protocol 
3) Link quality based ARA: due some design limitations 
in the basic Ant routing algorithm, since it do not gives the necessary consideration to the ad hoc networks characteristics in the process of pheromone update and route selection. LQARA 
4) AntNet -Distributed Stigmergetic Control for Communications Networks: 
The idea in AntNet is to use two agents for discovering and establishing routes over the network (forward and backward ants) 
5) Ant based Self-organized Routing Protocol for Wireless Sensor Networks: 
this protocol 
Discussion
As described above several ant routing algorithm were proposed for wireless networks, trying to adapt and improve the strategy of ant colony heuristic for the specificities of mobile networks such as devices' constraints, radio medium as well as the nature of environment. However the majority of the proposed adaptations are useless for wireless sensor networks which are characterized by a very constrained devices, limited bandwidth and having different needs and traffic pattern compared to the conventional wireless networks. Therefore, any adaptation of ant colony heuristic for WSN must take into consideration the traffic pattern of WSN which is in the form of many to one in order to save the network resources and improve the network performance. In the other hands the proposed solution must implement a security mechanism in order to resist against attacks which are more frequent in WSN compared to conventional wireless networks, since a WSN is very often part of a hostile environment exposed to numerous attacks such as spoofing, eavesdropping and physical attacks. 
Optimized ARA for WSNs
In WSNs the traffic pattern is many to one where sensor nodes get environment measures and sends them to a sink node or a base station. Therefore, using the conventional ant routing algorithm to establish routes between each sensor and the base station is not efficient, because sending of a FANT by each sensor overheads the network and consumes sensors' battery power and decreases the network lifetime since the FANT is propagated using flooding. Thus, we propose to affect the task of route discovery to the base station which periodically sends a FANT to all sensors over the network in order to inform sensors in the network about the path to the base station rather than waiting for the FANT from each sensor which is very costly regarding the energy and the bandwidth consumed during the FANT propagation, especially when we take into consideration the increasing number of sensors. 
In this paradigm of ant routing algorithms we anticipate the possibility that each sensor sends a FANT to establish a route with the base station by launching this operation by the base station. The period of the FANT is fixed according to period used by sensors for sending environment measures to the base station in order to ensure the availability of routes 
Forward Ant (FANT)
Like the conventional ant colony routing algorithm the forward ant is sent in order to discover all possible routes between the base station and each sensor over the network. The FANT is propagated over the entire network and visit each sensor in the network, like real ants at each visit to an intermediate node it increases the local value of pheromone with ∆φ. 
In our proposed improvement of ant colony routing we propose to affect the task of sending FANTs to the base station, this FANT is destined to inform sensors over the network about the location of the base station, therefore each sensor over the network learns the path which leads to the base station and rebroadcast the FANT. In order to avoid routing loops and the network resources wasting the FANT is treated once by each sensor using a unique sequence number affected to each new FANT, therefore each sensor when receives a FANT it verifies the sequence number has been already treated or not, if the FANT was not treated it is propagated over the network until it arrives to the edge of the network, otherwise it is ignored. 
Backward Ant (BANT)
In conventional ARA the BANT is sent by the destination node over the reversed path taken by the FANT in order to establish the final route between the source and the destination node. Like the FANT the BANT modifies the value of pheromone by adding a constant amount of pheromone ∆φ at each intermediate node. In optimized ARA the BANT is sent by each sensor after receiving a FANT coming from the base station after a predefined delay in order to avoid network congestion. Each intermediate node decides the next hop for the BANT according to the value of probability computed using artificial pheromone using the Equation (1). 
Route Maintenance
After the establishment of the final route using the route discovery mechanism described above, this route is used by the source and the destination nodes as long as there is no link failure over this route, however whenever an intermediate node loses the connectivity with its next hop due to link failure an error packet is sent to the source node, the route error packet contains: This packet is forwarded over the reverse path taken by data packets by each intermediate node which looks if there is any alternative route; otherwise the error packet is forwarded until it arrives to the source node which waits for the next FANT if there is not an alternative route. 
Securing ARA
Security is very important issue in wireless networks, due to the broadcasting nature of the used medium giving the possibility to anyone in the neighborhood of the network to eavesdrop or modify the exchanged data. Hence, the use of security scheme is primordial to protect the exchanged data from outsider attackers. 
Consequently we propose to execute a handshake destined to establish a symmetric encrypting key used to encrypt ordinary traffic exchanged with the base station, we propose to use as support for this handshake the BANT sent by sensors to the base station. 
In order to make in practice the specifications of our security scheme over ARA we assume that: -The base station has a pair of keys (private and public key) used to authenticate the base station by sensors. -Each sensor is capable to use symmetric and asymmetric encryption. -Each sensor has the capacity to save at least the public key of the base station and a session key used for data encryption. -Each sensor node gets the public key of the base station before deployment from an off-line dealer. 
To establish the encrypting keys between sensor and the base station, each sensor launches the handshake when receive the first FANT, by generating a random symmetric key, encrypts this key by the public key of the base station and sends it included in the BANT. 
We propose to encrypt the symmetric key with the public key of the base station in order to guaranty the security of this key since only the base station has the valid private key to decrypt this message which guaranties its confidentiality, integrity and authenticity. The use of the BANT as support for the handshakes saves sensors' battery power, since each sensor need only to encrypt the symmetric key with the public key of the base station which is not significant regarding the energy consumption. 
After the reception of the BANT by the base station it saves all the received encrypting keys in a global table used to identify each sensor and secure communication with that sensor. In order to enforce security, a proactive key update can be launched periodically by sensors; the key update is Using ECC (Elliptic Curve Cryptography) on a Berkeley/Crossbow motes platform Mica2dots 
Compared to other schemes in literature 
Analysis of Optimized ARA
Energy Consumption
The energy cost of any routing protocol is determined by the energy required for the transmission and the reception of the protocol messages by each sensor such as FANT and BANT. 
The total size of FANT and BANT in ARA is around 5 Bytes, using a Berkeley/Crossbow motes platform Mica2dots as platform 
In order to test scalability of the OARA, we have varied the network size from 9 to 100 sensors and we have measured the energy consumption required for route discovery in AODV 
Scalability
This criterion deals with the possibility to keep the same network performance regarding the network overhead and the energy consumption according to the network widening, this is an important issue since future WSNs will increase in size to get thousands of sensors per region, as proven by simulation it seems that the OARA deals efficiently with the network widening, since each sensor treats the same number of FANTs and BANTs for each route discovery which keeps the network performance constant. 
Security
The security of a routing protocol is defined according to its capacity to guaranty the confidentiality of data and the authenticity of the network nodes. In OARA, we have proposed to share a symmetric encrypting key between each sensor and the base station used to encrypt ordinary traffic which guaranties both confidentiality and integrity of data. The authentication of sensors and the base station is guaranteed using public key cryptography, since only the legitimate base station have the valid private key used to decrypt messages sent from sensors and only legitimate sensors have the valid public key preloaded before deployment which guaranties a mutual authentication between sensors and the base station. 
Conclusion
In this paper we have optimized the ant routing algorithm for WSNs; the proposed optimization takes into consideration the characteristics of the WSNs such as the traffic pattern and devices' constraints. In the proposed protocol we have affected the task of route discovery to the base station which periodically launches FANTs, the FANTs are used by sensors to define the paths to the base station instead of doing this task individually which consumes the network resources and decreases the network lifetime due to broadcasting nature of FANTs. As shown by simulation the proposed OARA saves greatly the network energy and extends the network lifetime compared to the conventional ARA and AODV. 
In order to secure the established link with the base station, we have proposed to execute a handshake during the route discovery phase. Therefore, each sensor when receives a FANT, uses the BANT as support to execute a handshake to share a symmetric encrypting key with the base station. In the way that each sensor encrypts using the public key of the base station a random symmetric key and sends it to the base station included in the BANT. Using the BANT as support for the handshake has considerably saved the network resources, since it does not add any overhead for key establishment. The proposed security scheme ensures confidentiality, authentication and integrity of data over the network. 
Benamar 
"
"Introduction
Today, the Internet is an essential part of our everyday life and many important and crucial services like banking , shopping, transport, health, and communication are partly or completely dependent on the Internet. According to recent sources 
Approaches for Defending DoS/DDoS Attacks
Current DoS/DDoS defenses can be classified into three categories: preventive mechanisms, reactive mechanisms, and source-tracking mechanisms. 
Preventive Defence
The preventive schemes aim at improving the security level of a computer system or network; thus preventing the attacks from happening, or enhancing the resistance to attacks. A proactive server roaming scheme 
Source Tracking
The source-tracking schemes, on the other hand, aim to track-down the sources of attacks, so that punitive action can be taken against them and further attacks can be avoided. The existing solutions fall into four groups: packet marking, message traceback, logging, and traffic observation. Many different packet marking schemes have been proposed , for encoding path information inside IP packets, as they are routed through the internet. The idea is first put forward by Savage et al. 
Reactive Solutions
The reactive measures for DDoS defence are designed to detect an ongoing attack and react to it by controlling the flow of attack packets to mitigate the effects of the attack. One of the proposed reactive schemes, given by Yaar et al. 
A PacketScore 
 3 Designing an Effective Protection Scheme 
Generalizing from the various defense mechanisms, a good protection scheme against DDoS attacks should be based on continuous monitoring, precise detection and timely reaction to attacks. The following characteristics are de- sirable: 
@BULLET The scheme should be able to control or stop the flow of attack packets before it can overwhelm the victim. The timely detection and immediate reaction to an attack is essential, to prevent the depletion of resources at the victim location. The suitable place to deploy defense scheme are the perimeter routers or the firewall of a network. @BULLET In stopping the flow of attack packets to the victim, the scheme must ensure that packets from legitimate users are successfully received so that the service to the legitimate users is not denied or degraded. Any degradation in service would signify a partial success for the denial of service attack. @BULLET The implementation cost should be low. Unless most internet users fully recognize the threats posed by DoS/DDoS attacks, it is difficult to get cooperation from them in defending such attacks, especially when the investment required is costly. Therefore, any viable DDoS defence scheme should require minimal participation of third party networks or intermediate routers on the internet. A good defence mechanism should be able to precisely distinguish the attack packets from the legitimate packets . What makes it difficult to control or stop the DDoS attacks is the use of spoofed IP address. Spoofed packets are commonly used in DoS/DDoS attacks to hide the location of attackers and the compromised machines, so that the paths to them are concealed. Also, the success of the reflector attacks and many of the basic DoS attacks require the use of spoofed IP addresses in the attack packets 
 4 Distinguishing the Attack Pack- ets 
In this section, we present our packet marking method which will help us to distinguish DDoS attack packets from packets sent by legitimate users. Though source IP addresses can be spoofed by attackers , the paths packets take to the destination are totally decided by the network topology and routers in the Internet , which are not controllable by the attackers. Therefore , the path of a packet has taken can really show the source of it. By recording the path information, the packets from different sources can be precisely differentiated, no matter what the IP addresses appeared in the packets. Packet marking, which is firstly proposed by 
Computing the Packet Marking
The mark made by a router would be a function of its IP address. To fit the 32-bit IP address A of a router into the ID field, we employ a hash function h that converts A to a 16-bit value. We adopt the CRC-16 hash function which is easy to compute and has low collision rate. Since attackers can easily know the routers' IP addresses , they can spoof the marking on a packet if they know the hash function used by each router. We cannot expect every router in the Internet to participate in the marking scheme and mark all packets passing through it. If a packet with such a spoofed marking passes through a route where there are no co-operating routers, this packet is impossible to be identified as an attack packet. To avoid such spoofing of the marking, each router R uses a 16-bit key K R (which is a random number chosen by the router) when computing its marking. The marking for a router R is calculated as M R = h(A) XOR K R , where A is the IP address of the router. After receiving a packet the router computes the marking M = M R ⊕ M old , if an old marking M old exists in that packet, and replaces M old with M. 
Inserting Order Information
One possible drawback with the scheme mentioned above is that the marking on a packet depends only on the routers it passes through, but not on the order passing them. This means that the packets which pass the same routers on two different paths have the same marking. To make the marking scheme more effective, we let each router perform a Cyclic Shift Left(CSL) operation on the old marking M old and compute the new marking as M = CSL(M old )⊕M R . In this way, the order of routers influences the final marking on a packet received by the firewall. 
Filtering Scheme
The MDADF scheme employs a firewall at each of the perimeter routers of the network to be protected and the firewall scans the marking field of all incoming packets to selectively filter-out the attack packets (see 
Learning Phase
To distinguish the spoofed packets, the firewall needs to keep a record of the genuine markings. During normal time that no attacks are happening, the firewall can learn about the correct markings for packets sent from specific IP addresses. The (IP-address, Marking) pairs are stored in a Filter 
Normal Filtering Procedure
After the learning phase, the firewall begins to perform its normal filtering operations. To the packet from an IP address recorded in the Filter Table, it is accepted if it has a consistent marking; otherwise, it is dropped. For the packet from a new IP address, we accept it with probability p and put the (IP-address, Marking) pair to a Check List, so that the marking can be verified. The value of p is set to high (close to 1) initially. When an attack is detected, the value of p is decreased according to the packet arrival rate and the victim's capability for handling the incoming traffic. 
Marking Verification
To verify the markings in the Check-List, a random echo message is sent periodically to the source address for each (IP-address, Marking) pair in the Check-List, and a counter is used to record the number of echo messages have been sent for it. To avoid the reply being imitated by the attacker, the content of the echo message is recorded in the Check-List and compared with the content of reply received. On receiving an echo reply from the source, the marking can be verified and the (IP-address, Marking) pair is moved to the Filter Table; otherwise, it indicates the previously received packet was spoofed, then this pair is deleted from the Check List. If the counter in the Check List shows that more than d(= 10) echo messages have been sent to an IP address x, then the entry for this IP address is removed from the Check List and the pair (x,φ) is added to the filter table, where φ is a special symbol denoting that all packets having source IP address x should be discarded. Since in this situation, this source IP must be either non-existent or inactive, so that the packets received with this source address are coming from the attacker and need to be rejected. 
Attack Detection
To detect the start of a DDoS attack, we use a counter called Total-Mismatches-Counter (T M C), which counts the number of packets whose marking cannot be matched at the firewall. This includes both packets with incorrect markings as well as packets from unknown source addresses that are not recorded in the Filter 
Route Change Consideration
Though routes on the Internet are relatively stable, they are not invariable. Once the route between two hosts has changed, the packet received by the destination will have a different marking with the one stored in the Filter 
Complete Filtering Scheme
Using the techniques and criteria introduced above, a complete filtering procedure is described below. Any packet received by the firewall is judged by the filter according to the following rules: 1) If the (IP-address, Marking) pair is same with one of the records in the Filter 4) If the TMC value exceeds the threshold, an attack is signaled. 
 5) All echo reply messages that are received as responses to the firewall's requests are handled by the Check List verification process. They are not passed through the filter. 
 In general, our MDADF scheme has the following func- tions: @BULLET Distinguish and filter out spoofed packets by checking the marking of each packet using the Filter 
Pushback Implementation
 By employing the filtering scheme, the firewall can protect the victim Web site by filtering out attack packets. However, sometimes the attack flow may be too large and the firewall may not have enough resources to handle it. In that case, we may employ the method of pushback 
In the Pushback method, the victim of a DDoS attack sends the signatures of attack to upstream routers and ask them to help filtering out these packets. Since one IP address can be used in the attack packets from many different sources, if we use the markings of spoofed packets as the attack signatures, large numbers of comparison need be done by the upstream routers. Instead, we create a list of IP addresses with their corresponding markings from the Filter Table and send this list (called the Pushback List) to the upstream routers. Whenever the firewall adds new entries or updates old entries in the Filter Table, these entries are sent as updates to the upstream routers, so that the Pushback List can be updated. The upstream routers compare each packet with the Pushback List after marking it and discard spoofed packets. Most of the attack packets are filtered before arriving at the victim, so that the victim Web site can continue with its normal operations. In some instances, the upstream routers of the victim still cannot deal with the attack flow, then they need to pushback further. To perform this function, each router R transforms all original markings 
M i (i = 0, 1, · · · , n) in the Pushback List by computing M i = CSR(M i ⊕ M R ), 
where CSR (Cyclic Shift Right) is the inverse of the CSL operation. The router then sends the new generated 
markings M i (i = 0, 1, · · · , n
) to its upstream routers. This process can be performed recursively until the attack flow is controlled. 
Experimental Results
We have evaluated the performance of the MDADF scheme under various parameter settings by simulating DDoS attacks of different magnitudes. 
Simulation of Internet Traffic
 In our simulation, we have used the topological data obtained from the Internet Mapping Project 
Parameter Selection
The choice of values for different parameters affects the performance results of the MDADF scheme. In our experiments , we have come up with some suitable values for these parameters by trail and error and we have tested the effects of changing the values of these parameters. The data-set used in the experiments contained 10,000 hosts and 50,000 intermediate routers. The size of the filter table was varied from 5000 to 10,000. The participation rate of routers was varied from 100% to 0%. For the parameter p, the most suitable value were found to 0.75 and 0.1 respectively for the pre-attack and post-attack scenarios . A learning phase of 10 minutes gave good results in our simulation setting. In the following, we discuss the results obtained during the experiments. 
Performance under Spoofed Attack
 6.4 Performance under Randomized At- tack 
We tested our scheme's performance under randomized attack in which attackers use randomly generated IP addresses . Figures 7, 8, and 9 show the variations in the packet acceptance ratio on changing the magnitude of attacks, using various Filter Table size, or using different values for p respectively. The results show that our scheme is effective even under attacks of high magnitude. The acceptance ratio of bad packets increases rapidly with the value of p, because most attack packets contain IP addresses that are new to the system. Good Packets Bad Packets 
Attack Detection Time
 Under our simulation setting, the MDADF scheme detected the occurrence of an attack in 3 -4 seconds in most conditions as shown in 
Marking Scheme
In the Pi scheme, the ID field of each packet is divided into 16/n parts, and each router inserts n bits of marking into the packet. Since the last several bits of IP addresses are usually clustered at a few numbers, such as 0 and 1, then the markings of different routers cannot be distinguished in this way. To avoid the iteration, a router computes the MD5 hash value of its IP address, which makes the value of its last n bits address distributed. Furthermore, to the packets coming from different upstream routers, a router uses different markings to distinguish them. Since the space is limited, not all markings of routers along the path can be put into, so that the later routers will overwrite the markings of previous routers. To ensure the packets from one origination will have the same marking and will not be affected by the attacker's manipulation, the marking is adjusted every time so that the oldest marking always appears in a fixed location. To keep the markings of different routers in a packet as many as possible, n can be 1 or 2 and the internal nodes of autonomous systems can omit marking, because the internal routes can be known from the local administration department. In the MDADF scheme, all the routers on the path of a packet contribute to the marking. Each router computes a 16-bit hash value of its IP address, with a secret 16- bit value, to construct its marking. Every time the old marking of a packet XORs with the router's marking to form the new marking. 
Filtering Scheme
To filter packets, the Pi scheme needs to be informed about the attack packet first, learns its marking, then begins to block packets with the same marking. Therefore, it needs to store the attack markings. When the number of attackers is large, lots of markings will be blocked, then it is quite possible for legitimate packets to have the same markings and be dropped. Therefore, to decrease the false positive, a threshold can be applied so that a packet will be dropped only when the percentage of attack packets having the same marking exceeds the threshold. In the MDADF scheme, the genuine (IP-address, marking ) pairs are stored. The packets with mis-matching markings are considered as spoofed packets from attackers and dropped. For the packets with source IP addresses not kept in the archives, they are accepted with certain probability. Furthermore, the markings of packets with unknown IP addresses will be verified through a marking verification process, as well as those stored IP addresses from which many packets with different markings have been received. Both of the two schemes filter packets basing on the marking of packets. They do not use the marking to find the source of attack, but use it to separate attack packets. Then, the victim do not need to wait until enough packets have been received to reconstruct the path as the traceback mechanisms do. Therefore, these two mechanisms have low overhead to both the routers and the victim. Both mechanisms can be combined with pushback. The Pi scheme filters packets basing on known attack markings. A disadvantage of it is that, if an attacker spoofs the marking or inserts random value and the marking is not overwritten by other routers, the victim cannot recognize the attack packet. While the MDADF filtering scheme use genuine markings of IP addresses, the packets with mis-matching markings are dropped. Even if no router marks the attack packet, the genuine marking cannot be spoofed by attackers in our marking scheme, because each router's marking is a function of its own secret key, which is not available to attackers. The Pi scheme cannot distinguish between legitimate and attack packets by itself, unless it knows the markings of attack packets from an external source. Therefore, this source must be reliable and capable to provide precise information. Accordingly, the MDADF scheme differentiates attack packets automatically by using the records of genuine (IP-address, marking) pairs. Route change is considered in the MDADF scheme. Since the path between two hosts in the Internet is not invariable, when we use recorded markings to differentiate packets, good packets will be classified falsely if their routes have changed. To reduce the false positive caused by the change, echo messages are used to test the genuine markings marked by routers at current time. 
Though a threshold is used in packet filtering process of Pi scheme, the false positive and false negative are inevitable and determined by the threshold value. If the value is low, many legitimate packets will be dropped; otherwise, lots of attack packets will come through, yet higher threshold is more suitable when the number of attackers is large. Again, the threshold is the percentage of attack packets versus the total number of packets having the same marking. In practical implementation, it is difficult to know a packet is actually an attack packet or legitimate one, which is the aim of most DDoS defense schemes. Therefore, it is hard to say a packet is from attacker while another having the same marking is from legitimate user. Moreover, if the victim can really get the percentage, he would have already had the ability to precisely differentiate attack packets, then he does not need to recur to the threshold to filter packets and lead to good packets be dropped mistakenly. 
Performance Comparison
The Pi scheme can accept at most 60% more good packets than bad ones when 100% routers participate the marking , while the advantage decreases to as low as 5% when the participation is 50%. In MDADF scheme, we get an acceptance ratio gap of 70% until the participation rate decreases to 20% and the gap is still bigger than 60% when only 10% routers cooperate (as in in 
The mainly differences between PI and MDADF schemes are listed in 
Conclusions and Discussion
In this paper, we have proposed a low-cost and efficient scheme called MDADF, for defending against DDoS attacks , The MDADF scheme is composed of two parts: marking process and filtering process. The marking process requires the participation of routers in the Internet to encode path information into packets. We suggest the use of a hash function and secret key to reduce collisions among packet-markings. The scheme also includes mechanisms for detecting and reporting DDoS in a timely man- ner. The evaluation of the scheme under simulations, show that our scheme can effectively and efficiently differentiate between good and bad packets under spoofed attack when the routers' participation rate is as low as 20%, so the deployment cost of our scheme is very low. Also, most good packets are accepted even under the most severe attack , whose traffic is about 10 times of normal traffic. At the same time, the bad packet acceptance ratio is maintained at a low level. Our scheme performs well even under massively distributed DoS attacks involving upto 5000 attackers. Under both spoofed and randomized DDoS attacks, the MDADF scheme detected the occurrence of attack precisely within 3 -4 seconds. The quick detection is valuable to the victim so that appropriate actions can be taken to minimize the damage caused by a DDoS attack. Must be informed by an external source at the first time, then records its marking to filter. Automatically, by using the Filter Ta- ble. 
Attack Detection Functionality None Yes 
Acceptance Ratio Gap 60% when all routers participate, and decreases to 5% when the participation rate is 50%. 70% when all routers participate, and holds even if the participation rate decreases to 20%. 
"
"Introduction
A ring signature scheme 
Analysis
 of the Abe-Ohkubo- Suzuki Instantiation in 
Let L = {(p i , q i , g i , y i )} 1≤i≤n . 
Let H : {0, 1} * → {0, 1} be a hash function viewed as a random oracle, where is larger than the largest |q i |, 1 ≤ i ≤ n. Sometimes, we pass in the set L for hashing and we implicitly assume that certain appropriate encoding method is applied. 
(Signature Generation) For message m ∈ {0, 1} * and the group defined by L, a signer k, 1 ≤ k ≤ n, who owns the private key x k , generates a signature σ = (c 1 , s 1 , · · · , c n , s n ) as follows. 
1) For i = 1, · · · , n, i = k, randomly select s i , c i ∈ R Z qi and compute z i = g si i y ci i mod p i . 
2) Randomly select r k ∈ R Z q k and compute 
z k = g r k k mod p k . 3) Find c k such that c 1 ⊕ · · · ⊕ c k ⊕ · · · ⊕ c n = H(L, m, z 1 , · · · , z n ). 4) Compute s k = r k − c k x k mod q k . 
(Signature Verification) For a triple of group, message and signature, (L, m, σ), it is valid if 
c 1 ⊕· · ·⊕c n = H(L, m, g s1 1 y c1 1 mod p 1 , · · · 
, g sn n y cn n mod p n ). 
Anonymity Attack
The attack is straightforward. The idea of the attack is to study the possible information leak from the variation of the length of c i corresponding to the size of the corresponding group order, for each i from 1 to n. Notice that if c k computed in Step 3 of signature generation above is greater than q k , then it can be sure that the user who owns x k must be the actual signer because all other c i 's are in their corresponding domains [0, q i − 1], for 1 ≤ i ≤ n, i = k. Suppose = |q max | + l where q max is the largest q i , 1 ≤ i ≤ n, and l > 0. As H is viewed as a random oracle mapping to numbers over the range [0, 2 − 1], the distribution of c k will be uniform over 
An Improving Approach
There are several ways to fix the problem. One of the approaches we are focusing on is to minimize the effect on the complexity of the algorithm. We retain the basic operations and procedure of the scheme and have all the c i 's in Step 1 of signature generation be chosen randomly from the same space, instead of from the corresponding Z qi 's. Let Ψ be the common space. One proposal is to set Ψ to {0, 1} . Hence in Step 1 of signature generation, c i ∈ R {0, 1} for i = 1, · · · , n, i = k. The rest of the scheme will remain unchanged. By assuming that H is a random oracle, the value of c k obtained in Step 3 of signature generation will also be uniformly distributed over {0, 1} . Signer anonymity can therefore be ensured. However, there could be more than one possible value of c i over the range [0, 2 − 1] 'committed' in Step 1 of signature generation by each z i . That is, for each i, 1 ≤ i ≤ n and i = k, z i = g si i y ri i mod p i for all possible values r i over 
(Forgery Algorithm) A signature σ = (r 1 , s 1 , · · · , r n , s n ) 
of (L, m) can be forged as follows. 
1) For i = 1, · · · , n, randomly select s i ∈ R Z qi and c i ∈ R {0, 1} . 
2) Compute c = H(L, m, g s1 1 y c1 1 mod p 1 , · · · , g sn n y cn n mod p n ) 3) Find r 1 , · · · , r n over [0, 2 −1] such that r 1 ⊕· · ·⊕r n = c and r i ≡ c i (mod q i ), 1 ≤ i ≤ n. 
If the number of values n exceeds the number of bits , it can be shown that, with high probability, there exists at least one subset of {r i } 1≤i≤n whose XOR is any desired bit target c. We can use linear algebra to find the subset. 
Let c = a 1 r 1 ⊕ · · · ⊕ a n r n where a i = 1/0, for 1 ≤ i ≤ n. Let a = (a 1 , · · · , a n ) 
be the binary vector we want to find. We call it a selection vector. Let the binary representation of c be (c 1 , · · · , c ). For each i = 1, · · · , n, let the binary representation of r i be (r i,1 , · · · , r i, ). We solve the following set of linear equations to find a if a solution exists. 
c i = a 1 r 1,i ⊕ · · · ⊕ a n r n,i , for i = 1, · · · , . 
However, our goal is to represent c as the XOR of all the values r 1 , · · · , r n , rather than as an XOR of a proper subset of these values. To overcome this problem, we use the fact above, that is, there are at least two r i over 
[0, 2 − 1] satisfying r i ≡ c i (mod q i )
, and give details of Step 3 of the forgery algorithm above in the following. 
3a) Set c = c ⊕ c 1 ⊕ · · · ⊕ c n . 3b) For i = 1, · · · , n, set r i = c i ⊕ (c i mod q i ). 
3c) Use the algorithm above to find the subset of {r i } 1≤i≤n whose XOR is c . Suppose the corresponding selection vector is 
a = (a 1 , · · · , a n ). 3d) For i = 1, · · · , n, if a i = 0, set r i = c i ; otherwise, set r i = c i mod q i . 
Hence if the number of group members n exceeds the number of bits , anyone can forge a signature with high probability without knowing any of the private keys. One solution to counteract this attack is to let grow with n. However, performance is the tradeoff. Another solution is to make sure that there is only one value in Ψ committed to each z i in Step 1 of the signature generation algorithm. We propose to set the common space Ψ to {0, 1} but change the value of such that 2 ≤ q min , where q min is the smallest q i , 1 ≤ i ≤ n. For security, the value of should still be sufficiently large to thwart the birthday attack against H. For example, we can set = |q min | − 1 and therefore Ψ = {0, 1, · · · , 2 − 1} when Ψ is considered as a set of integers. Hence the hash function H is now mapping from {0, 1} * to {0, 1} |qmin|−1 . In Step 1 of signature generation, we randomly select c i ∈ R Ψ, for 1 ≤ i ≤ n, i = k. In Step 3, c k must be in Ψ and will be uniformly distributed (assume H is a random oracle). In practice, we recommend the value of to be at least 160. In the special case of |q min | = 160, we can set = 160 and Ψ to Z qmin . But we also need to make sure that c k obtained in Step 3 is also in Z qmin . This may require multiple iterations from Step 1 to 3. The number of iterations is expected to be at most 2. In the following, we give a formal security analysis to our modification. 
Security Analysis
 The security of a ring signature has two aspects: unforgeability and signer anonymity. For unforgeability, we follow the definition proposed by Abe et al. 
In the definition above, the signing oracle SO takes as inputs any L ⊆ U, |L | = n , and any message m , produces a valid signature σ . As shown by Liu and Wong 
) 
 In the following, we show that our modification is existentially unforgeable against adaptive chosen message and public-key attacks, and also signer anonymous. 
Theorem 1 (Existential Unforgeability). Given a set U of N public keys, suppose A is a PPT algorithm which outputs a valid signature with non-negligible probability in security parameter ∈ N as defined in Def. 1, then there exists a PPT algorithm B which solves the DLP (Discrete Logarithm Problem) with non-negligible probability in . Proof. Let ∈ N be a security parameter. Given a forger A that takes N public keys and generates a valid message-signature pair (m, σ) where m ∈ {0, 1} * and 
σ = (c 1 , s 1 , · · · , c n , s n ), n ≤ N 
 , we construct an algorithm B that solves at least one of N discrete-log problem (DLP) instances: 
Q 1 , · · · , Q N where Q i ∈ Z * pi of order q i , 1 ≤ i ≤ N . For i = 1, · · · , N , B sets y i ← Q i . Let U = {(p 1 , q 1 , g 1 , y 1 ), · · · , (p N , q N , g N , y N )
}. Suppose the corresponding discrete logarithms (i.e. secrets) are x i , 1 ≤ i ≤ N . B simulates A's view by answering queries of random oracle H and the signing oracle. Note that in our modification, we require that 2 ≤ q min where q min is the smallest q i , 1 ≤ i ≤ N . For a H-query, B randomly picks c ∈ R {0, 1} and returns provided that the value has not been assigned. Otherwise, B repeats the process until a 'fresh' one is picked. Without loss of generality, we assume that A only submits distinct queries as previous replies can be cached. For a sign query of some n-element subset L of U and message m ∈ {0, 1} * , the answer is simulated as follows. 
For simplicity, let 
L = {(p 1 , q 1 , g 1 , y 1 ), · · · , (p n , q n , g n , y n )}. For i = 1, · · · , n, randomly pick s i ∈ R Z qi and c i ∈ R {0, 1} . Set the evaluation of H(L, m, g s1 1 y c1 1 mod p 1 , · · · , g sn n y cn n mod p n ) to c 1 ⊕· · ·⊕c n . If collision occurs, repeat this procedure . Otherwise, output (c 1 , s 1 , · · · , c n , s n ). 
 First note that A cannot distinguish between B's simulation and a real simulation, under the assumption that H is a random oracle. In one successful simulation, suppose the forgery of A is (
c 1 1 , s 1 1 , · · · , c 1 n , s 1 n ) 
 on some nelement subset L of U. By the assumption of random oracle model, A has a query H(L, m, z 1 , · · · , z n ) where z i = g si i y ci i mod p i , 1 ≤ i ≤ n. Suppose this is done at the ρ-th query of H and B returns c 1 . Since 
c 1 = c 1 1 ⊕ · · · ⊕ c 1 n and 
by the assumption of random oracle model, there is at least one c 1 i , 1 ≤ i ≤ n, that is determined after c 1 is returned by B. Otherwise, c 1 is pre-determined by {c i } 1≤i≤n before c 1 is returned by B on answering the ρth query of H, and this contradicts the assumption of the random oracle model. Hence by applying the technique of rewind simulation 
c 2 1 , s 2 1 , · · · , c 2 n , s 2 n ) and 
the answer for the ρ-th query of H is c 2 . Since c 2 = c 1 , then c 2 i = c 1 i for at least one value of i, 1 ≤ i ≤ n. Then B can obtain the secret x i by computing 
s 1 i −s 2 i c 2 i −c 1 i mod q i . 
On the signer anonymity of our modification, we show that all components in any valid signature σ = (c 1 , s 1 , · · · , c n , s n ) are uniformly distributed over their corresponding domains. 
Theorem 2 (Signer Anonymity). The modification of Abe-Ohkubo-Suzuki witness indistinguishable signature scheme described above is signer anonymous under the random oracle model. Proof. For any valid signature 
σ = (c 1 , s 1 , · · · , c n , s n ) 
on some arbitrary message m and any set L of n public keys 
(p i , q i , g i , y i ), such that 2 ≤ |q i |, for all 1 ≤ i ≤ n
, we first show that all components in the signature are uniformly distributed over their corresponding domains. Suppose the actual signer is indexed by π where 1 ≤ π ≤ n. For any i = π, c i is uniformly distributed over {0, 1} and s i is uniformly distributed over Z qi . Due to the random oracle assumption, given {c i } 1≤i≤n,i =π , the evaluation c of 
H(L, m, z 1 , · · · , z n ) 
is uniformly distributed over {0, 1} , for some appropriate values of z i , 1 ≤ i ≤ n. Therefore, c π is also uniformly distributed over {0, 1} . We remain to show that s π is also uniformly distributed over Z qi . Since s π = r − c π x π mod q i where r is randomly chosen from Z qi , x π is the private key corresponding to y π . We can see that given x π and c π , s π must be uniformly distributed over Z qi as r is. Therefore, if π is also randomly chosen from {1, · · · , n}, then the probability of finding the value of π is exactly 1/n. 
Performance
We note that the authors' full-paper version in 
g si i y 
CRHi(ci) i mod p i where CRH i : {0, 1} * → Z qi is a collision resistant hash function. s k is computed as 
r k − CRH k (c k ) · x k mod q k where CRH k : {0, 1} * → Z q k 
is a collision resistant hash function. In the worst case, it requires n additional hash functions for generating a ring signature with group size n. For implementation, these additional hash functions are to be considered as special system-wide functions. This makes their scheme less conventional and has the risk of losing spontaneity property of ring signature schemes. In addition, they make the implementation more complicated and the system less scalable . Although single hash operation can be carried out quite efficiently by common computing devices, the additional complexity introduced by these hash operations can still be significant when n becomes large. In addition, the signature size depends mainly on the value of |q max | as all the c i 's are of size at least |q max | bits long. Our solution proposed at the end of Section 3 above, on the other hand, is more scalable, much easier to implement , and more efficient with smaller signatures when comparing to the enhanced version above. Our solution does not require any additional hash functions. The complexity is essentially the same as the original Abe- Ohkubo-Suzuki instantiation. In addition, the signature size depends mainly on the value of |q min | as all the c i 's are of size at most |q min | bits long. Therefore, our solution yields shorter signatures in the general case. 
Conclusion
We point out that the Abe-Ohkubo-Suzuki Instantiation of witness-indistinguishable signature schemes in 
"
"Introduction
The Internet and its main application – the web – have been growing continuously in recent years. In the last three years, Web contents have doubled in size, from 10 billion to over 20 billion pages 
 2) How different are SSL certificates used by webfraudsters from those of legitimate domains? 
3) Can we use information in SSL certificates to identify web-fraud activities, such as phishing and typosquatting , without compromising user privacy? 
Roadmap
First, we measure the overall prevalence of HTTPS in popular and randomly sampled Internet domains. Next, we consider popularity of HTTPS in the context of webfraud by studying its use in phishing and typosquatting activities. Finally, we analyze, for the first time, all fields in SSL certificates and identify useful features and patterns that can help identify web-fraud. Leveraging our measurements, we propose a novel technique to identify web-fraud domains that use HTTPS. We construct a classifier that analyzes certificates of such domains . We validate our classifier by training and testing it over data collected from the Internet. The classifier achieves a detection accuracy ranging from 99%, in the worst-case, to 96%. It only relies on data stored in SSL certificates and does not require any user information. Our classifier is orthogonal to prior mitigation techniques and can be integrated with other methods (that do not rely on HTTPS), thus improving overall effectiveness and facilitating detection of a wider range of malicious domains . This might encourage legitimate websites that require sensitive user information (thus, potential phishing targets) to enforce HTTPS. Finally, we highligh some indirect benefits of HTTPS: it does not only guarantee confidentiality and authenticity , but can also help combat web-fraud. 
Paper Organization. The rest 
 of the paper is organized as follows. In Section 2, we briefly overview X.509 certificates. Section 3 presents the rationale and details of our measurements, as well as their analysis. In Section 4, we describe details of a novel classifier that detects malicious domains, based on information obtained from SSL certificates. Section 5 discusses implications of our findings and limitations of our solution. Section 6 overviews related work. Finally, we conclude in Section 7. 
X.509 Certificates
The term X.509 certificate usually refers to an IETF's PKIX Certificate and CRL Profile of the X.509 v3 certificate standard, as specified in RFC 5280 
Measurements and Analysis of SSL Certificates
 In this section, we describe our data sets and our collection methodology. We then present our analysis leading to the design of a classifier that detects web-fraud domains. 
 3.1 HTTPS Usage and Certificate Har- vest 
Our measurement data was collected during the following periods:  We include three types of domain sets: Popular, Random and Malicious (Phishing and Typosquatting). Each domain was probed twice. We probed each domain for web existence (by sending an HTTP request to port 80) and for HTTPS existence (by sending an HTTPS request to port 443). We harvested the SSL certificate when a domain responded to HTTPS. 
Popular Domains Data 
Set. The Alexa 
Random 
Malicious Data Set
 Phishing. We collected SSL certificates of 5, 175 different domains of phishing urls. The number of unique certificates is 2, 310 (
Typosquatting. 
In order to collect SSL certificates of typosquatting domains, we first identified the typo domains in our *.com/*.net random domains by using Google's typo correction service 
with another domain in this data set is 87%. In this paper, we refer to this data set as Typosquatting set. We acknowledge that the size of the Typosquatting data set is relatively limited. Therefore, we do not claim conclusive results from its analysis, but we include it for completeness. We believe that the limited size of the Typosquatting set is due to the lack of incentives from using HTTPS in this context. Using HTTPS in typosquatting domains does not help in luring users (unlike Phishing ). Nonetheless, we believe that, being the first of its kind, such an analysis of HTTPS-enabled typosquatting domains is interesting and shows an initial insight into this fraudulent activity. 
Certificate Analysis
 The goal of this analysis is to guide the design of our detection method, i.e., the classifier presented in Section 4. One side-benefit is to reveal differences between certificates used by fraudulent and legitimate/popular domains. In total, we identified 9 relevant certificate features, listed in 
Analysis of Certificate Boolean Features
Features F1-F4 have boolean values, e.g., F1 (md5) is true if the signature algorithm used in the certificate is "" md5WithRSAEncryption. "" The results of analyzing these features are summarized in 
F1 (md5) 
 9.9% of Alexa certificates use "" md5WithRSAEnc ryption "" , much less than those in *.com/*.net (27.4%), Phishing (17.4%) and Typosquatting (26.1%). Note that rogue certificates can be constructed using MD5 (
F2 (bogus subject) 
A bogus subject indicates whether the subject fields have some bogus values (e.g., "" ST=somestate "" , "" O=someorganization "" , "" CN=localhost "" , ...). We identified a list of such meaningless values and considered subjects to be bogus if they contain one of these  15.8% of Alexa certificates are self-signed, somehow unexpectedly. One possible explanation is that some of the popular domains in the Alexa set represent companies having their own CA and issuing their own certificates (e.g., google, microsoft, yahoo ...etc.). The percentages of self-signed certificates in *.com/*.net, Phishing, and Typosquatting is higher (resp., 35.4%, 30.6% and 53.5%). This is expected for Phishing, since miscreants would like to avoid leaving any trace by obtaining a certificate from a CA (which requires documentation and a payment). For *.com/*.net, a higher percentage could be explained by the use of locally generated certificates, e.g., using OpenSSL 
F4 (host-common-name-sim) 
We expect the common name in the subject field to be very similar to the hostname in popular domain certificates, while we intuitively expect this to be lower in malicious domain certificates, e.g., because malicious domains may not use complying SSL certificates . In order to assess this, we define a feature called "" host-common-name sim "" . This feature measures the similarity between domain name of the SSL certificate and common name of the subject field in it. For instance, if the hostname and common name are www.google.com.sa and google.com respectively, host-common-name-sim is set to true. Whereas, if the hostname and common name are equal to www.domain-x.com and www.domain-y.com, respectively , the feature is set to false since domain-x is not equal to domain-y. 74.3% of Alexa certificates satisfies this feature. The percentages in *.com/*.net, Phishing, and Typosquatting are 10.4% and 9.8%, 0% respectively. The difference between Alexa on one side and Phishing, Typosquatting , and *.com/*.net on the other side is quite remarkable. This feature, together with the previous one (excluding bogus subject), suggests that there is some strong similarity among *.com/*.net, Phishing, and Typosquatting certificates. 
 3.2.2 Analysis of Certificate Non-Boolean Fea- tures 
We now present the analysis of non-boolean features. F5– F6 are related to the certificate issuer: common name, organization name and country, while F7–F8 are related to the country issuer or country subject and F9 is related the validity duration. ) First, we noticed that some values of issuers' common names are popular in only one domain set. For example, 10.1% of Phishing certificates are issued by UTN-USERFIRST-HARDWARE (only 4.2(5.3)% in Alexa(*.com/*.net)). Similarly, some issuers' organization names are popular only in one domain set. For example, 9.6% of Phishing certificates have COMODO-CA-LIMITED as their issuer's organization name, as opposed to 2.1% in 
 To quantify the difference in issuer's common/organization name, we measure change in the posterior probabilities of phishing certificates, i.e., the probability that a certificate is phishing given the common/organization name is equal to a specific value. To this end, we merge Phishing and Alexa sets and observe how the posterior probability changes. Similarly, we merge Phishing and *.com/*.net sets. In order to measure the changes in the posterior probability, we borrow the metric in 
M ax Pp Displacement in 
I = { 1 − Pa if Ppin I > Pa Pa otherwise (1c) 
The Posterior Change Ratio measures relative change of posterior probability P p from the prior probability, P a . For instance, a value of 0 indicates no relative change and a value of 1 indicates the largest relative change. Note that the average is weighted, such that more "" weight "" is given to common names corresponding to more certificates (For more information, readers can refer to 
F6 and F7 (issuer and subject countries) 
 Similarly, some of issuers' country names are popular only in one data set. For example, 10.3% of Phishing certificates have GB as their issuer's country name (only 1.4% in Alexa and 4.3% in *.com/*.net). Additionally, some of the subject country names are popular only in the Phishing set. For example, 5.3% of Phishing certificates have FR as their country in the subject field. This happens only in 2.2% of Alexa certificates (0.9% in *.com/*.net). 
Summary of Certificate Feature Analysis
We now highlight the most important observations from our analysis: 
1) For most of the features, distributions of malicious certificates are significantly different from Alexa certificates . Therefore, popular domains can be easily differentiated from malicious domains based on their SSL certificates. 
 2) A self signed certificate is more likely to be for popular domain. The percentage of self signed certificates in popular domains is only 15.8%. 
3) We observe strong similarities between the *.com/*.net set and malicious sets in many features . One reason is that certificates in both sets may be issued without applying appropriate control, as opposed to certificates obtained from popular 
Certificate-based Classifier
 The analysis above shows that several features have distributions that vary among different data sets. Relying on a single feature to identify malicious certificates will yield a high rate of false positives. Therefore, we combine all features and feed them to a set of machine learning classifiers in order to differentiate among certificates belonging to different data sets. We use several machine-learningbased classification algorithms and select the one with the best performance. Specifically, we consider the following algorithms: Random Forest 
Classifier Features
Classifier Results
We first train the classifier on a data set that consists of Alexa and Phishing certificates. The purpose is to train the classifier to differentiate malicious from popular certificates . Performance results of different classifiers are shown in 
Classifier Results with Different Set Sizes
To verify how the data set size would affect the classifier performance, we use as the negative set the same Alexa we use in the previous section. But for the positive set, we use 3 Phishing sets of different sizes; namely, 500, 1000, and 2000. The ten-fold cross validation results of Decision Tree is shown in 
Classifier Results With Minimal Set of Features
To see how the classifier perform when we use a feature set that is less vulnerable to being manipulated, we restrict the feature set to only the sub-features related to the issuer; namely, issuer common name, issuer organization and issuer country. We use Alexa as our negative set and Phishing as our positive set. 
Discussion
Based on measurements presented in previous sections, we find that a significant percentage of well-known domains already use HTTPS. It is possible to harvest their certificates for our classification purpose, without requiring any modifications on the domains' side. Furthermore, the non-trivial portion of phishing websites utilizing HTTPS highlights the need to analyze and correlate information provided in their certificates. 
Using information in certificates. 
 Our results show significant differences between certificates of popular domains and those of malicious domains. Not only is this information alone sufficient to detect fraudulent activities as we have shown, but it is also a useful component in assessing a website's degree of trustworthiness, thus improving prior metrics , such as 
Keeping state of encountered certificates. 
We deliberately chose to conduct our measurements as general as possible, without relying on user navigation history or on user specific training data. These components are fundamental for most current mitigation techniques 
Limitations. 
 We acknowledge that additional data sets of legitimate domains need to be taken into consideration, e.g. popular websites from DNS logs in different organizations and countries. Data sets of typosquatting domains can be strengthened by additional and more effective name variations. Also, we acknowledge that our phishing classifier may incur false positives when actually deployed. However, this is a common problem to many machine-learning-based mitigation solutions (e.g., spam filtering and intrusion detection based on machine-learning techniques) and the number of false positives can be minimized by training the classifier on larger and more comprehensive data sets. Our classifier does not provide a complete standalone solution to the phishing threat since many domains do not have HTTPS. Instead, integrated with pre-existing solutions (e.g., 
How malicious domains will adapt. 
Web-fraudsters are diligent and quickly adapt to new security mechanisms and studies that threaten their business. We hope that this work will raise the bar and make it more difficult for web-fraudsters to deceive users. If web-browsers use our classifier to  alyze SSL certificate fields, we expect one of two responses from web-fraudsters: (1) to acquire legitimate certificates from CAs and leave a paper trail pointing to "" some person or organization "" which is connected to such malicious activities, (2) to craft certificates that have similar values to those that are most common in certificates of legitimate domains. Some fields/features will be easy to forge with legitimate values (e.g., country of issuer, country of subject , subject common and organization name, validity period, signature algorithm, serial number ...etc). For some other fields this will not be possible (issuer name, signature ...etc) because otherwise the verification of the certificate will fail. In either case the effectiveness of web-fraud will be reduced. Additionally , we show in Section 4.4 how the classifier still performs well when only relying on issuer related fea- tures. 
Related Work
The work in 
Conclusion
 In this paper, we study the prevalence of HTTPS in popular and legitimate domains as well as in the context of web-fraud, i.e., phishing and typosquatting. To the best of our knowledge, this is the first effort to analyze information in SSL certificates to profile domains and assess their degree of trustworthiness. We design and build a machine-learning-based classifier that identifies fraudulent domains that utilize HTTPS. The classifier solely relies on SSL certificates of such domains, thus preserving user privacy. Our work can be integrated with existing detection techniques to improve their effectiveness. Finally, we believe that our results may serve as a motivation to increase the adoption of HTTPS. We believe that aside from its intended benefits of confidentiality and authenticity , HTTPS can help identify web-fraud domains. 
"
"Introduction
With the cheap costs of communication, the convenience of network storage and network connectivity, and easier access such as the S3 provided by Amazon or AppEngine provided by Google, many small enterprises(data owners in our approach, simplified for DOs) resort to delegate their data storage and database management to one or more DSPs. Database as a Service , DaaS for short, caters to these requirements and allows enterprises to delegate their data management and data storage to DSPs so as to relieve them from excess costs of employing DBA professionals and associated hardware and software. Access control, an important security mechanism in traditional DBMS, allows different users to have different access privileges. However, the DaaS paradigm, different from traditional client-server architecture in which the server is trusted and responsible for designing and enforcing the access control policy, is challenged because the DSP himself/herself is untrusted and may be one of the internal attackers. Therefore in DaaS paradigm if the trusted data owner is responsible for filtering unauthorized access to delegated database at DSP for each user in client, he/she will become the communication and performance bottleneck. Therefore it is necessary to enforce selective authorization on delegated database by DSP, at the same time to guarantee the confidentiality of the delegated sensitive data. Different from the proposed approaches 
 2) In our approach, the DSP can implement the selective authorization enforcement management by using delegated access control authorization tables and the re-encryption module. 
3) Our approach is efficient. The users in client need little computation knowledge to derive keys for authorized tuples , but only need to use his own private key to decrypt all the authorized encrypted tuples filtered and returned directly by DSP. 
4) Our approach provides dynamic policy updating and managing. Whenever the granting or revoking operations take place, both the data owner and the DSP update the access control authorization tables by invoking the standard SQL updating statement. 
Paper Organization. 
The rest of this paper is organized as follows. In Section 2 we first describe the introduced DSP re-encryption based system architecture, and then Section 3 introduces our DSP re-encryption mechanism and the first level encryption completed by data owner. We describe the concepts of general access control and necessary authorization information in Section 4. Section 5 comes the DSP re-encryption based approaches. Then we demonstrate the dynamic policy updating in DSP re-encryption based architecture in Section 6. In Section 7 we perform the experiment evaluation. Section 8 shows the security analysis of our proposed approach. We show the related work in Section 9 and finally conclude the paper in Section 10. 
System Architecture
In this section, as shown in 
E i , E i ∈ − → E 
 , to perform the first level encryption on tuples in Source DB. The first level encryption indicates that the source database(Source DB in 
Database service provider. 
The database service provider(DSP) is usually a professional database company and is responsible for the query response, access control enforcement and regular maintenance issues. DSP needs to complete the following two tasks. @BULLET Maintaining ACAT. The first important task of DSP is to maintain the ACAT which includes at least two authorization tables in our approach. Both authorization tables can be updated flexibly and conveniently in term of the requirement of DO. The updating can be completed through the standard SQL statements. In order to avoid the disclosure of the rekey during the transmitting we use the public key of DSP to encrypt the value of column "" rekey "" in table "" user-re-key "" . @BULLET Performing the second level encryption. The second most important task of DSP is to complete the second level encryption on the encrypted tuples in Encrypted DB. The second level encryption is implemented by the component RE, abbreviated for Re-Encryption, which is used to enforce the selective access control for different legitimate users. DSP re-encrypts the authorized encrypted tuples by using the re-encryption keys in authorization table "" user-re-key "" and the RE component. The result value of re-encryption on encrypted tuple is called as reciphertext . The re-ciphertext can only be decrypted by using the legitimate user's private key corresponding to his/her re-encryption key. 
Data 
Requester. The Data Requester(DR) may be a PDA, PC, Mobile Phone, or any other electronic equipment. The DR needs to do the following two tasks. @BULLET Implementing the query transformation. Query transformation function in client is used to transform the submitted query of user into a privacy preserving query form by using the correct public keys and additional index information , such as the bucket-id for bucket index. The component to implement this function is omitted in our architecture because our emphasis is on the access control enforcement mechanism. @BULLET Performing the decryption. The second task of DR is to decrypt the re-ciphertexts from DSP and get the corresponding authorized plaintext tuples. Decryption is a polynomial algorithm D i from the algorithm sets 
− → D, D i ∈ − → D
, and is used to decrypt the re-ciphertext under his/her private key. 
The DSP Re-encryption Mechanism and the first Level Encryption
Figure 2 demonstrates the new introduced DSP re-encryption mechanism for flexible access control enforcement management and shows the information flows among the five components of DSP Re-Encryption mechanism. The DSP in our system is semi-trusted as that in the proposed approaches 
DSP Re-encryption Mechanism
Definition As is shown in 
E ∈ 1 E DO pk m RE user DO rek → c m D ∈ 1 
− → E , RE, REKG, PKG and − → D. 
The rekeys as the input of RE module, generated by the REKG and stored into an authorization table of ACAT, are used to enforce the selective authorization. The selective authorization enforced re-ciphertext rc m can only be decrypted by using the corresponding legitimate user's private key sk user , such as private key sk A for the legitimate user "" Alice "" . The subscript arrow from DO to user in rek DO→user only allows the reencryption from the DO to user, not from the user to DO. This is an one-way delegation. For convenience, in the following we assume that m is any plaintext data such as the tuple in Source DB or encryptionkey for one tuple. Suppose E 1 is the chosen standard first level encryption algorithm. By inputting pk DO and m, E 1 ∈ − → E outputs a ciphertext c m . The ciphertext is encrypted again by using the RE module in terms of the delegated ACAT to get the selective authorization enforced re-ciphertext rc m . The rc m can only be decrypted by the authorized user under his/her private key sk user and the standard decryption algorithm D 1 . Now we describe the function and usage of the components in 
@BULLET P KG, − → E , − → D. 
 They are the standard key pairs generation , encryption, and decryption algorithms respectively. 
− → E and − → D 
 are the sets of standard encryption and decryption algorithms. The PKG algorithm outputs a key pairs (pk user , sk user ) for the legitimate user by inputting the system security parameters 1 k . Generating the key pairs for user "" Alice "" can be expressed as: 
P KG(1 k ) → (pk A , sk A ) 
 The relationship between the first level encryption algorithm E 1 and the second level decryption algorithm D 1 for user "" Alice "" can be expressed as: 
E 1 (pk DO , m) → c m D 1 (sk A , rc m ) = m 
 where rc m is the encrypted value of c m by using the reencryption key and the RE module described in Section 5. @BULLET REKG. REKG is an algorithm in the DO and is used to generate the corresponding re-encryption key. rek DO→user is for the legitimate user by inputting the key pairs: 
(pk DO , sk DO , pk user , sk user ) or (pk DO , sk DO , pk user ) 
Suppose the user is "" Alice "" and "" A "" is representative of "" Alice "" , then the following is true: 
Alice : D 1 (sk A , rc m ) = m 
The information flow above can be explained simply as fol
First Level Encryption by DO
 In DaaS paradigm the DSP is viewed as untrusted for the privacy of delegated sensitive data. So the DO should transform the sensitive data in Source DB into the corresponding private form(the encrypted form) against the privacy disclosure. We introduce the formal definition for the transformation(first level encryption) and give the corresponding tables in 
Definition 3.2 The first level encryption E 
DO : E 1 (pk DO , ti) → c ti : DSP DO : E 1 (pk DO , randki) → c randki : DSP 
Example 3.2 
Access Control and Authorization Tables
Access control is an important security mechanism in DBMS. Recently, 
Access Matrix
Access matrix A is a conceptual model which specifies the rights that each subject(user or process) "" S "" possesses for each Figure 4: Access matrix object(data tuple in our context) "" O "" . This is defined by the server in the Client-server architecture where the server is believed as trusted. A should be defined by the data owner in DaaS scenario. For simplicity, we assume the database is read only. Therefore, the access matrix has the characteristics that there is a row for each S, a column for each O and a cell A[s,o]=1 if S can read O. If S can't read O, A[s,o]=0. Access control lists(ACL t ), are associated with a data tuple "" T "" , indicates that which subjects can access data tuple T. Capabilities lists(CAP s ) denotes that which objects the user S can access to. Taking the following as an example, assume there are five legitimate users in our system who want to access tuples of table in 
 Our approach is different from the three above, it takes advantage of the strengths of each while mitigating their drawbacks . In our approach the DO should delegate some additional authorization tables to the 
Authorization Tables
There are two authorization tables which need to be created, "" user-re-key "" and "" user-tuple "" . @BULLET user-re-key(userid, name, rekey). This table in 
DSP Re-encryption based Approach
In this section we first present the concept of access control enforcement management in DaaS, then give our proposed ap- proach. 
Access Control Enforcement Management
Definition @BULLET Verification phase: Verify whether the requesting user is a legitimate user or not. If he/she is a legitimate user, then continue the process in the next step. However, if the verification fails the process movies directly to the terminate phase. 
Access control enforcement management. The DSP can make full use of the delegated access control authorization tables in Figure 5 from ACAT and the re-encryption polynomial algorithm in RE to enforce the selective access to the authorized data tuples.
Terminate 
@BULLET Retrieving phase: 
Fetch the unique value of rekey from table user-re-key, which resides in ACAT, into a variable var-rekey on the condition that the userid equals to the requesting user's id req-userid. The statement for fetching rekey from table user-re-key is as follows: 
SELECT rekey INTO var-rekey FROM user-re-key WHERE userid=req-userid 
The values of attribute tid are then fetched into a record set rectid. This allows an authorized user to access many tuples in terms of his/her practical privileges. The statement for fetching tids of authorized tuples from  From the two computations above and the property of reencryption algorithm we know that rc 1 t3 is only decrypted by using the unique private key sk M for Mike and rc 2 t3 is only decrypted under the unique private key sk J for Jack. 
The PKE Approach
The PKE approach applies the concept of DSP re-encryption mechanism to the DaaS paradigm directly. Assume there is only one read only relation in 
Example 5.2 From user-tuple in 
Jone : D 1 (sk Jone , rc t1 ) → t1 = t1 Jone : D 1 (sk Jone , rc t5 ) → t5 = t5 
However, the PKE approach does have drawbacks. The speed of asymmetric encryption algorithm(RSA) is much more slower than that of the symmetric encryption algorithm(AES) when encrypting large mount of data 
OneEK Approach
 In the OneEK approach we introduce another pair of symmetric encryption(SE) and symmetric decryption(SD) algorithms, which are different from the asymmetric algorithms in − → E and − → D. A symmetric algorithm needs one identical share key between the sender and the receiver to encrypt the plaintext and decrypt the ciphertext respectively. The OneEK approach is different from the PKE approach in two phases. One is the Random key generation phase, the other is the Re-encryption phase. We demonstrate the OneEK approach from DO, DSP and DR in sequence as follows. The DO first randomly chooses a symmetric encryption key e, and then he/she uses e to encrypt all tuples in Emp in 
 1) It makes full use of the different merits of symmetric encryption algorithm and the asymmetric encryption algorithm to improve the speed of data encryption and decryption . The efficient combination not only can enforce the selective authorization but can prevent against man in the middle attack which will be analyzed in detail in Section8. 
 2) It is easy to combine this approach into the existing ap- proaches 
However, this approach also has drawbacks. The largest drawback is that the disclosure of the one data encryption key may lead to the whole encrypted database exposed to both internal and external attackers. This is unacceptable, therefore we propose MultiEK approach to resolve this issue in the following section. One difference is in the first level encryption where the MultiEK approach is to generate one random encryption key randki for each tuple ti, not a random key e for all tuples. Although the encryption keys are as many as the number of tuples in Source DB, the key distribution isn't needed in our approach. We only need to add a new column ekey to the corresponding encrypted table, such as the encrypted table Encrypted-Emp in 
The MultiEK Approach
rithm E 1 . DO : SE(randki, ti) → c ti : DSP DO : E 1 (pk DO , randki) → c randki : DSP 
The DSP does the access control enforcement process as that in the OneEK approach, except for doing the following extra operation for the delegated encrypted c randki . 
DSP : RE(rek DO→user , c randki ) → rc randki : user 
Our approach not only can take advantage of the OneEK approach, but also can avoid the MultiEK approach struggle with data security when an encryption key is compromised. It also has the advantage of not much more key distribution, even if all the encryption keys are different from each other. In conclusion, our system can work efficiently as that in the client-server environment in which the server can take over almost all computation. Using our new architecture allows the DR to be any light DR, such as PDA or Mobilephone. 
Dynamic Policy Updating
There are three main kind of policy updating operations. These are composed of inserting or deleting a user, inserting or deleting a resource, and granting or revoking an authorization. Different from 
Inserting or Deleting A User
 When inserting a new user u, the DO first generates the reencryption key rek DO→u for u by using the REKG algorithm. He/she then inserts a tuple about u and the corresponding rek DO→u into the authorization table user-re-key through using the standard inserting SQL statement. 
Example 6.1 Assume the new user is Tutu, 100806, then DO does the following to generate the 
Inserting or Deleting A Resource
In our approach we assume the resources are tuples. However, they can be easily extended to the object resources, such as the tables or views. When inserting a tuple the DO can encrypt the tuple and send the encrypted tuple to the DSP without needing to update the policy, that is to say needn't to update the authorization tables. When deleting a tuple, the DO only needs to request the DSP to delete all tuples in the authorization table user-tuple on the condition that the tid equals to the designated tuple tid, and then does the same for the authorization table user-tuple in his/her own. 
Example 6.3 Assume the tuple deleted is t5, then DO and DSP do the following: 
DELETE FROM user-tuple WHERE tid=t5 
Granting or Revoking An Authorization
Given the users and the tuples, any granting and revoking in our approach only require the DO to ask the DSP to insert or delete the tuple from the corresponding authorization tables. This is done from table user-tuple on the condition that the tid value in the tuple equals to the designated tuple tid value, and the DO does the same operation on the table user-tuple of his/her own. 
Experiment Evaluation
The first goal of our experiment is to demonstrate the PKE approach, which applies the DSP RE-encryption mechanism into the DaaS paradigm directly. We intend to show this is not practical because the decryption of asymmetric algorithm(RSA ) spends much more time than that of two times decryption of symmetric algorithm(DES). When compared to the approaches proposed by 
Man in the Middle Security
In proposed approaches 
Possible Disclosure
There must exist the possibility of the collusion between two entities such as one user and the DSP or the collusion of different users. If the collusion takes place in two different users, only the authorized tuples for these two users are disclosed . However, if the DSP colludes with a user, the collusion user may access to all the unauthorized tuples as long as the DSP re-encrypts each tuple by using the re-encryption key of the collusion user. Those disclosures are also true in 
@BULLET 
Related Work
In 2002 Hacigumus et al
Access control in Database is an important mechanism for implementing security and data privacy. There are many access control models to apply to different data management sys- tems. 
Conclusions
 With the computation capability improving greatly and the efficiency of scale economy, DaaS paradigm is adopted by var-ious DOs. This includes medium or small enterprises, as well as individual users. However, there exists the potential security problems which must be resolved before its practical application . In this paper we address the problem of enforcing access control by DSP to make the system more usable by introducing new DSP re-encryption based approaches. Our approach efficiently combines a new DSP re-encryption mechanism with access control policy of DO in DaaS scenario. Moreover, the DSP re-encryption based architecture still satisfies the secure performance of the confidentiality and can reduce the computation complexity of the DR. At the same time our approach can eliminate the public catalog of tokens, but use some authorization tables. The PKG can be flexibly managed by a secure certificated authorization center or the DO himself/herself. The subsequent research under this new architecture is on how to design the efficient query transformation in DR. The accessory mechanism such as the integrity and query guarantee by DSP and the efficient implementation of role-based and the secret share based DSP re-encryption mechanisms. 
"
"Introduction
Session Initiation Protocol (SIP) 
Registration Phase
When a user wants to register and become a new legal user, the person must first submit his or her username and password to remote server. The username and password is used to verify the identity of the user and server. When the server receives this user's username and password, the server stores this username and password. 
Authentication Phase
If a legal user wants to login in system, this user must type his or her username and password. All steps of authentication phase execute as following. When the server receives the user's messages, the server uses username to get the user's pw from its database. The server uses pw to compute h(pw)⊕t 1 ⊕ h(pw) to get t 1 . Furthermore, the server generate a random number r 2 and use t 1 , r 2 , g, p, pw to compute 
t 2 ⊕ h(pw), K = t r 2 1 mod p, andh(t 1 , K). Then the server sends Challenge (reakm, t 2 ⊕ h(pw), h(t 1 , K)) 
to the user. 
 Step 4: When the server receives the Response message , the server uses username, realm, K to compute h(username, realm, K). If the computed h(username, realm, K) is the same as the 
 2.2 Review Durlanik et al.'s Authentication Scheme 
In this section, we review Durlanik et al.'s authentication scheme. This authentication scheme is based on Elliptic Curve Diffie-Hellman (ECDH). 
Registration Phase
In Durlanik et al.'s authentication scheme, both the server and the user have a pre-shared password for authentication and an elliptic curve public key pair. 
Authentication Phase
If a legal user wants to login in system, this user must type his or her username and password. All steps of authentication phase execute as following. 
Step 1: U −→ S: Request(username, 
d c G ⊕ h(pw)). 
 The user sends a Request message including its username and its public key xor by its hashed password. 
Step 2: S −→ U : Challenge(realm, d s G ⊕ h(P W ), h(d c G, d c d s G). 
When the server receives the Request message, the server computes d c G ⊕ h(P W ) ⊕ h(P W ) to obtain d c G. Then, the server computes a session key 
K = d s d c G, d s G ⊕ h(P W ), and h(d c G, d c d s G). Finally, the server send Challenge(realm, d s G ⊕ h(P W ), h(d c G, d c d s G) 
message to the user. 
Step 3: When the user receives the challenge messages, this user computes 
 d s G ⊕ h(P W ) ⊕ h(P W ) to obtain d s G. Then, the user computes h(d c G, d c d s G) to verify received h(d c G, d c d s G). 
If they are equal, the user computes a session key 
K = d s d c G. 
Step 4: U −→ S: Response(realm, username, h(username, realm, K)). 
The user computes h(username, realm, K). Then, the user sends 
Step 5: When the server receives 
Our Proposed Authentication Scheme
In this section, we reviewed our authentication scheme. Before we review our proposed authentication scheme for Session Initiation Protocol, we define the symbols first. 
Registration Scheme
When a user wants to register and become a new legal user, this user must first submit his/her username and password to remote server. The username and password P W is used to verify the identity of the user and server. When the server receives this user's username and password , the server stores the user's username and password P W . 
Authentication Scheme
If a legal user wants to login in system, he/she must type his or her username and password. All steps of authentication phase execute as following. 
Step 1: U −→ S: Request(username, N c ). The user generates a random number N c and sends Request(username, N c ). 
Step 2: S −→ U : Challenge(realm, N s ⊕ h(pw||N c ), h(pw||N s ||N c )). 
When the server receives the Request message, the server generates a random N s and uses N s , N c , pw 
to compute N s ⊕ h(pw||N c ). Then, the server uses pw, N s , N c to compute h(pw||N s ||N c ) and sends Challenge(realm, N s ⊕ h(pw||N c ), h(pw||N s ||N c )
) to the user. Step 3: U −→ S: Response(username, realm, 
h(N s ||pw||N c )). 
When the user receives the Response message, this user uses N c , pw to compute h(), the server rejects the user request. Otherwise, the server accepts the connec- tion. 
pw||N c ) and uses h(pw||N c ), N s ⊕ h(pw||N c ) to compute h(pw||N c ) ⊕ N s ⊕ h(pw||N c ) to get N s . Then, the user uses pw, N s , N c to compute h(pw||N s ||N c ). 
 Step 5: After the server and the remote user authenticate each other, they use N s as a session key SK = N s. 
Security Analysis
 Below we can examine whether the communication agreement is safety, we will examine our authentication scheme on various known attacks. 
Replay Attack
Our authentication scheme uses the random nonce to withstand the replay attack. Authentication messages 
N s ⊕ h(pw||N c ), h(pw||N s ||N c ), h(N s ||pw||N c ) 
 are generated by random nonce N c and the random nonce N s . The random nonce N c and the random nonce N s are generated independently, and both values will deficient in each session . Assume an adversary wants to enter the system. This adversary can not enter the system by re-sending messages ever transmitted by a legal user, because the random nonce N s is different. 
Password Guess Attack
 In our authentication scheme, it is impossible for an adversary to guess the user's password. The password of a user was protected by the random number Nc and the random number N s . The random number N s is generated by the server and the random number N c is generated by the user. They are different in the next session, so our authentication scheme can withstand password guess at- tack. 
Server Spoofing Attack
In our authentication scheme, we will obviously ask the user to know whether the server is the correct one before authenticating the user. Therefore, if an attacker wants to masquerade as the server to cheat the user, this attacker must have the user password P W . If someone is discovered to masquerade as the server to cheat the user, the user will disconnect all following transmissions. Hence, any server spoofing attack will fails. 
Impersonation Attack
In our scheme, an attacker can not masquerade as a legal user. To successfully perform the impersonation attack, the attacker must require the knowledge of P W to generate and interpret authentication messages correctly, because all the authentication messages between the server and the user are protected by pw. pw is memorized by the user. If someone is discovered to masquerade as the legal user to cheat the server, the server will disconnect all following transmissions. Hence, any impersonation attack will fails. 
Security of Session Key
A session key is generated by user's password P W and a random number. Whenever the communication ends between the user and the server, the key will self destruct immediately and will not be reused at the next time. When the user reenters the system, a new session key will be generated to encrypt the information during the communication process. Therefore assuming the attacking has obtained a session key, the person will not be able to use the session key to decode the information in other communication processes. Because the random nonce N s and random nonce N c is generated randomly, it will not be able to use a known session key to calculate the value of the next session key. 
 5 Compare with other Authentication Scheme 
 In this section, we compare our nonce-based authentication scheme with Yang et al.'s authentication scheme 
"
"Introduction
Public key cryptosystems based on hard mathematical problems are well approved for information authentication with digital signatures. An important practical problem is developing digital signature schemes (DSSes) with short signature length 
Notation: 
G m denotes multiplicative group (Z/mZ) * ; ω m (x) denotes the order of the element x ∈ G m ; |x| denotes the length of binary representation of the value x; 
(xy) denotes the concatenation of the values x and y; gcd(x, y) denotes the greatest common divisor of the values x and y; lcm 
Algorithms
 Based on a New Signature Formation Mechanism 
 2.1 Method for Reduction of the Signature Length 
The well known cryptosystem RSA 
α = h ϕ(n)/δ mod n ≡ (h (q−1) ) (r−1)/δ mod n ⇒ α ≡ (h (q−1) ) (r−1)/δ ≡ 1 (r−1)/δ ≡ 1 mod q ⇒ α − 1 ≡ 0 mod q ⇒ q|α − 1 ⇒ gcd(α − 1, n) = q. 
Thus, in the considered case it is possible to factorize modulus using Euclidean Algorithm. Therefore some restrictions should be imposed on generation of the public key. A way preventing the described factorization method is to use such numbers r and q that both of them contain the same required large divisor δ, the δ 2 value dividing neither r−1 nor q−1. If this additional requirement is imposed , then the α parameter can be generated as follows: α = h L(n)/δ mod n = 1, where L(n) = lcm [r − 1, q − 1] is generalized Euler's function. Thus, α = h uv mod n = 1, where u = (r − 1)/δ and v = (q − 1)/δ. If we use, while generating the α value, a value that is simultaneously primitive element modulo r and primitive element modulo q as the number h (i.e. h is a "" double "" primitive element ), then we will have simultaneously α ≡ 1 mod q and α ≡ 1 mod r. While using a "" double "" primitive element we deterministically generate a "" strong "" α value. But it is not strictly necessary to use "" double "" primitive elements. We can generate a "" strong "" value α selecting random values h. In this case we should check if α ≡ 1 mod q and α ≡ 1 mod r holds. The second way to generate "" strong "" public key is to use composite value δ, i. e. δ = δ δ , where δ |r − 1 and δ |q − 1 and δ and δ do not divide q − 1 and r − 1, correspondingly. For generating the parameter α we have the following formula: α = h L(n)/δ mod n = 1, i. e α = h uv mod n = 1, where u = (r − 1)/δ and v = (r − 1)/δ . Analogously to the first case, while using the value h that is "" double "" primitive element, we get α ≡ 1 mod q and α ≡ 1 mod r. Thus, we have two different ways to define difficulty of the n modulus factorization in the considered DSS. Unfortunately in the first way we have a problem to avoid possibility to calculate the secret parameter δ without factorizing the n modulus. Indeed, we have: 
n − 1 = (uδ + 1)(vδ + 1) − 1 = uvδ 2 + uδ + vδ = (uvδ + u + v)δ. 
 Usually the value n− 1 can be easily factorized. Therefore the secret δ can be recovered, if no new restriction requirements are imposed on selection of the n modulus. In the second way factorization of the value n−1 does not allows one to determine the δ secret. Thus, we should use the second way while generating the public key. To choose the size of the δ value we should take into account that the α value can be used to factorize the modulus n with the help of the calculation of the value gcd(α i mod n − 1, n) for i = 1, 2, . . . min{δ , δ }. Therefore we should use the 80-bit values δ and δ . Thus, we get the length of the value δ should be |δ| ≈ |δ | + |δ | ≥ 160 bit. 
Digital Signature Schemes
A secure variant of the DSS with the 320-bit signature length is described by the following verification Equation: 
k − g = F H (M α k+g(v−1) mod n), 
(1) 
where v is a specified 80-bit prime number and M is a message. The signature generation is performed as fol- lows: 
1) Generate a random number U and calculate H = F H (M α U mod n); 
 2) Solve simultaneously Equation k − g = H and 
gruence k + g(v − 1) ≡ U mod δ. 
The solution gives the k and g signature elements: 
g = U − H v mod δ and k = H + g. 
(2) 
The signature size is |k| + |g| ≈ |H| + |δ| ≈ 320 bits in the case of the 160-bit hash function. Note that without using the prime v the signature scheme is not secure, since in such case the secret δ is not used to calculate signature, if we have U > H. One can simplify the verification Equation and present the following modified DSS: k = F H (M α kg mod n). 
(3) 
In this case the signature is calculated using the formulas: 
k = F H (M α U mod n) and g = U k mod δ. 
(4) 
Note that, while generating the signature, the events corresponding to the case gcd(δ, k) > 1 have negligible probability (if one of such cases takes place, then the signer should repeat the signature generation procedure using another value U ). If k|U , then the value g is calculated without using the secret value δ. However such events have also negligible probability due to sufficiently large value |k| = |H| = 160 bits (we assume that hash function SHA-1 is used). 
Security Discussion
The assumptions underlying the schemes described above are the following two: 1) Problem to factorize n using the public key (n, α) is difficult. 
2) Finding discrete logarithm x = log α y modulo n is difficult. 
Solving one of these two hard problems allows one to determine the secrete key. For the first problem it is evident . For the second problem we should consider the following attack. Select two 160-bit numbers k and g and calculate the value y = α kg mod n. Finding discrete logarithm x = log α y (modn) and factorizing the value kg − x one can get the secret value δ. For the known value δ = δ δ the difficulty of finding discrete logarithm x = log α y (modn) can be estimated as √ z exponentiation operations, where z = max{δ , δ }. However for unknown value ω n (α) the difficulty of the second problem is approximately equal to √ δ δ operations. Let us note that difficulty of the second problem depends on the difficulty of the first one. Indeed, solving the first problem reduces the second problem to the problem of finding logarithms modulo q and modulo r 
Signature Schemes with Prime δ
The Used Function
Let us consider the main trick that provides to use prime values of secrete number δ. Suppose we would like to provide to a verifier possibility to check if a is congruent to b modulo q, the q value being a secret one. It is possible to be done using very simple mechanism avoiding direct calculations modulo q. Really, let p = 2n + 1 is a prime, where n is the product of two large primes q and r, i. e. n = qr, and β is an element in G p for which we have ω p (β) = q. Then we have 
β a ≡ β b mod p ⇔ a ≡ b mod q. 
 Thus, one can check validity of the last congruence performing calculations modulo p, i. e. without use of the secrete value q. In the DSS design we use the following function: 
y = β α x mod n mod p, where p = 2n + 1, n = qr, β ∈ G p , ω p (β) = q
, and the value α ∈ G q is such that ω n (α) = δ(r −1) and ω q (α) = δ, where δ is a prime such that δ|q − 1 and δ |r − 1. 
Signature Scheme
 Using this mechanism we modify the verification Equations (1) 
and 
(2), respectively, in the following way: 
k − g = F H (M β α k+g(v−1) mod n mod p), 
(5) 
k = F H (M β α kg mod n mod p), 
(6) 
where the public key is (p, β, α) and the secret key is (q, δ). We suppose the following minimum length of the key values: |δ| = 160 bits, |q| = 512 bits. In DSSes described by verification Equations (5) and (6) the g value in the signature is an element of the prime order group G δ : g ∈ G δ . In the modified DSS described by Equation (5) the signature elements g and k are defined by Formula (2), with exception that now we have Z = F H (M β α U mod q mod p). In the modified DSS described by Equation (6) the signature element k is calculated as follows: 
k = F H (M β α U mod q mod p). 
The second signature element is defined by Formula (4). 
Statement 1. The signature verification Equations 
(5) and (6) work correctly. Proof. For Equations (5) and (6) the proof is analogous. Let us consider the case of the verification Equation (6). Let (k, g) be a valid signature corresponding to some signed message M . Taking into account that 
ω q (α) = δ, ω p (β) = q, and g = U/k mod δ, from 
the verification Equation we get: 
k = F H (M β α kg mod n mod p) = F H (M β α kg mod q mod p) = F H (M β α k U k mod q mod p) = k. 
Since k = k the signature verification result is positive, i.e. the verification Equation (6) works correctly. Let us consider the case of the verification Equation (5). For some valid signature (k, g) for the message M we have (see Formulas (2)): 
k − g = H, where H = F H (M β α U mod q mod p) = F H (M β α U mod n mod p) and H = F H (M β α k+g(v−1) mod n mod p) = F H (M β α H+g+g(v−1) mod n mod p) = F H (M β α U mod n mod p) = H. 
 Since H = H the verification Equation (5) works cor- rectly. 
Security Discussion
In the DSSes proposed in Section 3.2 the β value should satisfy the following security requirement: the ω n (α) value should be sufficiently large, i.e. |ω n (α)| > 160 bits. This requirement is defined by the following possible attack against the Scheme (6) (analogous attack is possible against Scheme (5)): 
1) Calculate the value y = α kg mod n; 
2) Find the value x = log α y (modn); 
3) Calculate δ as one of divisors of the value kg − x. If ω n (α) > 160 bits (this condition is satisfied in the considered case, since we have ω n (α) = δ(r − 1) ), then Step 2) is computationally infeasible. Note that until modulus n is not factorized the divisors of ω n (α) are not known. We also suppose to follow the recommendations by 
Statement 2. An attack providing calculation 
of the secret value δ is as difficult as factorizing the modulus n is difficult. Proof. Suppose an attack allows to calculate δ. Since α δ ≡ 1 mod q, then (α δ mod n) ≡ 1 mod q ⇔ q|α δ mod n − 1 ⇔ gcd(α δ mod n − 1, n) = q. Thus, difficulty of factorizing the n modulus is comparable with difficulty of the considered attacks. 
In the DSSes proposed in Section 3.2 the hash function is calculated after the value R = β α kg mod q mod p is computed . Therefore the forger is forced to choose the value U before the determination of the hash value (we use the standard assumption the used hash function is secure, i. e. the function F H possesses no special properties that the forger can take advantage of), like in the case of Snorr's DSS (for details see pp. 25-26 in 
Security Estimation
 In this Subsection we estimate security of the DSS described by Formula (5) and (6) as complexity of the best known Algorithms providing calculation of the δ value given the function y = β α x mod n mod p. Due to "" double exponentiation "" construction of this function the methods based on calculating discrete logarithms are not efficient relatively required time and storage. The following Algorithm implements a more efficient method. 
Algorithm 1 
1: Select random U > 2 |δ|+10 and calculate y = β α U mod n mod p. 2: For i = 0 to N , where integer N ≈ √ δ, calculate z (i) = β α iN mod n mod p. 3: Order the Table of pairs (i, z (i)) according to the z (i) value and set j = 0. 4: Calculate z (j) = y 1 α j mod n mod p. 5: Check if in the Table there exists z (i 0 ) such that z (i 0 ) = z (j). If z (j) = z (i) for i = 0 to N , then increment the j counter j ← j + 1 and go to Step 4. 6: Calculate values U = i 0 N +j and factorize the U −U value. 7: Select divisor δ such that 
β α δ mod n mod p = β. 
The Algorithm computes the value U = i 0 N + j 0 such that y = β α U mod n mod p. Indeed, suppose for i 0 and j 0 we have 
y 1 α j 0 mod n mod p = β α i 0 N mod n mod p. Then y ≡ β α i 0 N +j 0 mod n mod p ⇒ β α U mod n ≡ β α U mod n mod p ⇒ β α U mod q ≡ β α U mod q mod p ⇒ α U ≡ α U mod q. 
Therefore U ≡ U mod δ ⇒ δ|U − U . 
Difficulty of Step 2 is about 2N = 2 √ δ exponentiation operations. Difficulty of Step 3 is about N log 2 N comparison operations performed on |p|-bit values. Difficulty of Steps 4 and 5 is about √ δ exponentiations plus 2 −1 N log 2 N comparison operations. Difficulty of Steps 1, 6, 7 and difficulty of all comparison operations are negligible in comparison with difficulty of all exponentiation operations . In total the difficulty of Algorithm 1 is W ≈ 3 √ δ exponentiation operations. For |δ| ≈ 160 bits we have W ≈ 2 81 exponentiation operations. The Algorithm requires very lage storage ≈ 2 90 bits for ≈ 2 80 |p|-bit numbers . Algorithm 1 is efficient on time, but not efficient on storage. Minimum storage requirement and efficient on time computations are achieved with the Floyd's Algo- rithm 
x i+1 = β α x i mod n mod p, 
where x i is selected arbitrarily. Such random sequence represents a a non-periodic part (tail) followed by the periodic rest (multiple repetition of some cycle). The average length of the tail is λ = πδ/8. The average length of the cycle is µ = 3πδ/8 
Conclusion
Using a novel mechanism in the DSS based on difficulty of factorization problem we have reduced the signature size in such schemes to 320 bits in the case of the minimum security level (2 80 operations). To obtain possibility to use prime value of the secrete key element δ we have proposed the DSSes defined by the "" three-level "" verification Equations, combining calculations in four different groups G δ , G q , G n , and G p . In such schemes, while generating signature, calculations are performed in three groups G δ , G q , and G p . While verifying the signature, the calculations are performed in two groups G n , and G p . In the streighfoward implementation of the DSSes based on "" three-level "" verification Equations with the modulus p = 2rq + 1 the public key generation procedure is sufficiently more complex than in the DSA standard and in RSA, however it can be essentially simplified using the p modulus with the structure p = erq + 1, where e is the t-bit even number, t = 10 to 16 bits. In this case for the selected primes q and r one can choose with high probability e such that p is prime. This variant provides sufficient computational efficiency of the key generation procedure. The DSSes described by verification Equations (1) and 
(3) use composite element δ, however they have simpler design and are faster while both the signature generation and the signature verification. Schemes (1) and (3) appear to be more interesting for practical applications. The computational efficiency of the signature generation in these DSSes is about the same as in the DSA standard . Complexity of the signature verification procedure is about two times lower than in DSA. For the proposed DSSes we have provided formal security evidence. Due to using a novel signature formation mechanism we have succeeded to develop DSSes having comprehensible design and providing at present the shortest signature size for the DSSes based on factorization problem. 
"
"Introduction
Cloud computing has become the most popular issue in recent years. More and more cloud services have bloomed all around the world such as storage space outsourcing, computing resource and many kinds of software. Since variant cloud services have been used, millions of messages have been transferred in the public network and the relative security issues have arisen including privacy, security , resource abusing, and so on. Therefore, people usually use an extra safeguard before adopting cloud services. For example, when users wish to store the documents in the cloud storage space, they usually encrypt the documents before uploading them. After encrypting, the documents have become sequential characters which cannot be recognized. But how users can search the encrypted data that they want? In 2000, Song et al. 
E A pub (M ), P EKS(A pub , w 1 ), . . . , P EKS(A pub , w m ) 
where A pub is Alice's public key, 
(w 1 , w 2 , . . . , w m ) 
denotes the keywords that Bob sets. When Alice wishes to read the emails that contain a specific keyword W, Alice sends a "" trapdoor "" to the mail server. The mail server should identify the corresponding encrypted emails without learning any information and route these emails to Alice. In a like manner, Alice wants to retrieve the encrypted emails which are associated to "" urgent "" , "" Monday , and "" Marketing "" , the initial security model of PEKS cannot achieve this work since the user can only use one keyword to search the encrypted emails. However, searching on a large amount of data with more than one keyword can shrink the searching scope and improve the querying performance. Therefore, Golle, Staddon and Waters 
In 2005, Park, Kim and Lee 
Requirements
To construct a secure conjunctive keyword searchable scheme, there are some security requirements needed to achieve as follows: 
Unforgeable of the trapdoor 
User authentication: After encrypting, no information can be derived from the ciphertexts and the trapdoors , but the server still has to recognize whether the users who send the trapdoor are the authorized users. This requirement means that the server must has the ability to authenticate the user's identities 
Efficiency: Most of the existing schemes need a large amount of computing time or produce long ciphertexts and trapdoors which are inefficient for users. This requirement means that the proposed scheme should be processed efficently in the reality. 
Against off-line keyword-guessing attack: All the messages in the keyword searchable scheme are transferred via a public network and easy to eavesdrop . Not only the outside adversaries, the malicious servers are also regarded as the inside attackers. This requirement means that the proposed security model should stand against outside and inside off-line keyword-guessing attacks 
Organization
 This paper is organized as follows: In Section 2, we introduce the development of conjunctive keyword searchable schemes and analyze their advantages and shortcomings. We further evaluate whether the schemes in Section 3 conform the requirements mentioned above, and make a performance comparison. Finally, we discuss future researches in Section 4 and conclude in Section 5. 
Security Model for Conjunctive Keyword Searchable Scheme
The notion of secret key encryption with conjunctive field keyword search scheme was proposed by Golle, Staddon and Waters 
 @BULLET The same keyword never appears in two different keyword fields. It other words, all the keywords in one document are different from each other. 
@BULLET Every keyword field is defined for every document. That is, every keyword field should assign a keyword even if one field is empty, we should assign "" NULL "" or some unmeaning symbols. 
 For each document, they identify with the vector of m keywords and denote the i th document by 
D i = (w i,1 , w i,2 , . . . , w i,m 
). The above assumptions have been adopted to many conjunctive keyword searchable schemes. Since it identify a fixed number of keyword in each document, we classify some existing conjunctive keyword searchable scheme into two categories: fixed keyword fields and variable keyword fields, and introduce in this section. 
Security Definitions
In order to prove the conjunctive keyword search scheme is secure, first we introduce three security games that Golle et al. defined in 
Security Game ICC (Indistinguishability of Ciphertext from Ciphertext)
Let A be a polynomially bounded adversary (the server) and B be a challenger (the user). The goal of Game ICC is that A has to distinguish two encrypted documents, where D 0 and D 1 are chosen by A. The conjunctive keyword search scheme is secure if A cannot distinguish between D 0 and D 1 successfully with non-negligible advan- tage. 
1) An adversary A adaptively requests the encryption Enc(ρ, K, D) of documents D and search capabilities (trapdoors). 
2) A chooses two documents D 0 , D 1 , and then sends them to the challenger B. 
3) B chooses b randomly from {0, 1} and gives A an encryption of D b . 
 4) A may again ask for encrypted documents and capabilities , with the restriction that A may not ask for a capability that is distinguishing for D 0 and D 1 . 
5) A outputs b ∈ {0, 1} and wins the game ICC if b = b. We say the adversary A has an -advantage if the adversary's advantage is 
Adv A (1 k ) = |P r[b = b] − 1/2| > . 
2.1.2 Security Game ICR (Indistinguishability of Ciphertexts from Random) Let A be a polynomially bounded adversary (the server) and B be a challenger (the user). The adversary chooses only on document D 0 and a keyword subset T of D 0 . The goal of Game ICR is that A has to distinguish two encrypted documents, where D 0 is chosen by A and D 1 is produced by B. 
1) An adversary A adaptively requests the encryption Enc(ρ, K, D) of documents D and search capabilities (trapdoors). 
2) A chooses a document D 0 and subset T ⊆ {1, . . . , m}, then sends it to the challenger B. 
3) B creates a document D 1 =Rand (D 0 , T ) and chooses a random bit b ∈ {0, 1}, then gives Enc(ρ, K, D b ) to A. 4) A again asks for encrypted documents and capabilities , with the restriction that A may not ask for a capability that distinguishes D 0 from D 1 . 
5) A outputs b ∈ {0, 1} and wins the game ICR if b = b. We say that the adversary A has an -advantage if the adversary's advantage is 
Adv A (1 K ) = |P r[b = b] = 1/2| > . 
Security Game ICLR (Indistinguishability of Ciphertexts from Limited Random)
Let A be a polynomially bounded adversary (the server) and B be a challenger (the user). The adversary chooses one document and a keyword subset T . Then, B generates two encrypted documents related to T . The goal of Game ICLR is that A has to distinguish two encrypted documents . This security game reflects a secure notion which guarantees that an adversary cannot gain the plaintext from the other documents 
1) An adversary A requests the encryption Enc(ρ, K, D) of any documents D and any search capabilities (trapdoors). 
2) A chooses a documents D, and then sends it to the challenger B. 
3) B creates two documents D 0 =Rand (D, T − {t}) and D 1 =Rand (D, T ), where T ⊆ 1, . . . , m and a value t ∈ T , and chooses a random bit b ∈ {0, 1}, and then gives Enc(ρ, K, D b ) to A. 
 4) A again asks for encrypted documents and capabilities , with the restriction that A may not ask for a capability that is distinguishing for D 0 and D 1 . 
5) A outputs b ∈ {0, 1} and wins the game ICLR if b = b. We say that the adversary A has an -advantage if the adversary's advantage is 
Adv A (1 k ) = |P r[b = b] − 1/2| > . 
Fixed Keyword Fields Schemes
Golle et al.'s Scheme
Golle, Staddon and Waters 
ρ = (G, g, f (·, ·), h(·)
). 
2) KeyGen(ρ): Return a secret key K ∈ {0, 1} k for the function f , and we denote f (K, 
·) by f K (·). 3) Enc(ρ, K, D i ): Set keywords of a document D i = (W i,1 , W i,2 , . . . , W i,m ) and compute V i,j = f K (W i,j ) for j = 1, . . . , m. Select random values a i ∈ Z * q and output the keyword ciphertext S = (g a i , g a i V i,1 , g a i V i,2 , · · · , g a i V i,m ). 4) GenCap(ρ, K, j 1 , · · · , j t , W j1 , · · · , W jt ): Select a random number s ∈ Z * q and compute Q = (h(g a 1 s ), h(g a 2 s ), · · · , h(g a n s )
). Then, we define the 
value C = s + ( t w=1 f K (W j w )). Output Cap = {Q, C, j 1 , j 2 , · · · , j t }. 5) Verification(ρ, S, Cap): The server computes R i = g a i C · g −a i ( t w=1 V i,j w ) and returns "" yes "" if h(R i ) = h(g a i s 
) and "" no "" otherwise. 
Park et al.'s Scheme
In Park, Kim and Lee's 
1) Bilinear: a mapêmapˆmapê : G 1 × G 1 → G 2 is bilinear ifê ifˆifê(aU, bV ) = ˆ e(U, V ) ab for all U, V ∈ G 1 and all a, b ∈ Z. 
2) Non-degenerate: if g is a generator of G 1 thenêthenˆthenê(g, g) is a generator of 
G 2 . 
 3) Computable: there is an efficient algorithm to 
putê e(U, V ) for any U, V ∈ G 1 . 
) + H(W 2 ) + · · · + H(W t )), T 2 = u, and I 1 , I 2 , · · · 
, I t are positions of the keywords in Q. 
5) Test(GP, A pub , S, T Q ): 
Let S = [A 1 , A 2 ,· · · , A m , B, C]. Check if A I 1 × A I 2 × · · · × A I t = ˆ e(T 1 , B + T 2 C). 
If so, output "" yes "" , and "" no "" otherwise. 
Ryu and Takagi's Scheme
In 2007, Ryu and Takagi 
= (e(α, g ri 2 ), g ri 2 , (h i,1 ) r i , . . . , (h i,m ) r i ). 3) Trapdoor(α, {j 1 , . . . , j t }, {W j1 . . . , W jt }): 
Select a random value s ∈ Z * p . It returns the trapdoor 
T w = (α t w=1 (H(W j w )) s , g s 2 ). 4) Test(T w , C i ): Let T w = (T 1 , T 2 ) and C i = (V i , C i,0 , C i,1 , . . . , C i,m ). Check if e(T 1 , C i,0 )/e( t w=1 C i,j w , T 2 ) = V i . 
If so, output "" yes "" , and "" no "" otherwise. 
Chen and Hrong's Scheme
Chen and Horng 
G 1 × G 2 → G t . The global parameter GP = (p, G 1 , G 2 , G t , ˆ e, H). 
1) KeyGen(GP): Pick a random value α ∈ Z * p , a generator P 1 ∈ G 1 and a generator P 2 ∈ G 2 . It returns public key 
A pub = [P 1 , P 2 , Y = αP 1 ] 
and private key 
A priv = α. 2) PECKS(GP, A pub , D): Choose a random value r ∈ Z * p and compute V i = rH(W i )Y . It outputs the keyword ciphertext S = [V 1 , V 2 , . . . , V m , rP 1 ]. 
3) Timestamp(S, k): When the server receives the ciphtertext S, it chooses a random value s ∈ Z * p−1 . Then, it outputs "" the encrypted timing data "" 
S k = [V 1 k, V 2 k, . . . , V m k, rP 1 , kP 2 ] 
and publishes the timestamp value kP 2 . 4) Trapdoor(GP, A priv , D, kP 2 ): Take a timestamp value kP 2 from the public information server and select a random value 
s ∈ Z * p . Compute T w = [T 1 , T 2 , T 3 , I 1 , I 2 , . . . , I t ] where I 1 , I 2 , . . . , I t are 
the keyword fields which the receiver wishes to search, and 
T 1 = t i=1 (H(W i ))sα(kP 2 ) T 2 = sP 2 T 3 = kP 2 
in which T 3 is a label for searching the corresponding groups of document for the server. 
5) Test(GP, S k , T w ): Let S k = [A 1 , A 2 , . . . , 
T 1 ) = ˆ e(A l 1 + A l 2 + . . . + A lt , T 2 )
. If so, output "" yes "" , and "" no "" otherwise. Although the encrypted timing data shorten the searching time of Test algorithm, Chen and Hrong's scheme is still threatened by the outside off-line keywordguessing attack. 
Variable Keyword Fields Schemes
Zhang and Zhang's Scheme
Zhang and Zhang 
 Assume that there are l keywords in the PECKS algorithm (l is fixed). This scheme uses three group G 1 , G 2 and G t of prime order p, two collision resistant hash functions: H : {0, 
1} * → Z * p and H : G t → Z * p . The bilinear group is GP = (p, G 1 , G 2 , G t , ˆ e). 1) Setup(1 k , l): First choose l + 1 parameters: b 0 , b 1 , . . . , b l ∈ G 1 . Select two random generators g 1 , g 2 ∈ G 1 , a random generator h ∈ G 2 and a random value α ∈ Z * p . Set h 1 = h α . Output the public key pk = (g 1 , g 2 , h, h 1 , b 0 , b 1 , . . . , b l ) 
and the private key sk = α. 
 2) PECKS(GP, pk, w 1 , w 2 , . . . , w l ): Choose random elements a, k ∈ Z p , 
 then construct a l-degree polyno- mial: 
f (x) = a · (x − H(w 1 ))(x − H(w 2 )) · · · (x − H(w l )) 
+k, 
= a l x l + · · · + a 1 x + a 0 . 
Select a random element r ∈ Z p , then compute and output the keyword ciphertext S: 
S = (h r k , H (e(g 2 , h) (a 0 +a 1 +...+a l )·r ); h a 0 r 1 , h a 1 r 1 , . . . , h a l r 1 ; b a 0 r 0 , b a 1 r 1 , . . . , b a l r l ). 3) Trapdoor(sk, w 1 , w 2 , . . . , w s ): Choose a random value r ∈ Z p , then compute and output T w =[g 1/α 2 · (g H(w 1 ) 0 +H(w 2 ) 0 +···+H(w s ) 0 /α·s 1 · b 0 ) r = g 1/α 2 · (b 0 ) r ; g 1/α 2 · (g H(w 1 ) 1 +H(w 2 ) 1 +···+H(w s ) 1 /α·s 1 · b 1 ) r ; . . . g 1/α 2 · (g H(w 1 ) l +H(w 2 ) l +···+H(w s ) l /α·s 1 · b l ) r ; g r 1 ; h r 1 ]. 4) Test(GP, pk, S, T w ): Set T w = (T 0 , T 1 , . . . , T l ; g r 1 , h r 1 ), S = (C 0 , C 1 ; H 0 , H 1 , . . . , H l ; B 0 , B 1 , . . . , B l ). 
Then compute the following parameters: 
A 1 = l i=0êi=0ˆi=0ê(T i , H i ); A 2 = ˆ e(g r 1 , C 0 ) = ˆ e(g r 1 , h r l ); A 3 = l i=0êi=0ˆi=0ê(B i , h r 1 ) = l i=0êi=0ˆi=0ê(b a i ·α·r i , h r ); Check if H (A 1 /(A 2 · A 3 )) = C 1 . 
If so, output "" yes "" , and "" no "" otherwise. 
Chen et al.'s Scheme
Chen, Wu, Wang and Li 
Let G =< g > and G t be two cyclic multiplicative groups of composite order n = pqr, andêandˆandê be an admissible bilinear map from G 2 to G t . Assume that it is a hard problem to factor p, q, r on n. Let G p , G q and G r denote the subgroups of order p, q, r of G, and G t,p , G t,q and G t,r denote as the subgroups of G t , respectively; then 
G = G p × G q × G r , and G t = G t,p × G t,q × G t,r . In Chen et al.'s 
scheme, the subgroup G q and G r are used in the anonymity of encryption purpose and the correlation hiding between random values; and the subgroup G p is used to prevent the vicious manipulation of the keyword ciphertext S or the trapdoor T w from the adversaries , and then evaluate a query on the incorrect inputs. Chen et al.'s scheme consists of the following algorithms: Let KS w denotes the keyword space. The whole algorithm are as follows: 
1) Setup(1 λ ): First generate the bilinear group G of composite order n = pqr where p, q and r(p, q, r > m) are random primes of size θ(λ). 
Then pick α ∈ Z p and v, w 1 , w 2 , (u 1 , h 1 ), . . . , (u l , h l ) ∈ G p randomly. And also pick random elements R v , R w,1 , R w,2 , (R u,1 , R h,1 ), . . . , (R u,l , R h,l ) ∈ Z q and computes V = vR v , W 1 = w 1 R w,1 , W 2 = w 2 R w,2 . For i = 1, . . . , l, computes U i = u i R u,i , H i = h i R h,i , E = ˆ e(v, g) α , 
Output the secret key 
msk = (α, v, w 1 , w 2 , (u 1 , h 1 ), . . ., (u l , h l )) and the public parameters GP = [g q , g r , V, W 1 , W 2 , (U 1 , H 1 ), . . . , (U l , H l ), E, KS w ]. 2) PECKS(GP, x): x = (x 1 , . . . , x l ) ∈ KW w denotes 
the conjunctive keywords vector that the data sender sets. First pick s 
∈ Z n and Z 1 , Z 2 , Z 3 , Z 4 ∈ G q , and outputs the ciphertext S = [C 0 , C 1 , C 2 , C 3 , C 4 ] 
 as fol- lows: 
C 0 = E s , C 1 = V s Z 1 , C 2 = W s 1 Z 2 , C 3 = W s 2 Z 3 , C 4 = ( l i=1 H i U x i i ) s Z 4 . 
3) Trapdoor(GP, msk, e): To generate a trapdoor for 
conjunctive keywords e = (e 1 , . . . , e l ) ∈ KS w , first randomly pick r 1 , r 2 , r 3 ∈ Z n , and Y 1 , Y 2 , Y 3 , Y 4 ∈ G r . 
Then compute and output the trapdoor 
T w e = [K 1 , K 2 , K 3 , K 4 ] 
as follows: 
K 1 = g α w r1 1 w r2 2 ( l i=1 h i u ei i ) r 3 Y 1 , K 2 = v r 1 Y 2 , K 3 = v r 2 Y 3 , K 4 = v r 3 Y 4 . 4) Test(GP, S, T w ): Check ifêifˆifê(C 1 , K 1 ) = C 0 4 i=2êi=2ˆi=2ê(C i , K i ). 
If so, outputs "" yes "" and "" no "" otherwise.  Although the number of keyword ciphertext and trapdoor will not linearly expand as long as the number of the keywords increases, the size of ciphertext and trapdoor in Chen et al.'s scheme still larger than other schemes; also, it causes a long communication time for users to effect the quality of uploading and querying the documents. 
Comparisons
Security Analysis
In this section, we analyze the security of the schemes that we have discussed in Section 2 and show the comparison in 
Performance Analysis
In this section, we analyze the performance of the schemes with the size of outputs and the computation load that each algorithm need, and display the comparison in 
Future Research
Most of the existing conjunctive keyword searchable schemes cannot possess both security and efficiency at the same time. If a method focuses on enhancing the security that to stand against the inside and outside 
Encryption (m + 1)E (m)P + 2M + (m)e (m+1)E+e (m + 1)M (2m+4)E+ e (m + 5)E + (m + 3)M Trapdoor (n)E (m)P + M 2E 2M (2m+4)E+ (m)M (m + 7)E + (m + 7)M Test 2E e 2e 2e (2m + 3)e 4e 
line keyword-guessing attack successfully, the algorithm might consists of an exponentiation operation, Maptopoint function, and so on, which causes a huge amount of computational load for users. Therefore, how to reduce the computational load and improve the efficiency at the same time is one of the most important issues that needs to be solved. Besides, in order to construct a secure enough conjunctive keyword searchable scheme, some schemes are constructed in symmetric-key cryptosystem. In contrast, it is securer in encrypting the keywords and searching the encrypted documents, but how to transit the secret key is another important question which need to further discuss. Hence, no matter what the cryptosystem is adopted, how to construct a thorough conjunctive keyword searchable scheme is another important issue that can be deeply researched in the future. 
Conclusions
The first keyword searchable scheme was presented by 
"
"Introduction
Data hiding is the process that embeds additional data into the cover media while causes distortion as little as possible to cover media. There are two main applications of data hiding. One is secure communication, which is always called steganography. It is considered much safer than traditional encryption, because it conceals the existence of secure communication. The other one is copyright protection and authentication for the cover media, which is often called digital watermarking. Reversible data hiding is the data hiding that can reversibly recover the original cover media after the additional data is extracted . Due to the reversibility, it can be used in a lager field, such as medical image surgery, military imagery, and remote sensing imagery and so on. Many reversible data hiding schemes have been proposed in recent years 
The scheme utilizes the quantized coefficients of the BTC compressed images to achieve the reversible data hiding. The high mean values and low mean values of every block in the BTC compressed stream can construct two matrixes that are just like two sampled prediction image of the original image. Therefore, some traditional data hiding schemes can be utilized in the design of data hiding scheme for BTC compressed images. This paper imposes the integer DWT on the constructed images, and embeds additional data into the histograms of the middle and high frequency sub-bands in the integer DWT domain . The scheme has achieved high embedding capacity and low distortion in the experiments. Besides, compared with some existing schemes, better performances have been achieved with the proposed scheme. The rest of the paper is organized as follows. Some related techniques are introduced in Section 2. The main algorithm is proposed in Section 3. Section 4 demonstrates the experimental results and the corresponding analysis, and Section 5 draws the conclusions. where x j is the j th pixel of the block. Then the low mean value l i and high mean value h i are calculated with: 
l i = 1 k × k − q × xj <¯ xi x j , 
(2) 
h i = 1 q × xj ≥¯ xi x j , 
(3) 
where q is the number of pixels greater or equal to the mean value ¯ x i of the block. Then the bitmap of the block bm i is calculated with: 
bm i = 1 if x j ≥ ¯ x i 0 otherwise (4) 
Finally, block X i is represented with (h i , l i , bm i ), and all the blocks constitutes the set (H, L, BM ). When decompressing the compressed image, every 1 in the bitmap bm i is replaced by the grey value h i and every 0 in the bitmap bm i is replaced by the grey value l i . An example that compresses one block from image Lena is presented in 
 2.2 Reversible Data Hiding in Integer- DWT Domain Based on Histogram Modification 
The reversible data hiding scheme proposed in 
). At last, data is embedded by expanding the histogram between and, and the histogram after embedding is as 
Reversible data embedding
The histograms of LH, HL, HH sub-bands are generated and data is embedded into the coefficients by histogram modification as presented in 
Data extraction and reversible recovery of the matrix before embedding
Generate the histograms of middle and high frequency sub-bands and shift these histograms to extract the hidden data. The original coefficients matrixes are reversibly recovered with the following steps. For every coefficient C of LH, HL, HH sub-bands, given an embedding strength parameter q. If C ≥ 2 × q, then C is shifted to C − q; else if C ≤ −2 × q + 1, then C is shifted to C + q − 1; else C ← f loor(C/2), and data is extracted: B = mod(C, 2). All the coefficients of sub-bands LH, HL, HH are reversibly recovered and the extracted B is the data embedded before. 
Proposed scheme
The BTC compression divides the original image into blocks, and then quantizes the blocks into the high mean values and the low mean values and a bitmap that indicates the quantized values. The quantized high mean values and low mean values of the blocks just construct two sampled images, which are utilized for reversible data hiding. The original image Lena and the sampled image constructed with its high mean values and low mean values after BTC compression are presented in 
1) Divide I into k × k sized blocks X = {X i , i = 1, 2, · · · , (m × n)/(k × k)}; 
2) Calculate the coefficients of compressed image (H, L, BM ) with method given in Section 2.1, where H and L are vectors with size (1, (m × n)/(k × k)), BM is a binary matrix with size (m, n); 
3) Construct the sampled images, denoted as I h and I l , respectively with the high mean values vector H and the low mean values vector L; 
4) Impose the integer DWT on I h and I l to get the four sub-bands LL, LH, HL, HH for data hiding; 5) Select the LH, HL, HH sub-bands of I h and I l after integer DWT to embed data with the method proposed in Section 2.2.1; 6) Impose inverse integer DWT on the corresponding sub-bands to get the I h and I l that contains hidden data; 7) Scan I h and I l to reconstruct the coefficients vectors H and L , and then (H , L , BM ) is the BTC compressed image with hidden data. In fact, the LL sub-bands after integer DWT can also be utilized for the data hiding, which may increase the embedding capacity. In the receiving end, with the encoded (H , L , BM ), the compressed image with hidden data is decoded. Besides, the hidden data is extracted, and the original BTC compressed image is reversibly recovered. 
The data extraction process is the inverse process of data hiding. Detailed steps for extracting the hidden data and reversible recovery of the original BTC compressed image are presented as follows. 
1) Construct the sampled image with hidden data I h and I l by scanning the coefficients vectors H and L ; 2) Impose the integer DWT on the I h and I l to get the four sub-bands LL, LH, HL, HH; 
3) Extract the hidden data from the four sub-bands of I h and I l , and then recover LL, LH, HL, HH with method proposed in Section 2.2.2; 4) Impose the inverse integer DWT on the four recovered sub-bands to get the recovered I h and I l ; 
5) Reconstruct the quantized coefficients vectors H and L, and then the (H, L, BM ) is recovered. 
 The additional data is hidden into the quantized vectors through histogram modification operated on the integer DWT domain. The bit map remains unchanged throughout the data hiding and extraction processes. In fact, the bitmap can also be incorporated for the data hiding , which will increase the embedding capacity further. Besides, the arrangements between the high mean values and low mean values can be utilized to present some data, which will also increase the embedding capacity. 
Experiments
 The proposed scheme has been imposed on different images to testify the validity. The standard images selected from the USC-SIPI image database are adopted for the demonstration. Random bit streams are embedded into these images as the hidden data. All the experiments are performed on the MATLAB 2012a running on a personal computer with CPU of AMD Phenom (tm) X4 810 
Processor 2.6GHz, memory of 4 GB, and the operating system of Windows 7 x64 Ultimate Edition. The original image Peppers, the image after BTC compression and the BTC compressed image with hidden data are presented in 
P SN R = 10 × log 10 255 2 M SE (dB), 
(5) 
where 
M SE = 1 N 1 × N 2 N1 1 N2 1 (I i,j − I i,j ). 
(6) 
 Different images with different BTC compression parameters and different embedding strength parameters are tested. Detailed data is presented in the following figures. The embedding capacity and PSNR of images that are compressed by BTC with block size 2 × 2 are presented in 
Conclusion
 A reversible data hiding scheme for BTC compressed images is proposed in this paper. Based on the high mean values and low values in the BTC compression, we constructed two sampled images for the data hiding process . A histogram modification based scheme in the integer DWT domain is utilized to achieve the high embedding capacity and low distortion. Through the proposed construction method of sampled images, traditional reversible data hiding schemes can be adjusted to realize the data hiding on the BTC compressed images. Besides, some other data hiding strategies were also mentioned in the paper to further improve the performances of the scheme.  Liang Yang born in 1992, is a M.S. candidate in in College of Software, Nankai University, China. His current research interests include information security, reversible data hiding, and multimedia security. 
"
"Introduction
The widespread use of the Internet and the popularity of computers have caused computer security to become an important issue. For many reasons, it is important to prevent the computers from the damage of viruses. Currently, anti-virus software is the most frequently used mechanism against viruses. Anti-virus software relies on a set of feature instructions, called patterns or signatures. Patterns are made from known viruses. A new virus can be detected only when its pattern is extracted and included in the pattern database of an anti-virus software. The period from the breakout of a virus to the extraction of its pattern is called zero-day. Serious damage is usually done on zero-day. For example, the famous Code- Red worm infected 359,000 computers within 14 hours 
 1) The damage during zero-day is inevitable. Virus detection relies on the update of patterns. Serious damage may be caused during zero-day 
2) Virus scanning is inefficient. The number of patterns increases very fast. This causes the time required for scanning viruses to be longer than ever before. 
3) Viruses easily mutate. For example, there were more than 26 mutations of the Netsky virus within two months. This problem causes an increase in the number of patterns. This increase in the number of patterns means that an anti-virus software must update In order to overcome the above problems, a memory symptom-based virus detection approach is proposed in this paper. It differs from the pattern-based approach. The idea comes from understanding how diseases are diagnosed in real life. Doctors diagnose diseases based on the symptoms that a patient has, such as a fever, a cough, etc., rather than the type of virus. Although a disease may be caused by many kinds of viruses, its symptoms are almost the same. Similarly, if a virus can be detected based on the symptoms it causes, regardless of how the virus mutates, it can still be detected. In this paper, we mainly focus on the memory symptom of a program. The memory is consumed after the execution of a program. The memory symptom is the timely memory usages of program execution. Memory usage is sampled in fixed intervals and analyzed to detect whether a program is a virus or not. Basically, it is easy to generate a new viruses but difficult for them to cause new symptoms. The symptom-based approach is a new approach for detecting viruses. The rest of the paper is organized as follows. Section 2 presents some related works. Section 3 explains further what the memory symptom is. Section 4 presents the proposed symptom-based approach. Section 5 presents the experimental study. And, finally, section 6 gives the conclusion of this paper. 
Related Works
Many studies were conducted on how to detect computer viruses. For example, Lee et al. proposed a Virus Instruction Code Emulation (VICE) system 
 3 The Memory Symptom of Program Execution 
A symptom is defined as the usage of computer resources when a program is running. Among all the resources, the CPU and the memory are necessary for program execution . In this study, memory symptom is used for virus detection. The memory usage of two programs is shown in 
The Virus Detection Approach
The procedure of the proposed approach is shown in 
2) Encode: Memory usages are encoded as a series of codes, called curve codes. 
3) Match and compute the CF value: The memory symptom of an unknown program is matched with symptoms of sample programs, and a CF value is computed to represent the probability that the unknown program could be a virus. A set of viruses and normal programs are chosen as samples. They are processed using steps one and two and then their memory symptoms are stored in the sample database. An unknown program is processed using 
. . . . . 
the same steps. Then its memory symptom is matched with those in the sample database, and a CF value is outputted. The above three steps are presented in more detail as follows: 
1) Sample memory usage Memory usage is sampled for a fixed interval and period after the program is executed. Assume the interval is three seconds and the period is 90 seconds. The memory usages of the Netsky virus are listed in 
2) Encode The sampled memory usages are encoded in this step. The encoded result, the curve code, is defined to represent the curve of the memory usage. The difference between two successive sampled memory usages is computed. Then the curve code is generated according to the encoding table given in 
Assume there are n curve codes. The ith curve code of the sample program and unknown program is denoted by CS i and CU i , respectively. The accumulated values of the sample and unknown programs are denoted as CS Sum i and CU Sum i , respectively. The equations of CodeSum are shown below. 
CS Sum i = i j=1 CS J (1) CU Sum i = i j=1 CU J (2) 
CodeSum = min( n i=1 |CS Sum i |, n i=1 |CU Sum i |) max( n i=1 |CS Sum i |, n i=1 |CU Sum i |) (3) 
Equations 
(1) 
and 
 (2) are used for the computation of the ith accumulated value. In Equation (3), CodeSum is defined as the minimum sum of CS Sum i or CU Sum i divided by the maximum sum. However, CS Sum i or CU Sum i may be negative . CodeSum is based on the absolute value of CS Sum i and CU Sum i . The result of CodeSum is within zero to one. If the result is close to one, this means that the CodeSums of the sample and unknown programs are close. Otherwise, the result is close to zero. For example, assume that the values of two curve codes, 'SBb' and 'OAC', are 19, 2, In Equation (4), minCurveCode is defined as the minimum value among all of CS i and CU i . The sum of the differences of the curve codes is divided by the sum of the maximum CS i or CU i . However, CS i or CU i may be negative or positive. Each maximum value is subtracted minCurveCode to ensure that the denominator is larger than the numerator. The division result is within zero to one. Then the number one is subtracted from the above result in order to represent the same meaning of CodeSum. That is, if CodeDif f Sum is close to one, it means that the curve codes of the sample and unknown programs are similar. Otherwise, the CodeDif f Sum is close to zero. For the same curve codes 'SBb' and 'OAC', the values are 19, 2, -2 and 16, 1, 3, respectively . minCurveCode equals -2. CodeDif f Sum 
equals 1 − (3 + 2 + 5)/(21 + 4 + 5) , i.e., 0.7. 
3) U sageSum: The measurement is based on the sum of memory usages. Assume there are n memory usages. The ith usage of the sample and unknown program is denoted by U S i and U U i , respectively. The computation of U sageSum is shown in Equation (5). In this equation, the minimum sum of memory usages is divided by the maximum sum of the memory usages. The result is within zero to one. If the result is close to one, it means that the usage sum of two programs is similar. Otherwise, the result is close to zero. For example, the usages of two programs are 3868, 3984, 4020 and 4000, 3510, 2000, respectively. U sageSum 
U sageSum = min( n i=1 U S i , n i=1 U U i ) max( n i=1 U S i , n i=1 U U i ) (5) 
 4) U sageDif f Sum: This measurement is used to realize the difference of memory usages at every sample time. The computation of U sageDif f Sum is shown in Equation (6). The numerator is the sum of usage differences, and the denominator is the sum of maximum memory usages. One minus the division result causes the result to have the same meaning as the other measurements. That is, if the result is close to one, it means that the memory usages of the two programs are similar. For the example with the memory usages 3868, 3984, 4020 and 4000, 3510, 2000, the usage differences are 132, 474, and 2020. 
U sageDif f Sum = 1 − n i=1 |U S i − U U i | n i=1 (U S i , U U i ) (6) 
In order to compute the final measurement from the above four measurements, each measurement is assigned a weight instead of the four measurements being simply averaged. Assume the weights are 1, 1, 5, and 5. An 
CF = M B − M D 1 − min(|M B|, |M D|) (7) 
The range of CF is between -1 and 1. If CF is close to -1, it means that the unknown program is normal. If CF is close to 1, it means that the unknown program is a virus. If the unknown program has a CF value between - 0.2 and 0.2, the CF value is treated as insignificant. That is, it is too weak to support or deny that the unknown program is a virus. An example of CF computation is shown in 
Experimental Study
Experimental Metrics
The accuracy of the detection approach is estimated by using the confusion matrix, as shown in 
2) False positive rate: Its equation is d/(d + e + f ). This indicates the rate that normal programs are detected as viruses among all the normal programs. 
 3) Unknown rate: It indicates the rate of unknown programs among all of the programs. It equation is 
(c + f )/(a + b + c + d + e + f ). 
These rates are used to verify the proposed approach. 
Experimental Results
A tool for recording memory usages was implemented. A screen shot of the tool is shown in 
There were a total of 109 test programs. Many popular programs were included in the test programs. In addition, 15 mutations of the Netsky virus were also included in the test programs in order to determine whether all of them can be detected. A partial experimental result of detection using the proposed approach is listed in 
Conclusion and Future Works
Currently, anti-virus software relies mainly on the update of virus patterns. However, they still suffer from the problems of damage occurring on zero-day, virus scan inefficiency , and mutation of viruses. In this paper, a memory symptom-based virus detection approach was proposed. Viruses can be detected after it starts executing. The experimental results show that viruses can be detected correctly. In the future, other symptoms, such as network bandwidth usages, can be incorporated into this approach to increase the accuracy of virus detection. 
"
"Introduction
Most of the products and standards that use public-key cryptography for encryption and digital signatures use RSA today. Recently, Elliptic Curve Cryptography has begun to challenge RSA. The principal attraction of ECC, compared to RSA, is that it appears to offer better security for a smaller key size, thereby reducing processing overhead. Elliptic curve cryptography makes use of elliptic curves in which the variables and coefficients are all restricted to elements of a finite field. In ECC we normally start with an affine point called P m (x,y). These points maybe the Base point (G) itself or some other point closer to the Base point. Base point implies it has the smallest x,y co-ordinates, which satisfy the EC. A character in a message is first transformed into an affine point of the elliptic curve by using it as a multiplier of P m . That is, if the ASCII value of a character is A, then we determine P m =A(P m ). This is one step towards introducing sophistication and complexity in the encryption process. The newly evaluated P m is a point on the EC, determined by applying the addition and doubling strategy of ECC technique. Then as per ECC algorithm, P m is added with kP B , where k is randomly chosen secret integer and P B is the public key of user B, to yield (P m +kP B ). This now constitutes second part of the encrypted version of the message. The other part, namely, kG, which is the product of the secret integer and the Base point, constitutes the first part. Thus the encrypted message is now made up of two sets of coordinates, namely, (kG, P m + kP B ). In this paper we have assigned kG=(x 1 ,y 1 ) and (P m +kP B )=(x 2 ,y 2 ). Not satisfied with the complexity involved in determining the encryption, we wish to introduce further complexity by applying the Knapsack concept to the encrypted version. The whole idea behind these rigorous exercises is to make decryption totally impossible , even if the Base Point G, secret integer k, the affine Point P m are known to the crypt analyst. Now to recover the information from the encrypted version, first the knapsack process has to be reversed. Then we apply the decryption process of ECC, by applying the private key of recipient (n B ) on the first element (kG). This is subtracted from the second element to recover P m . Lastly by using the discrete logarithm concept, it is possible to evaluate the ASCII value and thereby recover the plaintext. The encrypted contents may be stored in CD/DVD or transmitted over the Net to a beneficiary. This promises to afford maximum security from intruders and hackers. 
The originality of this paper rests on the following points. 1) Transforming the ASCII value of a character of the message into an affine point on the EC. 2) Knapsack algorithm introduces further non-linearity in the encryption . Other points such as applying ECC for encryption/Decryption , invoking the discrete logarithm concept to recover the ASCII value are already existing and well documented. As far as the authors knowledge go no such attempt seems to have been presented so far. For the sake of comparison of the knapsack based ECC encryption/decryption , another public key algorithm, namely RSA, is used to encrypt/decrypt the same message. Unlike the ECC procedure, this yields only one integer for each character of the message. The time and space implications for both the schemes are discussed and analyzed. The paper justifies that despite the harsher requirements of time and space for the ECC methods, it is far superior due to the resistance it offers to any brute force attack. Some recent works on application of ECC are cited here.Aydos et al. 
Proposed Method Description
The Weiestrass equation defining an elliptic curve over GF(p), for q > 3, is as follows: 
E : y 2 = x 3 + ax + b, (1) 
where x, y are elements of GF(p), and a, b are integer modulo p, satisfying 4a 3 + 27b 2 = 0 mod p. 
(2) 
Here p is known as modular prime integer. An elliptic curve E over GF(p) consist of the solutions (x, y) defined by Equations 
 (1) and (2), along with an additional element called O, which is the point of EC at infinity. The set of points (x, y) are said to be affine coordinate point representation. The basic Elliptic curve operations are point addition and point doubling. Elliptic curve cryptographic primi- tives 
S = [(3x 2 P + a)/2y P ] mod p. 
Then 2P has affine coordinates x R , y R given by 
x R = (S 2 − 2x P ) mod p, y R = [S(x P − x R ) − y P ] mod p. 
Now to determine 3P , we use addition of points P and 2P , treating 2P = Q. Here P has coordinates (x P , y P ) and Q = 2P has coordinates (x Q , y Q ). Then 
x R = (S 2 − x P − x Q ) mod p, y R = (S(x P − x R ) − y P ] mod p. 
Therefore we apply doubling and addition depending on a sequence of operations determined for k. Every point x R , y R evaluated by doubling or addition is an affine point (points on the Elliptic Curve). The proposed algorithm is shown in Algorithm 1. Calculate the P m ': 
P m ' = G.P m , 
where P m is the ASCII value of Plain Text and G is the base point of EC Send (U Bob , C m =((S
x 1 = Inverse Knapsack value (S[x 1 ]); 26: y 1 = Inverse Knapsack value (S[y 1 ]); 27: x 2 = Inverse Knapsack value (S[x 2 ]); 28: y 2 = Inverse Knapsack value (S[y 2 ]); 29: kG = (x 1 , y 1 ); 30: P m ' + kP Bob = (x 2 , y 2 ); 31: Calculate n Bob kG = n Bob (x 1 , y 1 ); 32: Calculate P m ' = P m ' + kP Bob –n Bob kG; 33: 
 Calculate the P m value from P m ' using discrete log- arithm 34: end if 35: End//End Algorithm KnapsackBasedECC 
Implementation Details Of Our Proposed Algorithm
Once the defining EC is know, we can select a base point called G. G has 
P B = n B G. 
(3) 
Suppose A wants to encrypt and transmit a character to B, he does the following. Assume that host A wants to transmit the character 'S'. Then the ASCII value of the character 'S' is used to modify P m as follows: P m = SP m . P m we said is an affine point. This is selected different from the Base point G, so as to preserve their individual identities. P m is a point on the EC. The coordinates of the P m should fit into the EC. This transformation is done for two purposes. First the single valued ASCII is transformed into a x,y co-ordinate of the EC. Second it is completely camouflaged from the would-be hacker. This is actually intended to introduce some level of complexity even before the message is encrypted according to ECC. As the next step of ECC, we need to evaluate kP B , here P B is a public key of user B. Determining this product involves a series of doubling and additions, depending on the value of k. For a quick convergence of the result, we should plan for optimal number of doubles and additions . The encrypted message is derived by adding P m with kP B , that is, P m +kP B . This yields a set of x 2 , y 2 coordinates. Then kG is included as the first element of the encrypted version. kG is another set of x 1 , y 1 coordinates . Hence the entire encrypted version for purposes of storing or transmission consists of two sets of coordinates as follows: 
C m = (kG, P m + kP B ) kG − → x 1 , y 1 P m + kP B − → x 2 , y 2 . 
Thus far the modified plaintext has been encrypted by application of the ECC method. The modification of the plaintext in conjunction with Pm is a new innovation of this paper. However the authors have gone a step further , to make the encryption more secure, by proposing the extension of the knapsack procedure. As far as our knowledge goes, this is perhaps the first attempt at applying the Knapsack algorithm to ECC encrypted message. This introduces thorough diffusion and confusion to shatter any attempt at brute force attacks. Knapsack requires that we generate a series of vectors called a i . There are several ways of generating these vectors . For the sake of illustration we shall take the first value as 1, and subsequent values as multiples of n Say 
a i = 1, n, n 2 , n 3 , · · · , n m 1 ≤ i ≤ m. 
Here, n may be assumed as some random integer less than 10, or computed involving the p and k integers. Here p is a prime integer used in the modular arithmetic, k is the secret integer and m is the length of the binary bit string. Next let us explore how the co-ordinates of the encrypted message are subjected to Knapsack process. Say, x i , is one of the coordinate points, which can be represented in its binary form as: 
x i = b 1 , b 2 , · · · , b m 1 ≤ i ≤ m. 
 As per the knapsack algorithm we calculate a 
tive sum S[x 1 ], S[x 1 ] = m i=1 a i x i . 
 In the final encrypted version the co-ordinate x i is replaced by its equivalent S
C m = ((S[x 1 ], S[y 1 ]), (S[x 2 ], S[y 2 ])). 
Recall that this two pairs of integers represent just one character in a message. Depending on the number of characters in the message, there will be as many such pairs of integers. This is either stored in archival device like CD/DVD or transmitted to a beneficiary through the Net. The recipient B has all relevant information for reversing the knapsack procedure and to recover bit pattern of the coordinates. For example B knows the ai series, his own secret key n B , the base point G, a, b, p values of the EC. B receives the encrypted message, 
C m =((S[x 1 ], S[y 1 ]), (S[x 2 ], S[y 2 ]
)). Let us discuss how to reverse the Knapsack process, by taking one example. Consider 
S[x 1 ] = m i=1 a i x i , 
which is the knapsack representation of x 1 . The x 1 value is recovered in an iterative fashion as follows: 
S[x 1 ] − n m . 
If this value is positive i.e., S
Subtract this from P m + kP B , to get P m as follows: 
P m = P m + kP B − n B kG. 
This subtraction is another ECC procedure involving doubling and addition. But the only difference is that the negative term will have its y co-ordinate preceded by a minus sign. With this subtle change in mind, the expression of determining the slope, new values of x R , y R are the same. Wherever y figures, it is substituted as -y. This will yield P m . Using the discrete logarithm concept the ASCII value of 'S' can be retrieved as follows, P m = S P m . 
 4 Implementation Of The Proposed Algorithm 
The Elliptic Curve is y 2 mod 487 = (x 3 − 5x + 25) mod 487. 
The base point G is selected as (0, 5). Base point implies that it has the smallest x, y co-ordinates which satisfy the EC. P m is another affine point, which is picked out of a series of affine points evaluated for the given EC. We could have retained G itself for P m . However for the purpose of individual identity, we choose Pm to be different from G. Let P m =(1,316). The choice of P m is itself an exercise involving meticulous application of the ECC process on the given EC. Further we need to generate the secret integer k, and the private key n B of the recipient B. We have at our disposal a series of random number generators. But that would be digressing from the main path of thought. Hence we shall assume that k = 225, and n B = 277. Plaintext is "" S "" , whose ASCII value is 83. Therefore, 
25 = −ve − → 0 6 − 5 = 1 − → 1 1 = 1 − → 1. 
 Therefore, x 99 = 1100011 (read from bottom up). Similarly other coordinates are recovered by applying reverse knapsack algorithm. Thus we are able to recover the encrypted version (99, 253), (51, 58). Therefore, S = 83. Thus we retrieve the character "" S "" . Detailed working for the remaining characters is given in the Appendix A. 
Discussions and Conclusions
A plaintext message 'SAVE' is taken for implementing the algorithm proposed in this paper. Each character in the message is represented by its ASCII value. Each of these ASCII value is transformed into an affine point on the EC, by using a starting point called Pm. This Pm may be selected to be different from the Base point G. Transformation of the plaintext ASCII value by using an affine point is one of the contributions of this work.The purpose of this transformation is two fold. Firstly a single digit ASCII integer of the character is converted into a set of co-ordinates to fit the EC. Secondly the transformation introduces non-linearity in the character thereby completely camouflaging its identity. This transformed character of the message is encrypted by the ECC technique. ECC itself is a very secure algorithm for encryption. However, not satisfied with it, we apply the knapsack algorithm, so that the entire encrypted version turns into an ensemble of confusing integers, thereby discouraging a potential cryptanalyst from attempting a brute force attack. Applying Knapsack algorithm to the ECC encrypted message is another new contribution of this work, which has not been attempted so far. The table below shows the results for the message "" SAVE "" . Encryption with Knapsack incorporated is shown in 
a i − → 1, n, (n + 1) 1 , (n + 2) 2 , (n + 3) 3 , · · · , (n + p) p , 
can be used. The limit to this selection is left to the imagination of the researcher. Even assuming that the opponent knows all the relevant parameters such as private key n B , the secret integer k, and the modular prime value p, he will not be able to figure out the ai vectors. More than that the 'n' used in the ai is another hurdle to be crossed. Thus, this work introduces two new concepts, such as the transformation of the ASCII into an affine point on the EC, and extending the Knapsack algorithm, which makes ECC algorithm one of the most challenging and formidable one, among all the encryption strategies. This is one of the most secure schemes for storing massive personal and sensitive data for archival purposes, such as National registry. The same message 'SAVE' is also encrypted using the public key cryptography scheme called RSA. The RSA used a public key e = 31, private key d = 159 and modulus value n = 667. It yields the following result as shown in the Tables 4 and 5. The RSA took 36.26ms to do the encryption and decryption. The same message with Knapsack based ECC approach took 60.9 ms to perform Encryption/Decryption . This is 24.64ms more than time taken for RSA. The storage space required for writing the ECC results on to the CD/DVD is more than twice, as compared to the space required for RSA results. The following table illustrate the execution time taken for with knapsack and with out knapsack. Then the question arises why use ECC strategy at all! In using the Knapsack based ECC algorithm we need to keep only the a i vectors secret. The crypt analyst may know all other values. The decryption process is totally infeasible in Knapsack based ECC and is worthwhile for storing/transmitting sensitive data of national significance . The RSA algorithm is equally secure. But many values like modulus operator n, and its prime factors like p,q need to be kept secret. Moreover RSA needs use of numbers of 120 digits for 'n' for better security. 
Future Scope
What further work can be done as an extension of the present work? There is plenty of scope to toy with the selection of the ai vector. Depending on the amount of memory requirement, one can analyze the use of Knapsack based ECC in small memory devices like smart cards and mobile devices. One can try using random and pseudo random number generators for choosing the secret integer k, and the private key n B . ECC is a vase field where there is large scope for higher research. 
Appendix A 
Plaintext is "" A "" , whose ASCII value is 65. Therefore, Hence the transmitted message is (18756,82031), ((656,3751). The recovery of bit pattern for x 280 is done as follows: 
P B = n B G = 277(0, 5) = (260, 48) P m = 65(1
3751 − 390625 = −ve − → 0 3751 − 78125 = −ve − → 0 3751 − 15625 = −ve − → 0 3751 − 3125 = 626 − → 1 626 − 625 = 1 − → 1 1 − 125 = −ve − → 0 1 − 25 = −ve − → 0 1 − 5 = −ve − → 0 1 = 1 − → 1. 
Therefore, x 280 = 100011000 (read from bottom up). Similarly other coordinates are recovered by applying reverse knapsack algorithm. Thus we are able to recover the encrypted version: (99, 253), (116, 280). From this P m should be retrieved, using B's private key n B . 277(99, 253) = (212, 151) 
P m = (116, 280) − (212, 151) = (298, 182). 
Now apply discrete logarithm concept to get the ASCII value of "" A "" . A(1, 316) = (298, 182). Therefore, A = 65. Thus we retrieve the character "" A "" . Plaintext is "" V "" , whose ASCII value is 86. Therefore, Encrypted version of the message is: ((99, 253), 
P B = n B G = 277(0, 5) = (260, 48) P m = 86(1
a i = 1
x 253 = 253 − → 11111101 [binary value of 253]. Therefore, S[x 1 ] = m i=1 a i x i S
3751 − 390625 = −ve − → 0 82031 − 78125 = 3906 − → 1 3906 − 15625 = −ve − → 0 3906 − 3125 = 781 − → 1 781 − 625 = 156 − → 1 156 − 125 = 31 − → 1 31 − 25 = 6 − → 1 6 − 5 = 1 − → 1 1 = 1 − → 1. 
 Therefore, x 253 = 11111101 (read from bottom up). Similarly other coordinates are recovered by applying reverse knapsack algorithm. Thus we are able to recover the encrypted version: (99, 253), (427, 287). From this P m should be retrieved, using B's private key n B : 277(99, 253) = (212, 151) 
P m = (427, 287) − (212, 151) = (68, 91). 
Now apply discrete logarithm concept to get the ASCII value of "" V "" . 
V (1, 316) = (68, 91). 
Therefore, A = 86. Thus we retrieve the character "" V "" . Plaintext is "" E "" , whose ASCII value is 69. Therefore, Hence the transmitted message is (18756, 82031), (96876, 406901). The recovery of bit pattern for x 135 is done as follows: 
P B = n B G = 277(0, 5) = (260, 48) P m = 69(1
3751 − 390625 = −ve − → 0 96876 − 78125 = 18751 − → 1 18751 − 15625 = 3126 − → 1 3126 − 3125 = 1 − → 1 1 − 625 = −ve − → 0 1 − 125 = −ve − → 0 1 − 25 = −ve − → 0 1 − 5 = −ve − → 0 1 = 1 − → 1. 
 Therefore, x 135 = 10000111 (read from bottom up). Similarly other coordinates are recovered by applying reverse knapsack algorithm. Thus we are able to recover the encrypted version: . She is currently a senior lecturer at Information Technology Department at Thiagarajar College of Engineering , Madurai. She has published fifteen papers in national and international conferences and two papers in international journals. Her research interests include machine learning applications and web mining. She is a life member of Computer Society of India, Institution of Engineers , India and Indian Society for Technical Education. M. Suguna is a research assistant in Thiagarajar college of Engineering, Madurai for the Smart and Secure Environment project. Her research includes wireless networks and system security. She has received a degree A.M.I.E in computer science from The Institute of Engineers(India ),kolkata. 
(
"
"Introduction
With the rapid development of network socialization, a large quantity of multimedia social network services and tools emerge, aiming at providing network tools, services and applications for transmission and sharing of the digital multimedia contents (such as digital images, audio and video, Java mobile applications, etc.) for MSN users. At present, the popular multimedia social networks throughout the world include Youtube, SongTaste, Youku, etc. These networks that are organized by users' social relationships are mainly used for using, sharing, and disseminating digital media content. They show obvious advantages in directly, quickly and flexibly transmitting digital contents. But, it also brings some risks for insecure transmission and uncontrollable sharing of the copyrighted digital contents. The unauthorized distribution, transmission and misuse of the digital contents make the problems of digital rights management increasingly promi- nent 
Related Works
 In recent years, some researchers have made extensive researches on social network platform and DRM. With regard to access control of digital contents for multimedia social networks, Barbara et al. 
Background Knowledge
Based on relations between users, the existing researches have mined potential transmission paths and credible potential paths for the multimedia social networks. Since the judgment of the credible potential paths is closely related to user-defined trust threshold, and the setting of the trust threshold has certain risk, the copyrighted digital contents transmitted and shared through the credible potential paths are still risky. For this reason, this article mainly carries out researches on risk assessment of digital right transmission via the credible potential paths. Thus, the relevant concepts of MSN potential paths are 
The Potential Paths in MSN
 A social network consists of several locally dense "" communities . "" In social networks, each community represents an actual social organization formed on the basis of social relationship or interest. That is, the node-node connection within community is relatively dense, but between which connections are very loose. A weak relation tends to transfer non-recurring information between different communities. Therefore, more alternation and information spread are performed between communities through weak relations, hence making it become an "" information bridge "" 
Trust Measurements of (Credible) Potential Paths
There is the direct trust between two users connected equivalent-edge and bridge-edge. So, the equivalent-edge trust calculation between communities is same to bridgeedge trust calculation, which can adopt a trust model for MSN in the reference 
RT u w = EDT v w · BDT u v . 
(1) 
The definition of potential paths indicates that all edges on the potential paths are rough-edges. The trust value of the potential paths (expressed as T pp ) is a product of all rough-edge trust values in the path, as shown in Equation (2): 
Security Risk Assessment on Potential Digital Rights Distri- bution
In order to effectively control the risk of digital rights transmission on the credible potential paths, the main problems focus on identification, quantification and evaluation of transmission risk. Through effectively analyzing and calculating risk of digital content transmission on the potential paths of multimedia social networks, and evaluating possible loss brought by the risks, the rights are flexibly and safely shared and transmitted between users. Security of digital content transmission is enhanced for the multimedia social networks. 
 4.1 Quantitative and Qualitative Analytic Approach 
 In order to evaluate transmission risks of copyrighted digital contents in multimedia social network, this article adopts both quantitative and qualitative risk assessment approaches. In risk management, Annualized Loss Expectancy (ALE) is a common quantitative analysis tool used for computing an expected loss for an annual unit, and in general it includes the following elements: @BULLET Asset Value (AV) denotes a tangible or intangible worth of digital assets by using monetary or other styles, and it is determined by the potential impact caused by the loss of assets. 
 @BULLET Annual Rate of Risk Occurrence (ARRO) is a prediction of how often a specific risk event is likely to happen each year. @BULLET Exposure Factor (EF) indicates the impact of risks on a target system. According to the transmission feature of digital rights among MSN users, we introduce Risk of Trust (RT), which refers to the transaction process by the trust relationship reflects the risk of the interactive event. With regard to such a risk severity factor as user demands for contents in DRM ecosystem, we introduce User Demand (UD). So, ALE is defined as Equation (3). 
ALE = AV · ARRO · EF · U D · RT, 
(3) 
where, for main parameters of ALE, Asset Value is easily acquired and depicted by the monetary value of digital contents, ARRO is calculated by Poisson Distribution of the annual risk occurrence, EF and UD are yielded through the fuzzy assessments, and RT is quantified through the relation between trust and risk. 
VaR-based Calculation on Maximum ARRO
Value at Risk (VaR) 
P rob(L ≤ V aR) = 1 − α, 
where L is an expected risk loss, V aR is the maximum loss, and α is determined by Content Providers' opinions on risks to a specific DRM ecosystem, that is,    0 ≤ α < 0.5 Risk-averse α = 0.5 Risk-neutral 0.5 < α ≤ 1 Risk-seeking 
(4) 
 Taking it into consideration that the Poisson Distribution is a common probability function depicting the likelihood of random events occurrence, we attempted to employ the Poisson Distribution and V aR to calculate the ARRO, that is an estimation on the maximum occurrence rate of a random copyrights infringement/illicit usage event of digital contents. Thus, the maximum occurrence rate is in line with Poisson Distribution with the parameter λ. And then, by using Equation (5), the maximum value of ARRO can be calculated. 
P rob(x ≤ M AX ARRO ) = 1 − α. 
(5) 
In this case, a Multimedia Social Network has the specific annual occurrence rate of copyrights infringements threat, as is compliant to Poison Distribution with λ that denotes the average ARRO of random risky events. According to Equations (4) and (5), when λ respectively is equal to 1.8, 5, 9, M AX ARRO can be gained for three different Providers' opinions, that is, When λ = 1.8, Obviously, the maximum of ARRO decreases with the increase of α. 
Fuzzy
In the calculations of ALE, UD and EF was normalized. That is, UD is 0.7, and EF is 90. 
Trust Risks of Credible Potential Paths
Digital content sharing between users under multimedia social networks is based on certain trust relation, which will directly affect sharing and transmission of digital contents . For multimedia social networks, trust is closely associated with the risk, i.e. the higher the trust value between two users, the smaller the risk in sharing content information. So trust values of two interacting parties can reflects the risks in interaction events. Other conditions being equal, the higher the trust, the lower the risk would be; otherwise, the higher the risk. So we can suppose that trust value plus value-at-risk is approximately equivalent to 1. Based on the relation between trust and risk, trust risk value RT of the credible potential paths is given by Equation 
(9): 
RT = 1 − T pp (0 ≤ T pp ≤ 1). 
(9) 
Algorithm Design
Risk assessment process of digital right transmission for multimedia social network divides into the following steps: first, all potential paths between two user-nodes in different communities are identified, and then the trust values of the potential paths are calculated. The credible potential paths in the range of user-defined trust threshold are found. Finally, the quantitative and qualitative approaches are proposed in this article to evaluate the risks in the credible potential paths. The process of the algorithm is described as the follows Algorithm 1. 
Algorithm 1 Mining and Risk Assessment of Credible Potential Paths between any Two Nodes in MSN 
Experiment and Analysis
 In order to verify the effectiveness of the quantitative-andqualitative-combined method for risk assessment, simulation experiment is made. The hardware of the simulation experiment is listed below: AMD Athlon(tm) X2 240 Processor 2.8G, 2G, and Microsoft Windows 7 ultimate. We made an experiment based on a representative realworld MSN YouTube dataset (http://socialnetworks.mpi-sws .org/data-imc2007.html), and further found a random multimedia social network with non-overlapped communities , as shown in 
. · · · · · · · · · · · · 129 < 
. · · · · · · · · · · · · 129 < 
Conclusion
As transmission and sharing of digital content information has some potential threats due to openness and dynamic characteristics of the multimedia social networks, risk assessment for digital right transmission is an effective way to address security problems. This article mainly evaluates security risks in the credible potential paths for the multimedia social networks, and then proposes a risk assessment method based on combination of quantitative and qualitative approaches. Next, an algorithm is designed and later used to evaluate the risks in the credible potential paths through simulation experiment. The experimental results show that the average risk occurrence and risk preference of content providers jointly influence the risk-associated loss. In the following work, we will provide the specific security risk control model based on the risk assessment results, so as to reduce piracy and misuse risks of the digital contents protected by copyrights in the multimedia social networks. 
"
"Introduction
Denial of service (DoS) attacks and more particularly the distributed ones (DDoS) are one of the latest threat and pose a grave danger to users, organizations and infrastructures of the Internet. A DDoS attacker attempts to disrupt a target, in most cases a web server, by flooding it with illegitimate packets, usurping its bandwidth and overtaxing it to prevent legitimate inquiries from getting through 
Artificial Neural Network
An Artificial Neural Network (ANN) 
 2) Self-organization: An ANN can create its own organization or representation of the information it receives during learning time. 
3) Real time operation: ANN computations may be carried out in parallel, and special hardware devices are being designed and manufactured which take advantage of this capability. 4) Fault tolerance via redundant information coding: Partial destruction of a network leads to the corresponding degradation of performance. However,  An ANN is a layered collection of small processing elements known as neurons and mathematically the output of a single neuron is given as 
y i = f i (w ji x i + b j ) 
Where f is the activation function, x is input and b is a bias and w is the weight for each input. The activation function determines the type of neuron and the application where the neuron is to be used. But the sigmoid activation function as shown in 
f i (s) = 1 1 + e −s j 
Network Architecture
 Artificial neural networks are interconnections of individual neurons. There are various network architectures based on the type of connection. A most important type of network is the feed forward neural network shown in 
y k = f ko ( N j=1 W h jk f k ( p j=1 W I ji X i + b j ) + b 0 ) 
Where @BULLET W h jk is connection weight from hidden layer to output @BULLET W I ji is connection weight from input to hidden layer @BULLET b 0 is bias of output @BULLET b j is bias of hidden layer @BULLET f k is activation function of hidden layer, and @BULLET f ko is activation function of output layer 
Learning in Neural Networks
Learning in the context of neural networks is the process of adjusting the connection weights and biases such that for a given input a desired output is achieved. There are two basic training modes. 
1) Supervised learning -This is a learning paradigm where the neural network is given samples of the input and desired output and the error between the desired output and the actual output of the neural network is used to adjust the connection weights. A famous algorithm of supervised learning is back prop- agation. 
2) Unsupervised learning -This does not need any feedback for adjustment of the weights. 
Back Propagation Algorithm
 Back propagation is a famous algorithm used to train neural networks. It uses the gradient decent optimization method to train a network. In most cases the sum of squared error is used as objective function. 
J = 1 M M j=1 (d j − y j ) 2 
 where d j and y j are the desired and actual network out- puts. 
Weight update algorithm is given by 
W new = W old + ∆W 
where∆W = −µ ϑJ ϑw 
Input and Output
 In feed forward neural network, a relationship is developed between number of zombies Y (output) and observed deviation in sample entropy X (input). Here X is equal to (H c − H n ). Our proposed feed forward neural network based approach utilizes this deviation in sample entropy X to predict number of zombies. 
Detection of Attacks
Analytical Model
 This section describes an analytical model which is constructed to detect a wide range of flooding attacks. Detecting DDoS attacks involve first knowing normal profile of the system and then to find deviations from this normal profile. Whenever incoming traffic goes out of the normal profile, anomalous system behavior is identified. Our approach detects flooding DDoS attacks by the constant monitoring of the propagation of abrupt traffic changes inside the ISP network. A high-level block diagram of DDoS detection system is given in 
Feature 
M = (m 1 , m 2 , . . ., m m ), F = (f 1 , f 2 , . . ., f n ), where f i = (m i 1 , m i 2 , . . ., m i m ) is i th flow. Consider a random process {m i j (t), t = wδ, ω ∈ N 
}, where δ is a constant time interval, N is the set of positive integers, and for each t, m i j (t) is a random variables. 1 ≤ ω ≤ l, l is the number of time intervals. Here m i j (t) represents the value of m j in flow i in {t−δ, t} time duration. These relations can be written in matrix form as follows: 
Z(t) =      m 1 1 (t) m i 1 (t) . . . m n 1 m 1 j (t) m i j (t) . . . m n j . . . . . . . . . m 1 m (t) m i m (t) . . . m n m (t)      
where, Z(t) contains values of different measures used in {t − δ, t}. m j (t) represent total value of j th measure during {t − δ, t} time. m j (t) can be calculated as follows: 
m j (t) = m 1 j (t) + m 2 j (t) + . . . + m i j (t) + . . . + m n j (t) 
where 1 ≤ i ≤ n, n is the number of flows. 1 ≤ j ≤ m, m is the number of measures. Normal traffic value of j th measures can be calculated using following equation: 
m * j (t) = 1 l l ω=1 m j (t) 
where t = ωδ. Vector A can be used to represent normal traffic measures value: 
A = (m * 1 (t), m * j (t), . . . , m * m (t)
). To detect the attack, the value of j th traffic measure m j (t) is calculated in time window δ continuously; whenever there is appreciable deviation from m * j (t), anomalous behaviors could be determined. Depending on the measures selected to use or network conditions, following events are defined to determine anomalous system behaviors: 
m j (t) − m * j (t) > ξ upper j m j (t) − m * j (t) < ξ lower j where ξ upper j and ξ lower j 
represent value of upper and lower bound of the threshold for j th measure, respectively. ξ upper j and ξ lower j can be set as follows: 
ξ upper j = γ upper j * σ j ξ lower j = γ lower j * σ j 
where σ i represent value of standard deviation for j th measure. r upper j and r lower j represent value of tolerance factor to calculate upper and lower bound of the threshold for j th measure, respectively. Effectiveness of an anomaly based detection system highly depends on accuracy of threshold value settings. Inaccurate threshold values cause a large number of false positives and false negatives. Therefore, various simulations are performed using different value of tolerance factors. The choice of tolerance factors varies for different network conditions. Values of tolerance factors also depend on the composition of the normal traffic and the desired degree of the ability to control a DDoS attack. Then, trade-off between detection and false positive rate provides guidelines for selecting value of tolerance factor r j for j th traffic measure for a particular simulation environment. 
Entropy Based DDoS Detection
Here, we will discuss propose detection system that is part of access router or can belong to separate unit that interact with access router to detect attack traffic. It makes use of analytical model given in the previous section. Entropy based DDoS scheme 
H(X) = − N i=1 p i log 2 (p i ) 
where p i is n i /S. Here n i represent total number of bytes arrivals for a flow i in {t − δ, t} and S = N i=1 n i , i = 1, 2, . . . , N . The value of sample entropy lies in the range 0 − log 2 N . To detect the attack, the value of H c (X) is calculated in time window δ continuously; whenever there is appreciable deviation from X n (X), various types of DDoS attacks are detected. H c (X), X n (X) and gives Entropy at the time of detection of attack and Entropy value for normal profile respectively. 
 4 Experiment Setup and Performance Analysis 
In this section, we evaluate our proposed scheme using simulations. The simulations are carried out using NS2 network simulator. We show that false positives and false negatives triggered by our scheme are very less. This implies that profiles built are reasonably stable and are able to predict number of zombies correctly. 
Simulation Environment
Real-world Internet type topologies generated using Transit-Stub model of GT-ITM topology generator are used to test our proposed scheme, where transit domains are treated as different Internet Service Provider (ISP) networks i.e. Autonomous Systems (AS). For simulations, we use ISP level topology, which contains four transit domains with each domain containing twelve transit nodes i.e. transit routers. All the four transit domains have two peer links at transit nodes with adjacent transit domains . Remaining ten transit nodes are connected to ten stub domain, one stub domain per transit node. Stub domains are used to connect transit domains with customer domains, as each stub domain contains a customer domain with ten legitimate client machines. So total of four hundred legitimate client machines are used to generate background traffic. Total zombie machines range between 10 and 100 to generate attack traffic. Transit domain four contains the server machine to be attacked by zombie machines. A short scale simulation topology is shown in 
Results and Discussion
Training Data Generation
Neural network has to be trained by giving sample inputs and corresponding output values and a training algorithm will adjust the connection weight and bias values until a minimum error or other stopping criteria is reached. The training data has to be taken carefully to consider the complete input range. Normalization and other preprocessing of the data improve the training performance. In our paper, in order to predict number of zombies ( ˆ Y ) from deviation (H c − H n ) in entropy value, training data samples are generated using simulation experiments in NS-2 network simulator. Simulation experiments are done at the same attack strength 25 Mbps in total and varying number of zombies from 10-100 with increment of 5 zombies i.e. mean attack rate per zombie from 0.25 Mbps- 2.5 Mbps. 
Network Training
 For the prediction of the number of zombies in a DDOS attack , three feed forward neural networks have been tested. The feed forward networks used have different sizes. The size of a network refers to the number of layers and the number of neurons in each layer. There is no direct method of deciding the size of a network for a given problem and one has to use experience or trial error method. In general, when a network is of large size, the complexity of the function that it can approximate will also increase. But as the network size increase, both training time and  its implementation cost increase and hence optimum network size has to be selected for a given problem. For the current problem, two layer feed forward networks with 5, 10 and 15 neurons are selected. The training algorithm used is the Levenberg-Marquardt back propagation algorithm of MATLAB's neural network toolbox. The training results are given in 
Network Testing
Conclusion and Future Work
 The potential of feed forward neural network for predicting number of zombies involved in a flooding DDoS attack 
H c (X) − X n (X)
 ) in sample entropy is used as an input and MSE is used as the performance measure. Two layer feed forward networks of size 5, 10 and 15 have shown maximum mean square error (MSE) of 2.91, 2.59 and 3.14 respectively in predicting the number of zombies. Therefore, total number of predicted zombies using feed forward neural network is very close to actual number of zombies. However, simulation results are promising as we are able to predict number of , respectively. He is currently a Professor at Indian Institute of Technology Roorkee, India. He has a vast teaching experience exceeding 38 years at graduate and postgraduate levels at IIT Roorkee. He has guided over 150 M.Tech and 25 PhD dissertations. He has published over 100 research papers at national and international journals and presented many in Europe, USA and Australia. He has been awarded Gold Medal by Institute of Engineers for best paper. He has chaired many national and international conferences and workshops. Presently, he is actively involved in research in the field of Database management system, Data mining, Bioinformatics, Information security, Reconfigurable systems and Mobile com- puting. 
"
"Introduction
Undeniable signature was introduced by Chaum and van Antwerpen in 1990 
Bilinear Pairings
Let G 1 be an additive group of prime order q and G 2 be a multiplicative group of the same order. A cryptographic bilinear pairing is a mappingêmappingˆmappingê : G 1 × G 1 → G 2 that satisfies the following properties: 
@BULLET Bilinearity: ∀P, Q ∈ G 1 , ∀a, b ∈ Z * q , we havêhavê e(aP, bQ) = ˆ e(P, Q) ab . 
@BULLET Non-degeneracy: ∀P ∈ G 1 , if P = 0, thenêthenˆthenê(P, P ) = 1. @BULLET Computability: The mappingêmappingˆmappingê can be efficiently computed. 
H 2 : {0, 1} × {0, 1} × {0, 1} → G 1 , H 3 : G 3 2 → Z q and H 4 : G 4 2 → Z q . It randomly chooses a master key s ∈ Z q and computes the corresponding public key P pub = sP ∈ G 1 . The system's parameters are params := {q, G 1 , G 2 , ˆ e, P, P pub , H 1 , H 2 , H 3 , H 4 } @BULLET Keygen: Given a user (signer or verifier) with an identity ID, the PKG computes Q ID = H 1 (ID) ∈ G 1 and the associated private key d ID = sQ ID ∈ G 1 which will be transmitted to the user. 
@BULLET Sign: To sign a message M ∈ {0, 1} * ,the signer Alice with identity ID A and private key d IDA computes γ = ˆ e(H 2 (M, r, ID A ), d IDA ) ∈ G 2 , where r ∈ {0, 1} is a random string picked by Alice. Then, the pair (r, γ) is the signature on M . 
@BULLET Confirm: To verify the signature, the designated verifier with identity ID B will run a confirmation protocol with the signer Alice to produce a proof (U, v, h, S), 
where S = R + (h + v)d IDA , h = H 3 (c, g 1 , g 2 ), U ∈ G 1 , R ∈ G 1 and 
v ∈ Z q are randomly selected by the signer, 
and c = ˆ e(P, U )ˆ e(P pub , Q IDB ), g 1 = ˆ e(P, R) ∈ G 2 and g 2 = ˆ e(H 2 (M, r, ID A ), R) ∈ G 2 . 
To check the validity of the signature, based on the proof (U, v, h, S) for the signature (r, γ) on the message M from the signer, the verifier will first compute 
c = ˆ e(P, U )ˆ e(P pub , Q IDB ), g 1 = ˆ e(P, S)ˆ e(P pub , Q IDA ) h+v and g 2 = ˆ e(H 2 (M, r, ID A )
, S)γ h+v and accepts if and only if 
h = H 3 (c , g 1 , g 2 ). 
@BULLET Deny: To convince a designated verifier with identity ID B that a given signature (r, γ) is not a valid signature, the signer Alice will run the denying protocol, and produce a proof (C, U, v, h, S, s), where 
C = ( ˆ e(H2(M,r,IDA), dID A ) γ ) ω , S = V + (h+ v)R ∈ G 1 , s = v + (h + v)α, h = H 4 (C, c, ρ 1 , ρ 2 ), U ∈ G 1 , V ∈ G 1 , v ∈ Z q , ω ∈ Z q 
 , are randomly selected by the signer, and 
c = ˆ e(P, U )ˆ e(P pub , Q IDB ) v , ρ 1 = ˆ e(H 2 (M, r, ID A ), V )γ −v ∈ G 2 and ρ 2 = ˆ e(P, V )y −v ∈ G 2 , y = ˆ e(P pub , Q IDA ), α = ω, R = ωd IDA . 
 Based on this proof (C, U, v, h, S, s) for the signature (r, γ) on the message M from the signer, if C = 1, the designated verifier will reject the proof immediately. Otherwise, the verifier will compute 
c = ˆ e(P, U )ˆ e(P pub , Q IDB ) v , ρ 1 = ˆ e(H 2 (M, r, ID A ), S)γ −s C −(h+v) and ρ 2 = ˆ e(P, S)y −s ∈ G 2 , y = ˆ e(P pub , Q IDA )
, and accepts the proof if and only if 
h = H 4 (C, c , g 1 , g 2 ). 
The Attack
Now, we present an attack on Libert et al.'s identity based undeniable signature scheme. Suppose that an attacker has the information that (r, γ) is a valid signature, signed by the signer Alice with identity ID A , for the message M , then the attacker is able to forge a signature for Alice on any message M * as follows. 
1) He picks a random string r * ∈ {0, l} ; 
2) Computes H 2 (M * , r * , ID A ) ∈ G 1 ; 
3) Then, computes 
k = H 2 (M * , r * , ID A ) H 2 (M, r, ID A ) mod q = H 2 (M * , r * , ID A )H 2 (M, r, ID A ) −1 mod q. 4) Finally, computes γ * = γ k modq. 
The pair (r * , γ * ) is a forged signature on the message M * . The following lemma shows that the forged signature is a valid signature on M * if r * is the random string selected by the signer in Sign algorithm. 
= ˆ e(H 2 (M * , r * , ID A ), d IDA ). 
From the revealed message-signature pair, 
γ = ˆ e(H 2 (M, r, ID A ), d IDA ) and since γ * = γ k mod 
q, based on the bilinear property ofêofˆofê, we have the following. 
γ * = ˆ e(H 2 (M, r, ID A ), d IDA ) k = ˆ e(kH 2 (M, r, ID A ), d IDA ). 
Note that k = H 2 (M * , r * , ID A )H 2 (M, r, ID A ) −1 mod 
q, we have the following. 
γ * = ˆ e(kH 2 (M, r, ID A ), d IDA ) = ˆ e(H 2 (M * , r * , ID A ), d IDA ) = γ . 
Thus, Lemma 1 is established. 
Lemma 2. Let (r * , γ * ) 
be the forged signature generated by the attacker. Going through the procedure Confirm algorithm , the signer will produce a proof showing that the signature is valid. On the other hand, the signer cannot deny this forged signature with the denying protocol. Proof. Based on Lemma 1, if r * is selected by the signer in the procedure Sign algorithm, then the signature produced will be exactly the same as (r * , γ * ). So, a correct proof will be produced by the signer by executing the confirmation protocol. Similarly, the signer is not able to convince the verifier that the signature is invalid using the denying protocol. Combining Lemmas 1 and 2, we show that the attacker is able to forge the signer's signature for any message once a valid message-signature pair has been revealed. 
Conclusion
In this paper, we have shown that the identity based undeniable signature scheme proposed by Libert and Quisquater is not secure. In particular, once a valid message-signature pair has been revealed, the attacker is able to forge signatures of the signer for any message. Interestingly, while the design of the signature scheme relies on the property of bilinear mapping, the attack is also based on the same property of the bilinear mapping. As a remark, we have published a secure version of an identity based undeniable signature scheme in 
"
"Introduction
 A Mobile Ad-hoc Network (MANET) is a group of wireless mobile nodes that form a dynamic network topology without any centralized administration or fixed infrastructure . The nodes mobility requires establishing and breaking connections whenever needed. Each node communicates directly with the nodes within its wireless range. However, the nodes need to collaborate together to deliver the information between nodes that are beyond the wireless range of the source. With this approach, in terms of transmission, each node operates in two modes; source or router. Source nodes generate the traffic on the network whereas routing nodes receive the packets and forward them to the intended destination. A routing protocol is used to detect the topology of the network and to enable each node to have a path to any of its intended destinations. The nodes use Link State Update (LSU) packets to share information among each other to build their respective routing tables and report any changes in network topology. The routing protocol should focus on energy conservation to increase the lifetime of the nodes while choosing routes with the least delay, jitter and congestion. Occasionally, the best routes for several sources, in terms of delay, go through the same node whose energy gets consumed at a higher rate compared to other nodes. This, eventually, leads to a premature loss of the battery of the node. A more efficient approach is to route packets through paths that may have higher delays but with more energy resources in order to extend the life time of the network. Another important factor to be considered is the security of the communication among nodes. The routing protocol should detect any attempt to change the LSUs in transit, reject fabricated routing messages, avoid the creation of routing loops that lead to denial of service attacks and exclude all unauthorized nodes from the routing process. 
 The optimized link state routing protocol (OLSR) pro- tocol 
Security Threats
A major focus is the security of the ad hoc network where the integrity of data is essential. Due to the absence of a central authority for authentication, simple network functions , such as packet forwarding, become susceptible to attacks as they are executed by the nodes on the network instead of trusted centralized routers. To introduce trust levels, a node must examine the trustworthiness of the nodes with which it communicates before adding them to its routing table. The lack of authentication can be a serious hazard to the proper operation of the routing protocol. OEDR does not provide any security measures to guarantee the confidentiality, integrity, availability and authenticity of the data and proper routing. Because of this, malicious nodes can perform a variety of attacks to obstruct the communication on the network. It should be emphasized, however, that the following vulnerabilities are inherent in all link state routing protocols and do not represent faults in the initial design of OEDR. 
Passive Attacks
 In this form of attack, a node resides within the communication range of another node to capture all the information sent. This vulnerability is especially harmful if the intruder is within the range of the original source of traffic and can only be solved using encryption. If, however, the eavesdropper resides within the range of an MPR and not the original sender then the effect can be reduced by fragmenting the traffic into different paths. One approach for traffic partition is to use the number of packets sent through each MPR as a factor in calculating the cost of the link. As a result, with every packet sent, the cost of the link will increase until a point is reached where another link has a lower cost and the routing tables are updated to use a different MPR. 
Active Attacks
These attacks can be categorized as fabrication, identity spoofing, modification, or replay. Since routing functions are performed by the nodes within the network, carrying out such attacks is easier which will result in worse consequences on the overall performance of the protocol. Moreover, the node mobility adds complexity to the design of a secure protocol. Following is an explanation of these attacks and their effect on the routing protocol. 
Fabrication
In this form of attack, a node generates false HELLO or TC packets to cause changes in the routing tables of the nodes. This could lead to routing loops or denial of service. For the recipient, there is no way to verify the correctness of the data received. Examples of such an attack include generating TC packets that contain either an incomplete list of the MPR or a list of imaginary nodes. Another example involves the false advertisement of bi-directional links to the nodes in its neighborhood which may results in having it selected as the MPR. At that point, the intruder can either drop or selectively forward the packets to the MPR selector. Finally, since the OEDR link cost calculation depends on the reciprocal of remaining energy in the MPR candidate, an adversary can sends a Hello packet showing that it has a large amount of energy remaining in its battery. This misleads the recipient node to calculate a low cost for the link and thus selects the malicious node as its MPR. At this point, all the traffic will be routed through this malicious node which can either drop (denial of service), selectively forward or change the contents of the packets. 
Identity Spoofing
Since no authentication takes place, a malicious node can masquerade as another node (identity spoofing). When the neighboring nodes receive its Hello packets, they will be misled to believe that the claimed node is within their range. Later on, the deluded nodes will advertise themselves as the last hop to the intruder (which they mistakenly believe to be the legitimate node). This would result in conflicting information in the network as well as denial of service. 
Modification
MPR nodes are responsible for forwarding the packets to other nodes. While doing that task, a malicious MPR may change the payload or even change the destination field before transmitting the packet to the next hop. Another form of modification is to change the packet sequence number to match one that was previously used, resulting in a packet drop. 
Replay
A malicious node may hold copies of LSU packets that were sent earlier and retransmit them at a later time to poison the routing tables of the recipients with incorrect routing information. Although the destination nodes check the sequence number of any received packet to avoid duplicates, replay attacks can succeed by simply changing the packet sequence number to a higher value. 
 These attacks can render the OLSR and OEDR protocols ineffective. Security extensions are necessary in order to ensure safe transfer of data which is discussed next. 
Trust Level Routing
 TLR is a proactive link state routing protocol. Its operation is table driven through periodically exchanging topology information with other nodes in the network. The objective of the protocol is to give the same functionality of OEDR as well as providing guarantees of the integrity, timeliness and authenticity of the packets. Our proposed protocol follows the lines of OEDR. However, the routing criteria differ for selecting the MPR nodes. There are several metrics to be considered. 
Energy consumed per packet 
For this metric, the best path is selected based on the least consumed total energy. Any packet going from source n 1 to destination n k through some intermediate nodes will consume 
E t = k−1 i=1 e(i, i + 1) (1) 
where E t is the total energy consumed and e(i, i+1) is the energy consumed to send the packet from n i to n i+1 
Delay per packet Similar to the energy metric, the goal is to find the path with the least total delay. For a packet going from node n 1 to node n k through some intermediate nodes, the total delay incurred is given by 
D t = k−1 i=1 d(i, i + 1) (2) 
where Dt is the total delay and d(i, i + 1) is the time that starts when a packet enters the queue of node n i until it reaches the queue of n i+1 . It is worth mentioning that a trade off between the two metrics exists. For example, in the network shown in 
d A,D + d D,E + d E,C < d A,B + d B,C . 
(3) 
Then according to the delay metric the route A, D, E, and C will be taken instead of A, B, C. However, according to the energy metric 
e A,D + e D,E + e E,C > e A,B + e B,C . 
(4) 
Which implies that route A, B, C should be chosen. Furthermore, by selecting only one of the two metrics, the paths will tend to be always the same (assuming no mobility). As a result, some nodes will have high energy consumption while others will retain their energy. Such variance in node energy can result in network partition. Thus we consider a third metric. 
D A E C B 
Residual energy levels 
This metric is used to guarantee that all nodes will have approximately equal rates of consumption by using the nodes with the highest energy levels. That is, the lower the remaining available energy in a node, the higher the cost of routing through it. The cost in this case can be taken as the reciprocal of the residual energy. 
Traffic partition 
 This metric serves multiple purposes. First, less congestion and delay will be incurred through the intermediate nodes. Second, a higher throughput will be achieved because data packets are going through different paths. More importantly, the traffic will be fragmented into multiple paths which can potentially reduce the ability of a malicious node to capture the whole stream of traffic. TLR depends on the following steps for proper operation: neighbor sensing, cost calculation, MPR selection, broadcast of costs and routing table calculation. The following notations will be used: @BULLET MPR(s): the set of nodes selected as MPRs by node s (MPR(s) ∈ N 1 (s)); @BULLET P x,y : number of packets sent from x through MPR y; @BULLET C x,y : cost of link between nodes x and y, where 
C x,y = w 1 (Energy x→y )(Delay x→y ) + w 2 P x,y . (5) 
@BULLET E x : Available energy of node x. 
Neighbor Sensing
Each node maintains a table called the neighbor table that stores information about the link status between the node and all its immediate neighbors. Information about the delay, energy consumption on each link, two-hop neighbors accessible through the immediate neighbors, the set of selected MPRs and the number of packets sent through each MPR are also stored in the table. The table is populated using the HELLO messages. Every node in the network periodically sends HELLO packets to all the nodes within its transmission range. Each message contains a list of neighbors of the originator, transmission time, energy level, and the amount of energy used for transmis- sion. When the message is received, an entry is added to the table (if one does not already exist). The delay is calculated as the difference between the time of reception and time of transmission (we make the assumption that the clocks are synchronized). Furthermore, the energy consumption on the link is calculated in a similar manner by comparing the energy level of the signal at the receiver to the level stamped in the HELLO message. If the originator of the HELLO message has no entries in the neighbor table of the recipient (i.e. it just moved into the vicinity of the recipient) then a reply will be sent back with information about all authentic nodes and the information associated with each (as will be discussed in Section 4). 
Cost Calculation
Since TLR bases the selection of MPRs on a composite metric that differs from that of OLSR or OEDR, the MPR set chosen does not necessarily have to be identical for the same network topology and conditions. Moreover, if the nodes are static, the MPR set for nodes running OLSR will always be the same until one or more nodes lose their battery power or the topology changes due to node mobility. In TLR, the MPR set is dynamically changed more frequently compared to OEDR. 
MPR Selection Algorithm
@BULLET Initially the MPR set MPR(s) is empty. @BULLET First, find all the nodes in N 2 (s) that have a single neighbor in N 1 (s). Add these nodes of N 1 (s) to the MPR set if they are not already in MPR(s). (Because there are no other MPR candidates). @BULLET While there exists a node in N 2 (s) for which MPR node is not selected, then for each node in N 2 (s), with multiple neighbors from N 1 (s), select a neighbor from N 1 (s) as multipoint relay node which results in minimum cost from s to the node in N 2 (s), C MP R according to Equation 5, and add it to the MPR set if it is not already in MPR(s). 
MPR and Costs Declaration
Every selected MPR will transmit LSU packets called the Topology Control (TC) packets that contain information about the MPR node's selector set (i.e. the nodes that have selected the originator of the TC message as their MPR). The TC messages, which include the link costs between the MPR node and its selectors, are forwarded throughout the network through MPR nodes only. When a node receives a TC message, it can use the information to build a 'topology table', in which it stores the information about the topology of the network and the associated link costs. An entry in the topology table consists of the address of a destination (an MPR selector in the received TC message), address of the last-hop node to that destination (originator of the TC message), and the cost of the link between the destination and its last hop. It implies that the destination node can be reached in the last hop through this last-hop node at the given cost 
Routing Table Calculation
 For each node, a routing table is maintained to route packets to their destinations. Each entry contains the destination address, next-hop address, estimated distance to destination (in hops) and the total cost of the path from the source to the destination. For every known destination , an entry is added to the routing table listing the next hops to be taken. Our proposed protocol uses the least cost spanning tree method. 
TLR Implementation
With the vulnerabilities listed in Section 2, it is clear that the operation of OLSR and OEDR would become ineffective in the presence of malicious nodes. The main requirements of security, i.e. origin authentication, timeliness and ordering, and data integrity are missing in the original implementations of both protocols. Every LSU packet should hold enough information to clearly prove that it originated from the claimed source. Furthermore, late control packets should be dropped to avoid replay attacks. Finally, a mechanism to detect any alteration of LSU en route is needed. In this work, we propose two methods for adding trust levels in the routing protocol: traffic partition and authentication. 
Traffic Partition
In this method, the traffic transmitted from source A to destination B is routed through different paths by continuously switching between different MPR candidates. This provides confidentiality and forbids eavesdropping in the presence of a single eavesdropping node. With this approach, each path contains only partial information of the data stream. Consequently, an intruder will not be able to reconstruct the whole flow. From equation 5, it is evident that as the number of data packets increases on a certain link, the cost of using that link will increase until the total cost becomes higher than that of another link. At this point, the MPR list of the source will be updated to force a route change. This scheme would not be sufficient, however, in the presence of multiple eavesdropping nodes that may monitor the different selected paths and combine the data for analysis. 
Authentication and Timestamps
Traffic partition provides a reasonable level of security when combined with a lightweight encryption algorithm. It is helpful in reducing the risk of passive attacks by limiting the ability of one intruder to analyze the data stream completely and in a timely manner. However, it has little potential in overcoming or even detecting active attacks. Authentication of the LSU source can be used to counter active attacks. Assuming that the authentic nodes in the network have a mechanism for sharing the encryption keys, node A uses the shared key and the contents of the LSU to calculate a Message Authentication Code (MAC) which is appended to the LSU packet. When Node B receives the packet, it calculates the MAC again using the same secret key and the body of the received packet (excluding the MAC). Node B accepts the packet if the resulting MAC equals the MAC field of the packet. If, however, a malicious node alters the message but does not alter the MAC (because it does not have the secret key), then Node B can easily detect the changes in the packet and drop it. This guarantees that the packet was sent by one of the authentic nodes because only they have the shared key and that the payload of the control packet was not changed en route. The scheme above, by itself, is not enough for authentication. Consider a scenario where node A sends control messages to Node B. Assuming that a malicious node M intercepts the control message, it could wait for a random period of time and then retransmit the same packet. When Node B receives the replayed message, it checks the hash code and accepts the packet as authentic. This will cause inconsistencies in the routing table of Node B. Timestamps can be used to overcome this problem. Whenever a node sends a packet, it adds the time of transmission as a field and includes that in the MAC calculation . This enables the recipients to check the time of transmission. We propose the use of one way hash functions to create a hash chain analogous to that given in 
g 1 = h(S) , g i = h(g i−1 
) where 2 < i ≤ m and m is the length of the sequence. The source node, A, starts using the hash chain in the backward direction (i.e. it starts with g m followed by g m−1 , g m−2 etc). If a receiver knows the value of g i (where 2 < i ≤ m) and has guarantees that it is authentic then the next packet coming from node A can be checked for authenticity because it must be stamped by g i−1 . The receiver needs to check that h(g i−1 ) = g i as a proof of authenticity. 
Secure Broadcast of g m
 We assume that every node in the network has a mechanism to verify the public keys of all other nodes. We further assume that all the clocks of the nodes are synchronized (this assumption is needed for the proper calculation of delays in the OEDR protocol). It is worth mentioning that the use of public key cryptography is only applied on the control packets which comprise a smaller part of the whole packets being communicated. The broadcast of the last calculated value in the chain gm is done using a new packet called the Chain Tip packet. The format of the packet is shown in 
C = E KR [N odeID||timestamp||ChainLength(m)||g m ] E KU [C] = N odeID||timestamp||Chainlength(m)||g m . 
After decryption, the Node ID field will be compared with the decrypted value to make sure that it originated from the claimed source. The timestamp field proves that the message is "" fresh "" and that it is not a replayed message . The chain length field informs all recipient nodes about how many iterations of the hash function had been calculated to construct the chain, thus allowing them to know when the last value in the chain has been reached. Finally, g m will be stored in the memory along with its associated Node ID. Note, however, that any node that moves into the neighborhood after g m has been delivered will not be able to authenticate the received packets. This can be solved by introducing a Hello-response packet that is sent back from every node that receives the HELLO packet. (Refer to Section 3.1). The Hello-response should contain the same information given in the Chain tip packet with the only difference that the CI field will be set to the number of chain elements already received. For example, if a node received g m , g m−1 , , g m−i , then it will set the CI field to hold the value of i before transmitting the Hello-response packet. Thus, the originator of the HELLO packet will receive the same information that it would have received through a chain tip packet and will also have proof that it was indeed sent from the claimed source (No other node can forge it). Furthermore, by knowing the CI field, the node can check for the authenticity of the any TC packet it receives by calculating the hash function on the received chain value i + 1 times. 
h i+1 (g m−i−1 ) = g m . 
(6) 
After that, the new node will have all the information that is available to all other nodes. To avoid multiple copies of Hello-response packets, each node waits for a random time before replying back with its Hello-response while overhearing the responses of other nodes. If any matching Hello-response packet is overheard then the node backs off and does not transmit, otherwise it sends its own version of the Hello-response. 
Broadcasting TC Packets
The main purpose of using authentication is to guarantee that the TC packet was generated by the claimed source and that the contents were not changed in transit. Following is a description of how the hash chain can be applied. 1) Select a new random seed and calculate the hash chain; 
2) Broadcast g m ; 3) For 1 ≤ i < m 
@BULLET Find the message digest of the concatenation of the node ID, timestamp, message, and g m−i . @BULLET Transmit the node ID, timestamp, message and the calculated hash value. @BULLET Wait for a set period of time to make sure that the TC message reached all destinations in the network then transmit g m−i to update the current hash value being used. @BULLET Increment i. @BULLET If (i = m) then the chain has been consumed and a new chain has to be created (Step 1). Otherwise , repeat Step 3 for the next TC packet. 
When a node receives the TC packet, it has no way to calculate the message digest because g m−i is not known yet. The node must wait until it receives the value of g m−i to verify the authenticity and integrity of the latest received TC packet. 
Note that this update packet must be received within a protocol specific period of time after the TC packet to guarantee that none of the intermediate nodes held the TC packet until g m−i was released. This is an essential condition without which any malicious node can modify the contents of the packet and calculate a new message digest before sending the TC packet followed by the value of g m−i . The recipient, in this case, will not be able to detect the changes. For that reason, the recipient node compares the current time with the timestamp. If it finds that updating with the value of g m−i took more time than expected then it drops the TC packet. 
Security Analysis of TLR
In this section, we analyze the ability of TLR to limit various attacks. 
Replay Attacks
 An adversary may hold old copies of TC packets to transmit them at a later instance of time. This would result in conflicting information in the routing tables since either the topology or the MPR nodes would have changed. TLR mitigates this threat with the use of a timestamp in packets which is further enforced by the hash chain. 
Identity Spoofing and Link Spoofing
Identity spoofing involves a node using an ID that does not belong to it whereas link spoofing attacks occur when a node sends out incomplete or forged information about its links. The presence of the mechanism for verifying the keys of other nodes limits the ability of an attacker to attempt identity spoofing. Furthermore, in normal cases, only the packets signed by trusted nodes are accepted and all others are rejected thus a malicious node can not run a link spoofing attack since it packets will not be accepted. 
Modification Attacks
An adversary may change the contents of TC packets in an attempt to add, delete or alter the entries of the routing tables. This may result in routing loops or dropped packets due to incomplete routes. In TLR, modifying the contents of a TC packet will be detected since the intruder has no access to the held hash value. A malicious node trying to relay a modified TC packet will need to wait for that unknown hash value which would make it too late for it to transmit its modifications since that packet would be dropped. 
Passive Attacks
As mentioned earlier, a node may listen in to capture the data stream. If the node is overhearing the source itself then only encryption may protect the data. However, if the eavesdropper is positioned around one of the MPR nodes then TLR can be helpful by partitioning the data stream through different paths. By doing so, the intruder will only have a part of the data stream. With multiple cooperating intruders, the stream may be gathered in full and the protection of encryption is the last line of defense. It is implicitly assumed that all the nodes that were selected as MPRs would cooperate in relaying the packets to the destination. In the case of a compromised node, some packets may be dropped instead of being relayed. TLR can not detect this but due to its frequent topology updates it may limit this attack by switching to different MPR nodes. The work in 
Optimality Analysis of TLR
Theorem 1. Only authentic nodes can be selected as MPR nodes. Each packet received is checked for authenticity of source and content. Non-authentic packets are dropped by the recipient and no entries are made in the neighbor table. Hence, non-authentic nodes do not qualify as MPR candidates. 
Theorem 2. The MPR selection will result in a trusted optimal route between the source and destination with added trust levels only if there are multiple MPR candi- dates. 
Case I: If a node in N 2 (s) has only one neighbor from N 1 (s), then that single neighbor will be selected as the MPR. This MPR will be selected always in an optimal route but there will be no traffic partition. Case II: If a node in N 2 (s) has multiple neighbors in N 1 (s), the MPR selection will follow the cost function to determine the path with the least cost. Since the costs are dynamic due to the nature of the network and traffic sent, the paths selected will be dynamic to allow for traffic partition in addition to the energy-delay considerations. Assume that a source node s that has multiple one-hop neighbors in N 1 (s) needs to reach a node d in N 2 (s) that has multiple neighbor nodes n 1 , n 2 , ..., n k (k > 1) belonging to N 1 (s). Let the cost to reach any of these neighbors from s be C s,ni (i = 1, 2, , k) and the cost to reach d from n i is given by C ni,d . The MPR node between s and d is selected as the node n i with the minimum cost min 
((C s,n1 + C n1,d ), (C s,n2 + C n2,d ), ..., (C s,n k + C n k ,d )). 
The cost values of C s,ni change frequently and a different MPR is selected. Consequently, the MPR selection of TLR will result in trusted optimal routes with trust levels from s to its two-hop neighbors in N 2 (s). Lemma 1. All intermediate nodes on the trusted optimal path are selected as multipoint relays by the preceding Proof. To be selected as an MPR, a node has to prove its authenticity, provide connection between the source node and its two-hop neighbors and have the lowest link cost. 
Case I: The node in N 1 (s) of the source node s does not provide connection to any node in N 2 (s). Node n 2 has no direct connection to node d. The two possible paths from s to d are  Considering the added delay and energy consumption , it is clear that n 2 is not on the optimal path from s to d. 
s → n 1 → d and s → n 2 → n 1 → d. s 1 n 2 n d 
Case II: The node n 2 in N 1 (s) of the source s does not provide proof of authenticity to any node in N 2 (s). Any packet received from it by any node in the N 2 (s) will be dropped. 
Case III: There is a trusted optimal path from source to destination such that all the intermediate nodes on the path are selected as MPRs by their previous nodes on the same path. Suppose that in an optimal path, s → n 1 → n 2 → ... → n k → n k+1 → ... → d, both MPR and non-MPR nodes exist. Also, based on the result of Cases I and II, we suppose that for each node on the path, its next node on the path is its one-hop neighbor, and the node two hops away from it is its two-hop neighbor. 
 1) Suppose that on the optimal route, the first intermediate node n 1 does not meet the criteria for MPR selection by source s. However, n 2 is the two-hop neighbor of s. Based on the basic idea of MPR selection , every two-hop neighbor of s must be covered by its MPR set, then s must select another neighbor as its MPR. In this case, n 2) Assume that on the optimal route s → n 1 → n 2 → ... → n k → n k+1 → ... → d, all the nodes on segment n 1 → ... → n k are chosen as MPR by their previous node, we now prove that the next hop node of n k is on the optimal route is an MPR. 
Suppose that n k+1 is not an MPR of n k . Same as in the previous situation, n k+2 is the two-hop neighbor of n k , so it must have another neighbor n k+1 which covers n k+2 . Since route 
s → n 1 → n 2 → ... → n k → n k+1 → ... → d is an optimal path then s → n 1 → n 2 → ... → n k → n k+1 → ... → d 
is also an optimal path because it has a lower cost. This implies that in an optimal route, the k th intermediate node selects the (k + 1) th node as the MPR. 
Based on I and II, all the intermediate nodes of an optimal path are MPRs of the previous node. 
Theorem 3. For all pairs of nodes s and d, if s transmits a broadcast packet P , d will receive a copy of that packet. Proof. The proof follows on similar lines to 
Assume that n k (k = 2) forwards the packet P to node d. Assume there exists a path n k → nk − 1 → n k−2 → ... → n 2 → n 1 → d. Based on Lemma 1, any packet received by n k−1 from n k must be relayed to n k−2 . Similarly, when n k−2 receives the packet, it forwards it to n k−3 . This repeats until node n 1 receives the packet where it is automatically forwarded to node d. 
Simulation Results
 To simulate the protocol modifications, the NS2 implementation of OEDR was extended to reflect the changes in cost calculation. A new table was added in each node to hold the number of packets sent on each link. The OEDR implementation in NS2 was modified by adding definitions for the malicious nodes and providing a mechanism for authentication. Every packet received is checked in the MAC layer, and any packet from a non-authentic source is dropped. This means that no entries will be added in the one-hop or two hop tables. The simulation scenarios were based on networks of 50 and 200 nodes. The data rates varied from 128 kbps to 4096 kbps with a packet size of 512 bytes. The nodes were stationary in an area of 1000×1000 meters, their locations and flows were randomly generated. The performance of the OLSR, OEDR and TLR was compared based on the end-to-end delay and the energy-delay product. 
Conclusions
With the rapid deployment of wireless networks, security of the routing protocols is essential for reliable operation. The threats presented in this paper indicate that more work is needed to guarantee the privacy and integrity of the data. This is especially important in military and safety critical environments. TLR, an extension of the OEDR protocol, resulted in better management of route selection for security purposes . The simulation results indicate that TLR delivered the packets with a noticeable decrease in the average end-to-end delay. This, however, increased the power consumed when longer routes were selected. The addition of the authentication model in NS2 demonstrated how the TLR protocol dropped nonauthentic control packets. Nevertheless, more work needs to be done to improve the model to enable the analysis of the computational overhead involved in computing the hash fields as well as the bandwidth utilized for the additional bytes inserted into the control packet in the form of the hash code. Another modification to be investigated is the weight calculation equation given in Equation 5. A dynamic model that allows assigning different weights to each factor would be more suitable in cases where, for example, delay is given higher priority than energy consumption. engineer at Engineers India Limited, New Delhi, as a Research Associate and Instructor from 1990 to 1991, at the University of Manitoba, Winnipeg Canada, and worked at Systems and Controls Research Division, in Caterpillar Inc., Peoria as a consultant during 1994 to 1998. During 1998 to 2001 he was at the University of Texas at San Antonio , and since September 2001, he is at the University of Missouri-Rolla where he is currently a professor and Site Director for the NSF Industry/University Cooperative Research Center on Intelligent Maintenance Systems. He has coauthored more than 180 refereed conference and juried journal articles and several book chapters and three books entitled "" Neural network control of robot manipulators and nonlinear systems "" , published by 
"
"Introduction
In vehicular ad hoc networks (VANETs) is a subset of mobile ad hoc networks (MANETs), which uses mobile vehicles as network nodes in order to enable communication . Such network nodes include both onboard units (OBUs) those equipped with in mobile vehicles and road side units (RSUs) those mounted on stable units (traffic posts etc.). These network nodes communicate among each other to so that they can access the application server for retrieving services. In general, services provided via VANETs include traffic information for drivers, such as traffic accident, traffic condition, weather forecast and multimedia infotainment dissemination, etc. 
 With this in mind, this paper proposes a novel communication protocol based on group signature to tackle the conditional privacy presentation and authentication for VANETs, called ECPB. Differing from the existing group signature based schemes, ECPB uses validity as a substitute for CRL checks. In other words, it is focused on rectifying problems caused by CRL, such as the overhead involved in storing, communication, updating and checking process. 
Remark 1. Unlike the existing schemes based on Pseudonym, our communication protocol(ECPB) does not require each vehicle to store a large number of keys and anonymous certificates, and so the storage overhead of our scheme is lower. Also ECPB guarantees anonymity and traceability, as it is based on group signature scheme, which is not found in VAST. Comparing to SPRING, ECPB does not require any RSUs for the purposes of authenticating messages, tracing the vehicles. Because of using the validity to be a substitute of checking CRL, it offers faster message authentication. While in CRSB, the time increases linearly as the number of revoked vehicles in the CRL grows when verifying messages. In addition, ECPB can support batch verification. 
√ × √ √ Revocability √ √ √ √ Efficient verification × √ √ √ Batch verification × × × √ 
System Model and Security Goals
 In this section, we present the main entities and attributes of VANETs, illustrated in 
System Model
The proposed system model of VANETs consists of a trust authority (TA), service providers (SP), RSU, OBU, as shown in 
SP: service provider, the group manager in the model; the service is chargeable and the group member can pay for a period of validity and then he can use the service in the validity; whose main mission is to authenticate vehicles by providing them with the group public key and group members secrete key for signature and verification. 
RSUs: infrastructure of VANETs, they act as the bridge between SP and OBUs or between two OBUs, connecting SPs by wire and connecting OBUs by a wireless channel respectively. OBUs: a unit that is embedded in vehicles, is the indispensable basic entity in VANETs; this unit is similar to the mobile terminal of communication systems, the hardware security module of it ensures the security of calculation, such as encryption and decryption ; and it is responsible for the communication of vehicles and RSU, and periodically broadcasts trafficrelated status information. 
Security Goals
Authentication: Node authentication, helps users to ensure that the node identity information to which they establish communication is real. Non-repudiation: Authenticated vehicles cannot deny messages after sending to the VANETs. Anonymity: Other vehicles and adversaries in VANETs cannot identify the sender's identity. Traceability: Manager of VANETs can identify the real identity of the senders for malicious and controversial messages. 
Forward and backward security revocation: Vehicle cannot access the services both before authentication and after revocation; other vehicles cannot access the services as an impostor. 
Preliminaries
 In this section, we briefly introduce the statistical techniques used in our protocol. 
Bilinear Pairing. Both G 1 and G 
 2 are two (multiplicative ) cyclic groups of prime order q. g 1 is the generator of G 1 , g 2 is the generator of G 2 . ψ is a computable isomorphism from G 2 to G 1 , with ψ(g 2 ) = g 1 . e is a bilinear map, and e : G 1 × G 2 → G T , satisfies the following rules: 
1) Bilinear: e(ag 1 , bg 2 ) = e(g 1 , g 2 ) ab , for all g 1 ∈ G 1 , g 2 ∈ G 2 and a, b ∈ Z * q . 2) Non-degenerate: e(g 1 , g 2 ) = 1. 3) Computable: e(g 1 , g 2 ) is efficiently computable, and e(g 1 , g 2 ) = e(g 2 , g 1 ). 
 Zero-knowledge Proof. Alice and Bob are two participants , and Alice has a secret M. Alice communicates with Bob to demonstrate that she has a secret but do not tell Bob any other messages of the secret. It means that Bob knows that Alice has a secret, but he does not know what the secret is. Strong Diffie-Hellman Hypothesis. Let G 1 , G 2 be cyclic groups of prime order p, where possibly G 1 = G 2 . Let g 1 be the generator of G 1 , g 2 be the generator of G 2 . Given (g 1 , g 2 , g r 2 , . . . , g rq every vehicle will achieve a unique identity V id from TA during vehicle registration, including an electronic license, legal certificate Cert V id , and a pair of public and secrete key (V sk , V pk ). Before providing services, SP submit an application to TA, and will obtain a unique identity S id , legal certificate Cert Si and a pair of public and secrete key (S sk , S pk ). The notations used in the following scheme are listed in 
System Initialization
 As the basis of the system group initialization, TA initiates the bilinear parameter (G 1 , G 2 , G T , e, g 1 , g 2 , q, ϕ, H), where e is a bilinear pair G 1 × G 2 → G T , and all groups G 1 ,G 2 ,G T are multiplicative cyclic groups of the prime order q. g 1 is the generator of G 1 , g 2 is the generator of G 2 , and ϕ(g 2 ) = g 1 , Hash Function is H : {0, 1} * → Z * q . 
Operation of ECPB
This subsection details the operation of our scheme. The operation includes five parts such as membership application (
1) SP chooses random numbers h ∈ G 1 and k 1 , k 2 ∈ Z * q , given u k1 = v k2 = h, where u, v ∈ G 1 , thus all g 1 , u, v, h ∈ G 1 . 
2) SP chooses hash function H 1 : {0, 1} * → Z * q , uses the unique identity S id , computes r = H 1 (S id ) and w = g r 2 , then g 2 , w ∈ G 2 ; and then chooses another hash function: H 2 : {0, 1} * → Z * q . SP provides group public parameter {u, v, h, w, H 2 }, where the master key is gmsk = (k 1 , k 2 ), and the public key of the group signature system is gpk = (g 1 , g 2 , h, u, v, w, H 2 ). Membership Application. SP broadcasts its service information {S pk , Cert Si , S id , Sig S sk (S id )}, where S pk is the public key, Cert Si is the PKC, S id is its unique identity, Sig S sk (S id ) is its signature. When a vehicle receives a message, it executes it as follows: 
1) The vehicle identifies the legality of S pk through its Cert Si , then verifies the validity of Sig S sk (S id ) by using S pk to confirm that the message source is real, not falsifying. 
2) The vehicle sends its application information 
{V pk , Cert V i , V id , Sig V sk (V id ), T i } 
to SP, where V pk is the public key of the vehicle, Cert V i is the PKC, V id is its unique identity, Sig V sk (V id ) is the signature, and T i is the membership va- lidity. Membership Registration. Receiving an application message from a vehicle, the operations of SP are illustrates as follows: 
1) SP identifies the legality of V pk through Cert V i , and then verifies the validity Sig V sk (V id ) by using V pk to confirm that the message source is real, not falsifying. 
2) SP selects a random number v i , computes 
x i = H 1 (V id v i ), gives f i = g 1/(xi+r) 1 
, then chooses a random number s i , computes 
s i = H 2 (s i ), then computes t i = H 2 (T i s i ), and gives f i = (f i ) ti 
as the group identity information of the vehicle, which establishes the corresponding relationship with V id , and stores it. The group membership secrete key of the vehicle is gsk
= (x i , f i , s i ), and it encrypts E v pk (x i , f i , s i ) 
by its public key V pk , then sends it to vehicle V id . Vehicle Safe Message Generation. Each vehicle in VANETs generates the signature on message M ∈ {0, 1} * before sending it. In our scheme, we take a common vehicle who has become a group member which obtain gpk = (g 1 , g 2 , h, u, v, w, H 2 ) and gsk
1) The vehicle checks the validity of T i . If T i is invalid, then the vehicle sends a new application to SP for a group member. If it is valid, it will compute s i = H 2 (s i ) initially and then computes t i = H 2 (T i s i ). 
2) The vehicle selects random numbers α, β, r α , r β , r x , r γ1 , r γ2 ∈ Z q and computes: 
A 1 = u α ; A 2 = v β ; A 3 = f i · h α+β ; γ 1 = x i · α; γ 2 = x i · β; R 1 = u rα ; R 2 = v r β ; 
R 3 = e(A 3 , g 2 ) rx · e(h, w) −rα−r β · e(h, g 2 ) −rγ 1 −rγ 2 ; R 4 = A rx 1 · u −rγ 1 ; R 5 = A rx 2 · u −rγ 2 ; 
Then it computes: 
λ = H(M, A 1 , A 2 , A 3 , R 1 , R 2 , R 3 , R 4 , R 5 ). 
3) The vehicle gives λ = λ/t i , then computes. 
s α = r α + λ · α; s β = r β + λ · β; s x = r x + λ · x; s γ1 = r γ1 + λ · γ 1 ; s γ2 = r γ2 + λ · γ 2 ; 
Based on the above computations, signature σ of M is (A 1 , A 2 , A 3 , λ, s α , s β , s x , s γ1 , s γ2 , s i , T i ). Now M and σ are broadcasted, and the concrete format of broadcasted message is shown in 
ID m t m σ M 
 Message Verification. Based on the strong Diffie- Hellman Assumption, group member authenticates a signature by using the zero-knowledge proof. It means that without the identity f i and other secrete information of the sender, such as x i , s i , verifiers can validate the legality of the senders. In order to prevent a message from replay attacks, the freshness of Algorithm 1 Message Verification Require: gpk = (g 1 , g 2 , h, u, v, w, H 2 ), M, σ = (A 1 , A 2 , A 3 , λ, s α , s β , s x , s γ1 , s γ2 , s i , T i ). 1: Begin 2: if T i is invalid then 
3: 
Drop the message; 4: else 
5: 
Compute: t i = H 2 (T i s i ); 
6: 
Set λ = λ/t i 
7: 
Compute: 
R 1 = A −λ 1 · u sα R 2 = A −λ 2 · v s β R 3 = e(A 3 , g 2 ) sx · e(h, w) −sα−s β · e(h, g 2 ) −sγ 1 −sγ 2 ·(e(A 3 , w) 1/ti /e(g 1 , g 2 )) λ R 4 = A sx 1 · u −sγ 1 R 5 = A sx 2 · v −sγ 2 8: Verify: λ ? = H(M, A 1 , A 2 , A 3 , R 1 , R 2 , R 3 , R 4 , R 5 ) 9: 
if true then 10: 
Accept M; 11: else 12: 
Drop the message; 
13: 
end if 14: end if 15: End t m is verified upon receiving the corresponding message , as illustrated in Algorithm 1. Traceability of Controversial Message. Upon receiving a controversial message, it is necessary to find out the real identity of the sender. The group manager will first verify whether the sender's M and σ are real and correct, similar to the verification process of Algorithm 1. Then, using the master key gmsk = (k 1 , k 2 ), the real identity 
A 3 /(A k1 1 · A k2 2 ) = f i of 
the sender is computed, thereby identifying the corresponding vehicle V id from the storage list. 
Batch Verification
Our proposed scheme supports batch verification, which helps improving the signature verification efficiency . Now, R 3 has been given in the new signature Sig n (M ) in advance, so it just needs to be verified and not to be calculated. Suppose that a vehicle receives n messages, the batch verification process of the traffic messages is executed as shown in Algorithm 2( τ 1 , . . . , τ n is random vector, and τ j ∈ Z q ). Successful completion of this batch verification allows the validation of n messages together. 
Sig n (M j ) = (A j,1 , A j,2 , A j,3 , R j,3 , λ j , s j,α , s j,β , s j,x , s j,γ1 , s j,γ2 , s j , T j ), 1 ≤ j ≤ n. Algorithm 2 Batch Verification Require: gpk = (g 1 , g 2 , h, u, v, w, H 2 ), M , Sig n (M j )(1 ≤ j ≤ n) 1: Begin 2: Compute: t j = H 2 (T j s j ); 3: while j ≤ n do 4: R j,1 = A −λ j j,1 · u sj,α 5: R j,2 = A −λ j j,2 · v s j,β 6: R j,4 = A sj,x j,1 · u −sj,γ 1 7: R j,5 = A sj,x j,2 · v −sj,γ 1 8: Verify: λ j ? = H(M j , A j,1 , A j,2 , A j,3 , R j,1 , R j,2 , R j,3 , R j,4 , R j,5 ) 9: 
j=j+1 10: end while 11: Give θ j = λ j /t j 12: Verify: 
e( n j=1 (A sj,x j,3 · g −θj 1 · h −sγ 1 −sγ2 ) τj , g 2 )· e( n j=1 (A θj j,3 · h −sj,α−s j,β ) τj , w) ? = n j=1 R τj j,
 5 Security Analysis and Performance Evaluation 
In this section, we present the security analysis and performance evaluations of our scheme. 
Security Analysis
 Group signature algorithm is not detailed in this section , as it is out of scope this paper. A detailed description of this algorithm can be found in the works of 
Correctness proof. When the verifier received 
gpk = (g 1 , g 2 , h, u, v, w, H 2 ), M , σ = (A 1 , A 2 , A 3 , R 3 , λ, s α , s β , s x , s γ1 , s γ2 , s i , T i ), he can calculate the correct value of R 1 , R 2 , R 3 , R 4 , R 5 , if T i is a real validity, then λ ? = H(M, A 1 , A 2 , A 3 , R 1 , R 2 , R 3 , R 4 , R 5 ) 
will be verified, and based on λ and other parameters, and R 3 can be calculated, which will be equal to the R 3 in signature. 
The correctness proof process is as follows: 
e(A 3 , g 2 ) sx · e(h, w) −sα−s β · e(h, g 2 ) −sγ 1 −sγ 2 · (e(A 3 , w) 1/ti /e(g 1 , g 2 )) λ =e(A 3 , g 2 ) rx+λ x · e(h, w) −rα−λ α−r β −λ β · e(h, g 2 ) −rγ 1 −λ γ 1 −rγ 2 −λ γ 2 · e(A 3 , w) λ/ti · e(g 1 , g 2 ) −λ =e(A 3 , g 2 ) λ x · e(h, w) −λ α−λ β · e(h, g 2 ) −λ γ 1 −λ γ 2 · e(A 3 , w) λ · e(g 1 , g 2 ) −λ · e(A 3 , g 2 ) rx · e(h, w) −rα−r β · e(h, g 2 ) −rγ 1 −rγ 2 =e(A 3 λ , wg 2 x ) · e(h −λ α−λ β , wg 2 x ) · e(g 1 , g 2 ) −λ · e(A 3 , g 2 ) rx · e(h, w) −rα−r β · e(h, g 2 ) −rγ 1 −rγ 2 =e(f i , wg 2 x ) λ · e(g 1 , g 2 ) −λ · e(A 3 , g 2 ) rx · e(h, w) −rα−r β · e(h, g 2 ) −rγ 1 −rγ 2 =e(g 1 , g 2 ) λ · e(g 1 , g 2 ) −λ · e(A 3 , g 2 ) rx · e(h, w) −rα−r β · e(h, g 2 ) −rγ 1 −rγ 2 =e(A 3 , g 2 ) rx · e(h, w) −rα−r β · e(h, g 2 ) −rγ 1 −rγ 2 =R 3 R 1 =A −λ 1 · u sα = u −λ α · u rα+λ α = u rα R 2 =A −λ 1 · v s β = v −λ β · v r β +λ β = v r β R 4 =A 1 sx · u −sγ 1 = (u α ) rx+λ x · u −rγ 1 −λ αx =A 1 rx · u −rγ 1 R 5 =A 2 sx · v −sγ 2 = (v β ) rx+λ x · v −rγ 2 −λ βx =A 2 rx · v −rγ 2 
In a similar way, the batch verification process can also be validated. Security proof. When a false user attempts to use an expired membership, he must forge a false validity T in advance. As the Hash Function is collision resistant , it is probably impossible that the false T can be an equivalent to the true T . During the signature verification, when someone sends his M , σ and T to the verifier, the verifier initially checks T . If this is not valid, the signature verification process cannot be progresses any further, otherwise, the verification process can be carried out as follows: 
1) t i = H 2 (T i s i ) 
2) Verify the equation: 
e(A 3 , g 2 ) sx · e(h, w) −sα−s β · e(h, g 2 ) −sγ 1 −sγ 2 · (e(A 3 , w) 1/ti /e(g 1 , g 2 )) λ ? = R 3 If t i is not equal to t i , e(f · wg x 2 ) 1/ti = e(g 1 , g 2 ) 
will be an impossible equation, thus the equation could not be verified and the verification process will be terminated here. And so the false validity cannot regain the membership and the vehicle needs to reapply for it. In this way, the forward and backward secure revocation is achieved in VANETs. It means that the false vehicle cannot access the services after revocation, and other vehicles cannot access the services as an impostor either. In a similar way, the batch verification process of the Security proof can be validated. 
Performance Evaluation
 In this section, firstly, we define the time complexity of the cryptographic operations required between our scheme and other existing schemes. Let m denotes the number of group member, N crl denotes the number of CRL items, T mul denotes the time to compute one point multiplication, T mac denotes the time of one message authentication code operation, T par denotes the time to perform one pairing operation, T exp denotes the time to compute one exponentiation. We consider the time of the four important operations above but neglect the time of the other operations such as additive and one-way hash function in this evaluation. Here, we adopt the experiments in paper 
AD msg = N i=1 M j=1 (T s + T t + αT C + (1 − β)T v ) + βT b N M α = 0 
the scheme does not need to check the CRL 1 the scheme needs to check the CRL β = 0 the scheme cannot support batch verification 1 the scheme can support batch verification where AD msg represents the average delay, N represents the total number of vehicles, M is the number of messages sent by a vehicle, T s is the signature time for a message, T t is the transmission time for a message , T C is the CRL checking time for a message, T v is the verification time for a message, and T b is the batch verification time for all the messages. As shown in 
Conclusion
In this paper, we introduce a new scheme (ECPB) based on group signature for privacy-preserving in VANETs. In our scheme, the validity of membership is required when a vehicle applies for group membership and the validity is used to check whether the requesting vehicle is genuine or not, This validation process can deployed as a substitute for CRL checks. Also, our proposed scheme supports batch authentication of the messages. The security analysis and experimental results show that ECPB delivers the higher efficiency verification requirements of VANETs, and also satisfies the Privacy-preserving Communication for VANETs. 
"
"Introduction
 We prove our identities everyday by showing the possession of access tokens. Using a key to open a lock may be the most common form, which has about 4000 years of history since ancient Egypt. As one may access many locks, traditional master keys were designed to enable accessing multiple locks with a single key. Nevertheless, master keys are not widely used. Instead, people carry multiple access tokens for entity authentications, for example, keys, magnetic stripe cards, smart cards, RFID tags, and other tokens. We propose the Master Key, which is a novel approach for digital access tokens to have the advantages of both master keys and multiple access tokens. In this paper, we use the term, key, for an access token or a digital access token and the term, lock, for a digital lock or a computing resource. Traditional master keys are convenient. One does not need to carry many keys and memorize relationships between keys and locks. However, traditional master keys have fatal problems that are not suitable for everyone's daily use. The delegation of a master key equals delegating access to all locks that one has privilege to access. Revocation of a master key to access a lock is costly because the lock and the keys of other owners need to be replaced. If an intruder acquires a master key, then the intruder may open many locks. In addition, locks that support master keys are vulnerable to the malicious insider who has a normal key 
We formally define the Master Key design problem. The mathematical properties of our encoding scheme are proved. Different exposure strategies and their respective advantages and disadvantages are discussed. We present three representative protocols for different key-lock relationships . The protocols are formally verified using BAN logic and our extension to ensure the protocols' freshness, binding, and privacy properties. Our implementation on PDAs shows that the approach is efficient. An authentication process between the Master Key and a lock is less than half a second. The rest of the paper is structured as follows. In Section 2, we discuss related work. Then in Section 3, we illustrate taxonomy of the key-lock interaction design and the requirements for the Master Key design. Section 4 presents the design and analysis of the automatic key selection process using code words. In Section 5, we discuss the support for various exposure orders and amount of exposure between keys and locks. In Section 6, we demonstrate three protocols for key-lock interactions with different requirements. We show the implementation and performance measurements of our protocols in Section 7. We discuss some related issues in Section 8. Last, in Section 9, we outline our future work and conclude our contribu- tion. 
Related Work
 Several technologies are used and embedded in small devices for entity authentication. Magnetic stripe cards are widely adopted as hotel guestroom keys, bankcards, and employee badges 
Key-Lock Interaction
 Since we have not seen any systematic analysis and comparison of digital key-lock interaction designs, we present taxonomy of existing designs. There are many choices when designing the Master Key. We discuss the potential choices for the Master Key to achieve good usability, private authentication, and security. Then, we formally define the Master Key design problem. 
 3.1 Taxonomy of the Key-lock Interac- tion 
The Master Key and Lock Interaction
To meet different key-lock interaction requirements and constraints, the Master Key supports all key-lock interactions except reactive and one-way authentication. Keys with reactive initial status are susceptible to attacks as discussed in Section 3.1. For contactless communication, the Master Key requires users to actively start an authentication process. For contact communication, the Master Key will be passive. We assume that the Master Key has appropriate physical and wireless channels to support passive and active keys. We require the Master Key and locks to mutually authenticate each other (no one-way authentication ). Thus, the Master Key can ensure that a key is exposed to the intended lock. Combining all characteristics together, 
{K L M } m ∈ N . 
The Master Key and lock achieve strong authentication via mutual authentication such that a lock believes 
K i ∈ {L L M } and 
the Master Key believes the lock is the intended lock, 
L j = L intended . 
Based on their requirements between a lock and the Master Key, they may choose partial or full exposure in a message. Therefore, the probability 
p(K i ∈ {K L M }) = a, a ∈ (0, 1) and the probability p(L j = L intended = b, b ∈ (0, 1)). 
 The Master Key and a lock may predetermine who exposes authentication information first. When a user operates a lock, L intended , the Master Key automatically selects the key, 
K L intended , K L intended ∈ {K i } i ∈ N . 
 4 Discover the Proper Key to Operate a Lock 
There are two situations. In the simpler situation, a lock actively notifies the Master Key its information. The Master Key can search all the key-lock pairs based on the lock's information. Thus, the Master Key can select the proper key, 
K L intended , K L intended ∈ {K i } i ∈ N 
, for authentication. This method can be applied to locks that require digital keys to physically contact them (passive initial status). The event that a lock and a key physically touch each other triggers the lock to notify the Master Key its information. 
For a key and a lock that communicate over wireless channels (contactless), a lock might continuously broadcast authentication messages. Nevertheless, locks waste almost all their power and communication because they are not being operated most of the time. For locks running on batteries such as Remote Keyless Entry on cars, the method may not be acceptable. When some locks do not continuously broadcast authentication messages, the Master Key has to actively discover locks. In the rest of the paper, we focus on this more challenging situation. The Master Key initiates a contactless entity authentication (active initial status). Locks are in the listening mode. (A battery powered lock may switch between sleep mode and listening mode periodically to save power.) When a user pushes a button to lock and unlock, the Master Key broadcasts a discovery message to query whether there is any lock in the vicinity that it can operate. The message is in the code word form. If a lock understands and recognizes the Master Key, it replies back. Then, the Master Key searches all the key-lock pairs and verifies the lock. If a proper key is found, the Master Key submits the proper key, 
K L intended , K L intended ∈ {K i } i ∈ N 
, for authentication. 
Private Discovery by Speaking Code Words
 In pervasive computing environments, a user may authenticate with different parties in unprotected network environments. If identities are exchanged in clear text, malicious attackers may infer sensitive information and associate it with the identities. To mitigate the threat, the Master Key and locks exchange only code words. A code word is generated at both the Master Key side and a lock's side. It is calculated from a secret shared between the Master Key and the lock. The shared secrets are unique. (We will discuss more details about shared secrets in Section 6.) The Master Key operates many locks at different places. Without knowing the existence of a lock, it broadcasts all potential code words to locks in the vicinity. The code words are encoded in an array as shown in 
Generating and Verifying One-time Code Words
Figure 4 illustrates the generation of a code word in an array. All bits in the array are initially set to zero. A time variant parameter (TVP) and the shared secret are the two inputs to h( * ). A TVP consists of a timestamp and a random number. We assume that the Master Key and the locks have loosely synchronized clocks, and thus they do not need to maintain large caches to verify whether or not a TVP is fresh. Function h( * ) is the hash-based message authentication codes (HMAC) proposed in 
.... Chunkk 
Mathematical Properties of the Code Words
p(codewordmatch|notkeyowner) = ( m n ) k . 
(1) 
Proof. 
1) The first part means if the Master Key has a key to operate a lock, the lock will always find the code word match in the array. No false negative case is a property of the Bloom filter. In our case, the Master Key and the lock use their shared secret and the same TVP as inputs. The hash results are the same. The Master Key uses the hash results to set the bits in the array and the lock uses the bits to verify. So, they will always find matches. 
2) False positive match is possible because the bits that are set to 1 in the array are not exclusive. When a lock finds a code word match, the same bits may be set by other code words or the random bits that the Master Key generates. If the code word bits follow uniform distribution, the false positive rate in our case is a typical sampling with replacement problem. By our design, the number of bits set and the array length, m/n, is a fixed ratio. Given that the array is not generated by a key owner, the probability to find a code word match is = ( m n ) k . Examining Equation 1, we find that a key-lock pair can control their false positive rate by selecting the code word length. When the length of a code word increases, the false positive rate decreases as shown in 
Proposition 2. The probability to find a hash result from an array of code words is 
1 n 2 k=n 1 ( m! 
(m−k)! ) 
, where k is a code word length and m is the number of bits set in the array. 
Proof. If an eavesdropper does not know the code word length, he needs to try different lengths from k = n 1 to k = n 2 . For each k, an eavesdropper needs to select k bits from m bits and then permutate the k chunks to guess a hash result. Assume the code word bits follow uniform distribution, there are ( m! (m−k!) ) permutations. Therefore, the probability to find the hash result is 
1 n 2 k=n 1 ( m! (m−k)! ) . 
Note that the hash result found may be only part of the hash result because code words may not use all the chunks to set the bits in the array. 
 4.4 Code Words Follow Uniform Distribution Over an Integer Set 
When we analyze the mathematical properties of the code words, we assume that the values of the chunks of the hash results follow the uniform distribution over an integer set 
shows the test results. If the value is greater than 8402.5, then it is significant, which means we reject the hypothesis . There is only one test, chunk 2 for the hash result using SHA-1, is insignificant. However, it may be given a false result. Because, given that the significance level is 5 percent, it seems reasonable that 1 out of 21 tests is false. Thus, we do 20 runs to test the chunk again. 
 5 Choose the Amount of Exposure and Exposure Orders 
Our code word encoding scheme is flexible that the Master Key and a lock can determine their amount of exposure based on their requirements. They may choose from partial exposure to full exposure. Meanwhile, their amount of exposure is independent of other code words in the array because an array always has the fixed ratio of bits set. To control the amount of exposure, the Master Key and a lock can simply choose the number of bits used in the code word. The more bits used the more precisely the Master Key exposed. If the code word is very precise (very low false positive rate), it is considered as full exposure. If a lock has privacy concerns or power constraints and only wants to expose to its key owners, the Master Key may specify a precise code word. If the Master Key wants to make sure a certain lock is in the vicinity before its precise exposure, the Master Key may specify a partial code word with few bits. The use of partial code word changes the order of precise exposure from a key to send authentication information first to a lock to send authentication information first. Precise exposure at a later time has the advantage to protect privacy. If there is any mismatch and unnecessary exposure, a party that exposes later can avoid exposure. If both the Master Key and a lock have concerns and want the other party to expose first, they may progressively expose partial code words and verify each other. We described this approach and analysis in 
ability p(K i ∈ {K L M }) = a, a ⊂ (0, 1) and the probability p(L j = L intended ) = b, b ⊂ (0, 1). 
From a lock's perspective, the preciseness that the Master Key exposes is determined by the probability , p(keyowner|codewordmatch). That is the probability that the code word is generated by a true key owner, if the lock finds a code word match. Based on the definition of conditional probability, it equals to p(keyowner|codewordmatch)/p(codeword, atch). After applying multiplication law and law of total probability to the numerator and denominator respectively, we get the following equation. (It is similar from the Master Key's perspective.) 
p(keyowner|match) = p(match|keyowner) ×p(keyowner)/p(match|notkeyowner) ×p(notkeyowner) + p(match|keyowner) ×p(keyowner) 
(2) 
where p(keyowner) is the percentage of key owners among all people who send discovery messages at a place; p(codewordmatch|notkeyowner) is the false positive rate; and is as we discussed in Proposition 1. Note that Equation (2) is only valid at the lock's place. At other places, p(keyowner|codewordmatch) is not defined because p(keyowner) is unknown. 
 6 Mutual Authentication Protocols for Different Key-Lock Types 
In this section, we present protocols for three types of key-lock relations. A unique key is owned by one or a few owners to open a lock. A lock and the Master Key share a unique secret. An individual key is that its key owner can be identified among a group of owners. Beside the unique shared secret for the group as the unique key, a lock and the Master Key share an individual secret with each owner. A group key is also owned by a group of people. The lock is able to verify a key, but key owners are not differentiable. They may share some plain text to discover each other. The initialization processes for all types of keys are the same. We assume that shared secrets are delivered from locks to the Master Keys via secure channels. Locks indicate the number of bits for code words. 
The Unique Key
The protocol for the unique key is shown in 
The Individual Key
Some locks need to identity individual key owners among many key owners. The challenge for the Master Key design is that the Master Key holds many keys and a lock has many key owners. If we directly use the unique key approach, locks find that many false positive cases happen . To address the problem, we use domain secrets and individual secrets. A domain secret is shared and used by all key owners to discover the lock, whereas an individual secret is used by the lock to identity an individual key owner. The individual secret is only shared between a key owner and the lock and it is unique. 
BF M (R M , t M , S Unique ), R M , t M 2 LM: R M , t M , MB L , R L , t L , Hash L (R M , t M , S Unique ) 3 ML: R L , t L , Hash M (R L , t L , S Unique ) 
(a). The protocol for the unique key. Msg No. Sndr/Rcvr Message 1 ML: 
BF M (R M , t M , S domain ), R M , t M , 2 LM: R M , t M , MB L , Hash L (R M , t M , S domain ), (Hash L (R M , t M , S domain )) KL 1 , BF L (R L , t L , S individual ), R L , t L 3 ML: R L , t L , MB M , Hash M (R L , t L , S individual ) 
(b). The protocol for the individual key. Msg No. Sndr/Rcvr Message 1 ML: 
BF M (R M , t M , S PlainText ), R M , t M 2 LM: R M , t M , MB L , R L , t L , Hash L (R M , t M , S Group ) 3 ML: R L , t L , Hash M (R L , t L , S Group ) (c). 
The protocol for the group key. are generated from the individual secrets using the same approach that the Master Key generates the code words. Furthermore, the lock sets random bits in the array to reach a fixed ratio of bits set. In the third message, the Master Key indicates its identity by specifying the matched code word and supplies the hash result as the key. 
The Group Key
 Unlike the individual key, the group key has the requirement that a lock cannot differentiate key owners from their keys. This means that all key owners should have the same key. Nevertheless, the Master Key initiates the authentication process. A malicious lock may provide different key owners with different keys. Based on the keys, the lock differentiates key owners and responds accordingly . We suggest the following two approaches. The Master Key and a lock may use the unique key protocol. But a key owner only speaks few bits of a code word. A short code word ensures that a lock cannot differentiate among key owners. Because when there are two different code words of length 1 or 2 bits, it is very likely that the lock will find a false positive match and a true match in the code word array. If the lock replies with an incorrect hash, the key owner knows that he has a different key than other owners. If the overhead caused by the false positive cases is large, a lock and its owners may use more bits for the code word. However, the code word is generated from some plain text, as shown in 
Revocation
 To revoke a unique key, a lock invalidates the shared secret . If there is more than one key owner, a new shared secret needs to be delivered to the other key owners. To revoke an individual key from a key owner, a lock inval-idates the individual secret, while notification of a new domain secret to other key owners may not be imminent. To revoke a group key from a key owner, all other key owners need to update their group keys. Not all key owners are online all the time, and thus timely delivery of the new key may be a problem. If an owner updates his key when he finds that the key has expired, the lock may be able to determine the owner's identity because he has just updated his key. We are designing an approach, which is similar to sending email to a group of recipients, so that a new group key is dispatched to all key owners at the same time. 
Formal Verification of the Protocols
We use BAN logic 
P ∞ y Q: P 
shares a secret Y with Q. Q may be an individual or a group. 
(M ⊂ G): M is a member of group G. 
P 
The message-meaning and nonce-verification rules are extended as follows: 
P Y ∞ ,P [CW ]Y 
P |≡Q| CW : When P finds a matched code word, P knows that Q once said it. If Q is a member of a group, P only knows that one member (including P ) once said the code word. If a hash is received, P is sure that Q said it. If the code word is received, P knows there is a probability that Q said it. 
P |≡Q| CW,,(CW ) P |≡Q|≡CW 
: Based on the freshness of the code word, P believes that the code word or hash is fresh. Since the verifications of the protocols are similar, we only discuss the protocol for the individual key type. Following the BAN logic's procedure, we convert the protocol to an idealized protocol as shown in 
K i ∈ {K L m }, and 
the Master Key believes that the lock is the intended lock, L j = L intended . 
Performance Measurement
We implemented our protocols and measured performance on a set of PDAs. Handheld devices such as cell phones or PDAs are good candidates for the Master Key, since people regularly carry them. Locks may have diverse processing and communication capabilities. Some may have limited processing power, while others may be powerful to support hundreds of key owners. We used a Compaq iPAQ as the Master Key (an ARM SA1110 206 MHz processor , 64MB RAM, and a D-Link DCF-650W wireless card) and a Dell AXIM X5 as a lock (Intel PX250 400 MHz processor, 64MB RAM, and a Dell TrueMobile 1180 wireless card). The PDAs run Microsoft PocketPC 3.0, and the wireless cards are set to 2Mbps in the 802.11 ad hoc mode. 
Discussion
 During our design of the Master Key, an interesting question was raised: when there are multiple locks at a place and the Master Key owner has the privileges to access them, to which lock should the Master Key send the operation code? For example, suppose Bob walks to his office and pushes a button on the Master Key to unlock the door. However, the mailroom is also close by. Which door should the Master Key unlock? If the antenna on the Master Key is directional, then the door at which the Master Key points is unlocked. Nevertheless, if Bob also uses the Master Key to unlock his computer in the office, then the office door and the computer may in the same direction. Should both the computer and the door be unlocked? It may be convenient that both the door and the computer are unlocked. Or perhaps the Master Key may store a rule such that unless the office door is unlocked , the computer will not be unlocked. Alternatively, the Master Key may utilize location or proximity information if such information is available. Only if the Master Key is within a certain distance, the computer accepts the unlock command. For a greater challenge, if Bob has two cars parked side by side outside his house, which car should be unlocked when Bob pushes the unlock button on his Master Key? The Master Key may list the locks that reply back in the order of the frequency of the locks' 
1 BF M {R M , t M } L| ≡ (⊂ G)| ≡ BF M 2 Hash L {R M , t M }, Hash L {R L , t L }} −1 KL , BF L {R L , t L } M | ≡ L| ≡ Hash Sdomain , M | ≡ L| ≡ BF L 3 Hash M {R L , t L }, L| ≡ M | ≡ Hash Sindividual (b) Assumptions M | ≡ M Sindividud ↔ L, L| ≡ M Sindividud ↔ LL, M | ≡ L Sdomain ↔ LL, L| ≡ M Sdomain ↔ LL L| ≡ (t L ), M | ≡ (t L ), L| ≡ (t M ), M | ≡ (t M ), L| ≡ (R L ), M | ≡ (R L ), L| ≡ (R M ), M | ≡ (R M ), M | ≡ K → L 
Conclusion and Future Work
 In this paper, we propose the Master Key approach for entity authentication in pervasive computing environments. Our approach improves usability such that a person carries one device for various authentication purposes while it maintains the favorable properties of carrying multiple access tokens. The Master Key exchanges code words with locks securely and privately and supports various key-lock interactions. Users do not need to remember the relations between keys and locks. The current design of the Master Key does not support multiple groups of key owners. Thus, a lock will find up to one code word that matches. However, if multiple groups are supported in a lock, the lock may find multiple code words that match several groups due to the false positive cases. The lock could determine the false positive cases by establishing multiple sessions and exchanging messages with the Master Key, but in the group key type this violates the privacy feature, and thus the Master Key owner will not be sure that his shared secret with the lock is the same as other owners. We are designing an approach to support multiple groups, while the group key type still maintains its desirable privacy feature. In the meantime, we are designing an approach to make the revocation of a group key easier. The Master Key needs contact and contactless interfaces for various locks. The current design requires users to identify the correct interface to access a lock. Different interfaces may reduce the usability or might even cause authentication errors if a user infrequently accesses a lock. Proper user interfaces or visual hints may help users to choose the proper interfaces. This is an interesting issue and it is out of the scope of this paper. The Master Key will be further extended to function more than just a set of keys. It may possibly represent a person's real life roles. For instance, the Master Key may be used as a remote control for a TV. Therefore, the TV channels that are available to Bob may be different from what are available to his son. Nevertheless, the more functions that the Master Key supports, the more challenging it is for automatic authentication. 
"
"Introduction
Mobile Ad Hoc Networks have undergone incredible growth of popularity during the last years. One of the most practical example of these networks is Vehicular Ad Hoc Network (VANET). The use of wireless communication in VANET implies an always increasing number of potential applications in these networks such as driving assistance, road traffic information or emergency braking alert. All these applications need to exchange data with other vehicles that may be related to the driver safety. The need of confident communications between such critical applications becomes obvious. One possible threat is the creation of multiple fake nodes broadcasting false information. This attack is known as the Sybil attack 
To present our contributions, this paper is structured as follows. Section 2 presents related works concerning Sybil nodes detection techniques. Section 3 provides a geometrical analysis and discussions of Sybil attacks. Section 4 presents our relative localization technique, based on received signal strength gradient. In Section 5, we present our technique to detect intruders and malicious nodes within VANET, by evaluating the distinguishability degree between nodes. Section 6 presents analysis, simulations and real tests in order to validate our contributions and finally Section 7 concludes this paper with our aimed future work. 
 2 Existing Sybil Detection Ap- proaches 
The Sybil attack was first described and formalized by Douceur in 
Notations
Let us consider S and R be two mobile nodes such that S sends some messages received by R. We assume that the transmission of a single message is immediate, allowing to consider the positions of S and R as fixed points of the space at the time of transmission. We denote by d(S, R) the distance between S and R. Let denote by P snd the sending power of the node S. With an isotropic antenna of gain G snd and for d(S, R) sufficiently large, the node R will receive a power P rcv equals to: 
P rcv = P snd × G snd × G rcv × λ 2 16π 2 × d 2 (S, R) , 
where λ denotes the wavelength of the radiation. By denoting G SR = G snd ×G rcv ×λ 2 /(16π 2 ) the gain of the link from S to R, the maximal power P max rcv (dist(S, R)) at distance d(S, R) from the sender can be rewritten as: 
P max rcv (d(S, R)) = P snd × G SR × 1 d 2 (S, R) (1) 
By taking into account signal attenuation, the power received by R is smaller than P max rcv : 
P rcv (d(S, R)) = α × P max rcv (d(S, R)) 0 ≤ α ≤ 1, (2) 
where α depends on several parameters (distance d(S, R), λ, atmospheric conditions...). We denote by d min the minimal distance between the antenna of a sender and the antenna of a receiver. We can define the maximal received power P max rcv for a receiver close to a sender as: 
P max rcv = P snd × G SR × 1 d 2 min (3) 
3.
Sybil Attacks and Hypotheses
We suppose that each vehicle is equipped with a standard embedded device, in such a way that antennas, gains and sending powers are fixed and known. We can legitimately assume that the future standard track defining the wireless communication in VANET will fix all these characteristics . Moreover we assume that each car periodically diffuses some hello messages containing their GPS posi- tion. To create a Sybil node F , a sender S could give some false GPS positions in its messages. However, thanks to the previous equations, a receiver R may detect a mismatch between the measured received power P rcv and the GPS positions inside the message. To complicate the Sybil node detection by the receivers, the sender may use a non standard equipment. This implies that its sending power could vary instead of being fixed. We assume that all the mobile nodes (cars) are on the same two-dimensional Euclidean space, approximating the earth's surface. To estimate the number of potentially cheated cars, we can equivalently compute the area of the Euclidean plan where the Sybil attack cannot be detected. 
Results
 In this section, we consider a real propagation environment using a signal attenuation α. Such an attenuation depends on different parameters, including the distance from the sender to the receiver. However, it is not known and a receiver can not deduce the exact distance from the sender. Instead, it can only deduce the maximal distance from the sender, corresponding to an attenuation factor α of 1 (free space propagation). We begin with some preliminary results needed to prove our main propositions. 
Proof. Let (S, − → SF SF , − → j 
) be an orthonormal frame. The coordinates of the O point in this frame are ( α α−1 , 0) and the equation of C α is: 
(x − α α − 1 ) 2 + y 2 = α (α − 1) 2 
We have: 
α × d 2 (F, R) = d 2 (S, R) ⇔ α × (1 − x) 2 + α × y 2 = x 2 + y 2 ⇔ (α − 1)(x 2 + y 2 ) − 2 × α × x = −α ⇔ x 2 − 2×α α−1 × x + y 2 = −α α−1 ⇔ (x − α α−1 ) 2 + y 2 = α (α−1) 2 
Hence, all the points R satisfying Proof. The receiver R does not know the value of α. Hence, when S sends a message with the sending power P snd , R will measure a power P α rcv and compute an erroneous distance d α (S, R) from S to R using the free space propagation model (see Equation 1): 
√ α × d(F, R) = d(S, 
d α (S, R) = P snd P α rcv × G SR . 
By Equation 1, P α rcv = α × P rcv and we have 
d α (S, R) = P snd α × P rcv × G SR = 1 √ α × d(S, R). 
In the message from the node S, the node R will read the position of the Sybil node F and will compute the distance d(F, R). To be cheated by the Sybil attack, the receiver node R must satisfy d α (S, R) > d(R, F ), that is d(S, R) > √ α × d(F, R). By Lemma 1, the cheated nodes are then on the circle C α . Moreover, R cannot receive a message if it is outside the sender's range equal to √ α×d max . Hence, the cheated receiver nodes are those (i) on the circle C α and (ii) inside the disk C α max . Proof. Let S be a sender and R be a receiver such that the sending power of S is β × P snd , (β > 1) and the received power of the node R is P β rcv . Then, we have: 
P β rcv ≤ P max rcv ; β × P snd × G SR × 1 d 2 (S,R) ≤ P snd ×G SR d 2 min ; √ β × d min ≤ d(S, R). 
Hence, if the transmission power of S is equal to β × P snd , β > 1, every node R inside the circle C β min received a signal power greater than P max rcv .  We now consider the combined action of the attenuation factor α and the tuning factor β. We denote by γ the product α × β. 
S F (D ) A B O C β C β max max d √ β β C β min 
γ = 1. S (D ) A B O F max d C γ C γ max γ √ γ C γ min 
Discussions
We can reasonably assume that a standard for vehicular communication would fix the transmission power of each vehicles. Such a standard transmission power would then be used by all honest transmitting nodes. The only vehicles that may voluntarily bypass this rule are then the attacking nodes. 
Increasing the sending power allows to increase the area of successful attacks. However it also increase the area of reception. And when a receiver node is not cheated, it detects the attack. Then, by vehicle cooperation, the attack has more chances to fail. Hence, increasing the sending power could decrease the impact of the attack and could also be compromising for the attacker. There is then a tradeoff between the area of successful attack, and the area of detection. To evaluate this tradeoff, we study the ratio area of successful attack over area of reception. 
 4 Relative Node Localization Using Signal Strength Gradient 
 Our Sybil detection approach is based on a relative localization technique using received signal strength variations , under the assumption that all messages are sent with the same signal power. This assumption is legitimate as we have validated that malicious nodes should use a steady transmission power (cf. Section 3.4). We present in the following this localization technique 
Our relative localization technique uses the received signal strengths gradient, to estimate distances between two nodes at different positions (according to the Friis model). From these distances is then deduced an angle between the two nodes, as illustrated in 
β = arccos d 2 − d 1 L . 
Let the coordinates of the receiver at the position P 0 be (x 0 , y 0 ), and the coordinates of the sender (x s , y s ). Because of cos(x) = cos(−x), two localizations of the sender are possible, verifying the equation β = arccos((d2 − d1)/L). Thus: To be able to decide which position to choose for the sender, the receiver can measure the received signal strength from the sender, in the direction of one of the two localizations. Depending on the increase or the decrease of the received signal strength, the receiver decides which localization to choose for the sender. Calibration of Our Node Localization Technique. To avoid errors on the RSSI measurements, we should calibrate the exponent loss factor n used in the Friis Loss equation presented above. With d0 = 1, we have P L(d)
x s = x 0 + d.sinβ y s = y 0 + d.cosβ or x s = x 0 − d.sinβ y s = y 0 + d.cosβ 
d = 10 (P L(d)
 5 Sybil Nodes Detection Ap- proach 
 Our Sybil detection approach is composed of two complementary techniques. The first one is a localization verification technique based on received signal strength. This technique allows a node to verify the authenticity of another node by estimating its future geographical localizations , and compare them to its evaluated localizations . When a node is detected suspect (incoherent signal strengths gradient), our second technique should be used. This technique is a Sybil detection mechanism, based on the definition of a distinguishability degree metric. This mechanism can be launched individually by every node in the network in order to detect Sybil and malicious ones based on their geographical localizations. 
 5.1 Coherence Verification of RSSIs Mea- surements 
 We present in this section the first technique of our intrusion detection approach. This technique is based only on RSSIs measurements variations, and allows a node to detect malicious nodes within the network. We divide this section into two cases, according to the mobility of the verifier node. Fixed Verifier Node. Our objective is to allow a fixed node A to estimate (or predict) the signal strength received from a node B, based on the previous RSSI measurements . Such approach can detect the intrusion of a malicious node in the network, which is trying to usurpate the identity of another node. @BULLET Step 1: In this step, we compute the received signal strength measured by A, when the node B is at the position P 1 and P 2 , to delimit the estimated value of P rt (cf. 
Prt P r0 = ( d0 d t ) 2 ⇒ P rt = P r0 .( d0 d t ) 2 ⇒ P r0 . d 0 d 0 + V.T 2 ≤ P rt ≤ P r0 . d 0 d 0 − V.T 2 
Thus, we can conclude that at t = t 0 + T , the RSSI measured by the node A should belong to the interval 
[P r0 .( d0−V.T d 0 ) 2 , P r0 .( d0+V.T d 0 ) 2 ]
. Otherwise, the received message was sent by a Sybil node. In addition, the node A adds to the node B the label "" suspect "" . To illustrate our approach, we use the following example where the average speed V is equal to 30m/sec and the period T is defined at 1sec: each measured RSSI at t = t i should thus insure that: 
P r i−1 . d i−1 d i−1 + 30 2 ≤ P r i ≤ P r i−1 . d i−1 d i−1 − 30 2 
Figure 10: RSSI coherence verification Mobile Verifier Node. Our objective is to allow a mobile node A to verify the coherence of the localizations of node B, evaluated via our localization technique presented in Section 4, according to its mean speed V . 
LOC B i − LOC B i−1 <= V.(t i − t i−1 ). 
In the example given in 
Distinguishability Degree Between Two Nodes
To define the distinguishability degree between two nodes X and Y, a verifier node stores their localizations, evaluated through RSSIs measurements variations. We assume that the verifier node receives enough messages from the two nodes, in order to evaluate their localizations each t i . We define M the balanced geometric mean of the differences between the localizations of the two nodes X and Y at each t i , by adding to each difference between the localizations of the two nodes X and Y at each t i a weight, corresponding to the number of measurements until t i , as follows: 
M i (X <> Y ) = i k=1 (LOC X k − LOC Y k ) k 1/ i k=1 k 
We define D an evaluated distance which a node can traverse with the mean application speed within 2 seconds . We assume that two nodes localized at distance D at t i are distinct. For example, in VANET we can choose D = 60m (for a mean speed evaluated at 30m/sec). The distinguishability degree DD(X <> Y ) is thus evaluated as the percentage that nodes X and Y are distinct. DD(X <> Y ) is computed as follows: 
if (M i < D) DD(X <> Y ) = 100. Mi(X<>Y ) D % else DD(X <> Y ) = 100% end if If DD(X <> Y ) → 0
, the verifier node can determine that the nodes X and Y are malicious ; one of them is a Sybil node. Otherwise, the two nodes are considered distinct at DD(X <> Y ) %. Depending of the concerned wireless application and according to its established security policy, each node in the network can evaluate the distinguishability degree between two particular nodes in its neighborhood (labeled "" suspect "" for example), or evaluate a higher triangular matrix of distinguishability of all its neighbors, at each time t i , that we call M Distinguish . For a node I, with c neighbors, this matrix is evaluated as follows: 
M i Distinguish =     0 DD(0 <> 1) .... DD(0 <> c) 0 0 .... DD(1 <> c) .... .... 0 .... 0 .... .... 0     
Note that DD(i <> j) is stored at (row i, column j), DD(i <> i) = 0 and DD(i <> j) = DD(j <> i). According to this matrix and at each time t i , each node in the wireless network can evaluate the distinguishability of its neighbors and determine suspect ones. However, it can not determine precisely which entities are Sybil nodes and who creates them. 
Determination of Sybil and Malicious Nodes 
through Geometrical Analysis. At this step of our Sybil detection approach, a verifier node can isolate suspect entities within its neighborhood. To determine precisely Sybil and malicious nodes, it should proceed as follows . For each pair of nodes (S, F ) such that DD(S <> F ) ∼ 0, the verifier node can check if it is in a Sybil attack success area depending on the position of suspect nodes (S, F ) (cf. Section 3.3). Figures 11 and 12 shows how a node can identify the malicious node and the Sybil one in the pair (S, F ). If there was no inconsistencies in the received signal strength (the Sybil nodes is placed in the success area), the verifier can combine the distinguishability degree of a pair of nodes and the computation of the successful area of the Sybil attack to determine which node is the Sybil one and which is the malicious one. Having the verified position of the nodes S and F , a node V , the verifier can compute the Sybil attack success area considering the node S as the malicious node (
Analysis and Simulations
To validate the applicability of our contributions depicted above, we present in this section analysis and simulations we have done to evaluate the error margin of our localization technique and validate our distinguishability degree evaluation technique. 
Simulation Results
 In this section, we use the network simulator NS-2 to simulate our localization mechanism, described in Section 4. 
Our simulation parameters under NS-2 are as follow. The propagation model is Free Space, the MAC protocol is 802.11, the antenna model is omni-directional, the number of nodes is 2 and finally the simulated traffic is We carried out simulations to evaluate the localization error of our approach. In a first step, we evaluate the localization error according to the distance L between the two positions P 1 and P 2 , the distance d is fixed to 180.27 (cf. 
Validation by Real Measurements and Results
To validate our localization technique, we carried out RSSI measurements between two Nodes A and B, using the wireless tool iwspy. Our test framework is composed of a non isotropic source placed in free space as a transmitter antenna with P T watts and a directivity gain G T . At an arbitrary large distance d (d >> λ, where λ = cT = c/f is a wavelength) from the source, the radiated power is uniformly distributed over a surface area of a sphere of radius d. If P R is the power at the receiving antenna, which is located at distance d from the transmitter antenna and has a directivity gain G R , then the path loss in decibels is given by the following equation (where γ = 2): The distance d (km) is thus computed as follows: 
d = 10 (L F −34.44−20 log10f +10 log10G T +10 log10G R )/10 γ 
Our testbed is composed of two laptops, connected via an ad hoc network, equipped each by a Holux GPS and a wireless card (Avaya and Buffalo wireless cards) with external antennas (G T = 3dbm and G R = 3dbm). The frequency of the used channel is f = 2.457Ghz. The objective of our measurements is to verify through GPS that the distance evaluated using the received signal strength indicators is exact with an error margin to be determined. 
Validation of Our Distinguishability Degree Evaluation Technique
In this section, we validate through analysis the efficiency of our distinguishability degree metric. In our simulated example, our network is composed of four real Nodes (1, 2, 3 and 4) and a Sybil one (5 created by the Node 4). We study hereafter the behavior of the distinguishability matrix on the Node 1 throughout the simulation duration. 
and 4 and the Nodes 2 and
Figure 16: Distinguishability degree illustration then joined the University of Technology ofCompì egne (UTC), and the Heudiasyc lab (UMR CNRS 6599), France, were he is in charge of the computer systems and networks teaching specialty. His research work deals with highly dynamic ad hoc networks (such as inter-vehicles networks): networking, distributed algorithms, security, software architecture. . . . Dr. Ducourthial was advisor for several masters and PhD theses. He received the HabilitationàHabilitationà diriger des recherches in 2005. He is regularly involved in program committees of conferences. His work is funded by industrial, regional, national and European grants. Mohamed Salah Bouassida is a CNRS researcher at HEUDIASYC laboratory in France. He has a Ph.D. and master degree from Henry Poincaré University, Nancy France, within the MADYNES research team in the LORIA laboratory (in 2006 and 2003 respectively). His main research interests are localization within wireless networks, security services of group communications in the context of ad hoc networks, establishment of group key management protocols within MANETs and congestion control within wireless networks. 
Gilles Guette 
Mohamed Shawky received his MSc and PhD respec
tively in 1989 and 1992 on distributed computing from University de Technologie de Compine, France. He is currently associate professor, HDR, at the Computer Science Department, with Heudiasyc Laboratory. His research interests include real-time embedded computing for highly mobile networks. He is responsible of several European, industrial, national and regional research projects. He is co-ordinating the workgroup "" embedded diagnostic "" with the research cluster System@tic and the workgroup "" Robust Design "" of Eicose European Institute. He is also on the list of European Union experts for INCO programs. 
"
"Introduction
Due to the rapid development of computer technology, there are many digital applications that have become involved in our daily lives. In the past, people usually use pens to sign important messages; however, since the digital message has replaced traditional paper, people have started to use digital signatures to sign digital messages. Although many researchers have designed different signature applications with different requirements, like blind signatures 
 1) Integrity: When a person can verify the received message and signature, he or she can ensure that the message has not been modified by someone else during the transmission time. 
2) Unforgeability: By verifying the received message and signature, people easily can verify the legal identity of the signer. Conversely, the people who verify the signature can make sure that no one else is using a fake signature and message to impersonate the real signer. 
3) Non-repudiation: When someone maliciously denies a message and signature that he or she had signed, a good signature scheme can identify the true provider of the signature. In short, the signature must protect the verifier, in case he or she becomes the victim. 
 In a traditional digital signature system, the signer normally holds two keys, a private key and a public key. The private key can be used for signing important messages , and give the corresponding public key to the certificate authority and verifier. The certificate authority (CA) stores and manages every user's public key. Once the verifier receives a signature from a signer and wants to verify it, CA will give the corresponding certificate to the verifier which includes the signer's public key. Hence, the verifier can verify the certificate and the signer's public key immediately. It is secure and very convenient but places a heavy burden on CA because the CA has to store and manage many certificates. For this reason , Shamir proposed an ID-based public key system in 1985 
Related Works
 In this section, we briefly review Zhang and Mao's RSAbased certificateless scheme 
1) Setup phase: First, the KGC generates two large random numbers pand q, and computes N = pq. Then it generates e that satisfies gcd(e, φ(N )) = 1, where φ(N ) denotes Eular's totient function. After that, KGC gets d from computing ed mod φ(N ) = 1 and selects two cryptographic hash functions H 0 : {0, 
1} * → Z * N and H: Z 4 N · {0, 1} * → 
 {0, 1} l , where l is a security parameter. Finally, KGC sets the master secret key (M SK) = {d} and the master public key (M P K) = {e, N, H 0 , H}. 
2) Partial-private key extraction phase: KGC uses user's identity ID, where ID belongs to {0, 1} * , then computes the partial private key d ID = H 0 (ID) M SK = H 0 (ID) d . After that, KGC sends d ID to the user over a secure channel. 
3) Set user secret value phase: The user chooses a random number X ID and sets the X ID as a secret value. 
4) Set user public key phase: Given the partial private key d ID and the secret value X ID , the user uses identity ID to generate the public key 
P K ID = H 0 (U ID) X ID mod N . 
5) Set user private key phase: Given the partial private key d ID and the secret value X ID , the user can generate the private key SK ID = (X ID , d ID ). 6) Sign signature phase: First, the user chooses two random numbers r 1 and r 2 for computing R 1 = H 0 (ID) r1 mod N and R 2 = H 0 (ID) r2 mod N . Second, the user computes h = H(R 1 , R 2 , ID, P K ID , M ), where M is a message . Then, user computes 
u 1 = (H 0 (ID) d ) (r1−h) and u 2 = r 2 − X ID h
 . Finally, the certificateless signature on message M is δ = (u 1 , u 2 , h). 
7
) Verify signature phase: Upon receiving the message with the signature δ = (u 1 , u 2 , h), the verifier starts to com- pute 
R 1 = u e 1 H 0 (ID) h mod N and R 2 = H 0 (ID) u2 P K h ID mod N . Then, the verifier verifies whether H(R 1 , R 2 , ID, P K ID , M ) ? = h. If 
 the verification holds, the user can accept the signature and message; otherwise, the user will reject them. The correctness of the verification can easily be shown as follows: 
Step 1. Computes R 1 = u e 1 H 0 (ID) h mod N = ((H 0 (ID) d ) r1−h ) e H 0 (ID) h mod N = H(H 0 (ID) r1 ) mod N = R 1 . 
Step 2. Computes 
R 2 = H 0 (ID) u2 P K h ID mod N = H 0 (ID) r2−X h ID P K h ID mod N = H 0 (ID) r2−X h ID (H 0 (ID) X ID ) h = H 0 (ID) r2 mod N = R 2 . Step 3. Because R 1 = R 1 and R 2 = R 2 
, we can compute and verify 
H(R 1 , R 2 , ID, P K ID , M ) = H(R 1 , R 2 , ID, P K ID , M ) = h. 
Cryptanalysis of Zhang et al.'s Scheme
 Zhang and Mao improved upon the drawbacks of traditional signatures, and they were the first to start using the RSA crypto-system in certificateless signature scheme to reduce computational costs. Unfortunately, if we give more power to attackers, we find two defects in Zhang and Mao's scheme. The first problem is the signer's public key, and second is a royalty problem of KGC. 
Problem of Signer's Public Key
In Zhang and Mao's scheme, their public key is based on a traditional certificateless scheme. Therefore, their public key P K ID = H 0 (ID) X ID consists of the signer identity ID and secret value X ID . Apparently, the secret value is a random number that only the signer knows. Even if the verifier holds public key P K ID and the signer's real identity, he still cannot prove whether this public key is correct or not without the secret value X ID . Al-Riyami and Paterson 
Royalty Problem of KGC
Assume that Caesar is an attacker, Josh is a victim signer, and Janet is a victim verifier in Zhang and Mao's scheme. Caesar also is one of the KGC's members, who obtains the real master key d and stealthily generates a partial private key d Josh = H 0 (Josh) M SK = H 0 (Josh) d and randomly chooses the secret value X Caesar . After that, Caesar can impersonate Josh to generate the fake public key P K Josh = H 0 (Josh) X Caesar and fake private SK Josh = (X Caesar , d Josh ). Now, Caesar uses the fake P K Josh , SK Josh and Josh's identity to sign on the fake important message M 2 as follows: 
Step 1. Caesar randomly chooses two numbers r 1 and r 2 . 
Step 2. Then, Caesar computes 
R 1 = H 0 (Josh) r 1 mod N, R 2 = H 0 (Josh) r 2 mod N, h 2 = H(R 1 , R 2 , Josh, P K Josh , M 2 ), u 1 = (H 0 (Josh) d ) r 1 −h2 , u 2 = r 2 − X Caesar h 2 . 
 Step 3. After that, Caesar can generate the invalid signature δ = (u 1 , u 2 , h 2 ). Step 1. First, Janet computes 
R 1 = (u 1 ) e H 0 (Josh) h2 mod N R 2 = H 0 (Josh) u 2 (P K Josh ) h2 mod N. 
Step 2. After that, Janet can compute and verify whether H(R 1 , R 2 , Josh, P K Josh , M 2 ) ? = h 2 holds or not. If the verification holds, Janet believes the message and the signature; otherwise, Janet can detect that the message and signature are incorrect. The correctness of the verification can easily be shown as follows: 
Step 1. Compute R 1 = (u 1 ) e H 0 (Josh) h2 mod N = ((H 0 (Josh) d ) r 1 −h2 ) e H 0 (Josh) h2 mod N = H 0 (Josh) r 1 mod N = R 1 . 
Step 2. Compute 
R 2 = H 0 (Josh) u 2 (P K Josh ) h2 mod N = H 0 (Josh) r 2 −X Caesar h2 (P K Josh ) h2 mod N = H 0 (Josh) r 2 −X Caesar h2 (H 0 (Josh) X Caesar mod N ) h2 mod N = H 0 (Josh) r 2 mod N = R 2 . 
Step 3. Because R 1 is equal to R 1 and R 2 is equal to R 2 , we can compute and verify h 2 ? = h 2 by computing as follows: 
h 2 = H(R 1 , R 2 , Josh, P K Josh , M 2 ) = H(R 1 , R 2 , Josh, P K Josh , M 2 ) = h 2 . 
However, the message with the invalid signature can still pass the verification because the secret value X Caesar is a random number and nobody knows this secret value. Josh cannot prove that the fake public key P K Josh = H 0 (Josh) X Caesar and fake private SK Josh = (X Caesar , d Josh ) do not belong to him. Therefore, even though Zhang and Mao's scheme can be safe and efficient in most general cases, if we give strong power to an attacker , it cannot prevent the above-mentioned problem. 
The Proposed Scheme
 In this section, we propose a novel strong RSA-based certificateless scheme to improve Zhang and Mao's scheme. There are three participants in our scheme: key generator center (KGC), signer, and verifier. Our scheme consists of eight algorithms and the details are described as follows. Setup (1 c ) → (M P K, M SK) KGC inputs secret parameter to generate the master public key (MPK) and master secret key (MSK). The signer inputs the partial private key and signed secret value, then the algorithm returns the private key. Sign-Signature (SK U ID , U ID, M P K, M ) → (M, δ) The signer can input her/his private key, identity, master public key and message M, and then he or she can get a message M with signature δ from this algorithm. Verify-Signature (P K ID , M P K, M, δ) → Accept/Reject The verifier can input the public key of the signer, master public key, message M and the signature δ. After this algorithm runs the verification, it can give a response message to tell the verifier whether the signature is correct or not. 
Our proposed scheme can be divided into four phases: 1) setup phase, 2) blinding phase, 3) signing phase and 4) verifying phase. The details are described as follows: 
1) Setup phase. The KGC generates two large random numbers pand q, and computes N = pq first. Then KGC can choose e that satisfy gcd(e, φ(N )) = 1. Here, φ(N ) denotes Eular's totient function. After that, KGC can find one d from computing ed mod φ(N ) = 1 and selects two cryptographic hash functions h 0 : {0, 
1} * → Z * n and h: Z 4 n {0, 1} * → 
{0, 1} p , where p is a security parameter. Finally, KGC sets parameter d to be the master secret key (MSK) and parameters e, N , h 0 , and h to be the master public key (MPK). 2) Blinding phase. In the blinding phase, the signer chooses a random number R first, and then computes R −1 that satisfies R·R −1 = 1. After that, he or she uses R, secret value x U ID and KGC's master public key e to compute U ID . After that, the signer can compute the 
C = R e x U 
H s = h(R s1 , U ID, m 3 ), 
where UID is the public key of signer and m 3 is the message. Then, the signer computes 
u s1 = x Hs+rs 1 U ID and u s2 = ((x U ID U ID) d ) rs 1 
 −Hs to generate the signature δ = (H s , u s1 , u s2 ), and send a message with the signature to the verifier. 4) Verifying phase. When the verifier receives the message m with signature δ, he or she can use signer's public key (UID) and KGC's master public key e to compute R s1 = (u s2 ) e (U ID) Hs u s1 . Then, the verifier can use R s1 , signer's public key UID and the message m 3 to 
erate H s = h(R s1 , U ID, m 3 )
, and verifies whether H s is equal to H s . If the equation holds, then the verifier can believe that the signature is correct. The details of the equation are shown as follows: 
H s = h(R s1 , U ID, m 3 ) = h((u s2 ) e (U ID) Hs u s1 , U ID, m 3 ) = h((((x U ID U ID) d ) rs 1 −Hs ) e (U ID) Hs x Hs+rs 1 U ID , U ID, m 3 ) = h(((x d U ID U ID d ) rs 1 −Hs ) e (U ID) Hs x Hs+rs 1 U ID , U ID, m 3 ) = h(((x ed U ID U ID ed ) rs 1 −Hs ) (U ID) Hs x Hs+rs 1 U ID , U ID, m 3 ) = h((x rs 1 −Hs U ID U ID rs 1 −Hs ) (U ID)H s x Hs+rs 1 U ID , U ID, m 3 ) = h((x 2rs 1 U ID U ID rs 1 ), U ID, m 3 ) = h(R s1 , U ID, m 3 ) = H s . 
Security Analysis
As Cases 1 and 2 demonstrate, our scheme can withstand the forgery attack. 
Non-Repudiation
Here, we assume that Caesar is a malicious signer, who signed an important message m with his signature, but then denies his signature. In our proposed scheme, the signer must use her/his identity UID and secret value x U ID to compute R s1 = U ID rs 1 X 2rs 1 and uses secret value x U ID and private key (x U ID U ID) d to generate 
u s1 = x Hs+rs 1 U ID and u s2 = ((x U ID U ID) d ) rs 1 −Hs . 
ter that, he can generate the complete signature δ = (H s , u s1 , u s2 ), where H s = h(R s1 , U ID, m). Caesar cannot repudiate the signature because no one can generate the correct signature parameters without the correct secret value x U ID . Specifically, in our proposed scheme, when the signer generates a secret value x U ID , he or she has to use the blinding phase to let KGC sign the blind signature on value x U ID . Therefore, Caesar cannot choose another secret value and create x d U ID to generate the fake private key (x U ID U ID) d ) by himself. Hence, the proposed scheme can prevent signers from repudiating their signature. 
Problems of Signer's Public Key
In Zhang and Mao's scheme, the signer's public key P K ID = H 0 (ID) X ID consists of the signer identity ID and secret value X ID . When the verifier receives a signature from the signer, he or she cannot verify whether the public key is correct or not without the secret value. Another reason for the verifier cannot verify the public key is that there has no certificate to check signer's public key in certificateless signature system. Hence, in our proposed scheme, when the verifier receives a signature from a signer, the verifier can directly use the signer's identity to verify the signature. In short, we improved upon this weakness in Zhang and Mao's RSA-based certificateless scheme. 
Royalty Problem of KGC
Assume that there is an attacker, Caesar, who is one of the KGC's members, and he obtains the real master key d. Also, there is a victim signer (Josh) and victim verifier (Janet) in our proposed scheme. Caesar stealthily generates the partial private key d Josh = h 0 (Josh) M SK = h 0 (Josh) d and randomly chooses the secret value X Caesar . After that, Caesar can impersonate Josh to generate the fake public key P K Josh = Josh and fake private SK 
Josh = x d Caesar Josh d = (x Caesar Josh) d . 
Now, Caesar uses P K Josh and SK Josh to sign the fake important message m 4 as follows: 
Step 1. Caesar randomly chooses a number r c1 . 
Step 2. Then, Caesar computes 
R c1 = Josh rc 1 x 2rc 1 Caesar , H c = h(R c1 , Josh, m 4 ), u c1 = x Hc+rc 1 Caesar , u c2 = ((x Caesar Josh) d ) rc 1 −Hc . 
 Step 3. After that, Caesar can generate the invalid signature δ = (H c , u c1 , u c2 ). 
Step 4. Finally, Caesar sends invalid signature δ and important message m 4 to 
Step 1. Janet can compute R c1 = (u c2 ) e (Josh) Hc u c1 first. 
Step 2. Then, she can generate 
H c = h(R c1 , Josh, m 4 ) 
using parameter R c1 , Josh's identity and the received message m 4 . 
Step 3. She can verify whether H c is equal to H c or not. If it is not equal, then she knows that the signature and message are incorrect. Otherwise, she can believe the signature and message. The details of the equation are as follows: 
H c = h(R c1 , Josh, m 4 ) = h((u c2 ) e (Josh) Hs u c1 , Josh, m 4 ) = h(((x Caesar Josh) d ) rc 1 −Hc ) e (Josh) Hc x Hc+rc 1 Caesar , Josh, m 4 ) = h(((x d Caesar Josh d ) rc 1 −Hc ) e (Josh) Hc x Hc+rc 1 Caesar , Josh, m 4 ) = h(((x ed Caesar Josh ed ) rc 1 −Hc ) (Josh) Hc x Hc+rc 1 Caesar , Josh, m 4 ) = h((x rc 1 −Hc Caesar Josh rc 1 −Hc ) (Josh) Hc x Hc+rc 1 Caesar , Josh, m 4 ) = h((x 2rc 1 Caesar Josh rc 1 ), Josh, m 4 ) = h(R c1 , Josh, m 4 ) = H c . 
Apparently, even when Caesar uses a fake signature, it can easily pass verification because Caesar has the correct master private key d. Nevertheless, when Josh and Janet realize that the message and the signature are incorrect in our proposed scheme, Josh can provide his private key (X John John) d and blinded secret value (X John ) d to the police or the judge. Because we know that no one can create a private key and blinded secret value without the master private key d, the judge can that believe (X Caesar John) d and X d Caesar was created by KGC. Hence, if there were an attacker with strong power trying to impersonate the signer in our proposed scheme, our proposed scheme would protect the signer. 
Performance Analyzes
Here, we compare the computational cost between our proposed scheme and Zhang and Mao's scheme. In Zhang and Mao's scheme, they point out that one RSA's modulus of length is 1024 bits and one output length of the hash function is 160 bits. In addition, they also point out that the cost of one multi-exponentiation is about 20% more than the cost of one exponentiation. The details are shown in 
Conclusions
Recently, the certificateless-based signature scheme has been found to not only solve the certificate management problem, but also to overcome the key escrow problem. In this paper, we proposed a strong RSA-based certificateless signature scheme to improve the security of Zhang and Mao's scheme. Our proposed scheme makes the RSA-based certificateless signature system more useful and powerful. At the same time, it is capable of resisting more intense malicious behavior. Furthermore, we achieve lower computational cost in than in Zhang and Mao's scheme. For all of these reasons, our scheme is more suitable for certificateless-based signature systems. Chin-Yu Sun received the MS degree in Department of Information Engineering and Computer Science from Feng Chia University, Taichung, Taiwan in 2013. He is currently pursuing his Ph.D. degree in computer science from National Tsing Hua University, Hsinchu, Taiwan. He current research interests include information security , cryptography, wireless communications, mobile communications, and cloud computing. Shih-Chang Chang received his B.S. degree in 2005 and his M.S. degree in 2007, both in Department of Information Engineering and Computer Science from Feng Chia University, Taichung, Taiwan. He is currently pursuing his Ph.D. degree in Computer Science and Information Engineering from National Chung Cheng University, Chiayi , Taiwan. His current research interests include electronic commerce, information security, computer cryptography , and mobile communications. 
"
"Introduction
 With the rapid development of the contemporary society , new devices and powerful software make the world more digitalized and informationized. The wireless network offer ubiquitous channels to deliver and exchange various kinds of multimedia information, which becomes indispensable for humans' daily life. However, the potential brought about by freely uploading, downloading and manipulating data can't be fully realized without the protection of privacy. Data hiding technique 
Preliminaries
This section will review the Absolute Moment Block Truncation Coding (AMBTC) algorithm, (7, 4) Hamming code and the matrix embedding. 
Absolute Moment Block Truncation Coding (AMBTC)
In order to speed up the transmission process and save the bandwidth, Lema and Mitchell presented an AMBTC technique 
η = 1 k k i=1 x i , 
where x i denotes the gray-level intensity of the pixel i in a block. Since AMBTC is a one-bit quantizer, η is utilized as a threshold for binarizing all the pixels in each block into two clusters with p and k − p pixels, individually. If the intensity of the pixel is lower than η, it falls into the first cluster C 0 . Otherwise, it falls into the second cluster C 1 . The result is bitmap which is employed to record the distributions of the two quantization levels. The lowand high-mean used to represent a block is organized as follows: 
a = 1 p xi<η x i , 
(1) 
b = 1 k − p xi≥η x i , 
(2) 
 where the two variables a and b are the quantization levels . Finally, the reconstructed AMBTC compression image is expressed as follows: 
y i,j = a, if h i,j = 0, b, if h i,j = 1, h i,j = 0, if x i,j = C 0 , 1, if x i,j = C 1 , 
 where h i,j is the element in the bitmap BM and y i,j denotes the pixel in the compressed AMBTC image. From the above equations, we can see each compressed block will generate a trio (a, b, BM ), two quantization levels and one bitmap. An example of the AMBTC scheme is described in 
and 
 (2), we can obtain the two quantization level values of a and b as 135 and 166, respectively. Finally, the generated compressed trio (135,166, 0011001000001101) is transmitted to the receiver. Herein, the receiver decodes this trio and reconstructs the image block shown in 
104 158 192
(b) AMBTC bitmap (c) Reconstructed image block Hamming code 
 2.3 The Matrix Embedding with Hamming Code 
 Matrix embedding was originally introduced by Cran- dall 
y = Emd(x, m) = x + F (m − Hx), 
where y is the received stego vector and F (·) is a function that transforms (m − Hx) into a decimal value i and sets the i-th bit to '1' while other bits of the vector are set to '0'. At the receiving side, the receiver can easily extract the embedded message by: 
m = Ext(y) = H(x + F (m − Hx)) = Hx + m − Hx = m, 
(3) 
Proposed Scheme
In this section, the proposed scheme will be thoroughly presented. First, AMBTC algorithm is performed on the original grayscale cover image to obtain the compressed data that can be represented by a low-mean, a high mean, and a bitmap. Then, the secret message is concealed into the AMBTC compressed trio (a, b, BM ). The advantage is to achieve a higher payload compared to other data hiding schemes performed on the compression domain. There are two phases for embedding in our proposed scheme. We firstly perform using the (7, 4) Hamming code on the lowand high-mean values of each compressed trio and exploit the relationship between them to hide one more secret bit. Then, we embed secret message into the bitmaps. The reason why we choose (7, 4) Hamming code is based on two considerations. On the one hand, we can embed three secret bits into seven cover bits. On the other hand, only one bit in the cover vector will be flipped after embedding , which means the modification to the cover vector is reduced to the minimum. 
Embedding Phase
After the original grayscale image is partitioned into a set of n × n blocks for AMBTC compression, each block undergoes the same process, thereby the description below towards a single block processing is adopted for simplicity. Normally, n equals 4. The flowchart of the embedding procedure which contains two phases is illustrated in 
The embedding phase 1: Embed secret message into two quantization levels. 
, m =(101)} a' = (01011110) 2 =94 b' = (01011000) 2 =88 b'a'< 8
, cannot embed one more bit. Remain as (94,88) 
Steps 1-5 
Phase 2: Embedding group: 
{x = (0111101), m = (001)} {x = (1010010), m = (011)} {x = (1101111), m = (101)} {x = (0100101), m = (100)} Steps 6-9 y = (0111001) y = (1010011) y = (1101111) y = (0101101) y = (1110000) 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 (a 1 * =156 b 1 * =121) (a 2 * =94b 2 * =88) 
(a) AMBTC compressed trio 1 Secret message m='0001101001011101100' 
(b) AMBTC compressed trio 2 
(c) Watermarked compressed trio 1 
(d) Watermarked compressed trio 2 Step 2. Construct the parity check matrix H of (7, 4) Hamming code. Change the positions of the columns in H to ensure they array in ascending order in decimal form. 
 Step 3. Compute syndrome s = m − Hx, and transform s into a decimal number i. Change the ith bit of vector x to obtain the stego vector 
y = (y 1 , y 2 , y 3 , y 4 , y 5 , y 6 , y 7 
). Each time, three secret bits are embedded into seven bits. 
Step 4. Replace the four LSBs of low mean value a with (y 1 , y 2 , y 3 , y 4 ) and three LSBs of high mean value b with (y 5 , y 6 , y 7 ) to obtain newly generated quantization pair (a, b). 
Step 5. Compare the difference value between a and b to see whether they can be applied to the second embedding. If b − a ≤ 8, this pair belongs to the unembeddable category. If not, we can embed one more secret bit through interchange operation. If the corresponding embedded bit is '1', exchange the two quantization levels, which becomes (b , a ). Otherwise , leave the location of (b , a ) remain the same. Denote the final result of the quantization pair as 
(a * , b * ). 
It is notable that we set a threshold as eight to determine whether it is an embeddable group or not, because after operating Steps 1-4, the rangeability of a and b brought by matrix embedding may be eight and four respectively and only one of them will be changed. If a is larger than b , it may suffer from confusion in the data extraction phase, because we cannot figure out whether the result is obtained by matrix embedding or interchange operation. In order to avoid ambiguity, we set a threshold to distinguish between these two cases. After all the quantization level pairs are processed, we conduct embedding in the bitmaps. Step 6. Concatenate each bitmap as (BM 1 , BM 2 ,..., BM k ) to form a sequence of bitmaps. 
Step 7. Sequentially take seven bits from the bitmaps and denote them as cover vector x. Then read three secret message bits and denote them as vector m each time. 
Step 8. Perform the matrix embedding procedure which is the same as in Steps 2-3 in Phase 1 until the entire bit-stream in the bitmaps is successively processed. 
Step 9. Update the bitmaps (BM 1 , BM 2 ,..., BM k ) and combine them with the corresponding quantization levels to obtain the final compressed trios (a * , b * , BM i ). After the whole embedding process is done, we successfully conceal the secret message into the compressed trios. An embedding example is given in 
1   (4) 
 Firstly, we embed three secret bits into the first quantization level pair (123, 156) according to the operations in Steps 1-4 of Phase 1. Then the difference value between a and b is calculated to check whether it is suitable for hiding one more bit. In this case, the pair belongs to the embeddable category. Since the secret bit is '1', their location will be interchanged. Then the second quantization level pair is operated in the similar way as the first pair. When embedding is finished according to Phase 1, we will hide secret message into bitmaps. The final results are illustrated in 
Extracting Phase
In the extracting phase, the secret message is extracted from a received watermarked AMBTC compressed code stream by using our designed rules. The flowchart of the extraction procedure is depicted in 
Experimental Results
 In this section, we conduct some experiments and discuss the results to demonstrate the superiority of our proposed scheme. 
(7) and CS means the length of the output code stream. Hiding capacity HC is used to measure the number of embedded secret bits. Hiding efficiency HE is defined by 
255 2 M SE )(dB), 
(5) 
M SE = H i=1 W j=1 (x i,j − x i,j ) 2 /(L × W ), (6) 
BR = |CS| L × W (bpp), 
(7) 
HE = HC |CS| , (8) 
"
"Introduction
 Confidentiality, integrity, non-repudiation and authentication are the important requirements for many cryptographic applications. A traditional approach to achieve these requirements is to sign-then-encrypt the message. Signcryption, first proposed by Zheng in 1997 
 The rest of this paper is organized as follows. In Section 2, we define the indistinguishability against adaptive chosen plaintext attacks for signcryption. Chung et al.'s anonymous ECC-based signcryption scheme has been reviewed and analyzed in Section 3 and Section 4, respectively . Finally, the conclusions are given in Section 5. 
Formal Model of Anonymous Signcryption Schemes
We provide only those definitions relevant to our attack; see 
Generic Scheme
 A generic ring signcryption scheme consists of the following three algorithms. @BULLET Setup: Given a security parameter k, the algorithm generates public/private key pairs. The public and private key pairs for the ring signers and the verifier are (
Q 1 , d 1 ), · · · , (Q n , d n ), (Q V , d V ). 
@BULLET Signcrypt: To send a message m to the receiver whose identity is ID V , user Q i (i ∈ {1, · · · , n}) chooses some other users to form a group U including herself and computes Signcrypt(m, U, Q V , d i ) on behalf of the group U to obtain the ciphertext σ. We make the consistency constraint that if 
σ = Signcrypt(m, U, Q V , d i ), then m = Unsigncrypt(σ, U, d V ). 
Security Notions
 (Q 1 , d 1 ), · · · , (Q n , d n ) for the ring members and verifier . After that, C keeps (d 1 , · · · , d n ) secret and provides A with the public key (Q 1 , · · · , Q n ). 2) 
Review of the Chung et al.'s Scheme
Setup. Let q denote a large prime number, E denote an elliptic curve, P denote a base point on the elliptic curve E with order q, and H denote a one-way hash function for resisting collision, where q, E, P , and H are public parameters, and Z q is a finite field with q elements. Let a group member set be 
A = {U 1 , U 2 , · · · , U n } under the ECC, the private keys of Q 1 , Q 2 , · · · , Q n are d 1 , d 2 , · · · , d n respectively. The corresponding public keys Q 1 , Q 2 , · · · , Q n satisfies Q i = d i P , where i = 1, 2, · · · , n. 
The private and public keys of verifier 
U v are d v and Q v = d v P , respectively. 
Signcrypt. Let a member U i in A 
send the signcryption text of message m to verifier U v . U i executes the process of generating signcryption text as follows. 
1) Randomly select k ∈ R [1, q − 1] and r ∈ R [1, q − 1]; 2) Calculate (x i , y i ) = T i = kP , (x r , y r ) = R = rP , and (x e , y e ) = T e = rQ v ; 3) Select s t ∈ R [1, q − 1], where t = i + 1, i + 2, · · · , n, 1, · · · , i − 1 for t − 1 = n when t = 1; 4) Calculate c t = H(m ∥ x t−1 ) and (x t , y t ) = T t = s t P + c t Q t , where t = i + 1, i + 2, · · · , n, 1, · · · , i − 1 for t − 1 = n when t = 1; 5) Calculate c i = H(m ∥ x i−1 ) and s i = k − d i c i (mod q); 6) Encrypt the message m following m ′ = E xe (m) using the symmetric secret key x e ; 7) Send the encrypted text σ = (m ′ , c 1 , s 1 , s 2 ,· · · , s n , R) to the verifier U v . 
Unsigncrypt. On receiving the encrypted text 
σ = (m ′ , c 1 , s 1 , s 2 , · · · , s n , R), 
 the verifier U v performs the following steps to verify. 
1) Let (x r , y r ) = R, calculate (x d , y d ) = d v R and m ′′ = E x d (m ′ ); 2) For t = 1, 2, · · · , n − 1, calculate (x t , y t ) = T t = s t P + c t Q t and c t+1 = H(m ′′ ∥ x t ); 3) Calculate (x n , y n ) = T n = s n P + c n Q n and c ′ 1 = H(m ′′ ∥ x n ); 4) With c ′ 1 = c 1 , confirm that σ = (m ′ , c 1 , s 1 , s 2 , · · · , s n , R) 
is a valid anonymous signcryption text from the group 
A = {U 1 , U 2 , · · · , U n }; 
otherwise, reject the encrypted text. 
Cryptanalysis of the Chung et al.'s Scheme
 In this section, we show that the Chung et al.'s anonymous signcryption scheme is not even secure against chosen plaintext attacks. When A receives the challenge 
phertext σ * = (m ′ * , c * 1 , s * 1 , s * 2 , · · · , s * n , R * )
. A first makes a "" wild guess "" of b to be 0. Then, A goes as follows: 
1) For t = 1, 2, · · · , n − 1, calculate (x * t , y * t ) = T * t = s * t P + c * t Q t and c * t+1 = H(m b ∥ x * t ); 2) Calculate (x * n , y * n ) = T * n = s * n P + c * n Q n and c ′ * 1 = H(m b ∥ x * n ); 3) Checking whether the equation c * 1 = c ′ * 1 holds. 
If the above equations hold, then A knows that m 0 is the plaintext for the challenge ciphertext. If the above equations do not hold, A knows that m 1 is the plaintext for the challenge ciphertext. So Chung et al.'s anonymous signcryption scheme is not secure against the chosen plaintext attacks. The basic reason is that signcryption is achieved by using Encrypt-then-Sign paradigm in this scheme. This scheme lacks the binding between the encryption and signature; namely, the output of the encryption is not used as input in the hash of message, which is used for generating the signature. Informally, A is able to distinguish the ciphertext because, A knows that m b is either m 0 or m 1 which were produced to C during the challenge phase by A. Hence, A can find t ′ without having access to the private key of the receiver and this led to the proposed attack. 
Conclusions
Chung et al. 
"
"Introduction
Digital signature is one of the most important techniques in modern information security system for its functionality of providing data integrity and authentication. A normal signature holds self-authentication property, that is, the signature can be verified by anyone who gains access to the signature. So the normal signature is not suitable for the situation where the message signed is sensitive to the signature receiver. To solve the problem, Kim, Park and Won introduced a new type of signature, nominative signature 
In 1996, Mambo, Usuda and Okamoto 
 2 Review on Park-Lee's Nominative Proxy Signature 
 2.1 Concept of Nominative Proxy Signa- ture 
In a nominative proxy signature, not the original signer but the proxy signer generates the nominative proxy signature and sends it to the signature receiver. A nominative proxy signature is called original-nominative proxy signature if the original is the nominator. A nominative proxy signature is called proxy-nominative proxy signature if the verifier is nominated by the proxy. They can be applied in different situations. For instance, the originalnominative proxy signature is suitable for mobile communications in which the receiver is chosen by the mobile user (the original signer), not by the agent entity (the proxy signer). While the proxy-nominative proxy signature is favorable to electronic commerce. On the e-commerce, the manufacturer acts as the original signer in order to provide the customer with quality guarantee. But the manufacturer need not take part in every vendition after the manufacture delegates the vendor. The vendor sells goods to the customers, so the signature receivers (the customers) is determined by the vendor. The nominator should be personated by the vendor (proxy entity). A original-nominative proxy signature scheme satisfies the following requirements: 1) Only the original signer can nominate the receiver (verifier). 
 2) The original signer and the proxy signer cannot repudiate the nominative proxy signature after the signature is generated. 
3) Only the nominee can directly verify the nominative proxy signature. 4) If necessary, only the nominee can prove to the third party that the nominative proxy signature is valid. 
A proxy-nominative proxy signature should satisfy the Requirements 2), 3), 4) and the following condition: 1 ) Only the proxy can nominate the receiver (verifier). 
 2.2 Description of Park-Lee's Nominative Proxy Signature 
We will recall Park-Lee's nominative proxy signature 
r = g k (modp) s A = x A H(M ||T ) + kr(modq). 
2) Proxy Delivery: A sends (M, T, r, s A ) to the proxy signer B in a secure manner. 
3) Proxy Verification: B computes d = H(M ||T ) and checks if g sA ? = y d A r r (modp). If the equation holds, B accepts the delegation. 4) Nominative Proxy Signature Generation: B chooses k 1 , k 2 ∈ R Z * q at random and computes 
R = g k1−k2xB (modp), Z = y k1 C (modp), e = H(y C ||R||Z||M ), s = k 2 x B − k 1 es A (modq). 
The nominative proxy signature on message M is (M, T, r, R, Z, k 1 , s). 
5
) Nominative Proxy Signature Delivery: B sends the signature (M, T, r, R, Z, k 1 , s) to the verifier C. 
6) Verification of Nominative Proxy Signature: C computes 
d = H(M ||T ), e = H(y C ||R||Z||M ), y p = y d A · r r (modp). 
And then C verifies the nominative proxy signature by checking 
(g s · y k1e p · R) xC ? = Z(modp). 
Cryptanalysis of Park-Lee's Scheme
Park-Lee's scheme is a proxy-unprotected partial proxy signature scheme. The proxy signer's public key y B is not be used during the signature verification, the scheme can not provide non-repudiation. In existence, the scheme is insecure against the original signer's forgery. The attack is as follows. A malicious original signer chooses a, b, c, k 1 ∈ R Z * q and computes 
r = g a (modp) 
R = g b (modp) Z = y c C mod p d = H(M ||T ) e = H(y C ||R||Z||M ) s = c − x A dk 1 e − b(modq). Then, (M, T, r, R, Z, k 1 , s
) is a valid nominative proxy signature. This is because: 
(g s · y k1e p · R) xC = [g s · (y d A r r ) k1e · g b ] xC mod p = [g s · g xAdk1e · g ark1e · g b ] xC (modp) = g cxC = Z(modp). 
 Another original signer's forgery attack against Park- Lee's scheme can be found in 
g s · y k1e p · R ? = g k1 (modp), y k1 C ? = Z(modp). 
 3 Review on Seo-Lee's Nominative Proxy Signature 
Description of Seo-Lee's Nominative Proxy Signature Scheme
 The system parameters are the same as those in Park- Lee's scheme. Seo-Lee's scheme 
1) Proxy Signature Key Generation Phase: The phase is executed between the original signer A and the proxy B. a. Proxy Generation: A chooses a random k ∈ R Z q \{0}, and computes r = g k (modp), and s A = x A ·H(M w ||r||T )+k ·r( mod q), where M w is a warrant. b. Proxy Delivery: A sends (s A , M w , T, r) to the proxy signer B. 
c. Verification and Alteration of the Proxy: 
The proxy signer B validates the delegation by checking if the following holds 
g sA = y H(Mw ||r||T ) A · r r (modp). 
If the above equation holds, B generates a proxy signature key s p . 
s p = s A + x B · r(modq). 
2) Nominative Proxy Signature Generation Phase: This phase is executed between the proxy signer B and the nominee C. 
The proxy signer B chooses random integers k 1 , k 2 ∈ R Z * q , and computes: 
R = g k1−k2 (modp) Z = y k1 C (modp) e = H(M ||M w ||y C ||R||Z) s = k 2 − e · s p (modq). 
Thus, B creates a nominative proxy signature (M, M w , T, y C , r, R, Z, s). B transmits the nominative proxy signature to C. 
3) Nominative Proxy Signature Verification Phase: The nominee C computes the proxy signature public key y p . 
e = H(M ||M w ||y C ||R||Z) y p = y H(Mw ||r||T ) A · (y B · r) r (modp). 
And then, the nominee C verifies the nominative proxy signature by checking a congruence 
(g s · y e p · R) xC ? = Z(modp). 
(1) 
This is a proxy-nominative proxy signature. The scheme does not need a secure channel between the original signer A and the proxy signer B. 
Cryptanalysis of Seo-Lee's Scheme
In this subsection, we analyze Seo-Lee's scheme. The scheme tries to overcome the weakness of Park-Lee's scheme. However, there exists a same weakness as Park-Lee's scheme holds. The scheme does not still provide non-repudiation. A dishonest original signer A can create a nominative proxy signature on behalf of the proxy signer B. We show the attack of the original signer's forgery in detail. Proxy Signature Key Generation: A chooses two random a, b ∈ R Z q and computes the proxy signature key: 
r = y −1 B g a y b A (modp) s p = x A · H(M w ||r||T ) + a · r + x A · b · r(modq). 
Nominative Proxy Signature Generation: 
The original signer A uses the proxy signature key s p to produce the nominative proxy signature as the proxy signer B does in Seo-Lee's scheme. 
Nominative Proxy Signature Verification: 
After the nominee C receives the signature 
(M, M w , T, y C , r, R, Z, s), C computes e, y p and checks 
the Congruence (1). As a result, Congruence (1) holds. In other words, A forges a nominative proxy signature successfully. This is because: 
g sp = g xA·H(Mw ||r||T )+ar+xAbr mod p = y H(Mw ||r||T ) A · g ar+xAbr mod p = y H(Mw ||r||T ) A · g ar · (r · y B · g −a ) r mod p = y H(Mw ||r||T ) A · (r · y B ) r mod p = y p mod p. 
(g s · y e p · R) xC = (g k2−spe · y e p · g k1−k2 ) xC mod p = g k1xC mod p = Z mod p. 
In addition, a malicious original signer can frame the proxy signer by forging a nominative proxy signature on any message M . First, the original signer A randomly chooses a, b, d in Z * q . Then A computes 
r = y −1 B g a mod p R = g b mod p Z = y d C mod p. e = H(M ||M w ||y C ||R||Z) s = d − e(x A H(M w ||r||T ) + ar) − b mod q. Thus, (M w , T, y C , r, R, Z, S) 
is a valid nominative proxy signature on message M . This is because: 
e = H(M ||M w ||y C ||R||Z) y p = y H(Mw ||r||T ) A · (y B · r) r (modp) = g xAH(Mw ||r||T )+ar (modp). 
So, the following equations holds: 
(g s · y e p · R) xC = (g s+e(xAH(Mw ||r||T )+ar)+b ) xC mod p = y s+e(xAH(Mw ||r||T )+ar)+b C mod p = y d C mod p = Z. 
Proposed Nominative Proxy Signature Schemes
Two Nominative Proxy Signature Schemes
We first present our original-nominative proxy signature scheme. The system parameters are the same as those in Seo-Lee's scheme. The original-nominative proxy signature scheme comprises of the following phases. . Delegation Phase: 
 1) Proxy Generation: The original signer A generates a warrant m w , which records the delegation limits of authority, valid period of delegation, and the identities of the original signer and proxy signer. A chooses a random k ∈ R Z * q and computes 
r = g k (modp) s A = x A · H(M w ||T ||r||y C ) + k(modq). 
The original signer sends (m w , T, r, y C , s A ) to the proxy signer B. 
2) Delegation Verification: After the proxy signer B receives the delegation warrant and delegation key (m w , T, r, y C , s A ), B checks wether g sA = ry H(Mw ||T ||r||yC) A 
(modp). If so, B begins to execute the proxy signature key generation algorithm. Otherwise , B refuses this delegation. 
3) Proxy Signature Key Generation: The proxy signer B computes the proxy signature key: 
s p = s A + x B H(M w ||T ||r||y C )(modp). 
Proxy Signature Generation Phase: 
To generate an original-nominative proxy signature on message M , the proxy signer B does the same as in Seo-Lee's Scheme and generates a nominative proxy signature (M, M w , T, y C , r, R, Z, s). Then the proxy signer B sends the signature to the nominee C. 
Nominative Proxy Signature Verification Phase: 
The verifier C first checks if message M signed conforms to the warrant M w , then computes the proxy signature public key y p . 
y p = g sp = r(y A y B ) H(Mw ||T ||r||yC) (modp). 
And then, the nominee C verifies the nominative proxy signature by checking 
(g s · y e p · R) xC ? = Z(modp), 
(2) 
where e = H(M ||M w ||T ||r||y C ||R||Z). 
Nominative Proxy Signature Confirmation Phase: 
 If necessary, the nominee C (prover) proves the validity of the signature to the third party (verifier) V. The nominee C proves that (g s · y e p · R) xC = Z(modp) and g xC = y C (modp) in a zero-knowledge manner. The zero knowledge confirmation protocol is executed between C and V as follows. 
1) C computes u = g s · y e p · R(modp), and sends (u, M, M w , T, r, y C , 
R, Z) to the verifier V. 
2) V computes e = H(M ||M w ||T ||r||y C ||R||Z) and checks if u = g s · y e p · R(modp). 3) C proves to the verifier V that log u Z = log g y C in a zero knowledge fashion. We can construct a proxy-nominative proxy signature scheme in a similar way. For completeness, we list the components of a proxy-nominative proxy signature scheme. 1) Delegation Phase: A computes: 
k ∈ R Z * q , r = g k (modp) s A = x A · H(M w ||T ||r) + k(modq). A sends (M w , T, r, s A ) to B. Next, B checks g sA ? = r · y H(Mw ||T ||r) A ( mod p) and then computes s p = s A + x B · H(M w ||T ||r)(modq). 2) Signing Phase: B computes: k 1 , k 2 ∈ R Z * q , R = g k1−k2 (modp), Z = y k1 C (modq) e = H(M ||M w ||T ||r||y C ||R||Z) s = k 2 − e · s p (modq). Then B sends (M, M w , T, y C , r, R, Z, s) to C. 
3) Verification Phase: C checks: 
y p = r · (y A y B ) H(Mw ||T ||r) (modp), 
e = H(M ||M w ||T ||r||y C ||R||Z) (g s · y e p · R) xC ? = Z(modp). 
Security Analysis of Proposed Schemes
We can make analysis of both the proposed nominative proxy signature schemes in a similar way. For simplification , we only present the analysis of the proposed originalnominative proxy signature scheme. Firstly, the signature scheme does not require a secure channel between the original signer and the proxy signer. Secondly, the nominative proxy signature scheme holds nonrepudiation. An original signer cannot forge any valid proxy signature key as mentioned in Section 3.2. It is intractable for the original signer to choose a proper r and compute s p from the following the equation: 
g sp = r(y A y B ) H(Mw ||T ||r||yC) (modp). 
 The proxy signature key s p is in essence a Schnorr signature on message M w using private key (x A +x B ). Schnorr signature scheme is provably secure 
Conclusion
 In this paper, we classify the nominative proxy signature into original-nominative proxy signature and proxynominative proxy signature. Then we analyze Park and Lee's nominative proxy scheme. The scheme does not satisfy the foundational property of nominative proxy signature: only the nominee can verify the signature. It suffers from universal verification. We show that Seo and Lee's scheme is insecure against the original signer's forgery. Finally we present our nominative proxy signature schemes which hold all the properties of a nominative proxy signature scheme. Compared with the scheme recently proposed by Wang, our scheme is more efficient. 
"
"Introduction
 Computational difficulty of the discrete logarithm problem (DLP) in the finite fields is used in different cryptographic schemes such as public key distribution proto- cols 
Zero-knowledge Protocol
 Any public key agreement cryptoscheme can be transformed into some zero-knowledge protocol. The idea of such transformation relates to the possibility of two users' generating a common secrete value with the help of their public key exchange. For example, in the Diffie-Hellman scheme the public key y is generated as follows, using sufficiently large prime p, such that some another large prime q divides the number p − 1, and a primitive element α modulo p. Some user generates his private key as a random value k < p − 1 and computes the value 
y = α k mod p. 
Another user generates his private key as a random value u < p − 1 and computes his public key R = α u mod p. Each of them is able to compute the common secret 
Z = y u mod p = R k mod p. 
Any other person is not able to compute Z until the DLP modulo p is solved and value k or value u is computed from the known values p, α, y, and R. Suppose the first user (claimant) is the person to be authenticated by the second user (verifier). Suppose also the verifier has been provided with a trusted copy of claimant's public key y. The proposed zero-knowledge protocol, in which there is used some specified hash function h( * ), includes the following two steps: 
1) The verifier generates a random number u < p − 1 and computes the one pad public key R = α u mod p. Then he computes common secret Z related to R and claimant's public key y: Z = y u mod p, and hash function value H = h(Z). Then verifier sends to claimant the pair of numbers (R, H) that is verifier's request. 
2) Using his private key k the claimant computes the values Z = R k mod p and H = h(Z ). After that he compares the values H and H. If H = H, then the claimant sends to verifier the value Z that is claimant's response, otherwise he sends to verifier the message "" The request (R, H) is not correct "" . On receipt of the response Z the verifier compares Z with the value Z. If Z = Z, then the verifier accepts the claimant as valid owner of the public key y, otherwise the procedure fails. Let us note that computing the hash function values and comparison of the values H and H performed by the claimant at Step 2 is sufficiently important. The identity of the values H' and H proves that the verifier has computed correctly the value R and knows the value Z , i.e. no information about the private key x is provided to the verifier with the response Z . Computation of the hash function values at the first and second steps prevents the following attack on the claimant's private key. The verifier selects a value R having sufficiently small prime order ω modulo p and sends R as his request to the claimant. After receiving the response Z = R k mod p the verifier will be able to compute with the baby-step-giant-step algorithm 
1) The verifier generates a random number u < 2 s − 1 and computes the one pad public key R(x) = (α(x)) u mod p(x). Then he computes common secret Z(x) related to R(x) and claimant's public key y(x) : Z(x) = (y(x)) u mod p(x). Then verifier sends to claimant the binary polynomial R(x) that is verifier's request. 
2) Using his private key k the claimant computes the values Z (x) = (R(x)) k mod p(x) and sends to verifier the value Z that is claimant's response. 
 To reduce the computational complexity of the protocol one can use the low weight irreducible polynomials p(x). 
Commutative Encryption
Encryption algorithm EA K (M ), where M is the input message; K is the encryption key, is called commutative, if the ciphertext produced by two consecutive encryptions with keys K 1 and K 2 does not depend on the order of using the keys, i.e. 
EA K2 (EA K1 (M )) = EA K1 (EA K2 (M )) . 
Analogously to the Pohlig-Hellman algorithm 
d = e −1 mod p s − 1. 
Encrypting the message M represented as an element of the field GF (p s ) is performed as follows: 
C = M e . 
Decrypting the ciphertext C is performed as follows (performance of the decryption procedure does not depend on the selected value λ, since the size of value d does not depend on λ): 
M = C d . 
The following protocol 
 1) The sender encrypts the message M with his encryption key e s : C 1 = M es and sends cryptogram C 1 to the receiver. 
2) The receiver encrypts the cryptogram C 1 with his encryption key e r : C 1 = M er and sends cryptogram C 2 to the sender. 
 3) The sender decrypts the cryptogram C 2 with his decryption key d s : C 3 = C ds 2 and sends cryptogram C 3 to the receiver. Then the receiver recovers the message M as follows M = C dr 3 . 
Correctness. Proof of the protocol: 
C 3 dr = C 2 ds dr = (C 1 er ) dsdr = (M es ) erdsdr = M esds = M. 
Protocols like this one are used, for example, in mental poker 
Public-key Encryption
The ElGamal public-key encryption algorithm 
1) The sender generates the single-use private key u and computes the single-use public key R = a u mod p. Then he computes the single-use secret key Z = y u mod p and encrypts the message M : C = M Z mod p, where C is the produced ciphertext. 
2) Then the values C and R are send to the owner of public key y. 
The decryption procedure is performed as follows: 
 1) Using the value R and private key k the receiver computes the single-use secret key Z = R k mod p. 
2) Then he decrypts the ciphertext C and obtains the message M = CZ −1 mod p. 
Suppose except large prime q some small primes r i (i=1, 2,..., g) divide the number p − 1. Then an adversary can implement some potential known-decryptedtext attack on the ElGamal algorithm that relates to the following scenario. The attacker selects a value R < p having composite order ω = g i=1 r i modulo p, generates a random value C < p, and then sends the values R and C to the owner of the public key y. The receiver computes the value M = (CZ −1 mod p) = (CR −k mod p) that become some way known to the attacker. The last computes the value Z = (CM −1 mod p) = (R k mod p). Then using the baby-step-giant-step algorithm 
 1) The sender generates at random the single-use private key u < 2 s − 1 and computes the single-use public key ρ(x) = (α(x)) u mod π(x). Then he computes the single-use secret key as the polynomial ζ(x) = (χ(x)) u mod π(x) and encrypts the message M (x) : C(x) = M (x)ζ(x) mod π(x), where C(x) is the ciphertext. 
2) Then the values C(x) and ρ(x) are sent to the owner of public key y, i.e. to the receiver of the message M (x). 
The decryption procedure is performed as follows: 
 1) Using the single-use public key ρ(x) the receiver computes the value ζ(x) = (ρ(x)) k mod π(x). 
Conclusions
In this paper it has been proposed a new construction of the zero-knowledge protocol, which is based on the public key agreement scheme. The most simple design of the proposed protocol relates to the case of using binary finite fields GF (2 s ) for which the value 2 s −1 is a Mersenne prime. It has been also shown that such fields represent significant interest for using them in the commutative and public-key encryption algorithms. Besides, potential application of the Mersenne primes relates to the deniableencryption schemes 
"
"Introduction
 Recent technological advances in wireless networking, microelectronics integration and miniaturization, sensors, and the Internet allow us to fundamentally modernize and change the way health care services are deployed and delivered. Focus on prevention and early detection of disease or optimal maintenance of chronic conditions promise to augment existing health care systems that are mostly structured and optimized for reacting to crisis and managing illness rather than wellness 
Related Work
 Security issues in WBAN are particularly important because sensitive medical information must be protected from unauthorized use for personal advantage and fraudulent acts that might be hazardous to a user's life (e.g., alteration of system settings, drug dosages, or treatment procedure). The security mechanisms employed in Wireless Sensor Networks do generally not offer the best solutions to be used in Wireless Body Area Networks for the latter have specific features that should be taken into account when designing the security architecture. The number of sensors on the human body, and the range between the different nodes, is typically quite limited. Furthermore, the of the person carrying these devices. This means that it is difficult for an attacker to physically access the nodes without this being detected. When designing security protocols for WBAN, these characteristics should be taken into account in order to define optimized solutions with respect to the available resources in this specific environment 
TinySec
 TinySec is proposed as a solution to achieve link-layer encryption and authentication of data in biomedical sensor networks 
Hardware Encryption
 As an alternative to TinySec, one could utilize hardware encryption supported by the ChipCon 2420 Zig- Bee complaint RF Transceiver, one of the most popular radio chip on wireless sensor nodes. Based on AES encryption using 128-bit keys, the CC2420 can perform IEEE 802.15.4MAC security operations, including counter (CTR) mode encryption and decryption, CBC- MAC authentication and CCM encryption plus authentication . It can also perform plain stand-alone encryption of 128 bit blocks 
Elliptic Curve Cryptography
Recently, elliptic curve cryptography (ECC) has emerged as a promising alternative to RSA-based algorithms, as the typical size of ECC keys is much sorter for the same level of security. There have been notable advances in ECC implementation for WSNs in recent years. Uhsadel et al. 
Biometric Methods
A key establishment method to secure communications in biomedical sensor networks has emerged to be biomet- rics 
Our Contribution
 In this section, we present our protocol (Trust key Management Scheme for Wireless Body Area Network) which secures keys exchange between sensor nodes and the base station with minimal resource consumption. 
Assumptions
 Before describing the protocol, let us identify the assumptions underlying our model. We assume that: @BULLET The base station has more resources than a regular sensor node. @BULLET The base station can keep a record of the keys it shares with each sensor node and can use these keys to send confidential messages to individual nodes. @BULLET The base station can make long range radio transmissions to reach a node anywhere within the sensor network. However, in order for messages to travel from a sensor node to the base station, the message has to hop from node to node in order to maximize the energy conservation. @BULLET The base station has a pair of keys (private and public key). @BULLET Each sensor is capable to use symmetric and asymmetric encryption, by implementing (hard or soft) each of these operations. @BULLET Each sensor node gets the public key of the base station before deployment from an off-line dealer. @BULLET The base station gets a template reference (biometric features generated from the ECG signal of the user) before deployment from an off-line dealer. 
Notation
We will use the following notation to illustrate different primitives in our cryptographic operations: @BULLET Biokey: is the ECG-generated key. @BULLET Biokey Ref : is the template reference. It is used to authenticate sensor nodes by the base station. @BULLET E k (M ): an encryption of message M with a symmetric key K. @BULLET E P ub (M ): is an encryption of message M with the Base station's public key.  @BULLET cmp 1 , cmp 2 , . . .: are examples of a counter (initialized to some random values). @BULLET N A , N B , N C , . . .: are examples of a nonce generated by nodes A, B, C, . . ., respectively. 
The Protocol
The protocol is divided in four steps, key generation phase, key setup phase, key authentication phase and key update phase. 1) Key Generation Phase: While many physiological features can be utilized as biometrics, the ECG has been found to specifically exhibit desirable characteristics for WBAN applications . More specifically, it has delivered promising prospects for security in the WBAN settings. In this emerging area of research, the relevant ECG techniques ostensibly appear to be mere examples of fiducial methods. Fiducials are essentially points of interest on a heartbeat. The P, PQ, QRS, QT, T and RR time intervals as well as the amplitudes of P, R and T fiducials 
 Good cryptographic keys need a high degree of randomness , and keys derived from random time varying signals have higher security, since an intruder cannot reliably predict the true key. This is especially the case with ECG, since it is time-varying, changing with various physiological activities 
 From a cryptographic perspective, the ECGgenerated binary sequence (in our work, it is noted Biokey), is already suitable for a symmetric encryption scheme. However, we use its morphed version using a morphing block (here we use the MD5 function  for the morphing function M(.)) to ensure user privacy and confidentiality. As noted in 
In the next section, is given how to exchange securely the generated session keys (K session ) between each sensor node and the base station. 
2) Key Setup Phase: The node closest to the BS (base station) initiates the key setup phase by issuing the "" join-network "" message. In our sample topology shown in 
Let us assume node A initiates the key exchange phase: 
A → BS : IdA, E pub (BiokeyA), M AC(K sessionA , Id A ||Biokey A ). A BS B C D E IdA, Epub( BiokeyA), MAC(KsessionA, IdA||BiokeyA) E KsessionA (Ok; cmp A ) 
Figure 4: Node A initiates the key setup phase 
The base station decrypts the received message with its corresponding private key, compares the received Biokey A to the template reference Biokey Ref . On confirming the validity of the device (i.e. the check is successful), the base station computes the session key K sessionA using the received Biokey A . It uses this key (K sessionA ) to check the MAC message and to send the following encrypted information to node A: an Ok message and a counter cmp A , initialized to some random value. The counter is used to assure freshness. Every time the counter is used, the value gets incremented by 1. 
BS → A : E K sessionA (Ok, cmp A ). 
The first nodes that manage to complete the key setup procedure with the base station act as gateways for the other nodes in the network. The next sensor node (assume C as shown in 
@BULLET Generates the biometric key Biokey C . @BULLET Computes the session key K sessionC , (K sessionC = M (Biokey C )) 
@BULLET Encrypts the Biokey C with the base station's Public key. The request "" join-network "" is then broadcast by the node C. 
E KsessionC (Ok; cmp C , Id A , K A-C ) A BS B C D E Id C , E pub ( Biokey C ), MAC(K session , Id C ||Biokey C ) Id A , E KsessionA (N A ) Id C , E pub ( Biokey C ), MAC(K sessionC , Id C ||Biokey C ) 
Figure 5: Node C initiates the key exchange phase 
In our sample topology, the request will be received by the node A. 
C → A : Id C , E pub (Biokey C ), M AC(K sessionC , Id C ||Biokey C ). 
Node A generates a nonce N A and encrypts it with its session key. It appends its identifier Id A and the encrypted N A to the request. The request is finally forwarded to the BS: 
A → BS : Id C , E pub (Biokey C ), M AC(K sessionC , Id C ||Biokey C ), Id A , E KsessionA (N A ). 
The base station performs the routine validity checks on the sensor node and sends node C the information it needs to be a part of the network. In addition to the Ok message and the counter cmp C the base station also sends node C the identifier Id A of node A and the key authentication K A−C 1 
(K A−C = M (K sessionA ||N A )
) to inform it that node A is its gateway to reach the base station.  Once this information is available at node C, it attempts to authenticate its gateway A as described in the next section. Node C is setting up a secure key with the base station . The gateway node A has already successfully complete the key setup procedure with the base sta- tion. 
BS → C : E KsessionC (Ok, cmp C , Id A , K A−C ). 
3) Key Authentication Phase: On receiving the Id A and the key authentication K A−C , node C attempts to authenticate its gateway A using a challenge response. To do so, node C generates a nonce N C , encrypts it with the key authentication K A−C and transmits it to the node A. Node A, on receiving an "" authenticate me "" message, computes its own copy of K A−C = M (K sessionA ||N A ) and responds with the original nonce N C and a new nonce N A , both encrypted with the newly agreed key K A−C . To complete node C's authentication, node C responds with the nonce N A encrypted with the shared key K A−C . 
N odeC → N odeA : Id C , E KA−C (N C). N odeA → N odeC : Id A , E KA−C (N C, N A). N odeC → N odeA : Id C ; E KA−C (N A). 
 The same process is then carried out for all the remaining sensor nodes as they join the network. 4) Key Update Phase: a key update tries to prevent long term attack aiming to extract the encrypting keys by analyzing the encrypted traffic over the network for long time. In a WBAN an automatic key update must be defined, since a network can be deployed for many days or months. In our approach, we propose a periodic key update for each established session key. 
The key update is initiated by the base station by launching a key update request. On receiving the key update request, each sensor node generates a new biometric key Biokey', Encrypts it with the base station's public key and sends the encrypted message to the base station. 
N ode → BS : E P ub (Biokey ). 
On receiving the encrypted message, the base station decrypts it, checks the node's validity, computes the new session key from Biokey' (the new key K 
session = M (Biokey )
), updates the session key and sends an Ok message and a new counter cmp to the node. 
BS → N ode : E K session (Ok, cmp ). 
The period of the key update is relative to the key length and the complexity of the used algorithm which means that this period is fixed by the administrator of the WBAN. 
Analysis
Security Services
@BULLET Confidentiality: This aspect is ensured by the use of symmetric encryption to encrypt the exchanged traffic between the base station and sensor nodes. The confidentiality is enforced using automatic key update to prevent long term attacks. @BULLET Integrity: The integrity in our approach can be ensured using MAC (Message authentication codes) computed and joined to each sent packet between the base station and any sensor over the network. @BULLET Authentication: Authentication between sensor nodes over the WBAN as well as between each sensor node and the base station is ensured using the ECG-generated keys. @BULLET Data freshness: the use of the counter avoids replay attacks and ensure data freshness. 
Energy Cost Analysis
 The energy cost of any key management scheme is determined by the energy required for the execution of cryptographic primitives and the energy needed for transmitting the encrypted data. According to 
¡ ¢ £ ¤ ¥ ¦ ¦ ¦ § ¦ ¨ © © ¦ 
Concluding Remarks
Wireless Body Area Networks (WBANs) are an enabling technology for mobile health care. These systems reduce the enormous costs associated to patients in hospitals as monitoring can take place in real-time even at home and over a longer period. A critical factor in the acceptance of WBANs is the provision of appropriate security and privacy protection of the wireless communication medium. The data traveling between the sensors nodes should be kept confidential and integrity protected. Certainly in the mobile monitoring scenario, this is of uttermost im- portance. In this paper, we have presented a trust key management scheme for wireless body area network. Our protocol attempts to solve the problem of security and privacy in WBANs. It also aims to securely and efficiently generating and distributing the session keys between the sensor nodes and the base station to secure end to end transmission . It also allows to secure communication links between the nodes themselves. Compared to other approaches, our approach is more suitable for wireless body area network because it is efficient and energy saving. 
"
"Introduction
 Key authentication is very important in a public key cryptosystem . In such cryptosystem, each user has two keys: a public key and a private key. There is a possible threat in public key cryptosystem: an intruder can revise the public key from the public key directory and substitute the public key of a target user. In this way, the intruder can impersonate the user by means of his/her public key and, hence, raise a security threat of fabrication. The purpose of key authentication is to verify the public key of a legal user and prevent a forgery of the public key. In 2003, Lee et al. 
In this paper, we will show that a dishonest user can successfully deny his signature in the ZK-scheme. That is, the ZK-scheme does not achieve non-repudiation of the user's public key either. 
Review of the ZK-scheme
Similar to the LHL-scheme, the ZK-scheme also employs a password table that needs a trusted server. The system parameters of the ZK-scheme are as follows: Let p and q are prime numbers such that q|p − 1, g is a generator with order q in Z * p . The one-way function f is defined by f (x) = g x mod p. The p, q, and f (x) are made public. The user of the system has P rv as his/her private key and PWD as his/her password. Let P ub of the user's public key is: 
P ub = g P rv mod p. 
During the registration phase, the certificate of the public key is generated by the user with his/her password and private key. Each user chooses a random number r ∈ Z * q , and then calculates f (P W D + r). The certificate C of the user's public key is: 
C = P W D + r + P rv · P ub mod q. 
The user then sends f (P W D + r), R = g r mod p and his/her ID to the server secretly. The server verifies them by checking whether the following verification equation holds: 
f (P W D + r) = f (P W D) × R mod p. 
If the above equation holds, then the server stores ID and f (P W D + r) in public password table. To protect against illegal modification, the server uses the access control mechanisms. The certificate C and the public key P ub are opened to the public over the network. During the key authentication phase, the sender first downloads the receiver's certificate C, public key P ub and f (P W D + r) from the public directory in the network and public password table in the server. Then the sender checks the certificate C of the public key by computing the following equation: 
f (C) = f (P W D + r) × P ub P ub mod p. 
(1) 
 If the above equation holds, the sender accepts the public key P ub of the receiver; otherwise, the sender rejects it. Obviously, this scheme works correctly. 
Weakness
of the Non- Repudiation Service Dispute of digital signatures is a common problem that could jeopardize electronic commerce applications. Nonrepudiation is an essential security attribute, which can provide evidence to enable dispute resolution. Unfortunately , the ZK-scheme does not achieve this attribute that secure key authentication schemes should achieve. In the ZK-scheme, when the public key P ub is odd (a dishonest user can ALWAYS choose his private key P rv in such a way that P ub will be an odd number, and this will later allow him to cheat), a dishonest legal user can deny the signature signed using his private key P rv. To do this, the dishonest user does as follows: 
Computes P ub = p − P ub . We have (P ub ) P ub mod p = (p − P ub) (p−P ub) = p p−P ub + p − P ub 1 × p (p−P ub)−1 × (−P ub) 
+ · · · + p − P ub p − P ub − 1 × p × (−P ub) (p−P ub)−1 + (−P ub) (p−P ub) = (−P ub) (p−P ub) = (−P ub) (p−1) × (−P ub) (1−P ub) = (−P ub) (1−P ub) mod p. 
Since P ub is odd, 1 − P ub is an even number. So we have 
(−P ub) (1−P ub) mod p = (P ub) (1−P ub) = (g P rv ) (1−P ub) = g (P rv−P rv×P ub) mod p. 
Let C = C+P rv−2×P rv×P ub mod q and substitutes the fabrication certificate C and public key P ub in the public key directory. We can verify that (C , P ub ) will pass the verification Equation (1). So anyone will also be convinced of the integrity of the forged public key P ub via the same previously mentioned checking Equation (1). However, the signature generated using P rv and verified using P ub, cannot be verified using the forged public key P ub . As a result, the dishonest user can deny his signature. Therefore, we have shown that the ZK-scheme does not achieve the non-repudiation service. 
f (P W D + r) × (P ub ) P ub mod p = g (P W D+r) × g (P 
Jie 
"
"Introduction
 RSA and McEliece are the oldest public key cryptosystems . They are based on intractability of factorization and syndrome decoding problem respectively. However, McEliece was not quite as successful as RSA, partially due to its large public key and another main handicap of the belief that McEliece could not be used in signature. Recently , Nicolas T. Courtois, Matthieu Finiasz, and Nicolas Sendrier (CFS) show a new way to build practical signature schemes with the McEliece public key cryptosystem . They are based on the well-known hard syndromedecoding problem that after 30 years of research is still exponential. Thus it would be very interesting to dispose of some other cryptosystems (such as ring signature and blind signature schemes) based on such hard decoding problems. In this letter, we propose an approach of constructing ring signature that is based on the coding theory . The main motivation for the suggested cryptosystem comes from previous studies of McEliece-Based digital signature scheme 
Ring Signature
 Following the formalization about ring signatures proposed in 
@BULLET Key-Gen is a probabilistic polynomial algorithm that takes a security parameter(s) and returns the parameters (system parameters, private parameters and public parameters). @BULLET Sign is a probabilistic polynomial algorithm that takes system parameters, a private parameter, a list of public keys p k1 , · · · , p kl of the ring, and a message M , the output of this algorithm is a ring signature σ for the massage M . @BULLET Verify is a deterministic algorithm that takes as input a message M ,a ring signature σ, and the public keys of all the members of the corresponding ring, then outputs "" True "" if the ring signature is valid, or "" False "" otherwise. 
The resulting ring signature scheme must satisfy the following properties: 
 @BULLET Correctness: any verifier with overwhelming probability must accept a ring signature generated in a correct way. 
 @BULLET Anonymity: any verifier should not have probability greater than 1/l to guess the identity of the real signer who has computed a ring signature on behalf of a ring of l members. If the verifier is a member of the ring distinct from the actual signer, then his probability to guess the identity of the real signer should not have greater than 1/(l − 1). 
 @BULLET Unforgeability: any attacker must not have nonnegligible probability of success in forging a valid ring signature for some message M on behalf of a ring that does not contain him, even if he knows valid ring signatures for messages, different from M, that he can adaptively choose. 
Review of Courtois et al.'s McEliece-based
Signature Scheme 
Signature Algorithm: 
@BULLET Hash the document M into s = h(M ); 
@BULLET Compute s i = h([· · · s · · · |i]) for i = 1, 2, 3 · · · ; 
 @BULLET Find i 0 the smallest value of i such that s i is decod- able; @BULLET Use the trapdoor function to compute z such that 
Hz T = s i0 ; 
@BULLET Compute the index I z of z in the space of words of weight 9; 
@BULLET Use [· · · I z · · · |i 0 ] 
as a signature for M , where I z is the index of z by 
I Z = 1 + ( i1 1 ) + ( i2 2 ) + · · · + ( i9 9 ), and i 1 < i 2 < · · · < i 9 
denote the positions of the non-zero bits of z. 
Verification Algorithm: 
@BULLET Recover z from its index I z ; 
@BULLET Compute s 1 = Hz T with the public key H; 
@BULLET Compute s 2 = h([· · · h(M ) · · · |i 0 ]
) with the public hash function h; @BULLET Compare s 1 and s 2 : if they are equal the signature is valid. 
Our Proposed Ring Signature Scheme
As we know, the SD problem is NP-hard, all among several known attacks for SD are fully exponential and nobody has ever proposed an algorithm that behaves differently for complete decoding and the bounded decoding problems within a distance accessible to the owner of the trapdoor. In this section, we will extend T.Courtois et al.'s signature scheme to a practical ring signature scheme, which is also based on syndrome decoding problem. The three-tuple of our ring signature is as follows. 
Key-Gen (Key generating Algorithm): Each potential signer A i selects a t-error correcting Goppa code C i of dimension n − tm and length n = 2 m , chooses a generating matrix G 0 i and chooses a generating matrix H 0 i , selects randomly two non-singular matrixes U i , V i and a permutation matrix P i , computes G i = U i G 0 i P i and H i = V i H 0 i P i , then makes H i as A i 's public key and G i , U i , V i , P i as A i 's private keys. As discussed in 
3) (Generate forwarding ring sequence): 
For i = r + 1, · · · , l − 1, 0, 1, · · · , r − 1, choose randomly z i,q ∈ F n 2 (of weight t) and compute s i+1,q = h(L|h(M )|H i z T i,q ⊕ s i,q ); 
4) Find ¯ q the smallest value of q such that s r,¯ q ⊕ ¯ s ¯ q is decodable; 5) Use the trapdoor function to compute z r,¯ q such that 
H r z T r,¯ q = s r,¯ q ⊕ ¯ s ¯ q ; 
6) Compute the index I Zi,¯ q 's of z i,¯ q 's in the space of words of weight 9'as follows: 
I zi,¯ q = 1 + ( i1 1 ) + ( i2 2 ) + · · · + ( i9 9 ), Where i 1 < i 2 < · · · < i 9 
 denote the positions of the non-zero bits of z i,¯ q ; 7) Output the ring signature: select 0 as the glue value, the resulting signature for M and L is the l + 1 -tuple: 
(s 0,¯ q , I z0,¯ q , I z1,¯ q , · · · , I z l−1, ¯ q ). For simplicity , let s i = s i,¯ q , z i = z i,¯ q , I zi = I zi,¯ q , i = 0, · · · , l − 1
, then the resulting signature is denoted as 
(s 0 , I z0 , I z1 , · · · , I z l−1 ). Verify: Given (s 0 , I z0 , I z1 , · · · , I z l−1 ), M , and L: 1) Recover z i from the index I zi , i = 0, 1, · · · , l − 1; 2) Compute s i+1 = h(L|h(M )|H i z T i ⊕ s i ) for i = 0, 1, · · · , l − 1, Accept if s l = s 0 , and reject other- wise. 
Analysis of the Proposed Scheme: From the procedure of ring signature generation, we have: 
s r+1 = h(L|h(M )|¯ s ¯ q ) s r+2 = h(L|h(M )|H i+1 z T i+1 ⊕ s l+1 ) . . . . . . s l = h(L|h(M )|H l−1 z T l−1 ⊕ s l−1 ) = s 0 s 1 = h(L|h(M )|H 0 z T 0 ⊕ s 0 ) s 2 = h(L|h(M )|H 1 z T 1 ⊕ s 1 ) s r = h(L|h(M )|H r−1 z T r−1 ⊕ s r−1 ). Since H r z T r = s r ⊕ ¯ s ¯ q , we have s r+1 = h(L|h(M )|H r z T r ⊕ s r ) = h(L|h(M )|¯ s ¯ q ). 
Cost and Length
Signature Cost: The essential operation in our ring signature is to make t! decoding attempts, as in the CFS signature scheme 
2) Solve the key equation: the signer needs to apply Berlekamp-Massey algorithm to obtain the locator polynomial, with the costs O(t 2 ) operations in 
F 2 m . 
 3) Find the roots of the locator polynomial. If the syndrome is decodable, the locator polynomial splits in In Step 1 of our scheme, the signer has to compute l hash functions and l − 1 multiplication H i z i,q , in Step1 of CFS signature scheme, the signer only needs to compute one hash functions and no multiplication H i z i,q . In Steps 2 and 3 of our scheme, the signer does the same thing as one does in the Steps 2 and 3 on CFS scheme. Due to the fact that the head cost in the signature generation of our signature scheme or CFS scheme are Steps 2 and 3, we may say the total cost of our signature generation is approximate to that of CFS. 
Verification Cost: 
The verifier needs to recover z i 's from I zi , computes l multiplication H i z T i , and l + 1 hash functions. Each multiplication H i z T i can be computed by adding the t corresponding columns. The total cost of this verification is lt column operations and l + 1 hash computations. 
Ring Signature Length: 
The signature for massage M and ring L is 
(s 0 , I z0 , I z1 , · · · , I z l−1 )
, if we take m = 16 and t = 9, then the length of s 0 is 144 bits (n − k bits = tm bits= 16 × 9 = 144 bits), the length required to store I zi is about 126 bits, the total length of our signature is about 144 + 126l. For the discrete logarithm based ring signature schemes 
Security
Anonymity: For the anonymity, We have that any attacker outside a ring of l possible users has probability 1/l to guess which member of the ring has actually computed a given signature on behalf of this ring, because all z i but z r are taken randomly from F n 2 , in fact, z r = ¯ z ⊕ ¯ z r , because ¯ z is distributed uniformly over F n 2 , this results in that z r is uniformly over F n 2 . Unforgeability: When n = 1, our ring signature scheme degenerates into the McEliece-Based signature proposed by N.T. Courtois et.al (let H 0 z T 0 ⊕ s 0 = i 0 in N.T.Courtoiss scheme). N.T.Courtois's scheme is non-forgeability under the two assumptions: 
@BULLET Solving an instance of decoding problem is difficult. @BULLET Recovering the underlying structure of the code is difficult. 
For n > 1, we denote L a fix set of ring members, and suppose that an attacker A's public key does not belong to L, he may do the following: @BULLET A1 Choose randomly a word s 0 ∈ F n−k 2 
; 
@BULLET A2 Do the same as "" generate forward ring sequence "" 
of Sign for i = 0, 1, · · · , l − 2; 
@BULLET A3 Assign s 0 to h(L|h(M )|H l−1 z T l−1 ⊕ s l−1 ); @BULLET A4 Output the ring signature 
(s 0 , z 0 , z 1 , · · · , z l−1 ). 
Since h acts as a random oracle and z i 's are taken randomly from F n 2 , the probability of s 0 = h(L|h(M )|H l−1 z T l−1 ⊕ s l−1 ) is 1/2 n . So we say that our proposed ring signature is unforgeable. 
Conclusion
 We presented a code-based ring signature scheme. The security is based on the well-known hard syndrome-decoding problem that is still exponential. The signature length and the verification cost will always remain extremely small. The unique features make our coding-based ring signature scheme an exclusive choice for some applications while excluding other. 
"
"Introduction
Singular cubic curve 
y 2 + axy = x 3 + bx 2 mod p, 
(1) 
where a, b ∈ Z p may produce a number of solution. The set of all solutions (x, y) ∈ F p F p to Equation (1), is called the solution space of the given singular cubic curve. This paper uses faster RSA-type schemes based on curve, 
y 2 + axy ≡ x 3 (modn), 
and uses the concept of isomorphic mapping 
M = [m ij ]. R = [r ij ] is a matrix, so M − [r ij ] 
is a valid matrix. Therefore, matrix manipulation can be applied for any cryptographic exploitation. Selective encryption may not be effective when selected subset is small 
k i = k i−1 ⊕ D i foralli > 0 
Where, D i = Data exchanged in session. The security models differ according to the application domain. The network security model 
Related Work
Koyama 
Public Key Cryptosystem Scheme I
This cryptosystem is based on the singular cubic curve 
C n (0, b) : y 2 ≡ x 3 + bx 2 (modn) 
where n = pq is the product of two distinct large odd primes greater than 3. The encryption key e is chosen such that (e, N ) = 1 where N = lcm (p − 1, p + 1, q − 1, q + 1). The decryption key d is chosen such that ed ≡ 1 mod N . The public key is the pair (n, e) and the private key is d,p and q. To encrypt a plain text pair M = (m x , m y ) the sender first computes b = 
(m 2 y −m 3 x ) m 2 x 
(modn) and then the cipher text is computed as C = (c x , c y ) = 
, m y ) by computing d(c x , c y ) = (m x , m y ) 
over the singular cubic curve C n (0, b)as in the case of any other cryptosystem. 
Public Key Cryptosystem Scheme II
This cryptosystem is based on the singular cubic curve of the form, 
C n (a, 0) : y 2 + axy ≡ x 3 (modn) 
where n = pq is the product of two distinct large odd primes greater than 3. The encryption key e is chosen such that (e,N)=1 where N = lcm(p − 1, q − 1). The decryption key d is chosen such that ed ≡ 1 mod N . The public key is the pair (n, e) and the private key is d, p and q. To encrypt a plain text pair 
M = (m x , m y ) the sender first computes a = (m 3 x −m 2 y ) 
(mxmy) (modn) and then the cipher text is computed as C = e M on the singular cubic curve C n (a, 0). The complete cipher text is (C, a). The receiver, who knows the decryption key d, can get back the plain text (m x , 
m y ) by computing d(c x , c y )=( m x , m y ) 
over the singular cubic curve C n (a, 0). The computational method differs from the earlier one is a significant way. 
Public Key Cryptosystem Scheme III
This cryptosystem is based on the singular cubic curve of the form, where n = pq is the product of two distinct large odd primes greater than 3 
C n (α, β) : (y − αx)(y − βx) ≡ x 3 (modn) 
M = (m x , m y ) 
the sender first choose α randomly and computes β= 
(m 3 x −m 2 y +αmxmy) (mx(amxmy) 
(modn). The cipher text is computed as C = e M on the singular cubic curve C n (α, β). The complete cipher text is obtained as (C, α, β). The receiver knows the decryption key d can get the plain text back (m x , 
m y ) by computing d(c x , c y )=(m x , m y ) 
over the singular cubic curve C n (α, β). However, all three schemes are equivalent to each other in term of robustness, speed and complexity 
Proposed Scheme of Public Key Cryptosystem
 The proposed scheme is based on Selective Encryption with time variant key (AVK). To construct such a scheme, random part of plain text is chosen for encryption/decryption ; then applying AVK in this selective 
Proposed Generalization of Koyama Scheme-II Using AVK with Application of Selective Encryption
The algorithm demands the implementation of the steps of key generation, encryption and decryption. The steps can be elaborated as follows: 
Select a part [r ij ] from M . 
Step 1: Key Generation. 
1) Select large prime number p, q. 
2) n = p * q. 
3) N = lcm(p − 1, q − 1). 4) Select integer e. 
If (gcd(e, N ) == 1) where, 1 < e < N then generate public key n, e. 
Calculate: d p and d q . Using, 
d p = e −1 mod (p − 1) d q = e −1 mod (q − 1). 
The secret key (p, q, d p , d q ) is generated. Key generation process allows the sender and receiver to use the key for further communication. 
Step 2: Encryption. Plain text (m x , m y ) and public key (e, n). 
c = ( m 3 x m 2 y ) e mod n a = ( m 3 x − m 2 y m x m y ) mod n. 
Sender sends (c, a) to the receiver. 
Step 3: Decryption. The shadow cipher text (c, a) with secret key (p, q, d p , d q ) is received by receiver. 
C p = c mod p 
is known. Therefore, 
m p = c dp p mod p 
can be calculated. Again, 
C q = c mod q 
is known, therefore 
m q = c d q q mod q 
can be calculated. 
 Receiver computes, (m x p , m y p ) and (m x q , m y q ) us- ing, C p (a, 0), a p = a mod p C q (a, 0), a q = a mod q. 
Using isomorphic mapping, following can be obtained 
m xp = a 2 p m p (m p − 1) 2 mod p m y p = a 3 p m p (m p − 1) 3 mod p m xq = a 2 q m q (m q − 1) 2 mod q m y q = a 3 q m q (m q − 1) 3 mod q. 
Finally we obtain, 
(m x p , m y p ) ∈ C p (a p , 0) a p = a mod p (m x p , m y p ) ∈ C p (a p , 0) a q = a mod q. 
By application of Chinese Remainder Theorem 
(m xp , m yp ) ∈ C p (a p , 0) (m xq , m yq ) ∈ C q (a q , 0). 
We get, (m x , m y ) ∈ (a, 0). Encryption has been applied on selected part of the text message M only and therefore named as selective encryption technique. Decryption has been applied on selected part using the similar algorithm by the receiver. Finally, the full text document can be constructed by merging the decrypted selected part of M , namely 
 4 Experimental Set up of Proposed Algorithm 
Experimental setup consists of a desktop Pentium IV 3.20 GHz CPU. Performance data is collected or stored on such node. In the proposed methodology, the RSA based singular cubic curve has been applied on selected part of M and DES 
Speed of encryption = P lain text (in bytes)/Encryption time. 
 The CPU clock cycle is a standard measure. This metric identifies the energy consumption of the CPU when operation is performed in the encryption process. CPU may consume some amount of energy per cycle indicative of the overhead. The following tasks have been accomplished as the part of the experimental analysis: @BULLET A comparison is conducted between the results of the selected plain text and whole plain text. Selected part is encrypted/decrypted using singular cubic curve and rest of the part is encrypted/decrypted using DES algorithm. @BULLET A study is performed on different size of text data. Each algorithm is further tested for power consump- tion. 
Experimental Analysis
Result of the selective encryption for different file size obtained is shown in 
Analysis on Selective Encryption
The choice of R = 
Simulation Results
The simulation designed had a set of choices. There are cryptography testing tools, benchmark programs for cipher suite like openSSL, SSL DiggerTM etc 
Comparison between Koyama Scheme and Proposed Scheme
 Koyama scheme provides that the length of the transmitted message is 2 log n bits, where n is block size. Since encryption scheme key e can be set as a small value and decryption keys d p and d q are large enough such that log d p ≈ log p and log d q ≈ log q. From Koyama scheme we have calculated number of modular multiplication for decryption 
The comparison clearly shows that the proposed scheme is providing better results in terms of robustness, speed and complexity when compared with its existing sibling algorithm. Though this algorithm is an enhancement of Koyama scheme, the proposed scheme is able to stand in its own merit to be applicable in an e-commerce application domain. 
Conclusion and Future Work to Be Undertaken
 A generalization of the Koyama scheme is achieved by using selective encryption with AVK methodology. Implementation of the method/algorithm is evaluated by the speed of encryption. The algorithm for the selection of blocks out of all the blocks of the plain text is presented. The selection of blocks is the prime factor of desired speed of security performance. Comparison between selected and whole part of the message using proposed technique has been made to show the proof of concept and it is found that time consumed is less in selective encryption of blocks than whole text encryption. The observations for comparisons are presented through graphs for analytical view. The security feature of generalized scheme is analyzed to reveal very promising result taking limited number of parameters. Considering exhaustive set of security parameters is out of the scope of this paper. However , it is intended to take up such study in future. The future direction of research is likely to show some interesting results as observed by employing limited number of parameters in the present study. Comparisons are presented between Koyama and the proposed scheme using graphical analysis in terms of time complexity. It is evident that time complexity of proposed scheme is less than the Koyama scheme. 
 Further studies may be taken up on selective encryption in RSA based non singular cubic curve using AVK. Multivariant rational function on Koyama scheme may provide some interesting results. RSA based super singular cubic curve using selective encryption with AVK can be formulated in order to complete the scheme of variable security level required in different application domain of e-commerce and e-business where robustness is of paramount importance. 
"
"Introduction
Communication in next generation wireless networks will use multiple access technologies, creating a heterogeneous network environment. Practically, a single network cannot cater for all different user needs or provide all services. Nowadays the availability of multimode mobile devices capable of connecting to different wireless technologies provides users with the possibility to switch their network interfaces to different types of networks. Vertical handovers among heterogeneous networks should be supported to guarantee the service continuity. To achieve a seamless handover, a mobile user needs to obtain information of existing networks nearby, in order that he can choose a suitable target network and do some preparations for possible handover. However, the neighbor information discovery is the most time-consuming phase in the handover process 
Related Work
The extended Canetti-Krawczyk (eCK) model 
@BULLET EphemeralKeyReveal(sid ). Σ returns the ephemeral private key of the session sid. @BULLET SessionKeyReveal(sid ). Σ returns the session key of the session sid. @BULLET Establish(Φ). Using this query, the adversary registers an arbitrary public key on behalf of an adversary controlled party Φ. Σ only checks the validity of the public key, but does not need to check the possession of the corresponding private key. 
 A session sid (role, Φ, Ψ, comm) is fresh if the following conditions hold: 
@BULLET Both Φ and Ψ are honest parties. @BULLET ∆ did not query the session key of sid or its matching session sid * (if the matching session exists). @BULLET ∆ did not query both the static private key of Φ and the ephemeral private key of Φ in this session. 
@BULLET If sid * exists, then ∆ did not query both the static private key of Ψ and the ephemeral private key of Ψ in this session. 
@BULLET If sid * does not exist, then ∆ did not query the static private key of Ψ. Security of an AKE is defined as follows. In an eCK experiment, ∆ issues Send, StaticKeyReveal, EphemeralKeyReveal , SessionKeyReveal, and Establish queries polynomial times (in a security parameter) in any sequence . Then ∆ selects a completed session sid, and makes a query Test(sid ). To answer Test(sid ), Σ chooses a bit b∈ {0, 1}uniformly at random. If b = 1, then Σ sets the session key of sid as K. Otherwise, Σ selects K from the key space uniformly at random. Σ then returns K as the answer of Test(sid ). ∆ continues to query Send, StaticKeyReveal, EphemeralKeyReveal, SessionKeyReveal , and Establish polynomial times. At last, ∆ outputs a bit b * , and terminates the game. If the selected test session is fresh and b * = b,then ∆ wins the game. The advantage of the adversary ∆ in the eCK experiment with AKE protocol Π is defined as ADV Π (Ω) = Pr{∆ wins}−1/2. 
eCK Security 
An AKE protocol is secure (in the eCK model) if no efficient adversary ∆ has more than a negligible advantage in winning the above experiment, i.e., ADV Π (∆) < 1/Q(µ) for any polynomial Q(·) when µ sufficiently large. 
 3 Network Information Acquirement Protocol with User Anonymity 
 This section focuses on a new proposal for anonymous network information acquirement using an efficient Schnorr like ID-based signature scheme 
Network Initialization
We consider a network model as depicted in 
3) Chooses two hash function H 
1 : {0,1} * −→ Z * q , H 2 : {0,1} * ×G×{0,1} * −→ Z * q . 
4) Chooses one key derivation function f : G−→{0,1} k . 
5) Outputs system parameters {q, p, E/F p , P, G, PK HAS , H, f }, and keeps s secret. Later, the HAS computes the private keys of all users and the IS. This algorithm takes the master secret key s and an identifier (ID) as input and generates a private key corresponding to that ID. In order to achieve MIIS access anonymity, the HAS selects a pseudo-ID (PID) for each MU and based on the PID a private key is generated . The HAS works as follows for each MU with identifier PID M U . It chooses at random 
r M U ∈ R Z * q ,compute R MU = r MU P and h MU = H 1 (PID MU , R MU ). Then it computes s MU = r MU +h MU s. 
The MU's private key is the tuple (s MU , R MU ) and is transmitted to the MU via a secure channel, namely encrypted by the key shared between the HAS and the MU. The MU's public key is defined as PK MU =s MU P, which can also be computed with R MU , PID M U , and PK HAS from the equation: PK MU =R MU +H 1 (PID MU , R MU )PK HAS . The HAS also generates a private key for the IS as above procedure using ID IS . The private key and public key of the IS are denoted as (s IS , R IS ) and PK IS =s IS P, respectively. 
 3.2 Anonymous Secure Channel Estab- lishment 
When a MU moves to a new place, it should contact the IS to get information about neighbor networks. Suppose that the MU is now in coverage area of network V and he is already connected with the network. Then an anonymous authentication and key establishment process will be conducted between the MU and the IS. The flow chart of our scheme is depicted in 
1) MIIS Authentication Request (MU−→IS): 
PID MU , A, t MU , σ. 
The MU selects a random number a ∈ Z * q , and computes A = aP . He sends a MIIS authentication request message to the IS. The message content is as the following, {PID MU , A, t MU , σ}, where t MU is the timestamp of the MU, and σ is a signature generated by the schnorr like ID-based signature using s MU . Denote {PID MU , A, t MU , σ}as m, then σ is generated as follows 
2) MIIS Authentication Response (IS−→MU) 
ID IS , R IS , B, A, c, MAC. Upon receiving the request message from the MU, first the IS checks the time stamp t MU . If it is fresh, the IS computes the MU's public key by the equation PK MU = H 1 (PID MU , R MU ) PK HAS +R MU (Note R MU can be extracted from σ). Then the IS verifies the signature σ using PK MU by checking the following equation: yP=xP+ H 2 (PID MU , xP, m)PK MU . Successful signature verification implies the message is actually sent by a valid user of the HAS. Hence, the IS accepts the message. Otherwise the protocol is terminated at this stage. Next the IS selects a random number b ∈ Z * q , and computes B = bP . Then it computes the shared secret k IM as follows: 
K IM =(b+s IS )(PK MU +A), k IM =f (K IM , PID MU , ID IS )
. The IS randomly chooses a temporary ID (TID MU ) for the MU and stores an item {TID MU , PID M U , R MU }. The IS generates a ciphertext c by encrypting TID MU using k IM and a symmetric cryptographic algorithm. Later it sends a MIIS authentication response message to the MU. The message content is as the following, {ID IS , R IS , B, A, c, MAC }, where MAC is a value computed using a secure message authentication function λ by the equation MAC =λ(ID IS , R IS , B, A, c, k IM ). On receiving the response message from the IS, the MU computes as bellow. 
P K IS = H 1 (ID IS , R IS )P K HAS + R IS ; K M I = (a + s M U )(P K IS + B
). Then the shared session key k MI is derived from the equation: k MI =f (K MI , PID MU , ID IS ). Next the MU checks whether the MAC equals to σ(ID IS , R IS , B, A, c, k MI ). If it does not hold, the IS fails to pass the authentication. Otherwise, the IS passes the authentication and a secure channel between the IS and the MU is established using the shared key. The MU decrypts c and stores TID MU . Then neighbor network information of the MU can be acquired from the IS through the secure channel. Notes. The MU authentication is achieved by verifying the signature of the user. On the other side, the MU authenticates the IS by MAC generated using the shared key. It is easy to see that K MI =K IM . Later, if the MU moves to another place and wants to access the IS again, the MU will uses TID M U as his identity. The ANIA protocol will be performed except that the message sent in Step (1) consists of {TID MU , A, t MU , xP, y}. Note that R MU (a part of the MU's signature σ composed of {xP, y, R MU }) is not sent in the message, since the PID MU and R MU are stored in the IS. The IS identifies the MU by the TID MU , and it generates a new temporary identity TID * MU for the MU during the authentication procedure. 
Security Analysis
We assume that the cryptography suites employed in our protocol are all secure, such as, hash function, message authentication function and ID-based signature scheme. Then our protocol is secure under the extended Canetti- Krawczyk (eCK) model 
 2) Key-replication attack. ∆ succeeds in forcing the establishment of another session that has the same session key as the test session. 
 If random oracles produce no collisions, the keyreplication attack is impossible as equality of session keys implies equality of the corresponding 3-tuples (which are We only take the adversary against sig for example. The construction of attacker against λ is very similar. The input to Ξ consists of the parameters of the signature scheme, which includes access to a signature oracle. Ξ selects at random one party as MU. For session executed by MU, instead of using the MU's private key to compute the signatures, Ξ will make use of the signature oracle that it has access to in the signature security game that Ξ is simultaneously playing. Therefore, if the active attack occurs, Ξ succeeds in breaking the unforgeability of sig. By assumption forging a valid signature can only occur with negligible probability, the protocol is resilient against active attacks. 
Case 2: Passive attack. In this case, the Test session has a matching session owned by another honest party. We show that if the adversary performs a successful forging attack, the CDH problem could be solved by a solver Ξ. The input to the Ξ is a CDH problem instance (U = uP, V = vP ), where u, v ∈ Z * q and U, V ∈ G. The goal of Ξ is to compute CDH(U,V ) = uvP. For simplicity, we use γ, ω and Γ, Ω denote the static secret keys s MU , s IS and public keys PK MU , PK IS respectively. 
The adversary ∆ is allowed to reveal a subset of (γ, a, ω, b), but it is not allowed to reveal both (γ, a) or both (ω, b). We only take the subcase for example that (γ, b) is revealed by ∆. Other subcases are similar. Ξ selects random matching sessions executed by MU and IS, and modifies the experiment as follows. Ξ sets the ephemeral public keys of MU in the test session to be U, and sets the static public key of IS in the matching session to be V (namely, A=U, Ω = V ). If ∆ wins the game, it must queries f on the same 3-tuple θ, thus it successfully forges K =(γ+u)(v +b)P. Then Ξ can solve the CDH problem as below: CDH(U,V )=K −γbP −γV −bU. With the hardness of the CDH assumption, the adversary could not win the experiment and hence the protocol is secure. 
 In the following, we further discuss some security properties of our protocol. User Anonymity. In our scheme, the pseudo ID, instead of the MU's real identity, is used in access MIIS for privacy protection. Key Freshness. The session key k MI is computed from a function using random numbers from the MU and the IS respectively, which assures the freshness of session key. Forward Secrecy. The random numbers used in session key generation are unpredictable for any party except the MU or the IS. Even if the intruder attacks long term secret information of the MU and the IS, he can not compromise the past random numbers and the past session keys. Resistance to Replay Attack. Replay attack involves the passive capture of data and its subsequent retransmission to produce an unauthorized effect. A replay attack can be prevented by checking the timestamp or the MAC. 
Performance Analysis
 Computation and communication overheads are considered as two important metrics of authentication protocols . We present performance comparison of 802.21a pro- posal 
Conclusion
In this paper, we focus specifically on security of MIIS, and propose a new anonymous access authentication protocol for MIIS. We apply an identity-based Schnorr like 
"
"Introduction
There is a steadily increasing amount of services offered through mobile networks since mobile networks provide a very convenient way of communications. The exponential growth of wireless and mobile networks and their use for business applications over the Internet have brought vast changes for the mobile devices, middleware development , standards and network implementation 
Review of BGSW Scheme
Before describing BGSW scheme, some notation should first be defined. X → Y : Z denotes that a sender X sends a message Z to a receiver Y . U , V and H denote a roaming user, visiting network and home network, respectively . U ID, V ID, and HID denote the identity of U , V and H, respectively. R0, R1, R2 and R3 are random numbers. K V H denotes the secret key shared by V and H, K UH denotes the secret key shared by U and H, and K UV denotes the session key shared by U and V . 
Step 1. U → V : Request, R0 
3) U → V : [R3]K UV Step 8. 1) Decrypt [R3]K UV 2) Verify R3 3) V → U : [[R3]K UV ]K UV Step 9. 1) Decrypt [[R3]K UV ]K UV 2) Verify [R3]K UV 
Challenge-response technique 
Our Scheme
 The idea behind our scheme is that we do our best to promote the overall performance of the authentication process as efficiently as possible. The overall efficiency of our scheme is expected to be better than BGSW protocol in case honest communications constitute the majority of total communications. The details are described as follows and 
Step 1. U generates a random number R0, and then sends a service request and R0 to V . 
Step 2. V forwards the service request to H. 
Step 3. H generates a random number R1 and sends it to V . 
Step 4. V generates a random number R2 and session key K UV used by U and V , and then sends 
M 1 = [U ID, K UV , R1, R2]K V H to H. 
Step 5. H decrypts the message M 1 to get U ID, K UV , R1 and R2, and verifies whether R1 is the same as what was sent to V in Step 3. If R1 passes the verification , V is authenticated by H. Then, H sends 
M 2 = [V ID, K UV , R2]K UH and R2 to V . 
Step 6. V verifies whether R2 is the same as what was sent to H in Step 4. If R2 passes the verification, H is authenticated by V . Then sends M 2 and 
M 3 = [R0]K UV to U . 
Step 7. U decrypts the message M 2 to get V ID, K UV , and R2, and then decrypts M 3 to get R0. Then U verifies whether R0 is the same as what was sent to V in Step 1. If R0 passes the verification, V is authenticated by U . And then U responds to V s challenge with the 
M 4 = [R2]K UV . 
 Step 8. V decrypts the message M 4 to get R2 and verifies whether R2 is the same as what was sent to H in Step 4. If R2 passes the verification, U is authenticated by V . 
Step 4. 11 ) Generate KK UVV 
2) Sendd M11 too HH 
Step 6.. 
1)Verify R22 2)Sendd M22 , M33 too UU 
Security Analysis
 Anyone trying to masquerade as the U , V , or H cannot pass the authentication process, because all possible fraudulent behaviors will be found through fraud detection . In our protocol, H authenticates V by verifying R1 in Step 5, V authenticates H by verifying R2 in Step 6, U authenticates V by verifying R0 in Step 7, and V authenticates U by verifying R2 in Step 8. 
H Authenticates V by Verifying R1 in Step 5
In Step 5, H decrypts the response message [U ID, K UV , R1, R2]K V H return from V to obtain R1. Because H is the only other entity that knows the key K V H and therefore V is authenticated if the decrypted authenticator R1 is the same as what was sent to V in Step 3. It is impossible to forge the message [U ID, K UV , R1, R2]K V H to pass the authentication process because there is no way to alter bits in ciphertext to produce the desired changes in the plaintext without knowing key K V H . A forged V will be detected by H in Step 5. 
V Authenticates H by Verifying R2 in Step 6
By verifying the correctness of the authenticator R2, V can assure the responder has the ability to decrypt the message M 1 = [U ID, K UV , R1, R2]K V H sent to the alleged H in Step 4. Only H known the secret key K V H can decrypt the message M 1 to get R2 in Step 5, so without knowing the key K V H , any fraudulent behaviors from H will be detected by V in Step 6. 
U Authenticates V by Verifying R0 in Step 7
By decrypting the message M 2 to get K UV firstly and then decrypting M 3 to obtain the authenticator R0, V can be authenticated if the value R0 is the same as what U was sent to V in Step 1. Because the key K UV is hidden in M 2, any attacker has no way to forge the message M 3 = 
V Authenticates U by Verifying R2 in Step 8
To response M 4 = 
Performance Analysis
Communication situations consist of honest and dishonest communication behaviors. In honest communications, the entire authentication process must be completed. However , in dishonest communications, the authentication process will be terminated upon detecting fraudulent behavior . The evaluation of efficiency of the protocol is based on the number of computations including encryption and decryption. It is reasonable to assume that each encryption or decryption takes the same CPU time in conventional cryptosystem 
Honest Communication Situation
In an honest communication situation, both our scheme and BGSW protocol need to complete the entire authentication process so that all computations must be performed . The total number of computations for BGSW protocol is ten including five encryptions (2K V H , 1K UH , and 2K UV ) and five decryptions (2K V H , 1K UH , and 2K UV ). Compared to BGSW protocol, eight computations including four encryptions (1K V H , 1K UH , and 2K UV ) and four decryptions (1K V H , 1K UH , and 2K UV ), two less than BGSW scheme, are needed in our scheme. 
Dishonest Communication Situation
Frauds can be found by verifying the authenticators R0, R1 and R2 in different steps. However, each kind of fraud detection takes a different number of computations in the two protocols. According to the different case of fraudulent behaviors mentioned in Sections 4.1 ∼ 4.4, the following sections calculate the numbers of computations of BGSW and our scheme and the results are shown in Ta- ble1. The detailed computations for each U , V and H are shown in 
H Authenticates V
The fraud occurred from V will be detected by H in Step 5 by verifying R2 and R1 for BGSW and our protocol, respectively . Four (4 K V H ) and two (2 K V H ) computations are needed for BGSW and our protocol, respectively. 
V Authenticates H
In BGSW protocol, the fraud occurred from H will be detected by V through verifying R1 in Step 4 and two computations (2 K V H ) are needed. In our protocol, the fraud will be detected by V through verifying R2 in Step 6 and three computations (2 K V H and 1 K UH ) are needed. 
U Authenticates V
The fraud occurred from V will be detected by U in Step 9 through verifying 
V Authenticates U
The fraud occurred from U will be detected by V in Step 8 through verifying R3 in BGSW protocol. Eight (4 K V H , 2 K UH and 2K UV ) computations are needed. In our protocol , the fraud will be detected by V through verifying R2 in Step 8. Eight (2 K V H , 2 K UH and 4 K UV ) computations are needed. 
Comparisons of the Two Protocols
"
"Introduction
 Intrusion Detection Systems (IDSs) have become important and widely used tools for ensuring network security. In recent years, intrusion detection based on statistical pattern recognition methods has attracted a wide range of interest in response to the growing demand of reliable and intelligent IDSs, which are required to detect sophisticated and polymorphous intrusion attacks 
Related Work
 Feature selection is a process that selects a subset of original features. The significance of FS can be viewed in two facets. The frontier facet is to filter out noise and eliminate irrelevant and redundant features. FS is compulsory due to the abundance of noisy, irrelevant or misleading features in a data set. Second, FS can be considered as an optimization problem for an optimal subset of features that better satisfy a desired measure 
Proposed ACO-based Method for Intrusion Detection System
In the early nineties an algorithm called Ant System (AS) was proposed by Dorigo and colleagues as a novel natureinspired meta-heuristic approach for the solution of combinatorial optimization problems. First, algorithm was applied to the traveling salesman problem. Recently, it was extended and/or modified both to improve its performance and to apply it to other optimization problems . Improved versions of AS include, among others, Ant Colony System (ACS), MAX-MIN, AS and AS-rank. An ant colony optimization algorithm is essentially a system based on individuals which simulate the natural behavior of ants, including mechanisms of cooperation and adaptation. The inspiring source of ACO is the foraging behavior of real ants 
Graph Representation
 The main goal of ACO algorithm is to model a problem as the search for a minimum cost path in a graph. Here nodes can be considered as features, with the edges between them denoting the choice of the next feature. The search for the optimal feature subset is then an ant traversal through the graph where a minimum number of nodes, features, are visited that satisfies the traversal stopping criterion. Nodes are fully connected to allow any feature to be selected next. On the basis of this reformulation of the graph representation, the transition rules and pheromone update rules of standard ACO algorithms can be applied. In this case, pheromone and heuristic value are not associated with links. Instead, each feature has its own pheromone value and heuristic value 
Heuristic Information
 Generally, the representation of heuristic value is the attractiveness of features and the basic ingredient of any ACO algorithm is a constructive heuristic for probabilistically constructing solutions. A constructive heuristic assembles solutions as sequences of features from the finite set of features. A subset construction starts with an empty subset. Then, at each construction step the current subset is extended by adding a feature from the set of features. A suitable heuristic desirability of traversing between features could be any subset evaluation. In proposed method classifier performance is mentioned as heuristic information for FS. In other words, the classifier accuracy of each feature on training set is considered as heuristic information for each feature. The heuristic information of traversal and node pheromone levels are combined to form the so called probabilistic transition rule, denoting the probability that ant k will include feature i in its subset at time step t: 
P k i (t) = [τi(t)] α .[ηi] β u∈J k [τu(t)] α .[ηu] β if i ∈ J k 0 otherwise (1) 
where J k is the set of feasible features that ant k can be added to its subset; τ i and η i are respectively the pheromone value and heuristic information associated with feature i. α and β are two parameters that determine the relative weight of the pheromone value and heuristic information. The transition probability used by ACO is a balance between pheromone intensity (i.e. history of previous successful moves), τ i , and heuristic information (expressing desirability of the move), η i . This effectively balances the exploitation-exploration trade-off. The best balance between exploitation and exploration is achieved through proper selection of the parameters α and β. If α=0, no pheromone information is used, i.e. previous search experience is neglected. The search then degrades to a stochastic greedy search. If β=0, the attractiveness (or potential benefit) of moves is neglected. 
Pheromone Update
Pheromone updating is an important part for working the ACO algorithm suitably. After all ants have completed their solutions, pheromone evaporation on all nodes is triggered using Equation (2) and then according to Equation (3) all ants deposit a quantity of pheromone, ∆τ i (t), on each node that they have used. 
τ i (t) = (1 − ρ)τ i (t) (2) τ i (t + 1) = τ i (t) + ∆τ i (t) (3) with ∆τ i (t) = m k=1 ∆τ k i (t) (4) 
where m is the number of ants at each iteration and ρ ∈ (0, 1) is the pheromone trail decay coefficient. The main role of pheromone evaporation is to avoid stagnation , that is, the situation in which all ants constructing the same solution. All ants can update the pheromone according to 
∆τ k i (t) = ω.γ(S k (t)) + φ.(n/|S k (t)|) if i ∈ S k (t) 0 otherwise (5) 
 where S k (t) is the feature subset found by ant k at iteration t, and |S k (t)| is its length. The pheromone is updated according to both the measure of the classifier performance , γ(S k (t)), and feature subset length. ω and φ are two parameters that control the relative importance of classifier performance and feature subset length, ω ∈ 
e. α, β, ρ, m, τ 0 , φ, ω, T. 3: Let t = 1. 4: for Each node i do 5: τ i (t) = τ 0 . 6: end for 7: Place m ants, k = 1, , m. // 
13: 
Add node i to subset S k (t). 
14: 
end while 
15: 
Calculate the subset length |S k (t)|. 
16: 
Calculate the classifier performance γ(S k (t)). 
17: 
end for 18: 
for Each node i do 19: 
 Apply pheromone evaporation using Equation (2). 
20: 
Calculate ∆τ i (t) using Equations (4,5). 
21: 
Update pheromone using Equation (3). 
22: 
end for 23: t = t + 1 24: end while 25: Return the subset S k (t) with highest γ(S k (t)) as the solution. 26: End 
Solution Construction
The overall process of ACO feature selection can be seen in 
Experiments and Results
 The following sections describe the data sets and implementation results. 
Data sets
Since 1999, KDD Cup 99 data set from UCI repository is widely used as the benchmark data set for IDS eval- uation 
@BULLET User to Root (U2R): Is an attack in which the attacker starts accessing a normal user account on a machine and gains root access to the machine by exploiting vulnerabilities. 
@BULLET Remote to Local (R2L): Occurs when an attacker does not have an account on a remote system, but who has the ability to send packets to a system over a network and exploits vulnerabilities to gain local access as a user of that system. 
 @BULLET Probing: An attack in which the attacker scans network to collect information about its systems for the apparent purpose of circumventing its security con- trols. 
KDD Cup 99 features can be classified into three groups: 1) Basic features: this category encapsulates all the attributes that can be extracted from a TCP/IP connection . Most of these features leading to an implicit delay in detection. 
2) Traffic features: this category includes features that are computed with respect to a window interval. 
3) Content features: Most of the DoS and Probing attacks have many intrusion frequent sequential patterns , this is due to the fact that these attacks establish many connections to the host(s) in a very short period of time. Unlike these attacks, the R2L and U2R attacks donot have any intrusion frequent sequential patterns. The R2L and U2R attacks are embedded in the payload of the packets, and normally include only a single connection. To identify these kinds of attacks, some relevant features are needed to identify suspicious behavior in the packet payload. These features are called content features 
Performance Measures
The False Positive Rate (FPR) is defined as the number of normal records that are incorrectly detected as intrusions divided by the total number of normal records. The detection rate is defined as the number of intrusion records classified by the IDS divided by the total number of intrusion records present in the test data set. These are good performance measures, since they measure what percentage of intrusions the system is able to detect and how many incorrect classifications are made in the process. Usually True Positive Rate (TPR) and false positive rate are used for performance measurement. TPR also known as Detection Rate (DR) or sensitivity or Recall and FPR also known as the False Alarm Rate (FAR). They showed in the following equations: 
Recall = T P R = T P T P + F N (6) P recision = T P T P + F P (7) F DR = F P T N + F P (8) Accuracy = T P + T N T P + T N + F P + F N (9) 
where TP, TN, FP, FN are the numbers of true positives, true negatives, false positives and false negatives, respectively . Another commonly used measure is F-measure that is defined in Equation (10) 
F − measure = 2 × Recall × P recision (Recall + P recision) (10) 
Results
 A series of experiments was conducted to show the utility of proposed method. All experiments are executed on a machine with Intel(R) Core(TM) i7 CPU 3.2 GHz and 4 GB of RAM. The operating system was Windows 7 Professional. All tested models were implemented on MATLAB R2011b. Various values were tested for the parameters of the proposed method. The results show that the highest performance is achieved by setting the parameters to values as follow: α = 0.4, β = 0.6, ρ = 0.2, the initial pheromone intensity of each feature is equal to 1 (τ 0 = 1) the number of ant in every iteration is 50 (m=50) and the maximum number of iterations is 100 (T=100). These values were empirically determined in our preliminary experiments; however, we make no claims that these are optimal values. Parameter optimization is still a topic for future research. Many papers have focused on improving detection rates of the IDSs using efficient classifiers, i.e. this is a quiet difficult approach. This paper puts forward a modified ACO-based FS algorithm aiming at building a classifier to detect intrusion attempts. In the experiments, the whole training set and test set were applied. Each record in the training set or the test set consisted of 41 features. Initially the proposed FS algorithm is used to select important features for each type of the previous discussed attacks. Later on, a classification system was used to classify the attacks were the results are reported. Each feature value is standardized using Equation (11) and then unimportant features are removed, i.e., leaving only the important features, as listed in 
StandardScore = X − µ σ (11) 
where X is a feature value to be standardized, µ is the mean of feature values and σ is the standard deviation of the feature values. Results of classification using the proposed method are reported in Tables 5 and 6. In each class (normal or attack ), each row shows the performance of the proposed method and baseline approach (using all features) in detecting attacks. Experimental results show that FS phase of the process improves the detection rate. In the normal class, studying input features with regard to the output shows that there is no linear relation between the input features and output. Therefore, comparing it versus the baseline approach, implementing the proposed method has significantly improved accuracy of the classification models. The best performance of this system, in terms of its accuracy, is reported to be 98.90%, with only 2.59% false positive rate. Several experiments are performed to compare the two different IDSs. Results show that an IDS combined with the proposed ACO-based algorithm has higher detection rates in detecting attacks than the baseline approach. 
Conclusions and Future Works
 This paper addresses the problem of dimensionality reduction using ACO in intrusion detection problem area. ACO has the ability to converge quickly. It has a strong search capability in the problem space and can efficiently find minimal feature subset. Experimental results demonstrate a competitive performance. More experimentation and further investigation into this technique is required. The pheromone trail decay coefficient and pheromone amount have an important impact on the performance of ACO. Selection of the parameters is proved to be problemdependent . The deposited pheromone expresses the quality of the corresponding solution. Evaporation becomes more important for more complex problems. If ρ = 0, i.e. no evaporation, the algorithm does not converge. If pheromone evaporates too much (a large ρ is used), the algorithm often converged to sub-optimal solutions. In many practical problems, it is difficult to select the best ρ without trial-and-error. α and β are also key factors in ACO for FS. For large data sets, to speed up the calculation of FS process, a parallel algorithm can be imple- mented. 
 The proposed method uses ACO algorithm and a simple classifier (nearest neighbor classifier) to select important features and a trained classifier to identify any kind of new attacks. Tests and comparisons are performed on KDD Cup 99 and NSL-KDD data sets, the test sets contains 17 kinds of different attacks. The proposed method reduced the number of features by approximately 88% and the detection error reduced by around 24% using KDD Cup 99 test data set. The proposed method will significantly reduce both the memory size and the CPU time required for intrusion detection by reducing number of the features used for the detection. This shows that the proposed method is very reliable for intrusion detection. Results indicate that the proposed ACO-based detection method outperforms other methods since it can provide better and more robust representation of the data. This is due to the fact that it can accurately detect a broader range of attacks using smaller number of features. As for the future work, intention is to apply the proposed intrusion detection method using complicated classifiers to improve its performance and to combine the proposed method with other population-based algorithms. Analyzing packet payload is recently attracting lots of attention and many researchers report works carried-out in this area. It is notable that feature selection for the payload-based intrusion detection is not mature yet. Intension will be to extract and selection appropriate features from the packet payload to improve the detection rate. 
"
"Introduction
 The web browser security model is rooted in the sameorigin policy (SOP) 
@BULLET We design and prototype UserCSP to automatically generate Content Security Policies and then we evaluate the compatibility of the inferred security policies on websites. @BULLET We propose an approach for applying security policies on the client-side. Our approach allows savvy users to specify their own custom Content Security Policies. 
 Our experiments show a lack of Content Security Policy implementations in real-world websites and the necessity for tools like UserCSP to help promote adoption. UserCSP provides developers with an easy mechanism to create an effective, comprehensive, and strict Content Security Policy that secures their users and does not break website functionality. The rest of this paper is organized as follows: Section 2 presents our experimental evaluation and analysis. Section 3 describes the design of UserCSP. Section 4 describes evaluation of our approach, and we conclude the paper in Section 6. 
Experimental Evaluation and Analysis
We conducted empirical measurements to obtain the data for evaluating Content Security Policy (CSP) usage in wild. Our measurements are mainly conducted on a Dell server running Ubuntu 12.04 64bit, with Xeon 4-core 2.67GHz CPUs and 32GB RAM. 
Measurement Goals
Our measurements aim to measure the following: 
Goal 1: Measure inconsistency in real-world websites in enforcing CSP. 
Goal 2: Identify errors in existing CSP policies applied by developers on their websites that nullify the defense provided by CSP. 
Goal 3: Estimate the amount of efforts required by web developers to adapt to CSP for their websites. 
Measurement over Alexa Top 100,000 Desktop Websites and 289 Mobile Websites
In our experiments, we used Scrapy framework to crawl desktop and mobile websites. Our results show that out of 100,000 Alexa top websites 
 We also observed that 24 unique websites are using unsafe-inline, unsafe-eval or eval-script options. According to W3C standard and effective CSP protection on website, it is crucial to move all inline scripts and style sheets to the trusted external sources to allow web browsers to identify injected scripts by an attacker. Next, we explain how we measure these metrics and present their results. 
Goal 1: Inconsistency in CSP Enforcement. To measure the inconsistency in enforcing CSP policies in real-world websites, we measured different headers used by developers to send CSP policy to clients. There is an inconsistency in CSP supporting browsers in CSP enforcement headers they obey. Firefox version 4.0 onwards supports X-Content-Security-Policy header and Google Chrome and Safari support X-WebKit-CSP header for CSP enforcement. Whereas, Firefox doesn't respect X- WebKit-CSP header and Google Chrome neglects X- Content-Security-Policy header used by websites for CSP enforcement. Due to this inconsistency across web browsers, Candidate Recommendation of CSP specification of the W3C Working group of Web Application Security proposed a standard header name for CSP enforcement: Content- Security-Policy. At the time of writing of this paper, Firefox version 23 and Google Chrome version 25 support Content-Security-Policy header. We scanned in total 100,000 Alexa top desktop websites and 289 mobile websites and checked response for various possible CSP headers such as X-Content- Security-Policy, X-WebKit-CSP, and Content-Security- Policy. Web applications can detect user agents and send appropriate CSP header in the response; therefore, we scanned all websites multiple times by using separate user agent strings 
@BULLET Non-effective CSP policies: We observed websites are setting CSP policy incorrectly and thus keeping open doors for content injections. For example, http://www.metro-partner.ru/ website sets following CSP policy: X-Content-Security-Policy: allow 'self '; img-src *; script-src *; options eval-script inline-script; 
The http://www.metro-partner.ru/ website has defined CSP policy in incorrect way and thus made it non-effective to protect users from content injection attacks. It allows scripts to be executed from any domain and images to be loaded from any arbitrary domains. Furthermore, it also allows execution of inline scripts and eval() usage. We observed CSP policy errors on eight (8) websites. 
 Goal 3: Estimate the Amount of Efforts. To measure the amount of developer efforts required to change their website to adapt to CSP, we measured the number of inline scripts, and inline event handlers used in  1. Developers misunderstood the usage of unsafeeval and unsafe-inline, and used them incorrect way. 2. Injected content will not be prevented by CSP because inline scripts are allowed. X-Content-Security-Policy: allow *; options inline-script eval-script; frameancestor' , "" allow *; options inline-script eval-script; frame-ancestor 'self'; Arbitrary domains are allowed and inline scripts are also allowed. Thus nullifies CSP protection. X-Content-Security-Policy: allow 'self'; img-src *; script-src *; options evalscript inline-script; It allows scripts and images from arbitrary domains as well as allows inline scripts. Content-Security-Policy: default-src https: 'unsafe-eval' 'unsafe-inline' It allows arbitrary https: sources and inline scripts. 
world websites. Inline scripts mean JavaScript code that is embedded in < script > tag, JavaScript URIs, and inline event handlers such as onclick, onmouseover, etc. In our test bed we examined home page as well as three internal pages of desktop and mobile websites using the Scrapy framework. We noticed on an average seven inline scripts and eleven inline event handlers are used on Alexa top 100,289 desktop and mobile websites. Moreover, we measured the amount of changes require to remove inline scripts using phpBB a real-world web forum application 
UserCSP Design
The goal of UserCSP is to allow users to specify and apply security policies on web content. UserCSP helps developers and users write comprehensive policies for websites by providing them with a GUI to add and modify CSP policies.  UserCSP monitors the browser's internal events (including HTML parsing, HTTP requests, and XHR requests  triggered by scripts running in the JS engine). It then dynamically analyzes the content type loaded by a webpage and the source of that content. The HTML parser component in the browser parses the webpage and initiates HTTP requests to load resources such as images, scripts, and stylesheets included in the page. The Database manager component is responsible for storing the webpage's user specified policy in a local database and later retrieving the policy when the user visits the webpage in the future. As shown in 
Web Browser 
@BULLET If a user has specified a CSP policy for a website, but the website administrator hasn't, then the user's policy is enforced. @BULLET If both a user specified CSP policy and a website defined policy exist, then the user has a choice to either apply their own policy or adopt the website defined policy. Moreover, users can choose to combine their custom policy with an existing website policy by selecting a strict (intersection) or loose (union) combination policy. 
@BULLET If neither the user nor the website specify a CSP policy, but the user has specified a global policy that can be used for websites that do not have site-specific policies defined, then UserCSP will apply the global policy. 
@BULLET If neither the user nor the website specify a CSP policy, and there is no global policy, then UserCSP does not affect the content loading on the website. 
Automatic Policy Enforcement
 There are several challenges in automatic CSP policy enforcement as listed below. 
@BULLET Dynamic content on a website that can introduce new code into the website at run-time after web page load. @BULLET The list of resources origin changes on websites that use rotating advertisements, DNS load-balancing, etc. 
 @BULLET Heavy usage of inline-scripts on websites make it difficult to derive strict CSP policy that blocks inlinescripts , eval, etc. 
@BULLET Run-time content injection into websites by browser extensions. 
To allow automatic policy inference for websites, UserCSP uses an algorithm that performs dynamic analysis to monitor content loaded by a webpage and recommends a CSP policy based on the content types and content sources included in the webpage. It also monitors the resources dynamically added to the webpage by JavaScript. To record new content introduced by websites at run-time, UserCSP during learning phase continuously monitor websites even after website is completely loaded. It records inferred policy into local database. Next time, when the user visits the same site UserCSP takes previously inferred policy and combine it with the currently inferred policy. Due to rotating advertisements that change periodically may lead to a load request from different origins, therefore, the continuous inferring process of UserCSP helps users to detect changes in the resource origin and apply changed domain to reload resources. Our inferred policy derives strict CSP policy, which blocks inline scripts, styles, eval, and event handlers. However, UserCSP also provides features to users to allow inline scripts and eval on their favorite websites manually. In modern browsers, extensions are high privilege than websites and run with the privileges of the browser. Browser extensions are used to enhance user experience and provide new functionality. Therefore, UserCSP honors content included by extensions into web pages and includes them into inferred policy. 
Evaluation of UserCSP
 We tested UserCSP's user defined CSP feature and automatically infer CSP feature with the Alexa Top 100 websites 2 
Conclusion
 Content Security Policy is an effective mechanism to prevent against content injection attacks. In this paper, we did a large-scale study of CSP usage and infer difficulties in the CSP adoption. CSP has not been widely adopted because of the challenges involved in creating a comprehensive and functional policy, and limited knowledge of CSP among developers. Since adoption is controlled by developers, users lack control over their own security. Users do not have a mechanism to apply Content Security Policies on the websites that they visit and cannot protect themselves from Cross-Site Scripting and Clickjacking attacks. UserCSP helped to break down the challenges involved in adopting Content Security Policy with UserCSP feature to automatically infer policies and puts control into the users hands by providing them a mechanism to protect themselves with custom policies that they can create and modify. Our analysis and results show that another barrier to Content Security Policy adoption is the use of inline JavaScript. To overcome this, we would like to experiment further with the proposed script-nonce and scripthash directives that are under discussion for inclusion in the CSP 1.1 specification. 
"
"Introduction
With advances in sensor technologies in recent times, wireless sensor networks (WSNs) are increasingly popular in commercial, government and military settings (see 
Preliminaries
Bilinear Pairings
Definition 1. Let k be a security parameter and q be a k-bit prime number. Let G 1 denote a cyclic additive group of prime order q and G 2 a cyclic multiplicative group with the same order. We assume that the discrete logarithm problem is hard in both G 1 and G 2 . LetêLetˆLetê : G 1 × G 1 → G 2 be a map with the following properties: 1) Bilinearity: For any P, Q ∈ G 1 and a, b ∈ Z * q , ˆ e(aP, bQ) = ˆ e(P, Q) ab . 
2) Non-degeneracy: There exists P, Q ∈ G 1 such thatê thatˆthatê(P, Q) = 1 G2 . Therefore, when P is a generator of G 1 , ˆ e(P, P ) is a generator of G 2 . 
3) Computability: There is an efficient algorithm to computê e(P, Q) for all P, Q ∈ G 1 . The above bilinear map is also known as a bilinear pairing . The mapêmapˆmapê can be derived from either Weil pairing or Tate pairing on an elliptic curve over a finite field, and we refer the reader to 
Mathematical Assumption
 Let G be an abelian group of prime order q and P a generator of G. We describe the following three mathematical problems in the additive group (G, +). Discrete Logarithm Problem (DLP): Given P, Q ∈ G, find an integer n ∈ Z * q , such that Q = nP whenever such an integer exists. Decision Diffie-Hellman Problem (DDHP): For a, b, c ∈ Z * q , given P, aP, bP, cP ∈ G, decide whether c ≡ ab mod q. 
Computational 
Diffie-Hellman Problem (CDHP): For a, b ∈ Z * q , given P, aP, bP ∈ G, compute abP . CDHP assumption: There exists no algorithm running in polynomial time, which can solve the CDHP problem with non-negligible probability. 
(t , ε )-CDH group: A probabilistic algorithm A is said (t , ε )-break the CDHP in G if A runs at most time t , computes the CDHP with an advantage of at least ε . We say that G is a (t , ε )-CDH group if no probabilistic algorithm A (t , ε )-breaks the CDHP in G. 
Framework
Definition 2. The online/offline ID-based signature scheme comprises five polynomial time algorithms, namely: Setup, Extract, Offline Sign, Online Sign, Ver- ify. 
 Setup. The master key and parameter generation algorithm is a probabilistic algorithm. On input a security parameter 1 k , the algorithm will output a master key msk and a parameter list params. Extract. The signing key issuing algorithm is a deterministic algorithm. On input a user's identity id and a master key msk, the algorithm will return a pair of matching public and secret keys (pk id , sk id ). Offline Sign. The offline signing algorithm is a probabilistic algorithm. On input a parameter list params, the algorithm will return the generated offline signature σ of f . 
 Online Sign. The online signing algorithm is a probabilistic algorithm. On input a parameter list params, an identity id, a message m, and an offline signature σ of f , the algorithm will return the generated signature σ. Verify. The verification algorithm is a deterministic algorithm . On input a parameter list params, an identity id, a message m, and a signature σ, the algorithm will return accept if σ is valid and reject otherwise. 
Revisiting Kar's Online/Offline ID-based Signature Scheme
Kar's Scheme
Kar's Scheme 
Setup. Given security parameters k, the Private Key Generator (PKG) chooses two groups G 1 and G 2 both of prime order q, a generator P of G 1 , a bilinear mapêmapˆmapê : 
G 1 × G 1 → G 2 and two collision resistant hash functions H 1 : {0, 1} * → G 1 , H 2 : {0, 1} * → Z * q . 
Next, PKG will choose a master-key s ∈ Z * q and compute P pub = sP . The system public parameters are given by 
P = (G 1 , G 2 , q, ˆ e, P, P pub , H 1 , H 2 ). 
Extract. Given an identity ID ∈ {0, 1} * , the secret key will be 
d ID = s · Q ID , where Q ID = H 1 (ID). 
Offline Sign. At the offline stage, the signer computes 
ˆ α i = ˆ e(P, P pub ) 2 i , ∀ i = 0, 1, . . . , |q| − 1. 
 Online Sign. During this phase, the signer randomly selects β ∈ Z * q and computes two index sets D = {1 ≤ i ≤ |q| | β
σ = (α, U, V ). 
 Verify. The signature is valid only if the following equation holds: 
ˆ e(V, P ) ? = α · ˆ e(Q ID , P pub ) r · ˆ e(U, P pub ). 
(1) 
 3.2 Previously Unpublished Vulnerabili- ties We will now show that Equation (1) does not hold for general cases, even in the event that (α, U, V ) is a valid signature for the message m and the identity ID. First we have 
ˆ e(V, P ) = ˆ e((γ + β)P pub + rd ID , P ) = ˆ e((γ + β)P pub , P ) · ˆ e(rd ID , P ) = ˆ e(P pub , (γ + β)P ) · ˆ e(rsQ ID , P ) = ˆ e(P pub , γP ) · ˆ e(P pub , βP ) · ˆ e(rQ ID , sP ) = ˆ e(P pub , U ) · ˆ e(P pub , P ) β · ˆ e(rQ ID , P pub ) = ˆ e(P pub , P ) β · ˆ e(Q ID , P pub ) r · ˆ e(U, P pub ). 
Thus, Equation (1) holds if, and only if, α=ê(P pub , P ) β . However, we have 
α = ψ 1 ψ 2 = ( i∈Dˆα i∈Dˆ i∈Dˆα i−1 )( i∈Cˆα i∈Cˆ i∈Cˆα i−1 ) = ˆ α 0 ˆ α 1 · · · ˆ α |q|−1 = ˆ e(P, P pub ) 2 0 ˆ e(P, P pub ) 2 1 · · · ˆ e(P, P pub ) 2 |q|−1 = ˆ e(P, P pub ) 2 0 +2 1 +···+2 |q|−1 = ˆ e(P, P pub ) 2 |q| −1 . 
Since β is randomly selected from Z * q , it is clear that β = 2 |q| − 1 mod q in general, which results in 
α = ˆ e(P pub , P ) β ; 
thus, Equation (1) does not hold. 
In addition to the above design flaw, we will show that the scheme is vulnerable to an existential forgery attack, in violation of their security claim. We reasonably assume that A is an attacker who has the public parameters 
P = (G 1 , G 2 , q, ˆ e, P, P pub , H 1 , H 2 ). 
A can execute the following steps to forge a signature σ = (α , U , V ) for a message m and a legitimate identity ID. 
1) A selects U ∈ G 1 and computes 
r = H 2 (ID, U ||m ). 
2) A selects V ∈ G 1 and computes 
α = ˆ e(V , P ) · ˆ e(r Q ID + U , P pub ) q−1 , 
where Q ID = H 1 (ID). 
3) A sends the forgery signature σ = (α , U , V ) for the message m and the identity ID to the verifier. 
When the verifier receives the signature σ = (α , U , V ) for the message m and the identity ID, the verifier will compute 
r = H 2 (ID, U ||m ) and check whether Equation (2) holds. ˆ e(V , P ) ? = α · ˆ e(Q ID , P pub ) r · ˆ e(U , P pub ) (2) 
We now obtain: 
α · ˆ e(Q ID , P pub ) r · ˆ e(U , P pub ) = α · ˆ e(r Q ID , P pub ) · ˆ e(U , P pub ) = α · ˆ e(r Q ID + U , P pub ) = ˆ e(V , P ) · ˆ e(r Q ID + U , P pub ) q−1 · ˆ e(r Q ID + U , P pub ) = ˆ e(V , P ) · ˆ e(r Q ID + U , P pub ) q = ˆ e(V , P ) 
The above equation holds because the group G 2 has prime order q. Therefore, the forgery signature σ = (α , U , V ) for the message m and the identity ID will always be successfully verified. In other words, it is trivial to forge a signature. Consequently, this violates the claim by Kar that the scheme is secure against existential forgery on chosen message attack. 
Our Proposed Signature Scheme
The Basic Scheme
 In this section, we propose an improved online/offline signature scheme whose security is based on the assumption that CDHP is hard to solve. Our scheme contains the following five polynomial time algorithms. Setup. Given a security parameter k ∈ Z, this algorithm works as follows: 
1) Generates a prime q, two groups G 1 and G 2 of order q and a bilinear pairingêpairingˆpairingê : 
G 1 × G 1 → G 2 . Chooses a generater P of G 1 . 
2) Selects a random s ∈ Z * q as the master key, and sets P pub = sP . 
3) Chooses two cryptographic hash functions H 1 : {0, 1} * → G 1 and H 2 : {0, 1} * → Z * 
q , which will be viewed as random oracles in our security proof. The system parameters are 
P arams = {G 1 , G 2 , P, P pub , q, ˆ e, H 1 , H 2 }. 
 Extract. For a given identity ID ∈ {0, 1} * , the algorithm computes the associated private key S ID = s · H 1 (ID), where Q ID = H 1 (ID) plays the role of the associated public key. Offline Sign. During the offline stage, the signer com- putes: 
Y i = ˆ e(P pub , P ) 2 i , i = 0, 1, . . . , , 
where = |q| − 1. 
Online Sign. During the online stage, given a private key S ID and a message m ∈ {0, 1} * , the signer computes the followings: 
1) Randomly chooses a number y ∈ Z * q and com- putes Y = 0≤i≤ Y y[i] i , 
where y
2) Randomly chooses x ∈ Z * q , and computes R = xP and 
h = H 2 (m, R, Y ). 3) Computes Z = (x + y)P pub + hS ID . 
The signature is σ = (Y, R, Z). Verify. In order to verify the signature σ of a message m for an identity ID, the verifier computes the follow- ings: 
1) Computes h = H 2 (m, R, Y ). 
2) Verifies whether the following equation holds. 
ˆ e(Z, P ) ? = Y · ˆ e(R + hQ ID , P pub ) (3) 
Accepts if the above verification returns true, and rejects otherwise. 
Consistency. Let σ = (Y, R, Z) be a valid signature of a message m for an identity ID (in the case 
Z = (x + y)P pub + hS ID , R = xP , h = H 2 (m, R, Y ), and Y = 0≤i≤ Y y[i] i ), we have ˆ e(Z, P ) = ˆ e((x + y)P pub + hS ID , P ) = ˆ e((x + y)P pub , P ) · ˆ e(hS ID , P ) = ˆ e(P pub , (x + y)P ) · ˆ e(hsQ ID , P ) = ˆ e(P pub , xP ) · ˆ e(P pub , yP ) · ˆ e(hQ ID , sP ) = ˆ e(P pub , R) · ˆ e(P pub , P ) y · ˆ e(hQ ID , P pub ) = ˆ e(P pub , P ) y · ˆ e(R + hQ ID , P pub ) 
Then, Equation (3) holds if and only if Y = ˆ e(P pub , P ) y . On the other hand, 
Y = 0≤i≤ Y y[i] i = Y y[0] 0 Y y[1] 1 . . . Y y[] = ˆ e(P pub , P ) y[0]2 0 · ˆ e(P pub , P ) y[1]2 1 · · · ˆ e(P pub , P ) y[]2 = ˆ e(P pub , P 
) y
2 = ˆ e(P pub , P ) y . 
Thus, we show the consistency of our signature scheme. In the next section, we will prove that our signature scheme is secure against existential forgery on adaptively chosen message attack under the CDHP assumption. 
Security Proof
Let S = (Setup, Extract, Offline Sign, Online Sign, Verify) denotes an online/offline ID-based signature scheme. We consider the following game, denoted by Game EUF-ACM S,A 
, involving a probabilistic polynomial time algorithm A: 
 1) The challenger, denoted by F, runs the Setup algorithm to generate the system parameters Params and sends them to A. 
2) A performs the following queries as he wants: @BULLET Hash function query. F computes the value of the hash function for the requested input and sends the value to A. @BULLET Extract query. When A produces an identity id, F will return the private key sk id corresponding to id, which is obtained by running Extract. @BULLET Sign query. Proceeding adaptively, A requests signatures on at most q s messages of his choice m 1 , . . . , m qs ∈ {0, 1} * . F responds to each query with a signature σ i (1 ≤ i ≤ q s ), which is obtained by running Offline Sign and Online Sign. 
3) After a polynomial number of queries, the adversary A produces a tuple (id * , m * , σ * ) whose secret key was not asked in any Extract queries and the pair (id * , m * ) was not asked in any Sign queries. A wins the game if σ * is a valid signature of m * for id * . 
Definition 3. An adversary A (t, q h , q e , q s , ε)-breaks an online-offline ID-based signature scheme S if A wins the game Game EUF-ACM S,A with a non-negligible advantage (i.e. advantage of at least ε), running time at most t, and Hash functions, Extract and Sign queries at most q h , q e , q s times, respectively. S is considered (t, q h , q e , q s , ε)-existentially unforgeable under adaptively chosen message attacks if no adversary (t, q h , q e , q s , ε)breaks S. 
We now prove the following lemma using the technique used in the BLS scheme 
G 1 × G 1 → G 2 . In 
the random oracle model, the proposed signature scheme is (t, q h , q e , q s , ε)-secure against existential forgery under an adaptive chosenmessage attack, in which t and ε satisfy 
ε ≥ e(1 + q e )ε , t ≤ t − (q h + 2q e + 2q s )t m . 
Here, we denote by e the base of the natural logarithm and t m the time for computing scalar multiplication. Let q h , q e , q s respectively denote the number of H 1 queries, extract query and sign query, which the adversary is allowed to make. Proof. Suppose that A is a forgery algorithm who (t, q h , q e , q s , ε)-breaks the signature scheme and outputs a valid forged signature. The algorithm B simulates the challenger and interacts with the forgery algorithm A. We can then use A to construct a t -time algorithm B and solve the CDH problem with probability of at least ε . Let P be a generator of G 1 . We now describe algorithm B, which computes abP ∈ G 1 for a randomly given CDH instance (P, aP, bP ) where a, b ∈ Z * q . Setup. Algorithm B sets P pub = aP as the public key, and algorithm A obtains the system parameters {P, P pub } from B. H 1 -query. Algorithm A is allowed to query the random oracle H 1 at any time. In order to respond to these queries, algorithm B maintains a list of tuples ID j , α j , β j , c j denoted as L 1 , which is initially empty. When A queries the oracle H 1 at a point ID i ∈ {0, 1} * , B responds as follows: 
 1) If the query ID i already appears on the H 1 list in a tuple ID i , α i , β i , c i , algorithm B will respond with H 1 (ID i ) = β i . Otherwise, algorithm B generates a random coin c i ∈ {0, 1}, so that P r[
c i = 0] = 1/(1 + q e ). 
2) Algorithm B picks a random α i ∈ Z * q and 
putes β i = α i b 1−ci P . 
3) Algorithm B adds the tuple ID i , α i , β i , c i to the H 1 -list and responds to A by setting 
H 1 (ID i ) = β i . 
Extract query. Let ID i be an extract query issued by A. Algorithm B responds to this query as follows: 
1) Algorithm B runs H 1 -query to obtain 
H 1 (ID i ) = β i . Let ID i , α i , β i , c i 
be the corresponding tuple on the H 1 -list. If c i = 0, then algorithm B reports failure and terminates. 
2) Otherwise, we know c i = 1; hence, 
β i = α i P . Algorithm B computes S IDi = α i · P pub = a · (α i P ) and responds to algorithm A with S IDi . 
Sign query. Let m i be a sign query issued by A with the identity ID i , algorithm B responds to this query as follows: 
 1) Algorithm B runs the above algorithm for responding to H 1 -query to obtain a β i such that 
H 1 (ID i ) = β i . Let ID i , α i , β i , c i 
 be the corresponding tuple on the H 1 -list. 
2) Algorithm B randomly picks x i , y i , h i ∈ Z * q . Then, B computes Y i = 0≤k<|q| Y yi[k] k (where Y k = ˆ e(P pub , P ) 2 k , ∀k = 0, 1, . . . , |q| − 1), R i = x i h i P − y i P − β i h i , and Z i = x i h i P pub . 3) Algorithm B responds to algorithm A with σ i = (Y i , R i , Z i ). 
We also remark that σ i is always a valid signature on the message m i for the identity ID i . 
Y · ˆ e(R + hQ ID , P pub ) = ˆ e(P pub , P ) y ˆ e(R + hQ ID , P pub ) = ˆ e(yP + R + hβ, P pub ) = ˆ e(xhP, P pub ) = ˆ e(xhP pub , P ) = ˆ e(Z, P ). 
We apply the oracle replay attack (coined by Pointcheval and Stern 
H 1 (ID) = β = bαP . Algorithm B computes Z − Z = (h − h )S ID = (h − h )sQ ID = (h − h )abαP and abP = (Z − Z )(h − h ) −1 /α, 
where abP is the solution to the CDH instance (P, aP, bP ). We will now show that algorithm B solves the given CDH instance (P, aP, bP ) with probability at least ε . We analyze the three events required for algorithm B to suc- ceed: @BULLET ε 1 : Algorithm B does not abort as a result of any Extract queries of algorithm A. @BULLET ε 2 : Algorithm A generates a valid message-signature forgery (Y, R, Z). @BULLET ε 3 : The event ε 2 occurs and c = 0 for tuples containing ID on the L 1 -list. Algorithm B succeeds if all these events happen, and the corresponding probability is 
P r[ε 1 ∧ ε 3 ] = P r[ε 1 ] · P r[ε 2 |ε 1 ] · P r[ε 3 |ε 1 ∧ ε 2 ] (4) 
Claim 1. The probability that algorithm B does not abort as a result of any Extract queries asked by algorithm A is at least 
(1 − 1/(1 + q e )) qe . 
Proof. We assume that A does not query the signature of the same message twice. The probability that algorithm B does not abort is at least 
(1 − 1/(1 + q e )) i after i (0 ≤ i ≤ q e ) 
signature queries were asked by algorithm A. It is clear that the claim is true when i = 0. Let ID i be the i-th extract query asked by A, and ID i , α i , β i , c i be the corresponding tuple on the H 1 -list. Before issuing the extract query, only H 1 (ID i ) = β i depends on the random coin c i , and distribution on H 1 (ID i ) is the same as c i 's. Thus, the probability that the Extract query causes B to abort is at most 1/(1 + q e ). Based on the inductive hypothesis and the independence of c i , the probability that B does not abort after this signature query is at least 
(1 − 1/(1 + q e )) i . 
This proves the claim; as A makes at most q e extract queries, the probability that B does not abort is at least 
(1 − 1/(1 + q e )) qe ≥ 1/e. 
Claim 2. If B does not abort as a result of any extract queries of algorithm A, then A's view is identical to its view in the real attack. Hence, P r[
ε 2 |ε 1 ] ≥ ε. 
 Proof. As h 1 and h 2 are two collision resistant hash functions , responses to h 1 -queries and h 2 -queries are similar to those in a real attack. All responds to the Extract queries and signature queries are valid. Therefore, A generates a valid message-signature pair with probability of at least ε. Hence, P r[
ε 2 |ε 1 ] ≥ ε. 
Claim 3. The probability that algorithm B does not abort after A outputs a valid forgery is at least 1/(1 + q e ). 
Hence, P r[ε 3 |ε 1 ∧ ε 2 ] = 1/(1 + q e ). 
Proof. Suppose that events ε 1 and ε 2 occurred, algorithm B will abort only when A outputs a forgery messagesignature pair (m, σ) and c = 0 in the tuple ID, α, β, c on the h 1 -list. If A did not issue an Extract query for m i , only H 1 (ID i ) depends on the random coin c i , and distribution on H 1 (ID i ) is the same as c i 's. Due to that A could not issue an extract query for m, c is independent of A's current view. Therefore, 
P r[c = 0|ε 1 ∧ ε 2 ] = 1/(1 + q e ). 
According to Equation (4) and using the bounds from the above claims, algorithm B succeeds with probability at least 1/e · ε · 1/(1 + q e ). If algorithm A takes time t to run, algorithm B takes time t and with the time required to respond to 
(q h + q e + q s ) H 1 -queries, q e extract 
 queries, and q s signature queries. Each hash query and extract query require one scalar multiplication in G 1 , and each signature query requires four scalar multiplications in G 1 . We assume that one scalar multiplication in G 1 takes time t m . Thus, algorithm B takes time of at most t + (q h + 2q e + 2q s )t m . Lemma 2. (Lemma 4 in 
t ≤ 23q h t 0 /ε 0 . 
Combining the above lemmas, we have the following theorem. Theorem 1. If there is an algorithm A for an adaptively chosen message attack to our scheme which queries H 1 , Extract, and Sign at most q h , q e , q s times respectively, and has running time t and advantage ε ≥ 10e(
1 + q e )(1 + q s )(q h + q s )/2 k , 
then CDHP can be solved with probability ε ≥ 1/9 within running time 
t ≤ 23q h (t + (q h + 2q e + 2q s )t m )/(e(1 + q e )). 
Setup. Given a security parameter k ∈ Z, this algorithm works as follows: 
1) Generates a prime q, two groups G 1 and G 2 of order q and a bilinear pairingêpairingˆpairingê : 
G 1 × G 1 → G 2 . Chooses a generater P in G 1 . 
2) Selects a random s ∈ Z * q as the master key, and sets P pub = sP . 
3) Chooses two collision resistant hash function 
H 1 : {0, 1} * → G 1 and H 2 : {0, 1} * → Z * q . The system parameters are P arams = {G 1 , G 2 , P, P pub , q, ˆ e, H 1 , H 2 }. 
Extract. For a given identity ID ∈ {0, 1} * , compute Q ID = H 1 (ID) and set S ID = sQ ID as a private key of ID. Offline Sign. At the offline stage, the signer computes: 
Y i = ˆ e(P pub , P ) 2 i , i = 0, 1, . . . , , 
where = |q| − 1. Online Sign. During the online stage, given a private key S ID and n messages m j ∈ {0, 1} * ,1 ≤ j ≤ n, the signer computes the followings: 
1) For any 1 ≤ j ≤ n, randomly chooses y j ∈ Z * q and computes Y (j) = 0≤i≤ Y yj [i] i , 
where y j 
2) For any 1 ≤ j ≤ n, randomly chooses x j ∈ Z * q , and computes 
h j = H 2 (m j , R j , Y (j) ), where R j = x j P . 3) Computes Z j = (x j + y j )P pub + h j S ID , 1 ≤ j ≤ n and Z = n j=1 Z j . 
The signature is 
σ = (Y (1) , Y (2) , . . . , Y (n) , R 1 , R 2 , . . . , R n , Z). 
Verify. In order to verify the signature 
σ = (Y (1) , Y (2) , . . . , Y (n) , R 1 , R 2 
, . . . , R n , Z) for the n messages m j , j = 1, 2, . . . , n, and the identity ID, the verifier computes the followings: 
1) Computes h j = H 2 (m j , R j , Y (j) ), j = 1
, 2, . . . , n. 
2) Verifies whether the following equation holds 
ˆ e(Z, P ) 
? = n j=1 (Y (j) · ˆ e(R j + h j Q ID , P pub )) (5) 
Accepts if it is equal, and rejects otherwise. 
Consistency. Let σ = (Y (n) , R n , Z) be a valid signature for n messages m j , j = 1, 2, . . . , n, and identity ID, we have ˆ e(Z, P ) 
= ˆ e( n j=1 Z j , P ) = ˆ e( n j=1 ((x j + y j )P pub + h j S ID ), P ) = n j=1ê j=1ˆj=1ê((x j + y j )P pub + h j S ID , P ) = n j=1 (ˆ e((x j + y j )P pub , P ) · ˆ e(h j S ID , P )) = n j=1 (ˆ e(P pub , (x j + y j )P ) · ˆ e(h j sQ ID , P )) = n j=1 (ˆ e(P pub , x j P ) · ˆ e(P pub , y j P ) · ˆ e(h j Q ID , sP )) = n j=1 (ˆ e(P pub , R j ) · ˆ e(P pub , P ) yj · ˆ e(h j Q ID , P pub )) = n j=1 (ˆ e(P pub , P ) yj · ˆ e(R j + h j Q ID , P pub )) 
Then, Equation (5) holds if and only if Y (j) = ˆ e(P pub , P ) yj , j = 1, 2, . . . , n. 
On the other hand, for any 1 ≤ j ≤ n, we have 
Y (j) = 0≤i≤ Y yj [i] i = Y yj [0] 0 Y yj [1] 1 . . . Y yj [] = ˆ e(P pub , P ) yj [0]2 0 . . . ˆ e(P pub , P ) yj []2 = ˆ e(P pub , P ) yj [0]2 0 +···+yj []2 = ˆ e(P pub , P ) yj . Thus, ˆ e(Z, P ) = n j=1 (Y (j) · ˆ e(R j + h j Q ID , P pub )) 
and the verification is successful. We refer to 
Conclusion
In this paper, we proposed an online/offline ID-based signature scheme and proved that the scheme is secure against existential forgery on adaptively chosen message attack in random oracle model, under the assumption that CDHP is intractable. We also extended the basic scheme to provide the ability for a user to sign multiple messages. 
"
"Introduction
 During the last several decades, network defenders and researchers have developed approaches to detect malicious scans as well as coordinated port scans to keep enterprise networks secure. This is because cyber threats are becoming more sophisticated and more numerous, leading to more substantial damages to systems within short periods of time 
 A coordinated port scan is a part of a coordinated attack . Here, tasks are distributed among multiple hosts for their individual actions which may be synchronized. Such a port scan is an information gathering method used by an opponent to gain information about responding computers and open ports on a target network host. An opponent initiates the exploration of multiple hosts to scan a portion of the target network, with multiple sources focused on the portion of the target network which they want to compromise after getting relevant information from the target host. Intrusion Detection Systems (IDSs) are normally configured to recognize and report single source port scan activity. So, they cannot usually detect multiple source scans that collaborate with several hosts during scanning. The detection of port scans, particularly stealthy or coordinated port scans, is important for early detection to enable action against potential intruders. The attackers or intruders are technically sophisticated enough to remain undetected while gathering information but the network defenders are usually out in the open. Single source scan detection is comparatively easy to detect because detection usually works better when a single source communicates with a single or multiple destinations. But the detection of a coordinated port scan is difficult due to the lack of relevant feature information at both packet and flow levels. Therefore, we are motivated to develop an adaptive outlier based detection mechanism for coordinated port scans known as AOCD. This paper makes the following key contributions. @BULLET We formalize the problem of coordinated scan detection as a data mining problem and present an approach to transform network traffic data into a form where a classifier can be directly used. Specifically, we select random samples from the dataset and identify a set of features relevant for cluster detection for early detection of coordinated port scans. @BULLET We exercise special care during labeling and use the labeled dataset for training as well as testing. The source is real network traffic data in our TU- IDS (Tezpur University Intrusion Detection System) testbed 
Problem Statement
 Coordinated or distributed port scans originate at multiple sources and focus on a single machine or multiple target machines. It is of special interest to large organizations with high level network situational awareness or military operations to detect coordinated port scans. The following are key problems. @BULLET Coordinated scans compromise the victim machine earlier than single source port scans. @BULLET Coordinated port scans are distributed in nature. So, intruders or attackers self-propagate the traffic and consume network bandwidth and resources quickly. 
To overcome these problems, we develop an adaptive outlier based coordinated port scan detection approach. Let x be the captured, preprocessed current network traffic feature dataset, where x 1 , x 2 , · · · x s are the training samples, randomly selected from dataset x that contain only normal instances. We apply the fuzzy c-means algorithm to cluster each sample individually into k number of clusters. Each cluster uses as a range based profile for detection . Let x 1 , x 2 , · · · x t be the test instances to classify 
 3 Port Scans and Related Con- cepts 
In this section, we present preliminary discussions on port scans, outliers and network anomaly detection. 
Port scans and types
There are several forms of reconnaissance activity, which often precedes an attack. When an adversary uses an effective mechanism to remotely probe a network, it is known as port scanning. System administrators and other network defenders also use this mechanism to detect port scans as precursors to serious attacks 
5) UDP scan: A UDP scan attempts to discover open ports related to the UDP protocol. However, UDP is a connectionless protocol and, thus, it is not often used by attackers since it can be easily blocked. 
The list of port scan types discussed above along with firewall detection possibilities during the scanning process is given in 
Related Research
 There are various methods for detecting coordinated attacks . We describe some of them in brief next. Gates 
 5 AOCD: The Proposed Ap- proach 
We describe the required concepts first and then the AOCD algorithm to detect coordinated port scans. 
Outliers and Anomaly Detection
An outlier is an abnormal or infrequent event or object that varies significantly from the normal event or object in terms of a distance measure. A network administrator needs to define the abnormal event based on the normal statistics 
Outlier Score and Its Importance
A large number of outlier detection techniques have been proposed in the literature but only some of them have been applied to anomaly detection 
ROS(x) = 1 − D p (x, k) max 1≤i≤n D p (x i , k) 
(1) where D p (x, k) is min 1≤r≤R D(x, k, p r ). D p (x, k
 ) is the degree of neighborhood density of the candidate data point x with respect to the set of reference points p, n is the total number of data points, k is a reference based nearest neighbor, and R is the number of reference points. 
D
D(x, k, p) = 1 1 k k j=1 |d(x j , p) − d(x, p)| 
(2) 
where d(x, p) is the distance of x from the reference point p. p r is the closest reference point to p. The candidate data points are ranked according to their relative degrees of density computed on a set of reference points. Outliers are those with high scores. This scheme can discover multiple outliers in larger datasets. However three main limitations of this scheme 
Anomaly Detection
 Anomaly detection refers to the problem of finding nonconforming patterns in data. These patterns are often known as anomalies, outliers, exceptions, surprises, or peculiarities in different application domains. Anomalies and outliers are two terms used most commonly in the context of anomaly detection, sometimes interchangeably . The importance of anomaly detection is due to the fact that anomalies in data translate to significant, and often critical, actionable information in a wide variety of application domains. For example, an anomalous traffic pattern in a computer network could mean that a hacked computer is sending out sensitive data to an unauthorized destination. 
Feature Selection Using PCA
 Principal Component Analysis (PCA) is often used to reduce the number of dimensions in data for cost-sensitive analysis 
x 1 , x 2 , x 3 , · · · x p and y 1 , y 2 , y 3 , · · · y p 
be two p dimensional observations. PCA is concerned with explaining the variance-covariance structure of a set of variables through a few new variables which are functions of the original variables. Principal components are particular linear combinations of the p random variables 
x 1 , x 2 , x 3 , · · · x p with 
three important properties: (i) The principal components are uncorrelated, (ii) The first principal component has the highest variance, the second principal component has the second highest variance, and so on, and (iii) The total variation in all the principal components combined is equal to the total variation in the original variables 
x 1 , x 2 , x 3 , · · · x p . 
 They are easily obtained from an eigen analysis of the covariance matrix or the correlation matrix of 
x 1 , x 2 , x 3 , · · · x p . Let dataset x be denoted as {x 1 , x 2 , x 3 · · · x n } with n 
objects, where each x i can be a numeric or categorical attribute represented by a d-dimensional vector, i.e., 
x = {x i,1 , x i,2 , x i,3 · · · x i,d }. 
 Let A be a n × p covariance matrix of n observations in p dimensional space, i.e., p random variables 
ROS (x) = max 1≤i≤k S i k ×   1 − min 1≤i≤k dist(x i,j , R i,j ) × k i=1 min 1≤i≤k dist(x i,j , R i,j ) k i=1 max 1≤i≤k dist(x i,j , R i,j )   (3) x 1 , x 2 , x 3 , · · · x p . If (λ 1 , e 1 ), (λ 2 , e 2 ), (λ 3 , e 3 ), · · · , (λ p , e p ) 
are the p eigenvalue-eigenvector pairs of A, 
λ 1 ≥ λ 2 ≥ λ 3 , · · · , λ p ≥ 0
, the i th sample principal component of an observation vector, 
x = (x 1 , x 2 , x 3 , · · · x p ) is y i = e i z = [e i1 z 1 , e i2 z 2 , e i3 z 3 , · · · , e ip z p ] (4) 
where '' represents the transpose of the matrix, 
e i = (e i1 , e i2 , e i3 , · · · , e ip ) 
is the i th eigen-vector and z = 
(z 1 , z 2 , z 3 , · · · , z p ) 
 is the vector of standardized observations defined as 
z k = x k −x k √ s k 
 where x k and s k are the sample mean and sample variance of the variable x k . The features are selected based on the eigenvectors with highest eigenvalues from p dimensional space. Therefore, our approach works on reduced feature spaces given by PCAF, which is based on PCA. 
The Proposed Approach
 AOCD aims to detect anomalous patterns, i.e., coordinated port scans using an adaptive outlier based approach with reference to profiles. Initially, we select random samples , x 1 , x 2 , · · · x s using a linear congruential generator from the dataset x for training purpose. It is a maximum length pseudo random sequence generator 
C 11 , C 12 , C 13 , · · · C 1k , C 21 , C 22 , C 23 , · · · C 2k , · · · C s1 , C s2 , C s3 , · · · C sk . It 
timates the range based profiles for each cluster and matches each profile with others to remove redundancy. These profiles are used as reference during score computation . Finally, it computes score for each candidate object and reports as normal or outliers (i.e., attack) w.r.t. a threshold, δ. We present the Fuzzy C-means clustering technique for cluster formation in Algorithm 1. Let S i be the number of classes to which each of k nearest neighbor data objects belongs, where k is fixed for a particular dataset. Let x i,j be a data object in x and dist(x i,j , R i,j ) be the distance from the reference point R i,j to the data object x i,j , where dist is a proximity measure and x represents the whole dataset. The proposed approach is independent of the use of any particular proximity measure. However, in our experiments, we use Euclidean distance in computing proximity. The formula for the outlier score ROS is given in Equation (3). In this formula, 
max 1≤i≤k S i k 
is the maximum probability that a data object belongs to a particular Algorithm 1 FCM (x, k, m, l, ε) Input: x i is the i th data instance and u ij represents the whole data matrix, k is the number of clusters, m is a real number greater than 1, l is the number of iterations, ε is the termination criteria between 0 and 1. Output: Generate cluster, 
C 1 , C 2 , C 3 , · · · C k . Initialize U = [u ij ], U (0) . Compute the center vectors k (l) = [k j ] with U (l) : k j = N i=1 u m ij x i N i=1 u m ij Update U (l) , U (l+1) : u ij = 1 k l=1 x i −k j x i −k l 2 m−1 w.r.t. PCAF module. if U l+1 − U l < 
ε then Stop. else Return to Step 2. end if class; the remaining part is the summarized value of similarity measure within k nearest neighbors. The candidate data objects are ranked based on the score. Objects with scores higher than a user defined threshold δ are considered anomalous or outliers. δ is determined by a heuristic method. To test effectiveness, we consider seven different cases (illustrated in 
a) dist(x 1 , x 2 ) < δ and (b) dist(x 1 , x 2 ) = 0, if x 1 = x 2 . Definition 2. Profile: A profile of a cluster C i is a range value, µ(x µ,1 , x µ,2 · · · x µ,d ) 
of dataset x, where each x µ,j is the range of the j th column of the respective cluster C i . Definition 3. Outliers: Two data objects, O i and O j are defined as outliers w.r.t a cluster C i iff (a) ROS (O i , µ i ) ≥ δ where µ i is the profile of C i and, (b) for any other data object O j in 
C i , dist(O i , O j ) > δ. 
The symbols used to define the score based network anomaly detection algorithm are given in 
dist(x, y) = 0 if x = y (x 1 − y 1 ) 2 + (x 2 − y 2 ) 2 + · · · + (x n − y n ) 2 otherwise. 
AOCD: The Algorithm
The AOCD algorithm is based on the NADO 
C 11 , C 12 , C 13 , · · · C 1k , C 21 , C 22 , C 23 , · · · C 2k , · · · C s1 , C s2 , C s3 , · · · C sk are 
the set of clusters with cardinality sk. It generates the profiles, 
µ s1 , µ s2 , µ s3 , · · · µ sk for the clusters C s1 , C s2 , C s3 , · · · C sk obtained 
from the dataset x. Then it detects coordinated scans based on the outlier score ROS from the testing datasets. The major steps of AOCD are given in Algorithm 2. 
Algorithm 2 AOCD (x, δ) Input: x 
is the dataset, δ is the threshold Output: O i,j 's are the anomalous objects Select random sample, 
x 1 , x 2 , · · · , x s from the dataset x using α. Find clusters C s1 , C s2 , C s3 , · · · C 
Experimental Results
The main goal of the experiments is to apply AOCD to coordinated scan detection as well as to evaluate its capability in detecting outliers or anomalies or scans and compare it to the current best performing algorithms. To achieve this goal, we have implemented our algorithm and tested it with various real world datasets and datasets prepared by us on our TUIDS testbed in both packet and flow level. It has been used during attack generation in our TUIDS testbed for labeled coordinated dataset preparation . The network laboratory layout where we capture network traffic for coordinated port scans data is shown in 
Environment Used
 AOCD is implemented on an HP xw6600 workstation, Intel Xeon Processor (3.00 Ghz) with 4GB RAM. Java 1.6.0 version is used for the implementation in Ubuntu 10.10 (Linux) platform. Java is used to facilitate the visualization and reusability of code for further experimentation. 
Datasets Used
To evaluate the performance of AOCD, we use several real life datasets for experimentation. We use three datasets: our own datasets that are packet and flow based and KD- Dcup99 probe 
Results and Discussion
We use our feature datasets for experimentation in both packet and flow levels. The datasets are generated in our network security laboratory as discussed earlier. At packet level, we extract basic features, content based features , time based features and window based features (see in 
@BULLET Accuracy = T P +T N T P +T N +F P +F N @BULLET F alse P ositive Rate (F P R) = F P F P +T N 
Details of performance of AOCD for the real life TUIDS packet and flow level coordinated scan datasets are given in 
"
"Introduction
 One of the problems security administrators of organizations face is the lack of a global security policy specification . Organizations security policies are often described by access control rules (ACLs or other forms of simple rules) enforced on each protected object scattered all over the organization. This makes it very difficult for the organization to build a complete view of the policy enforced and makes it impossible to maintain coherency, thus undermining the trust in the system security. Organizations with MAC policies are less affected by this problem but they suffer from the lack of expressiveness of such policies. Because of such lack of expressiveness , MAC policies are never used alone, and therefore the problem remains. Having a single security model for the whole organization may not be appropriate at all. Moreover, having a single service, enforcing one or more closed models at the same time, does not scale in terms of efficiency and is not suitable for situations where users are not known in advance (open world assumption). However, the enforcement and mapping of users to policies 
The next section presents some related work. Section 3 presents the SPL structure and basic blocks (rules, entities , sets and policies) and describes how to express and enforce policies with those blocks. Section 4 shows how to express and enforce history and obligation policies. Section 5 describes how SPL handles conflicts. Finally, in Section 6 the paper is concluded. 
Related Work
Traditionally, access control policies are described by their implementation (ACLs, Unix protection bits, or database table permissions). This technique is not suitable for organizations with more than a few computers and certainly not suitable for large organizations with several domains, neither in terms of management or security . To cope with the management and scalability problems, recent commercial solutions have adopted the RBAC model 
SPL Structure & Basic Blocks
 In this section we present each one of the entities comprising SPL in detail and show how they are used in writing SPL security policies. SPL is a policy-oriented constraint-based language. It is composed of four entities: elements, groups, rules and policies (
Elements
 SPL elements are strongly typed entities with an explicit interface through which their properties can be queried. Every SPL element is a proxy to an entity of the underlying platform, thus enabling access to context information 2 . SPL elements may represent many different types, each one with its specific properties depending on the target platform. For instance, in a file system the elements are files, subjects and access requests (from now on referred to as requests) over subjects and files. In a router, the elements are networks, hosts and messages, each of these having different properties. Each property may be a ref-erence to another element, to a group or to one of the basic types number, string and boolean. In many SPL target platforms, the SPL element set may form a polymorphic hierarchy, where each element is a specialization of another element. 
Groups
 Elements can be divided into groups. Groups are essential in any policy as they provide the necessary abstraction to achieve compactness, generalization and scalability . Without groups, each rule would have to be repeated for each element to which the rule applies. Groups can be internal or external. Internal groups are internal SPL structures with references to the entities contained in the group. External groups are proxies for groups in the target platform. Some external groups are very useful for the definition of policies; for instance, the groups of all subjects and all elements known to the system (
(b) 
Figure 3: Example of: (a) external entities and sets; (b) a category and a group of other groups to the ones with particular properties . This is done by the SPL restriction operator (mygroup@{logical-expression}), which is a polymorphic operator that can be used in any type of group or rule (
Constraint Rules
Policies
An SPL policy is a collection of rules and groups that govern a particular domain of requests. Each policy has one "" Query Rule "" (QR) (identified by a question mark before the name of the rule) that relates all the rules specified in the policy. This rule uses the algebra defined before to specify which rules should be enforced and how. The domain of applicability of a policy is the domain of applicability of the QR. Unlike several logical based languages, in SPL there is not an implicit operation between rules, i.e. the rules inside a policy do not form an implicit disjunction or conjunction . The expression formed by the rules is given by the query rule. This solution provides flexibility to the language because the user building the language may choose whatever construction is the best. In fact, he may even choose to extend a predefined policy (see inheritance below in this section) which has a predefined query rule that performs the conjunction or the disjunction of every rule in the policy. In an SPL policy some of the groups can be parameters that are passed to the policy whenever it is instantiated (or, more correctly, activated). This allows for the construction of several abstract policies, which may be activated several times with different parameters. For instance, it is possible to have a generic DAC policy, a generic separation of duty policy, or a simple generic ACL policy (
Implementation
One of the problems of expressive security frameworks such as SPL, is the low efficiency of their implementations . While usual frameworks built upon access control lists, labels or Unix permission bits were designed to be efficient, SPL was designed to be expressive. In this section, we show that, using a mixture of compilation and query techniques, it is possible to achieve acceptable performance results, even for policies with thousands of rules. We have designed and implemented a compiler for SPL, which generates standard Java and is able to detect special SPL constructions and generate the most efficient code to implement them. Given the resemblance between SPL and Java structures , most of the compiler's actions are simple transla- tions: @BULLET each SPL policy is directly translated into a Java policy genericRole (user group Authorized, user group Active) { // Events inserting a user into // the Active group are allowed only if // that user is in the Authorized group ?genericRole:ce.action.name = ""insert""& ce.target = Active :: ce.par
Special Constraints
 The previously described language can be used to express several types of constraints, including complex constraints that require special implementation considerations . In this section we show how to express and implement two special types of constraints with a request monitor: history-based constraints and obligation constraints. 
History Constraints
Several security policies require requests to be recorded, in order to implement constraints with dependencies in the past. Among them, the Chinese Wall pol- icy 
The group contains all the targets with the same conflict of interest. The rule states that the current request is denied if the target of the request is in the "" interest class "" and there is a past request (pr) performed by the same subject on a target with a different owner that belongs to that "" interest class "" . Usually an organization implementing a Chinese Wall policy has several classes of conflicting interests. The above mentioned policy has just one class, but can be instantiated several times, one for each class of interest. The decide-expression of the rule has a constant value, which is consistent with the monotonicity of the Chinese Wall definition. This definition specifies which requests should be denied, but leaves it up to complementary policies to decided upon the ones that should be accepted. If, for instance, the expression cr.target.creator != pr.target.creator is moved from the domain-expression to the decide-expression, the policy result could either be allow or deny, which is against the monotonicity of the policy definition. 4.1.2 Implementing History Constraints A monitor-like security service has to decide, for each request, whether it should allow or deny the request. The decision must be taken at the time of the request with the information available. Thus, in order to implement history-based policies, any monitor-like security service has to record information about past requests. Some security services record requests implicitly in their own data structures 
Should 
Number of events Microseconds ¡ ¢ £ ¤ ¥ ¥ ¡ ¦ ¢ § £ ¨ ¤ © ¥ ! ! ! "" # $ ! $ % & ' ( ) 0 1' ) 2 3 0 4 5 $ 6 $ 5 # 7 8 4 5 $ 6 5 # 7 8 5 9 @ 7 5 # 7 8 
Obligation Constraints
 SPL is able to express the concepts of permission, prohibition and obligation. While the first two are usually supported by classical access control services, the last one is not. However, several access control solutions have recently started to recognize the importance of obligations to express current security policies 
Triggered obligations are those obligations activated by triggering actions. These obligations can be represented by the following generic expression "" if do TriggerAction then must do ObligatoryAction "" , which can be shown to be equivalent to "" cannot do TriggerAction if will not do ObligatoryAction "" 6 , which is a constraint with a dependency in the future. SPL specifies constraints with dependencies in the future the same way it expresses constraints with dependencies in the past, but with the special group PAR replaced by the special group FAR (Future Accepted Requests). 
Conditional Obligations
 Not every statement expressing a conditional obligation in common language requires an obligation-based policy to be enforced. For instance, the statement "" if someone executes some application he must register as a user "" contains a conditional obligation but it can usually be expressed as a conditional prohibition with a dependency in the past: "" Someone may execute an application if he has previously registered as a user "" . Thus it is necessary to clearly identify the situations where an obligation-based policy must be used. We have identified two generic situations where an obligation-based policy is required. The first one occurs when the two actions involved in a conditional obligation, oblige each other. The situation described before, where someone is obliged to register as a student of at least one discipline if he has registered as a student of the Online University and vice versa, can be given as an example. In this situation, it is not possible to rewrite the statement as a conditional prohibition because, whatever action is performed first, there is always an obligation to fulfill. The second situation occurs when the obligatory action is causally dependent on the trigger action. For instance, if the obligatory action requires a value obtained by the trigger action, the obligatory action cannot be executed before the trigger action, forbidding the transformation of the conditional obligation into a conditional prohibition. 4.2.
Implementing Obligation Constraints
Although the problem of expressing obligations with a constraint-based language can be solved by transforming them into constraints with dependencies in the future , they still cannot be easily implemented by a security monitor. In fact, Schneider 
(b) 
Figure 14: (a) Examples of the simplification and propagation rules. (b) Examples of incoherencies found by the tool. Goal 1 verifies if the policy is applicable to at least one request. Goal 2 and Goal 3 verify if the policy does not deny or allows every request, respectively. Each set of rules solves a specific type of constraint. For instance, 
Conclusion
 We have defined an access control language that simultaneously supports multiple complex policies, and either has a higher expressive power, or presents better results in terms of design than other multi-policy environments. The language uses its hierarchical based policy-oriented structure to solve conflicts between simultaneously active policies. We also provide a tool to verify incoherencies within policies which goes beyond conflict detection. The language was designed to be easily enforced by a security monitor. We have shown how index techniques can be applied to the policy structure to efficiently implement most security policies. Special care was taken with the enforcement of history-based policies. We have shown that by generating specific and special tuned logs for each history-based policy it is possible to implement SPL history-based policies as efficiently as handcoded labelbased implementations. The language goes beyond the permission/prohibition concepts of security and shows how to express and implement the obligation concept using ACID transactions. We have defined an easily described language, which does not have any specific predicate for any type of policy, yet it is able to express RBAC, history-based, obligationbased and delegation policies, among others. The language is composed of rules to decide about accesses, that can be composed using a simple logic with four basic operators (AND, OR, NOT and restriction) and their respective quantifiers (FORALL, EXIST and restriction quantifier), and uses policies to provide encapsulation, inheritance and parameterization , thus improving policy reusability. Preliminary results show that SPL should be mainly used to assemble big policy blocks. Simple rules and quantifiers should only be added if absolutely necessary, because they tend to clutter the policy with small and difficult things to read. We are currently developing a graphical interface which is expected to solve some of the low level usability problems identified in SPL 
"
"Introduction
 In the secure communication areas authenticated key exchange protocol is one of the most important cryptographic mechanism. In 1992, Bellovin and Merrit 
Password Authenticated Key Exchange Protocol based on Twin DH Problem
Cash, Kiltz and Shoup 
Twin DH Problem
Let G be a cyclic group of prime order q with a generator g. Define dh(X, Y ) := Z where X = g x , Y = g y and Z = g xy . Given random X, Y ∈ G, the problem of computing dh(X, Y ) is the DH problem. The DH assumption asserts that it is hard to compute dh(X, Y ) with random choice X, Y ∈ G. Define 2dh : 
G 3 −→ G 2 (X 1 , X 2 , Y ) −→ (dh(X 1 , Y ), dh(X 2 , Y )
), which is called the twin DH function. The twin DH assumption states that it is hard to compute 2dh (X 1 , X 2 , Y ), given random (X 1 , X 2 , Y ) ∈ G. 
Proposed Protocols
 In this Section we propose two secure key exchange protocols based on twin Diffie-Hellman assumption. The first proposed protocol (Protocol 1) is a variation of S-3PAKE protocol 
P A , U A ), where U A = g P1 and (P A , P 1 ) = H(P w A , id A , 
Protocol 1
The following are the detailed steps of the protocol as shown in 
Step 
z ∈ R Z q to compute L ← g z , g xz ← (g x ) z , g yz ← (g y ) z . Now, S computes X ′ ← g yz .H(P A , id A , id B , g x , (U A ) z ) and Y ′ ← g xz .H(P B , id A , id B , g y , (U B ) z ) and sends (X ′ ∥ L) and (Y ′ ∥ L )to B. 
Security Analysis
The security of Protocol 1 mainly relies on the difficulty of twin Diffie-Hellman problem. Unlike, in other protocols the passwords of the clients are only partially shared with the server. This prevents most of the attacks. 1) Trivial attacks: An attacker may directly try to compute the passwords and/or the session key 
SK = H ′ (id A , id B , g xyz ) from the transmitted messages (X, Y, X ′ , Y ′ , α, β
 ). But due to the difficulties of discrete logarithm problem, twin Diffie-Hellman problem and one-way ness of hash function, the trivial attack is not possible in our proposed protocol. 
2) Online password guessing attacks: In online guessing attacks, an attacker tries to confirm a guessed password in an online transaction. In our proposed Protocol 1, the passwords of the clients are only partially and not completely shared with the server. The part P 1 and P 2 of the passwords are kept secret with the users A and B respectively and are not transmitted through any messages. Even if an attacker or a malicious user B tries to guess A's password he can only 
guess P A get X ′ = M P ′ A 
 and send it in online transaction . But to verify the correctness of his guessed password he has to compute 
H(P A , id A , id B , g x , (U A ) z ) 
which is impossible since he requires the value of P 1 for getting the value of (U A ) z . Therefore online guessing attack is not possible on our proposed pro- tocol. 
3) Off-line password guessing attack: Assume that an attacker tries to mount off-line password guessing attack to guess the password. He intercepts the messages X and X ′ or Y and Y ′ but still he cannot verify his guessed password due to the difficulty of getting the values of P 1 or P 2 and one-way ness of hash function. Hence off-line password guessing attack is impossible in our scheme. 4) Man in the middle attack: The attacker in the middle attack involves interrupting a message and substituting it with his own message such that the communicating parties, without detecting the attacker compute the wrong session key. However authentication straight forwardly prevents this attack. For, suppose an attacker C tries to impersonate A and communicate with B, he computes C = M P wc and send id A ∥ C impersonating as A, in the subsequent steps to compute g yz 
← X ′ /H(P A , id A , id B , g x , (U A ) z 
) he needs the value of P 1 for calculating (U A ) z which is impossible to find. Hence there is no scope for man in the middle attack in our protocol. 
5) Replay attack: In a replay attack, an attacker may want to pretend to be A by replaying X to B. However, as he does not know the password of user A and x, y, z are randomly chosen in each session , the attacker has no ability to derive 
g yz ← X ′ /H(P A , id A , id B , g x , (U A ) z ) and produce a valid session key SK A = H ′ (id A , id B , K) where K = g xyz . 
Similarly the attacker also cannot pretend to be B. The replay attack will hence fail. 6) Forgery attacks: If a masked server tries to deceive the requesting users A and B he has to obtain the validity of messages 
g x , g y , H(P A , id A , id B , g x , (U A ) z ), H(P B , id A , id B , g y , (U B 
 ) z ) in Step 2 of our protocol where P A and P B are the passwords of A and B respectively. However, without knowing the users passwords P A , P B and also 
(U A ) z and(U B 
) z , it is not easy for a masked server to compute 
H(P A , id A , id B , g x , (U A ) z ) and H(P B , id A , id B , g y , (U B 
) z ) exactly so that users A and B can construct the common session key. Also, in the case of untrusted server, since the password of the user is not completely shared with the server, to get the value of P 1 from g P1 , an attacker has to face the difficulty of discrete logarithm problem. 7) Perfect forward secrecy: Even if the passwords P A and P B of the users are compromised, the attacker cannot calculate the session key as P 1 and P 2 are unknown. These values remain unknown even to the server and so there is no chance of any compromise. Also the session key is independent in each session and x, y, z are randomly chosen. 
Protocol 2
P A , U A = g P1 x ← R Z q y ← R Z q P B , U B = g P2 X ← g x .M PA Y ← g y .N PB idA∥X∥idB idA∥X∥idB ∥Y z ← R Z q , L ← g z g x ← X/M P A g y ← Y /N PB g xz ← (g x ) z g yz ← (g y ) z X ′ ← g yz .H(P A , id A , id B , g x , (U A ) z ) Y ′ ← g xz .H(P B , id A , id B , g y , (U B ) z ) (U B ) z ← L P2 X ′ ∥Y ′ ∥L g xz ← Y ′ /H(P B , id A , id B , g y , (U B ) z ) g xyz ← (g xz ) y α ← H(id A , id B , g xyz ) (U A ) z ← L P1 X ′ ∥L,α g yz ← X ′ /H(P A , id A , id B , g x , (U A ) z ) g xyz ← (g 
∈ R Z q to compute L ← g z ,(U A ) z , (U B ) z , a ← g xz ← (g x ) z , b ← g yz ← (g y ) z . Then S computes Z A ← b ⊕ H(P A , id A , id B , g x , (U A ) z ) and Z B ← a ⊕ H(P B , id A , id B , g y , (U B ) z ) and sends (Z A , L), (Z B , L) to B. 
 Step 3a. B, on receiving the message uses P 2 to 
pute (U B ) z ← L P2 and a← Z B ⊕ H(P B , id A , id B , g y , (U B ) z ) and authenticates S. Now, B uses y to compute K ← g xyz ← a y , α ← H(id A , id B , K) and forwards (Z A , L), α to A. 
 Step 3b. A, on receiving (Z A , L, α) uses P 1 to compute (U A ) z ← L P1 and b← Z A ⊕ H(P A , id A , id B , g x , (U A ) z 
) and authenticates the server. Then A uses x to compute K ← g xyz ← b x and checks whether α ← H(id A , id B , K) holds or not. If it does not hold, A terminates the protocol, otherwise A is convinced that K is the valid session key. Then A computes 
β ← H(id A , id B , K) and forwards it to B. Also, A computes the session Key SK A ← H ′ (id A , id B , K). 
Step 3c. Upon receiving β, B computes β← H(id A , id B , K
) and verifies whether computed β is equal to the received β. If both are equal then B authenticates A and computes the session key SK 
B ← H ′ (id A , id B , K). 
Security Analysis
1) Trivial attacks: Computing the session key from the transmitted messages α or β, is impossible due to the one-way ness of hash function. Also, for computing it from other transmitted messages Z A or Z B an attacker has to face the difficulty of discrete logarithm problem. So, our protocol is resistant to trivial attack. 
2) Password guessing attacks: Suppose an attacker or a malicious user B try to guess A's password as 
U serA U serB ServerS secret : P A , P 1 secret : P B , P 2 secret : P A , U A = g P1 P B , U B = g P2 x ← R Z q y ← R Z q N A ← N B ← g x ⊕ H(P A , id A , id B ) g y ⊕ H(P B , id A , id B ) NA,idA - NB ,idB z ← R Z q , L ← g z g x ← N A ⊕ H(P A , id A , id B ) g y ← N B ⊕ H(P B , id A , id B ) a ← g xz ← (g x ) z b ← g yz ← (g y ) z Z A ← b ⊕ H(P A , id A , id B , g x , (U A ) z ) Z B ← a ⊕ H(P B , id A , id B , g y , (U B ) z ) (U B ) z ← L P2 (ZA,L),(ZB ,L) a ← Z B ⊕ H(P B , id A , id B , g y , (U B ) z ) K ← g xyz ← a y α ← H(id A , id B , K) U z ← L P1 (Z A ,L),α b ← Z A ⊕ H(P A , id A , id B , g x , (U A ) z ) K ← g xyz ← b x verif y : α β ← H(id A , id B , K) SK A ← H ′ (id A , id B , K) β verif y : β SK B ← H ′ (id A , id B , K) 
Figure 2: Proposed Protocol 2 P ′ A , generates g x ′ ← N A ⊕ H(P ′ A , id A , id B ) and 
sends it to the server S in online transaction in Step 1 of our Protocol 2. To verify the correctness of his guessed password he needs to compute 
b ← Z A ⊕ H(P A , id A , id B , g x , (U A ) z ) and a ← Z B ⊕ H(P B , id A , id B , g y , (U B ) z 
) which is impossible as he needs the values of 
P 1 and P 2 for computing (U A ) z and (U B ) z . Similarly remaining off-line also, using the transferred messages N A , N B , Z A , Z B , L
 , an attacker cannot verify the correctness of his guessed password. 
 3) Man in the middle attack: In Step 2 of our protocol , S authenticates the 2 communicating parties A and B from the messages 
N A ← g x ⊕ H(P A , id A , id B ) and N B ← g y ⊕ H(P B , id A , id B ) sent by B. A and B authenticate S, from Z A ← b ⊕ H(P A , id A , id B , g x , (U A ) z ) and Z B ← a ⊕ H(P B , id A , id B , g y , (U B ) z ) as P A , P B are known only to S. Finally, A authenticates B from α ← H(id A , id B , K). 
Thus, in each step of our protocol each party authenticates the other communicating party and hence there is no scope for man in the middle attack. 4) Replay attack: Since one way hash function is used, our proposed protocol is invulnerable to this attack. 5) Forgery attacks: In case the server is compromised, the attacker is required to compute 
g x ← N A ⊕ H(P A , id A , id B ) and g y ← N B ⊕ H(P B , id A , id B ) 
 where P A and P B are the passwords of A and B respectively . However it is not possible to compute these values without the knowledge of the passwords and hence A and B cannot construct the common session key. 6) Perfect forward secrecy: In case, the passwords P A , and P B of the users A and B are compromised, the attacker cannot calculate the session key as P 1 and P 2 are unknown. These values remain unknown even to the server and so there is no chance of any compromise . Also the session key is independent in each session and x, y, z are randomly chosen. 
 4 Formal Verification and Validation of Proposed Protocols 
AVISPA
Automated validation of internet security protocols and applications (AVISPA) 
 4.2 Specification and Verification of Proposed Protocol 
We verified the security of proposed protocols using AVISPA and SPAN. For this we define three basic roles played by Alice (A), Bob (B) and Server (S). P W A and P W B are the passwords of A and B where P W A = (PA,P1) and P W B = (PB, P2). PA and PB are shared with S and hence represent the symmetric keys. P1 and P2 remain secret with A and B as their private keys. S gets UA = exp(G,P1) from A and UB = exp(G,P2) from B. Hence UA and UB are the public keys whose inverse is known only to A and B respectively. We then define the composed roles describing the sessions of the protocol and finally the top level role "" environment role "" . SPAN is used to symbolically execute the HLPSL protocol specification and hence provides a better understanding of the specification. For analyzing the protocol using AVISPA tool the following notations have been used. 
g → G
 5 Security and Efficiency of S- 3PKE Protocols 
In this Section we discuss the efficiency and security of the proposed protocols and existing competitive protocols based on Abdalla and Pointcheval's concept. We include the following protocols: Lu and Cao's S-3PAKE protocol 
Conclusion
Although many password based key exchange protocols are being developed, most of them are vulnerable to 
"
"Introduction
Increasingly organizations are connecting their Intranets to the Internet through firewalls and creating virtual private networks. This requires various types of information system to communicate in a secure manner. Many organizations use firewalls to provide a level of security by controlling access to information systems. By linking firewalls together via encrypted network connections it is possible to create a virtual private network 
Information Integrity through XML
 XML is emerging as the key standard for information interchange in e-commerce applications over the Internet 
Proxy Servers
A proxy server for a service is an application that from the perspective of the Internet behaves like the specified service , The proxy server activity passes complete messages to the application server located behind the firewall on the corporate Intranet. When combined with encrypted channels, proxying can be used to create a virtual private network 
The Xeon Architecture
The purpose of the XEON architecture is to provide a mechanism through which various applications (such as ecommerce systems) can have their application level protocols encoded via XML and transferred securely to another application. The ability of the architecture to function in a transparent manner from the perspective of an application allows for the creation of a virtual private network. A high level overview of the proposed architecture can be found in 
 1) The User message containing an XML format document M*, the Users IP address UIP (the From-IP) and the Firewall IP address FIP (the To-IP) is received by the Network Manager. 
2) The Network Manager strips out the FIP from the To-IP and passes the message to the HTTP Proxy. The Network Manager also applies a set of firewall specific rules and checks to the incoming connection. 
3) The HTTP proxy identifies the document as being in XML format and passes the message to the XML Router. 
 4) The XML Router identifies the appropriate Application Proxy by examining the DTD of the XML document and passes the message to that Proxy. In addition, the XML Router will: a. Strip the public DTD out of the XML document and insert the equivalent secure DTD into the document. b. Apply the incoming security policy to the XML document (see 
5) The Application Proxy parses the XML document, performing validation if specified. It performs any application specific checks and converts the result of parsing the document into a format appropriate to the Application. This converted document in native Application format is then passed to the Application. 
 6) The Application returns its response R to the Application Proxy. This is achieved by the application interacting with the application proxy using its own high-level application specific protocol, for example, SMTP. 
 7) The Application Proxy wraps the Application response in XML and strips out the Application IP address SIP, passing the resulting message to the XML Router. 
 8) The XML Router will: a) Apply the outgoing security policy associated with the secure DTD to the XML document (see 
9) The HTTP Proxy passes the message to the Network Manager. 
10) The Network Manager places the Firewall IP FIP into the From-IP field of the message and passes the message to the User IP address UIP. 
Network Manager
The purpose of a firewall is to restrict access between a protected network and the Internet, or between other sets of networks 
@BULLET Block all incoming connections from systems outside the internal network except incoming SMTP connec- tions. 
 @BULLET Block all connections to or from certain untrustworthy systems. @BULLET Allow e-Mail, FTP services, but block dangerous services such as TFTP, RPC, etc. 
There is a known set of vulnerabilities inherent within the TCP/IP protocol definition, such as SYN flooding and IP spoofing 
HTTP Proxy
A major concern regarding the security of HTTP proxies is the degree of damage that can be caused to the HTTP server by a malicious client. The security counter measures that are required to resolve the security concerns are well understood and well documented 
XML Router
 The XML router forwards XML documents to the appropriate application proxy. The XML router simply checks to make sure that a known public DTD is being used. If an unknown DTD is being used then the relevant security policy is applied. For example, the security policy for the detection of a malicious DTD could require the XML router to report the event to the security officer responsible for the management of the firewall. Within an XML document any embedded document type definition will take precedence over a DTD defined in the header of an XML document. Consequently an intruder may try to subvert the system by including an unauthorized DTD definition within an XML document. The XML router will therefore strip out and replace the DTD within an XML document. This ensures that a secure DTD is embedded within the XML document that is passed to the application proxy. The routing of XML documents to application proxies is achieved through the use of a simple database. Within the XML router a DTD is always associated with precisely one application. This association allows the router to pass a DTD to its appropriate application proxy. However, an application will be associated with a minimum of one DTD, it may be associated with many. A DTD is also associated with precisely one incoming security policy. An incoming security policy may be associated with many DTDs. This incoming security policy defines the actions to be taken and the rules to be applied when an XML document is received from the HTTP proxy. For example, an incoming security policy may define the maximum size of a document is 4096 bytes and the minimum size is 1024 bytes. In addition the security policy may also define that violations are only to be reported to the security officer (i.e. no error messages are to be reported to the source of the XML document). A DTD is also associated with precisely one outgoing security policy and an outgoing security policy may be associated with many DTDs. The outgoing security policy defines the actions to be taken and the rules to be applied when an application proxy receives an outgoing XML document. Typical incoming and outgoing security policies for specific DTD will include: @BULLET People from whom the system is allowed to accept XML documents encoded using a particular public DTD. @BULLET People who are allowed to send XML documents encoded using a particular secure DTD. @BULLET Maximum and minimum permitted size of an XML document. 
 @BULLET Comparison between the public DTD used in an incoming message and its secure version. @BULLET How and under what conditions errors are reported to the source of the XML document. @BULLET How and under what conditions errors are reported to the firewall and other security authorities. @BULLET Security countermeasures to be employed when a security rule is repeatedly broken. 
The routing for a particular DTD and the associated security policies are parameters of the XML firewall that are managed by the security officer. 
XML Application Proxy
The XML Application Proxy has three main functions. The first is to validate incoming XML documents and document content. The second is to convert incoming documents from XML format to native Application format . The final function is to convert outgoing native Application format documents to XML format. For an incoming message the public DTD will have been stripped by the XML Router and replaced with the equivalent secure DTD. The Application Proxy will also only receive documents sent with a DTD that can be handled by the Application Proxy. When dealing with an incoming message the proxy applies three processes: @BULLET XML Parser. This makes use of a look-up table that indicates the form of parsing that is to be performed on different document types according to their DTD. Each Application Proxy will contain a single parser. The look-up table specifies different forms of parsing depending on the DTD. The parser takes as input an XML document and produces as output a DOM tree or a series of parsed events depending on the type of parser. In the event of an unsuccessful parse, the message will be discarded and security policies will be followed. @BULLET Application Engine. The application engine contains a set of document specific rules that specify the way in which the DOM tree can be mapped into native Application format. These rules will be specialized to a specific document type within the Application . If the Application is XML enabled, it is possible that the output of the Application Engine may be an XML document. The Proxy also identifies the SIP of the Application it serves. The XML Parser, Validator and Application Engine will communicate though standardized APIs. When dealing with an outgoing message the proxy applies a single process, the application engine. The application engine contains a set of message specific rules that specify the way in which the native Application format message can be wrapped in XML. The Proxy also strips the SIP from the message to the XML Router. Outgoing XML messages are not validated as it is assumed that the wrapping generates valid XML documents. 
Conclusion
The XEON architecture provides a mechanism for secure communication between e-commerce applications over an unsecured network. XML is used to wrap application specific messages for network delivery via HTTP, SSL and HTTPS protocols. In addition to the standard XML validation , generic rules, application specific rules and security policies can be embedded in the system. It is also possible to extend the architecture to make use of emerging XML security standards such as XML signatures 
"
"Introduction
 With the development of Internet and P2P networks, people can publish their works, transmit important messages, and do trade network conveniently. Digital text is the most common and frequently used carrier (digital data) and has diverse forms, such as Webpage, e-mail, kinds of formatted text files including PS, PDF, DOC, TXT, and so on. Some data shows that the digital text's transmission accounted for a very large proportion of network traffic, so it is much easier to transmit the secret message in text. Text information hiding is to use text as the medium to hide information. The early methods of text information hiding are based on the physical formatting of text. Due to those methods exploited tolerances in typesetting by making minute changes in line placement and kerning, making them vulnerable to simple reformatting and OCR attacks 
Related Works 2.1 Natural Language Information Hid- ing
 Information hiding of natural language is one of text information hiding based on natural language processing techniques, which is becoming a hot spot. Publicly available methods for natural language information hiding can be classified into two groups. The first group of methods is based on generating a new text document for a given message with or without the given cover document 
The Mathematical Expression of a Chinese Character
mark(c) =          
0, c ∈ Γ AN D sum(l) < sum(r) OR c isn t in Γ 1, c ∈ Γ AN D suml ≥ sum(r) 
 3.2 Some Useful Linguistic Transforma- tions 
 Unlike the image, video and audio carrier, text has little redundant information. However, there are some redundancies by using linguistic transformation in natural language text. The proposed linguistic transformations involve the word level, the sentence level and the entire chapter. After the conversing the text into the binary strings, we exploit some useful transformations, such as synonym replacement, the replacement of variant forms of the same word, the adding or deleting of the empty words, and the shift conversion of the sentence. 
Definition 5. Broad synonyms: Absolute synonyms are the words which have the same meaning of all the senses in the same language. However, the number of absolute synonyms in Chinese is relatively small to nonexistent, so we define the broad synonyms, which exist in the dictionaries and have the same meaning in one or some senses. 
Definition 6. Variant forms of the same word are the different writing words, which are homophonic (sound, rhyme, tone), and have the same meaning. 
Definition 7. The empty words are the words, which have empty meaning but use frequently, such as the auxiliary word "" "" 
¡ "" ¢ "" ¡ "" £ "" and so on. 
 Definition 8. Shift conversion of the sentence is referring to that one part of the sentence, such as a word or a phrase can be placed in the different location in the sen- tence. 
Descriptions for the aforementioned transformtions are given below. 
1) The replacement of synonyms and variant forms of the same word. The synonyms and variant forms of the same word have the same meaning, so we can achieve the goal of embedding the secret message into the text by the replacement of them. We design a thesaurus based on words from the existing word lists of synonyms and variant forms of the same word, and moderately refine and augment or reduce manually by applying a Chinese lexical analysis system called ICTCLAS 
2) The adding or deleting of the empty words. Due to the empty meaning, the empty words can be selected to embed the secret message through adding or deleting them without changing the meaning of the text. In this paper, we just select the deleting the DE word in DE phrase. For example, the sentence "" ! "" . "" can be modified to "" ! "" . "" after deleting the DE word. 
3) The shift conversion of the sentence. Based on the characteristics of Chinese grammar, we rewrite the sentence by exploiting the shift conversion rules of the sentences that some linguists have summarized. For example, the preposition phrase can be placed before or after the verbs, even before the noun of agent. We just select the preposition "" "" 
# "" $ "" # "" % "" # "" & "" # "" "" # "" ' "" . 
Embedding Algorithm
 Let S be the sentences that have been selected for embedding the information in the text. The embedding process is as follows. 
1) Convert each sentence s i in S into a bit string B i according to Section 3.1, and then repeat the following Steps 2 -5 until all the secret message has been embedded. 
2) Text preprocessing: lexical analysis and part of speech tagging. 
3) We pick the information-carrying β bits within B i according to the key p, and verify if β bits match the desired values. If so return, else go to next step. 4) For each transformation in aforementioned transfor- mations; a. If there exists any synonym or variant forms of the same word in the sentence s i , replace it to get the sentence s i , and convert s i into B i . Then verify if the relevant bits of B i match the desired values. If so go to next step, else go to (b). b. If there exists any DE phrase in the sentence s i , delete DE to get the sentence s i , and convert s  c. If there exists any preposition phrase that satisfies the shift conversion rules in the sentence s i , re-write s i to get the sentence s i , and convert s i into B i . Then verify if the relevant bits of B i match the desired values. If so go to next step, else skip s i . 5) Obtain the transformed sentence s i , and select the next sentence s i+1 . 
Extracting Process
The extracting process is similar to the embedding process except that reading the bit value rather than modifying the sentences. We just describe it briefly as follows. Let S ' be the sentences that have been selected for embedding the information in the text. The extracting process repeats the following Steps 1 -2 until the last sentence in S '
 2) Pick the information-carrying β bits within B i according to the key p, and estimate whether the corresponding terms to β bits satisfy the aforementioned transformation or not. If so, read the β bits, else skip 
s i . 
3) Link the β bits and obtain the secret message. 
Experiments
We have implemented the binarization of all the 20902 CJK Chinese characters in UNICODE 3.0, thus we can lookup the corresponding bit value directly while embedding and extracting information. Compared with other 
Conclusions and Future Works
This paper proposed a text binarization method, which is based on Chinese mathematical expression. The method can achieve the goal of converting Chinese text into bit string conveniently, and the conversion is one-way and secure. Moreover, some useful linguistic transformations suitable for Chinese textual messages were introduced to embed the secret information. These available transformations is context based rather than format based, furthermore , they can not only enlarge the capacity, but also enhance the imperceptibility of the text information hid- ing. 
"
"Introduction
Cloud computing 
2) Server cannot cheat the client with the computing result of other input values. 
3) The verification of client should be efficient. 
The main results of this paper are two aspects, the function obfuscation technic and the secure scheme for verifiable delegation of polynomials. The function obfuscation technique which is based on the large integer factorization will make an efficient computation polynomial mix in the outsourcing polynomials, and due to the difficulty of large integer factorization the server cannot recognize the polynomial which is mixed in. The secure scheme for verifiable delegation of polynomials is based on this technic, with the mixed efficient computation polynomial client can easily verify the result returned by server. 
Related Work
In 2001, secure outsourcing for scientific computing and numerical calculation are studied for the first time by Atallah, Pantazopoulos and Rice 
Organization of this Paper
 The organization of this paper is as follows. Some preliminaries are given in Section 2. The algorithms of verifiable computation are given in Section 3. Then in Section 4 we give our protocol of verifiable delegation of polynomials. The security analysis is given in Section 5. The efficiency of our scheme analysis is given in Section 6. Finally, conclusion will be made in Section 7. 
Preliminaries
Some definitions and technics are listed in this section, which will be used in the following sections. 
Negligible Function
A negligible function is a function negl(x) such that for every positive integer c there exists an integer N c such that for all x > N c such that 
| negl(x) |< 1 x c 
 Equivalently, we have the following definition. A function negl(x) is negligible, if for every positive polynomial poly(·) there exists an integer N poly > 0 such that for all x > N poly | negl(x) |< 1 poly(x) 
Integer Factorization Problem
Given a number n = pq, where p and q are two large prime numbers, it is difficult to factorize n. This problem has many different versions. Here we introduce a decisional problem. Given n = pq, and an x with the corresponding y, it is difficult to determine that y satisfies which of the following equations: 
y = ax mod p or y = ax mod p * 
where, p * < n is a random prime number. It can be described as the indistinguishable way, see the following experiment. Let IFP be the integer factorization problem, A is a PPT adversary, d ∈ {0, 1}. Gen denotes the generation algorithm, and Compute denotes the computation algo- rithm. 
Experiment Exp p,p * IF P,A [n, x, y] (p, p * , n = pq, a, x) ← Gen(1 k ) and b 0 = p, b 1 = p * For i = 1 to t x i ← A(n, x 1 , y 1 , · · · , x i−1 , y i−1 ) y i = ax i mod b d ← Compute(x i ) x * ← A(n, x 1 , y 1 , · · · , x t , y t ) y * ← Compute(x * ) d ← A(n, x 1 , y 1 , · · · , x t , y t , x * , y * ) 
If d = d, output 1. Else output 0. The advantage of an adversary A in the above experiment is defined as: 
Adv ind IF P,A (n, p, p * ) = | Pr[d = d] − 1 2 | 
The factorization of integer n = pq is a difficult problem means 
Adv ind IF P,A (n, p, p * ) ≤ negl(k) 
where negl(·) is a negligible function. 
Homomorphic Encryption
Homomorphic encryption is a form of encryption which allows some computations (such as addition, multiplication and exponentiation) to be carried out on ciphertext and obtain an encrypted result the decryption of which matches the result of operations performed on the plain- texts. Assume E(·) is an encryption algorithm, and D(·) is an decryption algorithm, fully homomorphic encryption should satisfy the following properties. 
(σ x1 , σ x2 ) ← E(x 1 , x 2 ), then D(σ x1 + σ x2 ) = x 1 + x 2 . 
(σ x1 , σ x2 ) ← E(x 1 , x 2 ), then D(σ x1 · σ x2 ) = x 1 · x 2 . 
Fully homomorphic encryption is useful in outsourcing computations. 
Given σ x ← E(x), σ y = f (σ x ), then y = D(σ y )
, which satisfies y = f (x). 
Verifiable Computation
A verifiable computation scheme is with two parties client and server. Client outsources the computation of a function f to an untrusted server. Client expects server to evaluate the function on an input and server returns a result with a proof that the result is correct. Then the client verifies that the result provided by the server is indeed correct about the function on the input. In this scenario , client should verify the result efficiently with much less cost of computation resources. A verifiable computation scheme is defined by the following algorithms: KeyGen(f, k) → (P K, SK): Based on the security parameter k, the key generation algorithm generates a key pair (P K, SK) for the function f . P K is provided to the server, and client keeps SK. ProGen SK (x) → (σ x , V x ): The problem generation algorithm is run by client to uses SK to encode the input x as σ x which is given to server, and a verification key V x which is kept private by client. 
Compute P K (σ x ) → (σ y 
 ): Given P K and σ x , the algorithm is run by the server to compute an encoded version of the output σ y . Verify SK (V x , σ y ) → (y∪ ⊥): Using the secret key SK, the verification key V k , and the encoded output σ y , the algorithm returns the value y = f (x) or ⊥ indicating that σ y does not equal to f (x). A verifiable computation scheme should be correct, secure and efficient. A verifiable computation scheme VC is correct if the algorithms allow the honest server to output values that will pass the verification. Definition 1 (Correctness). A verifiable computation scheme is correct if the algorithms allow the honest server to output values that will pass the verification. That is, for any x, f and any (P K, SK) ← KeyGen(f, k), 
if (σ x , V x ) ← P roGen SK (x), (σ y ) ← Compute P K (σ x ), then f (x) ← V erif y SK (V x , σ y ) 
 holds with all but negligible probability. 
In other words, for an input x and a given function f , a malicious server should not be able to convince the verification algorithm on output σ y such that σ y = f (x). We use the following experiment to describe this. 
Experiment Exp V f A [VC, f, k] (P K, SK) ← KeyGen(f, k) For i = 1 to q x i ← A(P K, x 1 , σ x,1 , V x,1 , · · · , x i−1 , σ x,i−1 V x,i−1 ) (σ x,i , V x,i ) ← ProGen SK (x i ) x * ← A(P K, σ x,1 , V x,1 , · · · , σ x,q , V x,q ) (σ x * , V x * ) ← ProGen SK (x * ) σ y ← A(P K, σ x,1 , V x,1 , · · · , σ x,q , V x,q , σ x * , V x * ) y ← (P K, V x * , σ y ) If y = ⊥ and y = f (x * )
, output 1. Else output 0. A verifiable computation scheme is secure if an incorrect output cannot be accepted. That is, the probability that the verification algorithm accepts the wrong output value for a given input value is negligible. For a verifiable computation scheme, we define the advantage of an adversary A in the above experiment as: 
Adv V f A (V, f, k)=Pr[EXP V f A [V, f, k] = 1] 
Then we get the definition of security. 
Definition 2 (Security). A verifiable computation scheme is secure if for any function f , and any PPT adversary A, that 
Adv V f A (V, f, k) ≤ negl(k) 
where negl(·) is a negligible function. 
 In the verifiable computation scheme the time for verifying the output must be much smaller than the time to compute the function. Definition 3 (Efficiency). A verifiable computation scheme is efficient, if the time required for Verify(V x , σ y ) is o(T ), where T is the time required to compute f (x). 
 4 Verifiable Delegation of Polyno- mials 
 In this section, we give the polynomial obfuscation technique based on integer factorization problem, and give our implementation scheme. 
Obfuscation Technic
The polynomial 
f (x) = a 0 + a 1 x + a 2 x 2 + · · · + a d x d , a i ∈ Z p , 0 ≤ i ≤ d 
which is a high degree polynomial, is outsourced to server. Client asks the server to compute the function on the value of x. In this scenario, the client must be able to verify the correctness of the result efficiently. For the secure and efficient verification, an efficient computing polynomial v(x) is chosen, and a verifiable polynomial F (x) = af (x) + v(x) is constructed. This should satisfy the following requirements. 1) Server cannot get any information about a from f (x) and F (x). 
2) v(x) cannot be identified by server. 
3) f (x) and F (x) are indistinguishable. We use the following technic, which is based on integer factorization problem, to achieve the requirements. Client selects a prime q randomly (which satisfies |p| = |q| ), and computes n = pq. Then client randomly chooses a ∈ Z * p . F (x) is generated as where,b i ∈ Z p , 0 ≤ i ≤ d. 
Proposition 1. Server cannot get any information about a from f (x) and F (x). Proof. For a = (b i ) −1 a i mod p(i = 0, 1, · · · , d), if p is known, then a can be uniquely determined. Server just knows n = pq, where q is unknown. If p is undetermined, there are p choices of a, so a is secure. (p, p * , n = pq, a, x) ← Gen(1 k ) and b 0 = p, b 1 = p * For i = 1 to t x i ← A(n, x 1 , y 
(1) 1 = ax 1 mod p, y (2) 1 = ax 1 mod p * , · · · , x i−1 , y (1) i−1 = ax i−1 mod p, y (2) i−1 = ax i−1 mod p * ) y (1) i = ax i mod p, y (2) i = ax i mod p * ← Compute(x i ) x * ← A(n, x 1 , y (1) 1 , y (2) 1 · · · , x t , y (1) t , y (2) t ) y * = ax * mod b d ← Compute(x * ) d ← A(n, x 1 , y (1) 1 , y (2) 1 · · · , x t , y (1) t , y (2) 
t , x * , y * ) If d = d, output 1. Else output 0. The advantage of an adversary A in the above experiment is defined as: 
Adv ind IN D,A (n, p, p * ) = | Pr[A(p) = 1]− Pr[A(p * ) = 1]| Let v(x) = r 0 + r 1 x + r 2 x 2 + ˙ +r d x d , r i ∈ Z p , 0 ≤ i ≤ d then b i = aa i + r i mod p. 
We also have the equation 
b i = aa i mod p * 
where p * = p. For there exists a prime p * such that 
p * |(aa i − b i ), so aa i − b i = kp * , 
then the equation b i = aa i mod p * is existent. Due to the difficulty of large integer factorization, we know 
Adv ind IF P,A (n, p, p * ) ≤ negl(k) so Adv ind IN D,A (n, p, p * ) = | P r[A(p) = 1] − P r[A(p * ) = 1] |≤ negl(k). 
For n cannot be factorized, p is unknown, the server cannot distinguish b i = aa i + r i mod p from b i = aa i mod p * . So the coefficient r i is secure. Thus v(x) cannot be identified by server. 
= aa i + r i mod p, 0 ≤ i ≤ d. There exist b, r i ∈ Z p , such that a i = bb i + r i mod p. 
For p is unknown and a, r i , 0 ≤ i ≤ d are safe, a i and b i are indistinguishable. Thus f (x) and F (x) are indistinguishable. 
Verifiable Scheme with Efficient Computing Functions
We assume x is an encoded input and y is an encoded output, the function f is a homomorphic encrypted func- tion. Client wants to computes y = f (x) mod p (f (x) is a high degree polynomial), he/she delegates it to a server. And the client can verify the correctness of the result. Initialization. Client selects a prime q randomly, and computes n = pq. Then client randomly chooses 
e(0 < e < d) and r 1 , r 2 ∈ Z * p . Client keeps p, q, e, r 1 , r 2 and 
send n to server. 
Delegation. Client sends two polynomials 
y = f (x) mod n and y = af (x) + x e + r 1 + r 2 x mod n to server. Verification. Server sends y 1 = f (x) mod n and y 2 = af (x)+x e +r 1 +r 2 x mod n to client. Client computes m = x e + r 1 + r 2 
x mod n and verifies whether the following equation holds. 
y 2 = ay 1 + m mod n. 
If this equation does not hold, that means the server gives the wrong answer, y 1 is not correct. If the equation holds, that means y 1 is the right answer, and client can get the final result by computing y = y 1 mod p. 
Security Analysis
Security of Parameters
Theorem 1. The probability that server can get p from the computation process is no more than 1 s , where s satisfies s ln s = n. 
Proof. Server gets two polynomials 
y 1 = f (x) mod n and y 2 = af (x) + x e + r 1 + r 2 x mod n so server knows the coefficients a i (i = 0, 1, · · · , d) of y 1 = a 0 +a 1 x+· · ·+a d x d , and the coefficients b i (i = 0, 1, · · · , d) of y 2 = b 0 + b 1 x + · · · + b d x d . 
The random number a is unknown. Comparing the coefficients, server can get b i = aa i mod p, and server can compute a = (b i ) −1 a i mod p. But a is unknown, for every prim p there exists one a such that 
a = (b i ) −1 a i mod p. Server should find a number a, such that a = (b i ) −1 a i mod p (i = 0, 1, · · · , d). 
There are about s prime numbers which are less than n, where s satisfies s ln s = n. So the probability server can get p is no more than 1 s . If p is a 512 bit prime. the probability is negligible. On the other hand, server knows n = pq, so server can get p form decomposing n. But by the difficulty of integer factorization, the probability that p can be obtained from the factorization of n is negligible. In these two aspects, the probability that server can get p from the computation process is no more than 1 s . 
Theorem 2. The probability that server can obtain a from the computation process is at most max
( 1 p , 1 s ). Proof. For a = (b i ) −1 a i mod p(i = 0, 1, · · · , d), 
if p is known, then a can be uniquely determined. The probability that p can be uniquely determined is no more than 1 s , where s satisfies s ln s = n, so the probability a can be determined is also 1 s . In another way server just knows n = pq, where q is unknown. If p is undetermined, there are p choices of a, so the probability that a can be determined is at most 1 p . So the probability that server can obtain a from the computation process is at most max
( 1 p , 1 s ). 
2 = af (x) + x e + r 1 + r 2 
x mod n, and server do not know a, e, r 1 , r 2 and p. Server can determine the function parameters through some special values. The equations are as follows. 
y (0) 2 = af (0) + 0 + r 1 + 0 y (1) 2 = af (1) + 1 + r 1 + r 2 y (2) 2 = af (2) + 2 e + r 1 + 2r 2 y (3) 2 = af (3) + 3 e + r 1 + 3r 2 · · · · · · y (m) 2 = af (m) + m e + r 1 + mr 2 where, a, 2 e , 3 e , · · · , m e , r 1 , r 2 are unknown parameters. 
 There are m + 1 equations with m + 2 unknown parameters , so server cannot gain the parameters from these equations. If server guesses a value of the parameters, the other parameters can be computed out. Even if server gets Theorem 4. The two polynomials y 1 = f (x) mod n and y 2 = af (x) + x e + r 1 + r 2 x mod n are indistinguishable. Proof. The probability p can be determined is no more than 1 s , and a is at most max( 1 p , 1 s ). x e +r 1 +r 2 x is mixed in the polynomial y 2 = af (x) + x e + r 1 + r 2 x mod n. For the mixed polynomial x e + r 1 + r 2 x is safe in the computation process, the two polynomials y 1 = f (x) mod n and y 2 = af (x)+x e +r 1 +r 2 x mod n are indistinguishable. This theorem means that server cannot distinguish which polynomial is the one client wants to compute and which one is for verification. 
2 e , 3 e , · · · , m e , 
No Fraudulence
Theorem 5. The probability that the random values which the server provides without the evaluation of the two polynomials y 1 = f (x) mod n and y 2 = af (x) + x e + r 1 + r 2 x mod n can pass the verification is max( 1 p 4 , 1 sp 3 ). Proof. The probability that server can obtain a from the computation process is at most max
( 1 p , 1 s ), that is Pr{A(a) = 1} = max( 1 p , 1 s ). 
The probability that x e + r 1 + r 2 x can be obtained from the computation process is 1 p 3 , that is 
Pr{A(x e + r 1 + r 2 x) = 1} = 1 p 3 
The two random values of the two polynomials y 1 = f (x) mod n and y 2 = af (x) + x e + r 1 + r 2 x mod n should satisfy the equation y 2 = ay 1 + m mod n so that they can pass the verification. However server does not know a and m, so 
Pr{V(y 1 , y 2 ) = 1} = max( 1 p , 1 s ) 1 p 3 = max( 1 p 4 , 1 sp 3 ). 
Thus probability that the random values can pass the verification is max
( 1 p 4 , 1 sp 3 ). 
Theorem 6. The advantage that server uses the result of other input to cheat client is at most e p . Proof. Considering the following experiment. 
Experiment Exp x0 P E,A (k) X = (x 1 , x 2 , · · · x t ) ← Gen(1 k ) Y = ((y (1) 1 = f (x 1 ), y (1) 2 = af (x 1 ) + x e 1 + r 1 + r 2 x 1 ), (y (2) 1 = f (x 2 ), y (2) 2 = af (x 2 ) + x e 2 + r 1 + r 2 x 2 ), · · · , (y (t) 1 = f (x t ), y (t) 2 = af (x t ) + x e t + r 1 + r 2 x t )) ← A(X, (f (x), af (x) + x e + r 1 + r 2 x)) x * = x 0 ← A(X, Y ) Return 1, if af (x * ) + (x * ) e + r 1 + r 2 x * = af (x * ) + (x 0 ) e + r 1 + r 2 x 0 . Else, return 0. Adv A,x0 = Pr[A x * (k) = 1 | x * ← Gen(1 k ), x * = x 0 ] = Pr[Return1] 
Client wants to compute y 1 = f (x 1 ), but server gives the values y 
2 = f (x 2 ) and y 2 = f (x 2 ) + x e 2 + r 1 + r 2 x 2 (x 1 = x 2 ). If the equation y 2 = ay 2 + x e 1 + r 1 + r 2 x 1 
holds, the value y 2 can pass verification. That means 
x e 1 + r 1 + r 2 x 1 = x e 2 + r 1 + r 2 x 2 mod p(x 1 = x 2 ) 
should hold. There are at most e values in Z p can satisfy this equa- tion 
x e 1 + r 1 + r 2 x 1 = m mod p. 
So the probability the equation x e 
1 + r 1 + r 2 x 1 = x e 2 + r 1 + r 2 x 2 mod p (x 1 = x 2 ) holds is at most e p . Hence, Adv A,x0 = e p 6 Efficiency Analysis To compute f (x) = a 0 + a 1 x + a 2 x + · · · + a d x d client 
needs 2d times multiplications. If the polynomial f (x) is outsourced to server, the client need to compute x e + r 1 + r 2 x and verifies wether y 2 = y 1 +x e +r 1 +r 2 x holds. If the verification passed, client should compute y = y 1 mod p. In this way, client needs e + 1 times multiplications and a modular exponentiation computation. If e is chosen much less than d, the computation cost is much smaller than the cost of computing f (x). The time cost comparisons of verification and computation are as 
Conclusion
 Cloud computing has made a reality of computation outsourcing . A new protocol for publicly verifiable outsourcing of evaluation of high degree polynomials is given in this paper. And we introduce a function obfuscation technic , the secret polynomial can be mixed into the outsourced polynomial by using this technic. This technic can also be used in the information hiding. In the verification phase, the result returned by server can be easily verified by client. And the time cost is much less than the time computing the original polynomial. Client can choose the value of e, such that the computation of verification value can be controlled within the user's computational capabilities. 
"
"Introduction
In here we give some definition and list of symbols that we use. In this paper we use finite field GF (2 8 ) that we can represent as GF (2)
@BULLET ∪ is OR; @BULLET ∩ is AND; @BULLET is left circular rotation by one bit; @BULLET is right circular rotation by one bit; @BULLET ⊕ is bitwise XOR; @BULLET || is concatenation of two operators; @BULLET K nl is the left side of 2n-bit key. This key part has size of n bits; @BULLET K r is the right side of K. The size is a half of full key. 
 The rest of this paper is organized as follows. The Section 2 describes the new block cipher BC2, its randomizing part and key schedule, Section 3 explains how to implement BC2 at various platforms efficiently, Section 4 explains cryptanalysis of BC2, Section 5 explains the design rationale of BC2 and Section 6 gives conclusion. 
BC(Block Cipher 2)
The BC2 is a 128-bit block cipher using Feistel Network that supports 128, 192 and 256-bit key lengths. Like many other ciphers, we use Substitution Boxes to give confusion , linear layer to give diffusion and mixed key to give dependent on key. The structure of BC2 for 128-bit key length, is showed in 
F11 
Substitution Box
We use Camellia's S-Box 
Linear Layer L
We use MDS (Maximum Distance Separable) matrix to realize linear component to give high diffusion. We do not use XORs component like in Camellia cipher, because it does not give branch number exactly. We use circular matrix with low number in order to be able to be implemented efficiently in hardware. 
A linear [n, k, d] code C with generator matrix G = [I k×k L k×(n−k) ] is MDS if, and only if, every square submatrix formed from rows and columns of L is nonsingular (cf. 
            b 0 b 1 b 2 b 3 b 4 b 5 b 6 b 7             =            
                        a 0 a 1 a 2 a 3 a 4 a 5 a 6 a 7             
where a i is input of MDS and b i is output of MDS. So b = L a. 
Add Key AK
 In this part, we use only XOR component to avoid weakness that we can find in IDEA cipher. 
Key Schedule
We construct a new key schedule with the criteria: 1) simple and fast for many platforms 2) it should be resistant to related key attack 3) it should be hard to find masterkey if attacker can get (partial) subkey(s). 4) there are no weak keys. 5) every bit of masterkey gives influence to all subkeys. We use the basic instructions (like XOR, AND, OR, 1- bit rotation) to achieve Objectives 1, 2, and 3. We also use the matrix component(like in Rijndael) in key schedule to achieve Objective 4. This component gives high diffusion SBB 
aa 
    
1     
Outputs of MCs are XORed with constant (
Implementation
In this section, we explain how to implement BC2 at various platform. If input of F function is IF, substitution operation is SB, L is linear operation, AK is Add- Key, and output of F function is OF, then we can write OF = AK(L(SB(IF ))). 
64-bit Processors
In this platform, BC2 can be implemented very efficiently. Like as Khazad or Rijndael cipher, we can write 
            b 0 b 1 b 2 b 3 b 4 b 5 b 6 b 7             =             SB[x 0 ] SB[x 0 ] @BULLET 2 SB[x 0 ] SB[x 0 ] @BULLET 3 SB[x 0 ] SB[x 0 ] @BULLET 2 SB[x 0 ] @BULLET 3 SB[x 0 ] @BULLET 2             ⊕             SB[x 1 ] @BULLET 2 SB[x 1 ] SB[x 1 ] @BULLET 2 SB[x 1 ] SB[x 1 ] @BULLET 3 SB[x 1 ] SB[x 1 ] @BULLET 2 SB[x 1 ] @BULLET 3             ⊕             SB[x 2 ] @BULLET 3 SB[x 2 ] @BULLET 2 SB[x 2 ] SB[x 2 ] @BULLET 2 SB[x 2 ] SB[x 2 ] @BULLET 3 SB[x 2 ] SB[x 2 ] @BULLET 2             ⊕             SB[x 3 ] @BULLET 2 SB[x 3 ] @BULLET 3 SB[x 3 ] @BULLET 2 SB[x 3 ] SB[x 3 ] @BULLET 2 SB[x 3 ] SB[x 3 ] @BULLET 3 SB[x 3 ]             ⊕             SB[x 4 ] SB[x 4 ] @BULLET 2 SB[x 4 ] @BULLET 3 SB[x 4 ] @BULLET 2 SB[x 4 ] SB[x 4 ] @BULLET 2 SB[x 4 ] SB[x 4 ] @BULLET 3             ⊕             SB[x 5 ] @BULLET 3 SB[x 5 ] SB[x 5 ] @BULLET 2 SB[x 5 ] @BULLET 3 SB[x 5 ] @BULLET 2 SB[x 5 ] SB[x 5 ] @BULLET 2 SB[x 5 ]             ⊕             SB[x 6 ] SB[x 6 ] @BULLET 3 SB[x 6 ] SB[x 6 ] @BULLET 2 SB[x 6 ] @BULLET 3 SB[x 6 ] @BULLET 2 SB[x 6 ] SB[x 6 ] @BULLET 2             ⊕             SB[x 7 ] @BULLET 2 SB[x 7 ] SB[x 7 ] @BULLET 3 SB[x 7 ] SB[x 7 ] @BULLET 2 SB[x 7 ] @BULLET 3 SB[x 7 ] @BULLET 2 SB[x 7 ]             
where x i is input of SBox-i and x is input of F function. 
KF N 1 (KA l ∪ SK3) ⊕ KC l KF N 3 (SK1 ∪ SK5) ⊕ KC l ) KF N 2 (KB l ∪ SK4) ⊕ KC r KF N 4 SK2 ⊕ SK6 ⊕ SK11 SK5 (KA l ∪ KB l ) ⊕ KF N 2 SK15 (SK7 ∩ KC l ) ⊕ SK12 SK6 (KA r ∪ KB r ) ⊕ KC r SK16 (SK8 ∪ KC r ) ⊕ SK13 SK7 (
T 0 =             SB[x 0 ] SB[x 0 ] @BULLET 2 SB[x 0 ] SB[x 0 ] @BULLET 3 SB[x 0 ] SB[x 0 ] @BULLET 2 SB[x 0 ] @BULLET 3 SB[x 0 ] @BULLET 2             T 1 =             SB[x 1 ] @BULLET 2 SB[x 1 ] SB[x 1 ] @BULLET 2 SB[x 1 ] SB[x 1 ] @BULLET 3 SB[x 1 ] SB[x 1 ] @BULLET 2 SB[x 1 ] @BULLET 3             
and so forth, then we have: 
OF = T 0 ⊕ T 1 ⊕ T 2 ⊕ T 3 ⊕ T 4 ⊕ T 5 ⊕ T 6 ⊕ T 7 ⊕ SK, 
where SK is subkey at each round. All T tables require 16 k bytes. 
32-bit Processors
To this platform we can write: 
    b 0 b 1 b 2 b 3     = a 0     1 2 1 3     ⊕ a 1     2 1 2 1     ⊕ a 2     3 2 1 2     ⊕ a 3     2 3 2 1     ⊕ a 4     1 2 3 2     ⊕ a 5     3 1 2 3     ⊕ a 6     1 3 1 2     ⊕ a 7     2 1 3 1     or OF 0−3 = T [0] ⊕ T [1] ⊕ T [2] ⊕ T [3] ⊕ T [4] ⊕ T [5] ⊕ T [6] ⊕ T [7] ⊕ SK 0−3 .     b 4 b 5 b 6 b 7     = a 0     1 2 3 2     ⊕ a 1     3 1 2 3     ⊕ a 2     1 3 1 2     ⊕ a 3     2 1 3 1     ⊕ a 4     1 2 1 3     ⊕ a 5     2 1 2 1     ⊕ a 6     3 2 1 2     ⊕ a 7     2 3 2 1     or OF 4−7 = T [8]
8-bit Processors
For this platform, the method that we describe above is unsuitable. So we use other method. We can write linear layer as follows: 
r 0 = a 0 ⊕ a 2 ⊕ a 4 ⊕ a 6 r 1 = a 1 ⊕ a 3 ⊕ a 5 ⊕ a 7 r 2 = xT (r 0 ) 
r 3 = xT (r 1 ) b 0 = r 0 ⊕ a 5 ⊕ r 3 ⊕ xT (a 2 ) b 1 = r 1 ⊕ a 6 ⊕ r 2 ⊕ xT (a 3 ) b 2 = r 0 ⊕ a 7 ⊕ r 3 ⊕ xT (a 4 ) b 3 = r 1 ⊕ a 0 ⊕ r 2 ⊕ xT (a 5 ) b 4 = r 0 ⊕ a 1 ⊕ r 3 ⊕ xT (a 6 ) b 5 = r 1 ⊕ a 2 ⊕ r 2 ⊕ xT (a 7 ) b 6 = r 0 ⊕ a 3 ⊕ r 3 ⊕ xT (a 0 ) b 7 = r 1 ⊕ a 4 ⊕ r 2 ⊕ xT (a 1 ). 
In this method we need four registers, 30 exors, 10 xT operations, and 12 assignments for linear layer implementation . If we have six registers, then we can reduce the operation. We write r4=r 0 ⊕ r 3 and r5 = r 1 ⊕ r 2 . The operations of SBox and Addkey are performed per byte. 
Key Schedule Implementation
 We use the same component in key schedule and randomizing part to give efficiency in implementation. We also use the basic instruction (OR, XOR, AND, 1-bit rotation) in key schedule in order to be able to be implemented efficiently at various platforms. 
Cryptanalysis
Differential and Linear Cryptanalysis
In this section we discuss about how to measure maximum differential and linear probability (DP max and LP max ) of BC2 without FN and F N −1 functions. We use heuristic method to count the minimal number of active substitution boxes. For differential attack 
DP max =(2 −6 ) 26 = 2 −156 and LP max = (2 −4 ) 26 = 2 −104 . 
 This probability can cryptanalysis Camellia with 2R at- tack. 
Square Attack and Its Variant
 Cipher having byte oriented is vulnerable with square at- tack 
⊕p = 0 i=0 
where p is byte plaintext, is hold till the input of round 5. So square attack and its variant are very unlikely to succeed for full round (13 rounds). 
Higher Order Differential Attack
 In general, a cipher with a low non-linear order is vulnerable to this attack. Since BC2 use non-linear component with degree 7, so after a few rounds, the degree will increase rapidly. Moreover, BC2 has 13 rounds, so this attack is impossible to be done. Moreover, the FN function in BC2 increase resistance to this attack, like Misty cipher that have only low degree in its S-Box. 
Interpolation Attack
A cipher with S-Box having simple algebraic is vulnerable to interpolation attack 
Related-key Attack and Slide Attack
Related-key attack 
Design Rationale
Non-linear Component
We choose S-Box from Camellia and Hierocrypt because these components have very excellent features. They have maximum differential probability 2 −6 , maximum linear probability 2 −4 and degree 7. So these components can be resistance against differential, linear and higher order differential attacks. The affine function in these components can improve the BC2 strength to interpolation attack and other algebraic attacks. 
Linear Component
We use MDS (Maximum Distance Separable) to increase the number of active S-Box, so BC2 can be resistance to linear/differential attack. MDS gives high diffusion that is also important to face boomerang attack. 
FN Function
This component is made to face unknown attacks. FN component also can damage path of linear hull and impossible differential attack. FN is designed more complicated than Camellia has, in order to give more protection, for example, against truncated differential attack 
Appendix A: Substitution Boxes 
In this section, we can see the substitution box from Camellia called SB C and one from Hierocrypt called SB H. 
const byte SB C[256] = { 70x , 2cx
, b3x , c0x, e4x , 57x, eax, aex, 23x, 6bx , 45x , a5x, edx, 4fx, 1dx, 92x, 86x , afx, 7cx , 1fx , 3ex, dcx, 5ex , 0bx, a6x, 39x , d5x, 5dx, d9x, 5ax, 51x , 6cx, 
8bx , 9ax, f bx , b0x , 74x, 2bx, f 0x , 84x , dfx, cbx, 34x , 76x, 6dx, a9x, d1x, 04x, 14x , 3ax, dex, 11x , 32x , 9cx, 53x , f 2x , f ex, cfx , c3x, 7ax , 24x , e8x, 60x , 69x , aax, a0x, a1x , 62x, 54x , 1ex , e0x, 64x , 10x , 00x, a3x, 75x , 8ax, e6x, 09x , ddx, 87x , 83x, cdx, 90x , 73x , f 6x, 9dx, bfx, 52x, d8x, c8x, c6x, 81x , 6fx , 13x , 63x, e9x, a7x, 9fx , bcx, 29x, f 9x , 2fx , b4x, 78x, 06x , e7x , 71x, d4x, abx, 88x , 8dx, 72x , b9x, f 8x , acx, 36x, 2ax , 3cx, f 1x , 40x , d3x
, bbx , 43x, 15x, adx, 77x , 80x , 82x , ecx, 27x , e5x, 85x , 35x , 0cx, 41x , efx, 93x, 19x , 21x , 0ex, 4ex , 65x , bdx, b8x , 8fx, ebx, cex, 30x, 5fx , c5x, 1ax, e1x , cax, 47x , 3dx, 01x , d6x, 56x, 4dx, 0dx, 66x , ccx, 2dx, 12x , 20x, b1x, 99x , 4cx, c2x, 7ex, 05x, b7x , 31x , 17x, d7x, 58x , 61x, 1bx , 1cx, 0fx, 16x, 18x , 22x , 44x, b2x , b5x , 91x, 08x, a8x , f cx, 50x, d0x, 7dx, 89x , 97x , 5bx, 95x, f fx , d2x, c4x , 48x, f 7x, dbx, 03x, dax, 3fx , 94x , 5cx, 02x , 4ax , 33x, 67x, f 3x , 7fx , e2x, 9bx , 26x , 37x, 3bx, 96x , 4bx , bex, 2ex, 79x , 8cx, 6ex , 8ex, f 5x , b6x , f dx, 59x , 98x, 6ax, 46x , bax, 25x, 42x , a2x, f ax, 07x , 55x, eex , 0ax, 49x, 68x , 38x, a4x, 28x , 7bx , c9x, c1x, e3x, f 4x , c7x, 9ex , 
}; 
So, SB C
 Appendix C: The Speed Comparison of BC2 with Other Ciphers 
In this section, we give speed comparison of BC2 with other block ciphers at personal computer. We use C ANSI with Borland C++ v6.0 compiler, 1200 Mhz AMD Duron processor, 512 MB RAM, and Windows XP sp2 to compare them. The key schedule of BC2 is one of the fastest of all other ciphers. Yusuf Kurniawan received the B.S. degree and the master degree in electrical engineering from Institut Teknologi Bandung (ITB), Bandung, Indonesia, in 1994 and 1997, respectively. He is currently the Doctoral Student of School of Electrical Engineering and Informatics at the ITB. His research interests focus on the design of , 0, 0, 0, 0, 0, 0, 0 1 2 b 1 , 0, 0, 0, 0, 0, 0, 0 0,0,0,0,0,0,0,0 0 3 0,0,0,0,0,0,0,0 b 1 , 0, 0, 0, 0, 0, 0, 0 1 4 b 1 , 0, 0, 0, 0, 0, 0, 0 
c 1 , c 2 , c 3 , c 4 , c 5 , c 6 , c 7 , c 8 8 5 c 1 , c 2 , c 3 , c 4 , c 5 , c 6 , c 7 , c 8 0
, 0, 0, 0, 0, 0, 0, 0 0 6 0,0,0,0,0,0,0,0 
c 1 , c 2 , c 3 , c 4 , c 5 , c 6 , c 7 , c 8 8 7 c 1 , c 2 , c 3 , c 4 , c 5 , c 6 , c 7 , c 8 e 1 , 0
, 0, 0, 0, 0, 0, 0 1 8 e 1 , 0, 0, 0, 0, 0, 0, 0 0,0,0,0,0,0,0,0 0 9 0,0,0,0,0,0,0,0 e 1 , 0, 0, 0, 0, 0, 0, 0 1 10 e 1 , 0, 0, 0, 0, 0, 0, 0 
d 1 , d 2 , d 3 , d 4 , d 5 , d 6 , d 7 , d 8 8 11 d 1 , d 2 , d 3 , d 4 , d 5 , d 6 , d 7 , d 8 0
, 0, 0, 0, 0, 0, 0, 0 0 12 0,0,0,0,0,0,0,0 
d 1 , d 2 , d 3 , d 4 , d 5 , d 6 , d 7 , d 8 8 13 d 1 , d 2 , d 3 , d 4 , d 5 , d 6 , d 7 , d 8 f 1 , 0
, 0, 0, 0, 0, 0, 0 1 14 f 1 , 0, 0, 0, 0, 0, 0, 0 0,0,0,0,0,0,0,0 0 15 0,0,0,0,0,0,0,0 
f 1 , 0, 0, 0, 0, 0, 0, 0 1 16 f 1 , 0, 0, 0, 0, 0, 0, 0 g 1 , g 2 , g 3 , g 4 , g 5 , g 6 , g 7 , 
"
"Introduction
Global mobility networks (GLOMONET) provide the global roaming service that permits mobile users to use the services provided by the home agent in a foreign agent. However, with the rapid development of such environment , many security problems are brought into attention due to the dynamic nature and vulnerable-to-attack struc- ture. As user privacy becomes a notable security issue in GLOMONET, it is desirable to protect the privacy of mobile users in remote user authentication process 
Review of Chuang et al.'s Scheme
 Chuang et al.'s anonymous authentication scheme comprises three phases, namely registration phase, mutual authentication and key agreement phase and password changing phase. Each foreign agent F shares a secret key K F H with the home agent H. The abbreviations and notations used in their scheme are listed in 
Registration Phase
Step 1. M freely chooses ID M and P W M , then sends them to H through a secure channel. pre-shared secret key between F and H P ub H The home agent H's public key P ri H The matching private key of P ub H hold by H x The secret key of H h(·) A one-way hash function ⊕ Exclusive-OR operation String concatenation operation E k An encryption function with key the k D k A decryption function with key the k Adv The adversary 
Step 2. H computes R = h(ID M x) ⊕ P W M and k = h(P W M ). After that, H stores {ID M , R, k, P ub H , h(·), E(·)} 
into the smart card and submits it to M . 
Mutual Authentication and Key Agreement Phase
 Step 1. M inserts the smart card into the device and inputs P W * M . Then the smart card calculates k * = h(P W * M ) and checks whether k * ? = k. If yes, it means M is the cardholder; otherwise, the smart card terminates the procedure. Afterwards, the smart card randomly selects n M , r M ∈ Z * q , and computes 
AID M = E P ub H (r M ID M ). Finally, M sends m 1 = {ID H , AID M , n M } to F . 
Step 2. On receiving m 1 , F generates n F , r F ∈ Z * q and computes V 1 = E P ub H (r F ID H ID F AID M n M n F ). Subsequently, F sends m 2 = {ID H , ID F , V 1 } to H. 
Step 3. Upon receiving m 2 , H can obtain (r F ID H ID F AID M n M n F ) by decrypting V 1 using the private key P ri H . Subsequently, H can get (r M ID M ) from decrypting AID M . Then, H verifies the validity of M . If M successfully passes the verification, H generates a random integer 
n H ∈ Z * q and calculates C = h(ID M x) ⊕ r M , y = h(Cr M ) ⊕ n H ⊕ h(K F H r F ), z = h(Cn M r M ) ⊕ n H , V 2 = h(K F H n M n F r F yz), V 3 = h(Cz)
, and then sends m 3 = {V 2 , V 3 , y, z} to F . Step 5. After receiving m 4 , M calculates C = R ⊕ P W * M ⊕ r M and checks whether h(Cz) is equal to the received V 3 . If it is true, then M establishes trust with F . Otherwise, this session will be terminated. 
After the mutual authentication phase successfully, M and F share the session key SK = h(T Kn M n F ) = h(h(Cr M ) ⊕ z ⊕ h(Cn M r M )n M n F ). 
Password Changing Phase
When M wants to update P W M , M inserts his/her smart card into a terminal and inputs the origin password P W M to the smart card. 
Step 1. The smart card computes h(P W M ) and checks whether it is equal to the stored k. If yes, M inputs a new password P W new M ; otherwise, the smart card rejects the password change request and terminates this procedure. 
Step 3. The smart card computes  Generally, due to the inherent limitation of human cognition , the identity is easy-to-remember and hence the identity space is very limited, and it follows that the above attack can be completed quite effectively. The running time of the above attack procedure is O(|D P W | * T h ), where T h is the running time for Hash operation. And hence, their scheme cannot resist off-line dictionary at- tack. 
k new = h(P W new M ), R new = R ⊕ P W M ⊕ P W new M and replaces k, R by k new , R new , respectively. Step 2. Computes k * = h(P W * 
Failure to Provide Users' Anonymity
In Chuang et al.'s scheme, the home agent H stored ID M in M 's smart card. Hence, Adv can get ID M by monitoring the power assumption 
User Impersonation Attack
As explained above, if Adv successfully obtains M s smart card, he/she can get P W M and ID M corresponding to M . Then, Adv can impersonate M to make fool of both F and H as follows. In the mutual authentication and key agreement phase, Adv generates two random number 
n M , r M ∈ Z * q and computes AID M = E P ub H (r M ID M ). 
Then he/she sends the forged login request message 
m 1 = {ID H , AID M , n M } to F . 
 It is easy to see the forged login request is in the correct format. Upon receiving m 1 , H and F will execute the protocol normally and Adv will pass the verification successfully. 
Our Proposed Scheme
In this section, we propose a new authentication scheme with user anonymity in global mobility networks using elliptic curve cryptography. Our scheme consists of four phases, which are the registration phase, mutual authentication and key agreement phase, password changing phase and revocation phase. Before the system begins, H generates two distinct large primes p and q with p = 2q + 1 and chooses a generator P of order q on the elliptic curve E p (a, b). H computes the public key Q = x · P mod p with the master secret key x of H. Subsequently, H computes a secret key K F H = h(ID F x) for each foreign agent F . 
Registration Phase
 Step 1. M freely chooses his/her identity ID M and password P W M . Then, M sends them to H via a secure communication channel. 
Step 2. H calculates A = h(ID M x), B = h(ID M v) and C = h(ID M × P + P W M × P ), where v is a secret random number chosen by H for every mobile user. 
Step 3. After that, H maintains a registration table in the format (B, A). H can retrieve A from the registration table by B in the revocation phase and in the mutual authentication and key agreement phase. 
Step 4. H personalizes the smart card with {C, P , Q, E p (a, b), q, p, h(·)} and issues it to M . 
Mutual Authentication and Key Agreement Phase
Step 1. M inserts his/her smart card into a card reader and enters ID M , P W M . Then, the smart card 
ifies C ? = h(ID M × P + P W M × P )
 , if not, the login phase is terminated immediately; otherwise, the smart card generates a random number 
α ∈ [1, q − 1] and computes X = α × P , X 1 = α × Q, D = ID M ⊕ h(X + X 1 )
. Finally, the smart card sends 
m 1 = {ID H , X, D} to F . 
 Step 2. Upon receiving m 1 , F generates a random integer number β ∈ [1, q − 1] and calculates Y = β × P , 
Y 1 = β × Q, E = K F H × P + Y 1 . Then, F transmits the message m 2 = {ID H , ID F , X, D, Y, E} to H. 
Step 3. On receiving m 2 , H computes X 1 = x × X, ID M = D ⊕ h(X + X 1 ), B = h(ID M v), and A * = h(ID M x
). Then, H retrieves A from the registration table by B and checks A * ? = A. If B does not exist in the verifier table or A * = A, H terminates this session. If three continuous requests from M fail in a short interval, H will ignore M 's following request within a guard interval. If the all conditions hold, H verifies the legitimacy of M successfully . Afterwards, H computes 
Y 1 = x × Y , E * = h(ID F x) × P + Y 1 and verifies E * with 
the received E. If they are equal, the authenticity of F is ensured; otherwise, H rejects this request. After the verification of M and F , H computes 
I = ID M ×P +X 1 , J = h(h(ID F x)×P −Y 1 )⊕h(X 1 ) and K = h(ID F x) × Y 1 . 
 Finally, H sends the message m 3 = {I, J, K} to F . 
Step 4. After receiving m 3 from H, F firstly calculates K F H × Y 1 and verifies it with the received K. If it is valid, 
F computes h(X 1 ) = J ⊕ h(K F H × P − Y 1 ), T K = h(X 1 ) × β × X and sk = h(h(X 1 ) × P − β × X) 
and transmits the message m 4 = {I, Y, T K} to M . 
Step 5. Upon receiving m 4 , M computes I * = ID M × P + X 1 and T K * = h(X 1 ) × α × Y . Then M checks I * ? = I and T K * ? = T K, respectively. If the two equations hold, M successfully authenticates H and F , then sends m 5 = h(h(h(X 1 ) × P − α × Y )0) to F . If any of these two equations is false, the authentication fails. 
Step 6. After receiving m 5 , F computes m * 5 = h(sk0) and checks m * 5 ? = m 5 . If they are equal, F ensures the legitimate of M ; otherwise, terminates the session. After mutual authentication, M and F compute and share the session key SK = h(sk1) = h(h(h(X 1 ) × P − α × β × P )1) for future secure communication. 
Password Changing Phase
 When the user M wants to update his/her password offline , he/she inserts the smart card into a terminal and enters his/her identity ID M and old password P W M to the smart card. 
Revocation Phase
In case of lost or stolen smart cards, M could request H to revoke his/her smart card. In our scheme, M should transmit ID M to H via a secure communication channel, then H computes h(ID M v), and checks whether it exists in the registration table. If yes, H retrieves A from the registration table by B and checks whether h(ID M x) is equal to A. Supposing that these two conditions hold, H removes the entry (B, A) from the registration table. If M wants to re-register in H, he/she just needs to reregister in H through performing the registration phase again. 
Secure Analysis of Our Scheme
In this section, we analyze the security of the proposed scheme and show that it can resist different types of attacks and provides user anonymity and untraceability. We assume that the following problems are difficult to solve in polynomial time, in other words, there are no efficient polynomial-time algorithm to solve the following problems. 1) ECDLP 
2) CDHP 
D = ID M ⊕ h(X + X 1 ). If 
the adversary wants to retrieve the mobile user M 's identity from D, he/she should compute X 1 = α×x×P from α×P and Q = x×P to obtain the value h(X + X 1 ), then he/she will face with the CDHP. Furthermore, {X, D} varies in each session because they are generated by the random number α. It is difficult for the adversary to tell apart M from others in the communication channel. Hence, the proposed scheme satisfies user anonymity and untraceability. 
 Theorem 4. Our scheme could withstand user impersonation attack. Proof. In our scheme, if the adversary wants to forgery the legal mobile user M , he/she has to generate a valid message 
m 1 = {ID H , X, D}, where D = ID M ⊕ h(X + X 1 ), X = α × P . 
However, Adv cannot generate a valid D without the knowledge of ID M . Therefore, our scheme could withstand user impersonation attack. 
 Theorem 5. Our scheme could withstand server masquerading attack. Proof. In our scheme, if Adv wants to impersonate H or F to fool the mobile user M , he/she has to generate a valid reply message m 4 = {I, Y, T K}, where I = ID M ×P +X 1 , T K = h(X 1 )×β ×X and Y = β ×P . However, the adversary cannot generate I and T K without the knowledge of x. So, our scheme could withstand server masquerading attack. 
M F H { } 1 , , H m ID X D = { } 2 1 , , , F m m ID Y E = { } 4 , , m I Y TK = { } 3 , , m I J K = 5 ( || 0) m h sk = 
Theorem 6. Our scheme could withstand stolen smart card attack. Proof. If M 's smart card is lost and obtained by Adv, he/she can extract the stored data {C, 
P , Q, E p (a, b), q, p, h(·)} 
in the smart card through the differential power analysis 
m 1 = {ID H , X, D}, m 2 = {ID H , ID F , X, D, Y , E}, m 3 = {I, J, K}, m 4 = 
 {I, Y, T K}. If Adv wants to obtain ID M from these messages, he/she has to compute X 1 = α × x × P from α × P and Q = x × P . Adv will face with the CDHP. We should consider the off-line password guessing attack in this case, that is, the adversary uses a brute force search to find out the correct password from the value C. In our proposed anonymous authentication scheme, the user identity is protected against outsiders what can help to withstand the password guessing attack. Since there can be a huge number of users in the mobile system, it is infeasible for an adversary to do an exhaustive search for all the possible (ID, P assword) pairs. Therefore, our scheme can withstand stolen smart card attack. Proof. In our scheme, Adv may intercept the message m 1 = {ID H , X, D} and replay it to the foreign agent F . However, the adversary cannot generate valid m 5 without knowing x and α. Then, F can find the attack by checking the valid of m 5 . Moreover, Adv also cannot generate valid SK without knowing the value β to construct a communication channel with F . Therefore, our scheme could withstand replay attack. of the mutual authentication phase. From 
Conclusions
In this paper, we analyze several security flaws in Chuang et al.'s authentication scheme with user anonymity for roaming service in global mobility networks. Further, we propose a new authentication scheme for roaming service in GLOMONET to overcome these shortcomings. In addition , the security analysis and performance comparisons demonstrate our proposal is more secure and suitable to the practical application environment. 
Dianli Guo 
"
"Introduction
Signature schemes are a central cryptographic primitive. Besides being an important stand-alone application, they also constitute a building block in many cryptographic protocols. One of important applications of signatures is anonymous credential. The notion of anonymous credential was introduce by Chaum 
 In the next section, we define the definitions and requirements for signature on commitment values. The Section 3 contains some preliminaries required throughout the paper . In Section 4, we present a variant of Boneh-Boyen short signature scheme without random oracle and give its security analysis. In Section 5 we propose a signature on a committed message. In Section 6, we present a basic anonymous credential system based the proposed signature scheme. Section 7 concludes this paper. 
Definitions and Requirements
Our signature scheme consists of a committer, a signer, and a verifier. The committer commits to a value and the signer then signs the committed value. Any one can verify the correctness of the signature. The committer can prove to the verifier that he knows the committed value embedded in the signature. Our anonymous multi-show credential scheme is based the proposed signature scheme and consists of an organization , a group of users, and a service provider. The organization acts as the signer who issues credentials to users for some service provided by the service provider. 
 Definition 2. The proposed anonymous multi-show credential scheme is a 5-tuple of polynomial-time algorithms 
(KeyGen, CIssue, CVerify, CProve, CPVerify). @BULLET KeyGen(1 ). We define the security notion for our basic signature scheme only. It is easy to extend it to the anonymous multi-show credential scheme. Completeness property for the signature on commitment values is defined as follows. 
@BULLET CIssue: The user uses Commit and the signer uses 
Pr       (SK, VK, param0) ← KeyGen(1 ) ∧ (c, s) ← Sign(c, SK) ∧ true ← Verify(c, s) ∧ (PK, Proof) ← Prove(c, s) ∧ true ← PVerify(PK, Proof)       = 1. 
We require our schemes to meet the requirement of existentially unforgeable against the chosen message attacks . We split it into to properties: Security of signature of commitment and security of proving knowledge of committed message in a signature. Assume there exists a TTP adversary A who launches a chosen message attack against our signature scheme and at most asks n queries to the signing oracle. 
Pr true ← Verify(c , s ) ∧ (c , s ) ← A(c i , VK, param0, i = 1, · · · , n) = . 
Here, is negligible. For security of proving knowledge of committed message in a signature, we also require statistical zero knowledge ; that is, it is negligible for an adversary A to obtain any information on m. 
Pr A knows m|true ← Verify(PK(m), Proof) = . 
Preliminaries
Bilinear Pairings
In recent years, the bilinear pairings have been widely applied to cryptography and enable us to construct some new cryptographic primitives. We briefly review the necessary facts about bilinear pairings using the same notation as 
g 2 ) = g 1 . Definition 3. A map e : G 1 × G 2 → G T (here G T 
, then e(g 1 , g 2 ) generates G T . 
3) Computability: There is an efficient algorithm to compute e(u, v) for all u ∈ G 1 and v ∈ G 2 . 
We say that (G 1 , G 2 ) are bilinear groups if there exists a group G T , a computable isomorphism ψ : 
G 2 → G 1 , and a bilinear pairing e : G 1 × G 2 → G T as above. 
In this paper, we assume that G 1 = G 2 . In this case, the co-Decision Diffie-Hellman problem (co-DDH) in (G 1 , G 2 ) is easy, but we can still assume that the Decision Diffie-Hellman problem (DDH) in G 1 is hard. The following Strong Diffie-Hellman assumption is suggested by 
 Definition 4. (q-SDH problem) The q-Strong Diffie- Hellman problem in (G 1 , G 2 ) is defined as follows
: given a (q + 2)-tuple (g 1 , g 2 , g 2 γ , · · · , g 2 γ q ) as input, outputs a pair (g 1 1/γ+x , x) where x ∈ Z * p . 
An algorithm A has advantage in solving q-SDH in 
(G 1 , G 2 ) if P r[A(g 1 , g 2 , g 2 γ , · · · , g 2 γ q ) = (g 1 1/γ+x , x)] ≥ , 
 where the probability is over the random choice of generator in g 2 ∈ G 2 , of γ ∈ Z * p , and of the random bits of A. We say that the (q, t, )-SDH assumption holds in 
(G 1 , G 2 ) 
if no t-time algorithm has advantage at least in solving the q-SDH problem in (G 1 , G 2 ). 
 3.2 Proofs of Knowledge of Discrete Log- arithms 
We will use the notation introduced by Camenisch and Stadler 
P K{(α, β, γ) : y = g α h β ∧ z = g α h γ ∧ (a ≤ α ≤ b
Pedersen Commitment Scheme
Recall the Pedersen commitment scheme 
A Variant of BB0Signature Scheme
We describe the new signature scheme as follows. Let e : G 1 × G 2 → G T be the bilinear pairing where |G 1 | = |G 2 | = |G T | = p for some prime p. We assume that |p| ≥ 160. As for the message space, if the signature scheme is intended to be used directly for signing messages, then |m| = 160 is good enough, because, given a suitable collision resistant hash function, such as SHA-1, one can first hash a message to 160 bits, and then sign the resulting value. So the messages m to be signed can be regarded as an element in Z p . We also need a very efficient and suitable conversion function from 
G 1 to Z * p : [·] : G 1 → Z * p . The system parameter is (G 1 , G 2 , G T , e, p, g 1 , h, g 2 , [·]), here g 1 , h ∈ G 1 , g 2 ∈ G 2 are random generators. Key Generation. Randomly select x, y ∈ R Z * p , and compute u = g x 2 , v = g y 2 . 
The public key is u, v. The secret key is x, y. Signing: Given the secret key x, y ∈ R Z * p , and a message m ∈ Z p , compute the signature 
σ = (g m 1 ) 1 x+[g m 1 ]+yr ∈ G 1 . 
Here r is randomly selected from Z * p . The signature is (r, σ). 
Verification: Verify that e(σ, ug [g m 1 ] 2 v r ) = e(g m 1 , g 2 ). 
We now give the security theorems and proofs for the above instantiation. 
 Lemma 1. If there exists a (t, q S , )-forger F using 
a (t, q S , )-forger F for BB04 scheme. 
Proof. Recall that BB04 signature scheme is described as follows. The system parameter is same as the above scheme. 
Key Generation. Randomly select x, y ∈ R Z * p , and compute u = g x 2 , v = g y 2 . 
The public key is u, v. The secret key is x, y. Signing: Given the secret key x, y ∈ R Z * p , and a message m ∈ Z p , compute the signature 
σ = g 1 x+m+yr 1 ∈ G 1 . 
The signature is (r, σ). 
Verification: Verify that e(σ, ug m
Suppose that there exists a (t, q S , )-forger F using adaptive chosen message attack for the proposed signature scheme, i.e., after at most q S signatures queries and t processing time, F outputs a valid signature forgery (r, σ) on message m with probability at least , here e(σ, ug 
[g m 1 ] 2 v r ) = e(g m 1 , g 2 ). Let m = [g m 1 ], σ = σ m −1 
, then we have a forgery on BBS04 scheme. This is because of 
e(σ , ug m 2 v r ) = e(σ m −1 , ug [g m 1 ] 2 v r ) = e(g 1 , g 2 ). 
Theorem 1 (
q S < q, ≥ 2( + q S p ) ≈ 2 , t ≤ t − Θ(q 2 T ), 
where T is the maximum time for an exponentiation in 
(G 1 , G 2 ). 
So, we have the following theorem: 
Theorem 2. The proposed signature scheme is secure against existential forgery under an adaptive chosen message attack if the (q, t , )-SDH assumption holds in 
(G 1 , G 2 ). 
Obtaining a Signature on a Committed Value
 Following Camenisch and Lysyanskaya, in order to construct an anonymous credential system, it is sufficient to exhibit a signature on a committed value. We provide a new signature on a committed value based on the variant of BB04 signature scheme in this section. The system parameter is 
(G 1 , G 2 , G T , e, p, g 1 , h, g 2 , [·]), here g 1 , h ∈ G 1 , g 2 ∈ G 2 are random generators. KeyGen. Randomly select x, y ∈ R Z * p , and compute u = g x 2 , v = g y 2 . 
The public key is u, v. The secret key is x, y. 
Commit: Compute c = g m 1 h a . 
Sign: Given the secret key x, y ∈ R Z * p , and a commitment c ∈ G 1 , compute the signature as follows: 
Randomly select r ∈ R Z * p , compute σ = c 1 x+[c]+ry ∈ G 1 . 
The signature one 
c = g m 1 h a is (r, σ). 
Verify: Verify that e(σ, ug [c] 2 v r ) = e(c, g 2 ). 
Prove: The following protocol is a zero-knowledge proof of knowledge of a signed message for above signature scheme. 
Common input. The system parameter is (G 1 , 
G 2 , G T , e, p, g 1 , h, g 2 , 
Randomly select r 1 , r 2 ∈ R Z * p , and compute c = (ug [c] 2 v r ) r 1 = u r 1 g r 1 [c] 2 v rr 1 , σ = σ r 2 . 
Send (c , σ ) to the verifier. 2) PVerify. The prover and verifier compute the following values: 
A = e(σ , c ), B = e(g 1 , g 2 ), C = e(h, g 2 ) 
and then carry out the following zero-knowledge proof protocols: 
ZKP {(α, β, λ 1 , λ 2 , λ 3 )|A = B α C β ∧ c = u λ1 g λ 2 2 v λ3 ∧ λ 1 = 0}. 
Here α = mr 1 r 2 , β = ar 1 r 2 , λ 1 = r 1 , λ 2 = r 1 [c], λ 3 = rr 1 . 
blind the credential by using two randomly generate numbers r 1 , r 2 . The completeness of the proposed signature scheme on a committed value is obvious. Due to the using of two randomly generate numbers r 1 , r 2 , the protocol can provide the anonymity. The protocol above uses zero-knowledge proof, so, it is a zero-knowledge proof of a signature on a value. 
 6 A Multi-show Anonymous Credential Scheme 
 Based on the proposed signature scheme, we can now construct the multi-show anonymous credential scheme. We will follow the notations given previously in this paper. The system parameter is same as above signature scheme. @BULLET KeyGen(1 ): Generate public (u, v) and private signing key (x, y). @BULLET CIssue: The user commits to (m, a) by computing 
c = g m 1 h a . 
and the signer computes the signature on 
c: (r, σ = c 1 x+[c]+ry ). 
@BULLET CVerify. The user checks e(σ, ug 
[c] 2 v r ) ? = e(c, g 2 ). 
@BULLET CProve. Using Prove, the user proves to the service provider about his knowledge on (m, a) and (r, σ) on c and outputs (PK, Proof). Here, the (Proof) is the zero-knowledge proof: 
ZKP {(α, β, λ 1 , λ 2 , λ 3 )|A = B α C β ∧ c = u λ1 g λ 2 2 v λ3 ∧ λ 1 = 0}. 
@BULLET CVerify. The service provider checks the correctness of (PK, Proof) using PVerify. 
Our credential scheme is of multi-show, i.e., the user can blind the credential by using two randomly generate numbers r 1 , r 2 . The credential itself is never sent to the service provider in clear. Clearly, our scheme also supports non-transferability. To show a credential to the service provider, the user has to know his secret (m, a). Of course, we have to assume that his secret should not be given to others. However, it is also not hard for us to modify the scheme such that there exists a revocation manager who can revoke the identity of the user if needed. 
Conclusion
In this paper, we propose a variant of Boneh-Boyen short signature scheme without random oracle such that it can be used as a building block for cryptographic protocols. We provide a protocol to prove knowledge of a signature on a committed message and to obtain a signature on a committed message such that it can be converted into an efficient multi-show credential scheme. The proposed signature scheme on a committed value in this paper has many good properties, and for the further work, we expect to design a group signature scheme based on this signature scheme. research interests include elliptic curve cryptography, pairing-based cryptosystem and its applications. of Wollongong, Australia. He is currently an Associate Professor at the School of Information Technology and Computer Science of the University of Wollongong . He is the coordinator of Network Security Research Laboratory at the University of Wollongong. His research interests include cryptography, information security , computer security and network security. His main contribution is in the area of digital signature schemes, in particular fail-stop signature schemes and short signature schemes. He has served as a program committee member in a number of international conferences. He has published numerous publications in the area of digital signature schemes and encryption schemes. 
Xiaofeng 
"
"Introduction
In 1984, Shamir 
In 2010, Boyen 
In this paper, we propose a short lattice-based HIBS scheme without random oracles, which is obtained from 
Preliminaries
Notations
 The security parameter in this work is n. For a positive integer k, 
A = [a 1 , · · · , a m ] ∈ Z 
n×m , let A denote the Gram-Schmidt orthogonalization of A and let ||A|| = max i∈
Lattices
 In this work, we focus on integer lattices, which are contained in Z m . 
Λ = L(B) = {Bc : c ∈ Z m }. 
Definition 2.2. For a positive integer q, a vector y ∈ Z n q 
and a matrix A ∈ Z n×m , define two m-dimensional spaces 
Λ ⊥ (A) = {e ∈ Z m : Ae = 0 (mod q)}, Λ y (A) = {e ∈ Z m : Ae = y (mod q)}. 
 Gaussians on lattices. Here we briefly review the Gaussian function which is a useful tool in lattice-based cryptography . For any σ > 0, the Gaussian function on R m centered at c with parameter σ is defined as 
ρ σ,c (x) = exp(−π||x − c|| 2 /σ 2 ). 
The discrete Gaussian distribution over Λ with center c and parameter σ is 
∀x ∈ Λ, D Λ,σ,c = ρ σ,c (x)/ρ σ,c (Λ). 
Micciancio and Regev 
Basis Delegation
Let A ∈ Z n×m q be a random matrix, the one-way function f A , introduced by Gentry et al. 
f A (x) = Ax (mod q), with domain D n = {e ∈ Z m : ||e|| ≤ σ √ m} and range R n = Z n q . Namely, sampling from f −1 A (y) 
for any y ∈ R n is hard without a trapdoor. A trapdoor of f A is a short basis T A of Λ ⊥ (A). Some relevant facts about these functions are listed below. 
Proposition 2.1. Let q ≥ 2 and m > 
Λ ⊥ (A) and σ ≥ T A · ω( √ log m). 1) For any e ∼ D Z m ,σ
,0 , the distribution of the syndrome u = Ae (mod q) is statistically close to uniform over 
Z n q . 2) For any y ∈ Z n 
q , there is a PPT algorithm 
SamplePre(A, T A , σ, y) that outputs a vector e ∈ Λ y (A) satisfying ||e|| ≤ σ √ m with all but negl(n) 
probability. In addition, the set {x ∈ Z m : ||x|| ≤ σ √ m ∧ Ax = y} contains at least 2 ω(log n) elements. At CRYPTO 2010, Agrawal et al. 
Definition 2.4. Let q be a prime, m ≥ 6n log q and σ 
i = [a i1 , · · · , a im ] ∈ Z m×m q }, where a ij ∼ D Z m ,σ,0 for all j ∈ [m]. Proposition 2.2. Let q > 2, A ∈ Z n×m q and R ∈ Z 
HIBS Scheme and Its Security Model
HIBS Scheme
 A HIBS scheme consists of four algorithms: Setup, Extract , Sign and Verify. They are specified as follows: 
Setup. On input the security parameter n, the root PKG generates system parameters P P and a master secret key M SK. 
Extract. On input an identity ID and the master secret key M SK or parent's private key, this algorithm outputs a secret key SK ID for ID. 
Sign. Given a private key SK ID 
and a message M , the algorithm signs the massage M for ID and outputs the signature v = Sign(M, SK ID ). Verify. Given a signature v, a massage M and an identity ID, it outputs 1 if the signature is valid. Otherwise, it outputs 0. 
These algorithms must satisfy the standard consistency constraint, namely, for any massage-identity pair (M, ID) 
if v = Sign(M, SK ID ), then V erif y(v, M, ID) 
outputs 1 with overwhelming probability. 
Security Model
There are two security models for HIBS, i.e., the adaptive identity security model and the selective identity security model. The adaptive identify security model allows an adversary to adaptively issue queries on arbitrary identity . The selective identity security model demands that an adversary must announce its target identity before seeing the public key. Our HIBS scheme is strongly unforgeable under selective identity attack (SU-sIDA) which is formally defined in the following SU-sIDA game played between an adversary A and a challenger C. 
Init. On input the maximum depth of the hierarchy l+1, the adversary A outputs a target identity 
ID * = (ID 0 , ID * 1 , · · · , ID * k ), where k ≤ l. 
 Setup. The challenger C runs Setup and sends the system parameters PP to the adversary A. 
Proposed Lattice-based HIBS Scheme
 Assume that the maximum depth of the hierarchy, including the root PKG, is l + 1, where l ≥ 1. Let q ≥ 2 be a prime and m ≥ 6n log q. Choose two cryptographic hash functions H :  Setup: Given the security parameter n and the maximum depth l + 1, run TrapGen(1 n ) to generate a matrix A ∈ Z n×m q and a corresponding short 
sis T A ∈ Z m×m q . Select a random nonzero vector y ∈ Z n q , 2lλ 1 random matrices R 0 i,j , R 1 i,j ∈ D m×m (for 1 ≤ i ≤ l, 1 ≤ j ≤ λ 1 ) and λ 2 random matrices C i ∈ Z n×m q . Publish the system parameter P P = {A, R 0 i,j , R 1 i,j , C i , 
y} and keep the master secret key T A secret. Extract: On input a private key SK ID|d for the identity ID|d and an identity ID 
= (ID 0 , · · · , ID d , · · · , ID k )
, do the following steps: 
1) Set µ i = H(ID|i) for all i ∈ [k]. 2) Compute R µi = R µi[λ1] i,λ1 · · · R µi[1] i,1 ∈ Z m×m q and F ID|d = A(R µ d · · · R µ 1 ) −1 . 
Λ ⊥ (F ID|d ). 3) Let R = R µ k · · · R µ d+1 and F ID|k = F ID|d R −1 ∈ Z n×m q . 4) Run BasisDel(F ID|d , R, SK ID|d , σ d ) 
 to generate a private key SK ID|k for ID, where SK ID|k is a random basis for Λ ⊥ (F ID|k ). 
Sign: On input the secret key SK ID|k of the user ID|k and a message M ∈ {0, 1} * , do as follows: 
1) Select a random string r ∈ {0, 1} n and compute ν = h(M, r, ID|k). 
2) Set C = (−1) ν[1] C 1 +· · ·+(−1) ν[λ2] C λ2 ∈ Z n×m q . 3) Pick v 1 ∈ D Z m ,σ k ,0 uniformly at random. By Lemma 1, ||v 1 || ≤ σ k √ m with 1−negl(n) prob- ability. 4) Run v 2 ← SamplePre(F ID|k , SK ID|k , σ k , y − Cv 1 ). 5) Output the signature (v, r), where v = v 1 v2 in Z 2m q . Verify: Given a identity ID|k, a signature (v, r) and a massage M , do: 1) For all i ∈ [k], set µ i = H(ID|i). 2) Compute F ID|k = A(R µ k · · · R µ 1 ) −1 , where R µ i = R µ i [λ 1 ] i,λ 1 · · · R µ i [1] i,1 . 3) Set ν = h(M, r, ID|k) and C = (−1) ν[1] C 1 + · · · + (−1) ν[λ 2 ] C λ 2 . 
4) The verifier accepts the signature if and only if 
(C|F ID|k )v = y and ||v|| ≤ σ k √ 2m. 
Analysis
Correctness
According to the above definitions, we have 
F ID|k = F ID|d R −1 = A(R µ k · · · R µ 1 ) −1 (mod q). 
By Lemma 2.2, we know that F ID|k v 2 = y−Cv 1 (mod q) and the vector v 2 satisfies ||v 2 || ≤ σ k √ m with all but negligible probability in n. Therefore, 
(C|F ID|k )v = Cv 1 + F ID|k v 2 = y (mod q) and ||v|| = v 2 1 + v 2 2 ≤ 2(σ k ) 2 m = σ k √ 2m with overwhelming probability. 
Now we evaluate the Gaussian parameter σ k for each 
k ∈ [l]. Let σ 0 = O( √ m), by Proposition 2.2 we can obtain that the Gaussian parameter σ k ≥ σ 0 m kλ 1 +k/2 · ω(log kλ1+k m) since || SK ID|d−1 || ≤ σ d−1 √ m and σ d ≥ || SK ID|d−1 || · m λ 1 · ω(log λ 1 +1 m) for all d ∈ [l]. 
Comparison
The lattice-based HIBS scheme without random oracles constructed by Rückert is also provably secure in the above security model. However, the private keys and the signatures in his scheme are dependent on the identity length of the signer. In contrast, both the private key size and the signature size in our scheme are unchanged and much shorter. Therefore, our scheme is more practical, though the public key size in this scheme is larger than that of Rückert's scheme. For the signer 
ID|k = (ID 0 , · · · , ID d , · · · , ID k ) of depth k ∈ [l], 
m 1 = ˜ O(ln) and m 2 = ˜ O(λ 1 ln). 
Strong Unforgeability
Proof. 
Suppose that there is a t-time adversary A that succeeds in the SU-sIDA game with probability at least , then we can construct a PPT algorithm C that solves the SIS problem instance with non-negligible probability. Init: The adversary A first outputs a target identity 
ID * = (ID 0 , ID * 1 , · · · , ID * u )
 , where u ≤ l. To simplify the notation, let u = l (the proofs of other cases are similar and therefore omitted). 
Setup: The algorithm C picks a random matrix A 0 ∈ Z n×m q and λ 2 random matrices E 1 , · · · , E λ 2 ∈ Z m×m q , 
where each column of E i is selected independently 
from D Z m ,η,0 . Let C i = A 0 E i (mod q) for all i ∈ [λ 2 ]
. According to Lemma 2.2, we know that C i is statistically close to uniform over Z n×m 
q . C selects lλ 1 random matrices R i,j ∼ D m×m , where i ∈ [l], j ∈ [λ 1 ]. For ∀j ∈ [λ 1 ]
, the rest of the public parameters are chosen as follows: 
1) For each i ∈ [l], compute µ * i = H(ID * |i) and set R µ * i [j] i,j ← R i,j . 2) Define R µ * i = R µ * i [λ1] i,λ 1 · · · R µ * i [1] i,1 and compute A = A 0 (R µ * l · · · R µ * 1 ). 3) Select x 0 ∈ Z m q uniformly at random from D Z m ,η
,0 and let y = A 0 x 0 (mod q). If y = 0 (mod q), repeat this step until y is a non-zero vector. 4) For each i ∈ 
j = A · (R µ * i−1 · · · R µ * 1 ) −1 (R µ * i [j−1] i,j−1 · · · R µ * 1 [1] i,1 ) −1 , where A 1,1 = A. 5) Invoke SampleRwithBasis(A i,j ) to generate a matrix R ∼ D m×m and a short basis T B for Λ ⊥ (B = A i,j R −1 ). Return R 1−µ * i [j] i,j ← R. 6) Preserve the tuple (i, j, R, B, T B ). Finally, C sends the system parameters P P = {A, R 0 i,j , R 1 i,j , C i , y} to A. 
Extract queries: A queries the secret key of the identity 
ID = (ID 0 , · · · , ID w ). If w > l or ID = ID * |w, C answers ⊥. 
Otherwise, do these steps: 
1) For i ∈ [w], define µ i = H(ID|i) and R µ i = R µ i [λ 1 ] i,λ 1 · · · R µ i [1] 
i,1 . 2) Let (k, j) be the first position such that µ k 
= µ * k [j], where k ∈ [w], j ∈ [λ 1 ]. 3) Retrieve the tuple (k, j, R, B, T B ). By construction B = A k,j · (R µ k [j] k,j ) −1 . 4) On input T B , run BasisDel(B, (R µw · · · R µ k+1 )· (R µ k [λ 1 ] k,λ1 · · · R µ k [j+1] 
 k,j+1 ), T B , σ k ) to generate a private key for ID and sends the result to A. 
+ n (m 1 + kλ 1 m 1 ) 2 (1 + kλ 1 + λ 2 )m 1 + n This work (n + 2lλ 1 m 2 + λ 2 n)m 2 + n (m 2 ) 2 2m 2 + n 
Sign queries: On input a message M and an identity ID: 
@BULLET If ID = ID * , then F ID * = A(R µ * l · · · R µ * 1 ) −1 = A 0 . C does 
the following steps: 1) Choose a random string r ∈ {0, 1} n and evaluate ν = h(M, r, ID). 
2) Let E ν = i∈[λ2] (−1) ν[i] E i . 
We then have 
C = i∈[λ2] (−1) ν[i] C i = A 0 E ν . 3) Select a random vector v 1 ∈ D Z m ,η,0 and compute v 2 = x 0 − E ν v 1 (mod q). If E ν v 1 = 0, repeat this step. 4) It outputs v = v1 v 2 and r. 
Now we show that (v, r) is a valid signature. By the above process, we have (
C|F ID * )v = A 0 E ν v 1 + A 0 v 2 = A 0 x 0 = y (mod q). 
Forgery: The adversary A outputs a signature (v * , r * , M * , ID * ) such that the algorithm Verify returns 1. Let ν * = h(M * , r * , ID * ) and E ν * = i∈[λ 2 ] (−1) ν[i] E i . Then F ID * = A 0 and C = A 0 E ν * . 
There are two different cases that need to be considered. 
– Case 1. The message (M * , r * ) has been queried in the Sign queries phase, namely, this is a strong forgery. We have (
C|F ID * )v * = A 0 (E ν * v * 1 + v * 2 ) = y = A 0 (E ν * v 1 + v 2 ) (mod q) and v = v * (by 
the definition of strong unforgeability). Obviously, 
e = E ν * (v * 1 −v 1 )+v * 2 −v 2 satisfies A 0 e = 0 (mod q). 
According to the Lemma 26 in 
* = h(M * , r * , ID * ) is a new vector and (C|F ID * )v * = A 0 (E ν * v * 1 + v * 2 ) = A 0 x 0 (mod q). Let e = E ν * v * 1 + v * 2 − x 0 . 
||e|| ≤ ||E ν * || · ||v * 1 − v 1 || + ||v * 2 − v 2 || ≤ 2σ k √ 2m(λ 2 η √ m + 1) ≤ 2σ l √ 2m(λ 2 η √ m + 1), since ||E ν * || ≤ λ 2 η √ m and σ k ≤ σ l . 
Similarly, for case 2, we have 
||e|| ≤ ||E ν * v * 1 || + ||v * 2 || + ||x 0 || ≤ √ 2λ 2 ησ k m + σ k √ 2m + η √ m ≤ 2σ k √ 2m(λ 2 η √ m + 1) ≤ 2σ l √ 2m(λ 2 η √ m + 1). 
Thus, we can set 
β = 2σ l √ 2m(λ 2 η √ m + 1). 
Here we calculate the advantage of the algorithm C. 
 Suppose that each case will happen with the same probability , therefore, the PPT algorithm C has advantage ≥ /2 · 2/3 + /2−negl(n)= 5/6−negl(n) in solving the SIS problem instance. 
Conclusion
In this paper, we have constructed a new lattice-based HIBS scheme with short secret keys and signatures. We have also proven that this scheme is strongly unforgeable in the standard model. It is more practical than Rückert's HIBS scheme without random oracles. 
"
"Introduction
The key distribution problem is considered one of the most important issues for providing secure communication. Many protocols that address the key distribution problem for unicast or point-to-point communication 
In recent years, many approaches for solving the problem of group key distribution were proposed. These approaches can be classified as follows: 
Centralized approaches use one central entity to maintain the security of the whole group. For large groups, those protocols are not scalable. In addition, the central entity represents a single point of failure. 
Distributed subgroup approaches, 
where the whole group is divided into several subgroups. One subgroup controller maintains each subgroup. These protocols solve the problem of scalability. Another advantage of these protocols is that in case of failure of one subgroup controller, this does not lead to the failure of the whole group. 
Decentralized approaches, 
where the whole group members contribute in the group key generation. As for the centralized approaches, these protocols are not scalable for large groups since it requires large computations among the group members. 
In the present paper, a design of a high performance protocol for securing multicast communication is proposed. The proposed protocol is based on the idea of organizing the keys in a tree as in LKH protocols. In order to achieve lower computation overhead, the proposed protocol uses a multi-processor system. It has to be noted that LKH protocol relies heavily on one central point, therefore, it represents single point of failure and for a large tree; the server's throughput can represent a bottleneck. The use of multiple processors could solve this problem and enhance the server's throughput. The proposed protocol is analyzed according to: the number of processors, the tree height and the tree degree. The analysis shows that the use of multiprocessor system will enhance the system performance. Increasing the number of processors reduces the total execution time until reaching the system's saturation. Therefore, the use of multiprocessor system will significantly reduce the computation overhead which is considered an important factor for both real time and wireless applications. In addition, the proposed design is scalable. This is an important factor in real applications where the number of users changes repeatedly. The paper is organized as follows: in Section 2, a survey of group key distribution protocols is detailed. In Section 3, a background of multiprocessor systems is given. Then, the proposed protocol is detailed in Section 4. In Section 5, a performance evaluation of the proposed protocol is given. Finally, the paper concludes in Section 6. 
Related Work
As a result of the spread use of Internet applications characterized by multicast communication, the need to establish a group key becomes a vital requirement. Group key distribution protocols play an important role to deliver security. They are considered the main part to obtain a secure system. A good key distribution protocol must satisfy the following requirements 
In distributed subgroup approaches, the whole group is divided into several subgroups. One Subgroup Controller (SC) that shares a symmetric key with the Group Controller (GC) maintains each subgroup. The role of SC is to establish a subgroup key to be used within the subgroup and to translate messages sent from GC to the subgroup members. This approach solves the problem of scalability; any member change will only affect the subgroup where this member belongs. Another advantage is that the failure of one SC will not lead to the failure of the whole group. The disadvantage of this approach is the need of decrypting group messages at the SC and re-encrypting using the subgroup key. This solution reduces the communication and computation complexity to O(n/m), where m represents the number of subgroups. Examples of this approach can be found in 
Background of Multiprocessor Systems
A multiprocessor system can be defined as a collection of autonomous processors, which can communicate with each other through a communication medium. The communication between processors can be done either through a shared memory medium or through links which interconnect processes directly with each other (message passing systems). In the shared memory model, processors can access in parallel memory locations which they share with all other processors. The communication between processors is achieved by writing information to common memory locations. In the message passing model (distributed memory system), each processor can read and write information only to a local memory, thus it must exchange information by sending messages via links to other processors. It is assumed that all processors of the message passing system execute the same algorithm and work correctly for any possible interconnection of processors 
A Message Passing (MP) system involves connecting multiple independent nodes each contains a processor and its local memory. There is no sharing of primary memory, but each processor has its own memory. The contents of each memory can only be accessed by its processor. When a processor needs information owned by another processor, the information is sent as a message from one processor to the other. Messages can carry information between nodes, also synchronization node activities. There are no restrictions on the number of available processors. The processors of this type operate independently of one another 
Design of High Performance Implementation of a Tree-Based Multicast Key Distribution Protocol
The proposed protocol is based on the idea of organizing the keys in a tree as in LKH protocols as shown in 
In the next subsections, description of the proposed protocol will be detailed. The focus of this paper will be on the following cases: member join or member leave. 
Member Join
Assume a new member U d joins the group, to preserve backward secrecy; all the keys along its path must be changed as shown in 
{K new (0, 1)}K(0, 1), {K new (1, 1)}K(1, 1), …, {K new (h-1, 1)}K(h-1, 1), {K new (0, 1), K new (1, 1), …, K new (h-1, 1)}K(h, d) 
Therefore, GM needs to perform 2h keys encryptions. The encryption is done in Electronic Code Book (ECB) mode. There are two important aspects of any algorithm: (i) the amount of time required to execute the algorithm and (ii) the amount of memory space needed during run-time. Execution time, which refers to the total running time of the program, is the most obvious way of describing the performance of parallel programs. The aim of using parallel processing is to decrease the execution time of the problem implementation. In parallel systems, the total execution time (parallel time) T par is the sum of the computation time T comp , and the overhead time T ov. The sources of overhead are local communication overhead (access memory, memory contention, and synchronization), 
………….... …………… … K(0,1) K(1,1) K(1,2) K(1,d) K(h-1,1) K(h,1) K(h,d) U1 __________ _ Ud _______________________ Ud h 
processor network latency (message latency) and application overhead 
T par = T comp + T comm 
(1) Where T comp =max{T comp (P i )} 1 - M i 0   (2) T comm =                 t M i M j 1 0 1 0 1 ) j P , i (P mess T   (3) T mess (P i ,P j ) = Sender overhead + [(S mess (P i ,P j )/b g)* hop(P i ,P j )] 
+ Receiver overhead 
(4) 
where,  The sender overhead, receiver overhead, , and  1 are very small with respect to the message latency. Therefore, it will be neglected in our calculations.  Asynchronous communication mechanism  The number of hops between P i and P j hop(P i ,P j ) equals one for all i,j (fully connection).  Time of key generation is very small with respect to the encryption time. So, it will be neglected in our calculations.  Only P 0 and P i exchange messages with each other, i.e. no communication between P i and P j , where i and j range from 1 to M-1. Therefore, the communication time is given as follows: 
T comm =   g b 1 M 1 i i P , 0 P mess S    (5) 
In the following paragraphs, the sequential time T s , the computation time T comp , the communication time T comm , and the total parallel time T par are calculated for different values of M. First, for M =1, only one processor is used, there is no communication time and the number of tasks to be performed is 2h encryptions. The sequential time T s required to calculate these tasks is given by Equation (6). 
Th B * 2h comp T s T   (6) 
For the case of M=2, using Equation (2) and Equation (5), the computation time T comp and the communication time T comm are given by Equation 
(7) and Equation (8). 
Th B * h comp T  (7) g b B * h comm T  (8) 
Therefore, using Equations (1), (7) and (8), T par equals: 
                g b B * h Th B * h par T (9) …………… … Knew(h-1,1) K(h,1) K(h,d) Knew(1,1) K(1,d) 
       M h M h * processors. Therefore, the first        M h M h * 
processors are assigned one more row (2 tasks); thus the number of tasks to be performed is 
2 1 * M h        
tasks. For the case where (h mod M) = 0, using 
Equation (2) and Equation (5), the computation time T comp and the communication time T comm are given by Equation 
(10) and Equation (11). 
Th B * M h * 2 comp T        (10) g b B * M h - h * 2 comm T              (11) 
Therefore, using Equation (1), Equation (10) and Equation (11), T par equals: 
                                            g b B * M h - h * 2 Th B * M h * 2 par T (12) 
For the second case where 
(h mod M)  0, using 
Equation (2) and Equation (5), the computation time T comp , and the communication time T comm are given by Equation 
(13) and Equation (14). 
Th B * 1 M h * 2 comp T         (13) g b B * 1 M h - h * 2 comm T               (14) 
Therefore, using Equation (1), Equation (13) and 
Equation (14), T par equals: 
                                                      g b B * 1 M h - h * 2 Th B * 1 M h * 2 par T (15) 
In the next subsection, description of the proposed protocol for the case of a member leaves will be detailed. 
Member Leave
Assume U d leaves the group, to preserve forward secrecy; all the keys along its path must be changed as shown in 
{K new (0, 1)}K new (1, 1), {K new (0, 1)}K(1, 2), …, {K new (0, 1)}K(1, d), {K new (1, 1)}K new (2, 1), {K new (1, 1)}K(2, 2), …, {K new (1, 1)}K(2, d), . . {K new (h-1, 1)}K(h, 1), {K new (h-1, 1)}K(h, 2), …, {K new (h-1, 1)}K(h, d-1) 
Therefore, GM needs to perform d*h-1 keys encryptions. The encryption is done in Electronic Code Book (ECB) mode. As in the join case, the sequential time T s , the computation time T comp, the communication time T comm and the total parallel time T par are calculated for different values of M. First, for M=1, only one processor is used, there is no communication time and the number of tasks to be performed is d*h-1 encryptions. The sequential time T s required to calculate these tasks is given by Equation (16). 
  Th B * 1 - h) * (d comp T s T   (16) 
For the case of M=2, two cases arise: the first for h even and the second for h odd. For the case where h is even, the master processor P 0 has to calculate the last 
      2 h rows (i.e        1 2 
  g b B * 1 d 2 h                1 comm T (18) 
Therefore, using Equation (1), Equation (17) and Equation (18), T par equals: 
                                                g b 1 d 2 h 1 B * Th B * 2 h * d par T (19) 
For the second case where h is odd, the master processor P 0 has to calculate the last Equation (5), the computation time T comp and the communication time T comm are given by Equation (20) and Equation (21). 
       1 2 h rows (i.e                 1 * 2 d d h 
Th B * 1 2 h h * d comp T                      (20) g b B * 2 h * d 1 2 comm T                      h (21) 
Therefore, using Equation (1), Equation (20) and Equation (21), T par equals: and Equation (5), the computation time T comp and the communication time T comm are given by Equation (23) and Equation (24). 
                                                                                    
      M h * d tasks, while P 0 performs                     1 1 M h * d tasks. Therefore, P 0 sends        1 M 
comp T = Th B * 1 1 M h * d                     (23) …………… … Knew(h-1,1) K(h,1) K(h,d-1) U1 ________________ _ Ud-1 _______________________ Ud h Knew(1,1) K (1,2) K (1,d) …. .   g b B * 1 M h d M h * 1 - M comm T                (24) 
Therefore, using Equation (1), Equation (23) and Equation (24), T par equals: 
                                                                  g b B * 1 M h d M h * 1 - M Th B * 1 1 M h * d par T (25) 
For the second case where ((h-1) mod M)  0, two cases occur, the first for (h mod M) = 0 and the latter for (h mod M)  0. For the case where (h mod M) = 0, each processor from P 1 to P M-1 has to perform 
            M h * d tasks and P 0 has to perform                    1 M h * d tasks. Therefore, P 0 sends              1 M h keys to each processor (from P 1 to P M-1 ). 
After encrypting the keys, each processor (from P 1 to P M-1 ) 
sends             d * M h encrypted keys to P 0 . Using Equation (2) 
and Equation (5), the computation time T comp and the communication time T comm are given by Equation (26) and Equation (27). 
Th B * M h * d comp T        (26)   g b B * 1 M h M dh * 1 - M comm T                (27) 
Therefore, using Equation (1), Equation (26) and Equation (27), T par equals: 
                                                g b B * 1 M h M dh * 1 - M Th B * M h * d par T (28) 
Finally, for the case where (
(h-1) mod M)  0 and (h mod M)  0, P 0 has to perform                     1 1 M h * d tasks, and P j , where                M h * M h j 1 has to perform              1 M h * d tasks
, finally the remaining processors has to perform 
      M h * d . Therefore, P 0 sends        2 M h keys to         1 M h * M h processors and        1 M h keys to               M h * M h M 
processors. After encrypting the keys, the processors from P 1 to P M-1 send 
                                  1 1 M h * d 1 dh encrypted keys to P 0 . 
Using Equation (2) and Equation (5), the computation time T comp and the communication time T comm are given by Equation (29) and Equation (30). 
Th B * 1 M h * d comp T         (29)   g b B * 1 M h h * d h M h 2 M comm T                                (30) 
Therefore, using Equation (1), Equation (29) and Equation (30), T par equals: 
                                                                     g b B * 1 M h h * d h M h 2 M Th B * 1 M h * d par T (31) 
In the next section, evaluation of the proposed protocol and experimental results will be depicted. imitate the behavior of a real system in a controlled manner 
Performance Metrics and Scalability
Many performance metrics have been proposed to quantify the parallel systems. Among of them are execution time, speedup, efficiency, communication overheads, scalability and the degree of improvement 
Discussion of Results
The proposed protocol is evaluated for both join and leave cases; and for different values of d and h. In our proposed design, we assume that the multiprocessor system is based on message passing system and uses the Advanced Encryption Standard (AES) protocol in ECB mode for encryption. In our implementation, we assume that each processor's speed is 2.5GhZ, P mem equals 1Gbits, bg is 4.2Gbps, Th equals 1.4Gbps and B is 128bits. Figures 8-12 show the analysis of the proposed protocol according to the metrics given in Sec. 5.1. While 
        1 M h M , for 
the binary tree, as shown in 
 
The analysis shows that the use of multiprocessor system will enhance the system performance. Increasing the number of processors reduces the total execution time until reaching the system's saturation. In addition, the proposed design is scalable. This is an important factor in real applications where the number of users changes repeatedly. 
Conclusions
In the present paper, a design of a high performance implementation of a tree-based multicast key distribution protocol is proposed. In order to solve the problem of distributing a symmetric key between the whole group members, the group is organized in a logical key hierarchy as in LKH protocols. In order to achieve lower computation overhead, the proposed protocol uses a multi-processor system. It has to be noted that LKH protocol relies heavily on one central point, therefore, it represents a single point of failure and for a large tree; the server's throughput can represent a bottleneck. The use of multiple processors could solve this problem and enhance the server's throughput. The proposed protocol is analyzed according to: the number of processors, the tree height and the tree degree. Experimental results illustrate the improved performance of the proposed protocol compared to sequential system even with significant communication overheads. This improvement is true for all values of h and d even with highly size tree. It outperforms the sequential performance for both join and leave situations. When a new member joins the binary tree, the degree of improvement is 47.7%, 53.3%, 51.3%, 60% and 62.2% for h= 7, 10, 13, 20 and 30 respectively. On the other hand, for the leave situation, the improvement degree is 25.5%, 28%, 32.2%, 35% and 37.8% for the same values of h. The proposed protocol achieves lower communication time in case of a member joins the tree than its corresponding values of the leave situations. This is obvious since the distributed tasks required for re-keying in the join case is less than that in the leave case. In addition, the analysis shows that the proposed design is scalable according to the metrics given in Sec. 5.1. To test the scalability of the proposed protocol, it is tested on different problem sizes. The above results indicate that when the problem size d*h increases our protocol improves the overall system performance with respect to the increasing of the processor number. This leads to the conclusion that the proposed design is scalable. Another experiment which discusses the effect of increasing the value of d is done. The experiment shows that when the value of d increases both the computation and communication time increase. Therefore, the total execution time increases. Consequently, minimizing the value of d is the optimum solution, i.e. the binary tree is the best choice. The abovementioned analysis shows that the use of multiprocessor system will significantly reduce the computation overhead which is considered an important factor for both real time and wireless applications. 
"
"Introduction
 After having established the legal basis for the broad usage of biometric technology the adequate evaluation of resulting technical and societal risks in a holistic manner is of high importance. With the already started international governmentally supported standardization projects and working groups (for instance ISO/IEC SC 37) for biometric person identification and authentication technology , it can be stated that biometric technology should be available for example with standardized data formats for biometric data interchange, communication protocols, and unified programming interfaces for enabling the interoperability of different biometric systems and components in existing (national) information and communication technology (ICT) infrastructures. Biometric technology for person authentication, identification, surveillance, and other applications itself contains core processes and components, which are the main subject of the risk analysis and evaluation approach in this paper. The paper starts with selected terminology from IT security biometrics , privacy, safety, performance, and security risk analysis for biometric authentication technology and a comprehensive approach for biometric authentication systems in Section 2. In Section 3 a high-level component & process model for integrated security risk analysis of biometric authentication technology is presented. A holistic security risk analysis approach for biometric authentication technology based on the predefined model and on biometric authentication risk matrices is discussed in Section 4. This paper closes with conclusions in Section 5. 
Fundamentals
Fundamentals are given in two subsections dedicated to terminology (2.1) and biometric authentication systems for ICT infrastructures (2.2). 
 2.1 Terminology on IT Security Biometrics , Privacy, and Risks 
 IT Security Biometrics. For authentication, identification , and surveillance purposes IT security biometrics uses the mathematical definitions of metrics and metric spaces as explained in 
k) = ( d j=1 |x ij − x kj | r ) 1 
r , where r ≥ 1 and x (i|k)j is the j th feature of the (i|k )th pattern in a pattern matrix. The Minkowski metric defines for r = 2 the Euclidean distance , for r = 1 the Manhattan distance, and for r → ∞ the sup distance. If all features are binary the Manhattan distance is called Hamming distance, which is known from the comparison of iris codes (=biometric signatures) of Daugman's method in 
(2) hazard exposure or duration (
 Definition: A safety risk of biometric authentication technology is the risk of degradation of the biometric authentication system's safety and performance caused by failures and faults. Security Risk of Biometric Authentication Technology . Kossakowski describes in 
 Definitions: A threat to biometric authentication technology is the potential of a circumstance or an action that causes loss of security , degradation of the technology's reliability or performance, or the harm to a person's privacy. The vulnerability of biometric authentication technology is a flaw or weakness that makes it possible for a threat to biometric authentication technology to occur. Shirey defines in 
Definition: A security* risk of biometric authentication technology is a security risk of biometric authentication technology expressed along Brunnstein's Security & Application Risk Traffic Light Model. 
Biometric Authentication Systems for ICT Infrastructures
A biometric authentication system can be considered as a part of a biometric authentication infrastructure where a person is subjected to a general authentication process, which is given by 
 2) (Biometric) Authentication (1:c, 1:1): A person's authenticity is checked by an identification (1:c) or verification (1:1) comparison of the present computed biometric signature with the previously computed biometric signature (template|class) in the phase of biometric authentication with(out) being combined with authentication methods based on a person's knowledge, possession, location, and time ((single|multi)factor biometric authentication). 
3) Authorization: Implicit and explicit authorizations are given to the person in the authorization phase with respect to strong and weak authorizations. 
4) Access Control: 
In the access control phase the access to e.g. IT system resources or activity control within electronic business processes is granted by an access management system (AMS), which can be based on the concepts of mandatory, discretionary, or role-based access control (M/D/RBAC). 
5) Derollment and Authorization Withdrawal: 
 In the phase of derollment and authorization withdrawal a person is derolled and the access rights, relevant biometric and personal data are removed from a biometric database. 
A set of basic elements can be identified from which biometric authentication systems along the general (biometric ) authentication process can be constructed. These elements are wetware entities (persons) and hardware components, biometric communication channels, biometric processes for (en|de)rollment and authentication, biometric algorithms, biometric signatures, and biometric databases. 
 Definition: A biometric authentication system is defined as a set of hardware components , processes, algorithms, data structures, and databases fulfilling internal and/or external communication between the elements for the purpose of biometric authentication. Biometric Processes. Based on the general (biometric ) authentication process four core processes can be identified: sensing and biometric (en|de)rollment and authentication processes. 
A High-Level Component & Process Model for Integrated Security Risk Analysis of Biometric Authentication Technol- ogy
In this section a high-level component & process model for integrated security risk analysis of biometric authentication technology (ComProMiSe·Risk·of·BiT) (
What types of risks can be discussed with the model? The model enables different types of single and integrated risks to be discussed specifically for biometric authentication technology: classical security, privacy, safety, performance, and holistic security risks. The classical security risks are based on the criteria of confidentiality (e.g. secrecy, authenticity), integrity, and availability. The availability aspect obviously reveals that even in first approaches security risks are considered as an integrated aspect. Within the determination of classical security risks both qualitative and quantitative analysis has been done. One main research aspect are attacks threatening the resources and systemic assets by exploiting vulnerabilities. The understanding of faults has been inherited from IT safety for discussing availability. Privacy risks are mainly a qualitative analysis criterion and can be explained by considering misuses which can be (non-)intentional. Research aspects in privacy analysis are e.g. anonymity, pseudonymity, purpose binding of data processing, and necessity of data collection like presented by Fischer-Hübner in 
 4 A Discussion on a Holistic Security Risk Analysis Approach for Biometric Authentication Tech- nology 
 This section evaluates related security risk analysis approaches (4.1) and performance evaluation approaches (4.2). Finally, a holistic risk analysis approach enabled by the ComProMiSe·Risk·of·BiT model from Section 3 is proposed (4.3). 
Evaluation of Related Security Risk Analysis Approaches
Only few partial risk analysis studies with relation to biometric authentication systems exist. Selected related work, initiated and supervised mainly by Brunnstein and Brömme, is evaluated against the understanding of  tic security(*) risk of biometric authentication technology as defined here. Capture Risks. A risk analysis approach of biometric systems is presented in 
Evaluation of Selected Performance Testing Approaches of Biometric Algorithms and Systems
 Assuming that biometric systems and algorithms are understood as pure pattern recognition systems and algorithms test and evaluation approaches regarding their performance exist. An introduction to evaluating biometric systems with strong regard to their performance is given by Phillips et al. in 
The evaluation is understood as the quantification of how well biometric systems meet these properties. Phillips et al. present performance evaluations (focussing their results on the false alarm/reject rate -different terms for false acceptance/rejection rate) done within laboratory and/or scenario tests at the NIST with regard to face (face recognition technology -FERET 1993-98), voice, and fingerprint recognition. The FERET tests have been followed by face recognition vendor tests (FRVT) in 2000- 02. Phillips et al. are presenting within the FRVT 2002 report 
Towards a Holistic Security Risk Analysis Approach for Biometric Authentication Technology
By studying security risks of biometric authentication methods, researchers come to more secure and reliable prototypical research solutions like presented for instance with multimodal biometric methods (biometric fusion techniques) by Hong and Jain in 
(single|multi)factor monomodal to (single|multi)factor multimodal biometric authentication methods, a higher security benefit is produced. From a non-holistic security engineering point of view multifactor multimodal biometric authentication methods should be preferred with regard to (high) security requirements. This result enables a lot of research potential for promising innovative biometric authentication approaches. Instances from the application class of multifactor multimodal biometric authentication technology can be developed for fulfilling technical requirements for performance, safety (reliability), and classical security (confidentiality, integrity, availability). Therefore, the described type of biometric authentication technology from Section 2.2 can be used to derive different design patterns for modularization and interfaces of processes and components. Single technical risk analysis approaches and methods can show risks with regard to specific aspects like safety and performance issues. By also considering integrated risk analysis approaches like in classical security, generic data formats and interfaces between communicating modules and processes have to be defined for enabling an integrated view and evaluation of the resulting combined or overall system risks. So far, pure technical development methods for minimizing risks can be used as long as the different technical requirements from the different technical fields are fulfilled within an integrated system. For the technical aspects of security, safety, and performance it can be concluded for a holistic security risk analysis approach for biometric authentication technology that a well-defined understanding of the processes and components involved in the used biometric technology is necessary (2.2). For a holistic security risk analysis it can also be concluded that it is of little help that processes and components of biometric technology are covered as so called company secrets. Within a holistic security risk analysis approach also non-technical but societal demands arise, for example from the field of privacy. Here a simple technical solution for biometric systems with strong regard to the classical security aspect of confidentiality (especially secrecy) is stated for instance by Jain in 
⊥ e a ⊥ e d ⊥ e authentication e ⊥ a a ⊥ a d ⊥ a derollment e ⊥ d a ⊥ d d ⊥ d 
gle person with the intended reliability risk effect on the enrollment reclustering of the feature space for persons to be enrolled (enrollment C module) is described. Example PREC#3 aD attc ⊥ secu dC. This class outlines the manipulation of the decision within an authentication attempt (authentication D module) with the intention to falsely remove a person's biometric template within a subsequent derollment procedure (derollment C module). An automatic biometric authentication system with a limited number of entrance allowances per person for public transportation could be, for example, target of such an attack. For the holistic security risk analysis approach for biometric authentication technology presented here it can be concluded that the flexibility of the ⊥ relation in combination with a risk matrix enables the systematic exploration and discussion of holistic security risks by a security analyst with(out) help of an inference system guiding through the single implicit possible risk effects along the risk matrix of a biometric authentication system, which is under study. The holistic security risks are based on different security risks between the biometric processes and/or their components. 
Conclusions
This paper presents a systematic approach for a holistic security risk analysis of biometric authentication technology based on the high-level component & process model for integrated security risk analysis of biometric authentication technology also proposed here. The processes and components used within this model are developed together with a terminology for biometric authentication technology for the research field of IT security biometrics, which is comprehensively presented here for the first time. Current approaches for risk analysis of biometric authentication technology are limited to enrollment and identification/verification processes with biometric algorithms mainly considered as black-boxes, only. By using the biometric authentication risk matrices introduced here it is shown that more than seven thousand single possible risk effect classes can be identified, which should be examined for an overall holistic security risk analysis of biometric authentication technology. With the systematic discovery of such a large amount of possible risk effect classes in this paper, it can be concluded that current biometric authentication technology contains inherent holistic security risks, which are not systematically explored. For this reason , the specific risk analysis approach presented here has a strong advantage in comparison with other evaluation and risk analysis approaches in this area. More generally speaking, the presented approach is a significant contribution on the way to the possible development of more (holistic) secure biometric authentication technology. 
"
"Introduction
The increasing need for trustworthy distribution of digital multimedia in business, industry, defense etc. has lead to the concept of content-based authentication. Nowadays manipulating digital images efficiently and seamlessly has become very easy with the availability of powerful software and hence, it is necessary to ensure confidentiality as well as integrity of the images that are transmitted. Image encryption schemes 
The scheme proposed by Fawad Ahmed et al. in 
The paper is organized as follows. In Sections 2 and 3, the proposed orthogonal polynomials based transformation is described. Section 4 elaborates the proposed authentication scheme and in Section 5 the experimental results are presented. 
Orthogonal Polynomials Based Transformation
The Discrete Cosine Transform and Discrete Wavelet Transform are the most widely used transforms for image authentication. However the computational complexity of these transforms is quite high as they involve floating point operations. Motivated by the fact that integer transforms lower the computational complexity, the orthogonal polynomials based transformation which has proved to be efficient in image compression 
M ( i, t) = u i (t), i, t = 0, 1, …, n-1 
(1) 
The linear two dimensional transformation can be defined by the point spread operator M(x, 
y)(M(i, t) = u i (t)) as shown in Equation (2).              ' , , , ,      M x M y I x y dxdy y Y x X 
(2) Considering both X and Y to be a finite set of values {0, 1, 2 … n –1}, Equation (2) can be written in matrix notation as follows 
  I M M t ij   '  (3) 
where  is the outer product, | ij | are n 2 matrices arranged in the dictionary sequence, |I| is the image, | ij | are the coefficients of transformation and |M| is 
                  n n n n n n t u t u t u t u t u t u t u t u t u M 1 1 0 2 1 2 1 2 0 1 1 1 1 1 0         (4) 
We consider the set of orthogonal polynomials u 0 (t), u 1 (t), …, u n-1 (t) of degrees 0, 1, 2, …, n-1, respectively to construct the polynomial operators of different sizes from Equation (4) for n  2 and t i = i. The generating formula for the polynomials is as follows. 
u i+1 (t) = (t – ) u i (t) – b i (n) u i-1 (t) for i  1, u 1 (t) = t – , and u 0 (t) = 1 (5) where                n t i n t i i i i i i t u t u u u u u n b 1 2 1 1 2 1 1 , , and      n t n t n 1 2 1 1  Considering the range of values of t to be t i = i, i = 1, 2, 3, …n, we get       1 4 4 2 2 2 2    i i n i n b i (6) 
As shown in Equation (4), we construct the orthogonal polynomials operator |M| based on the orthogonal polynomials in Equation (5). |M| can easily be made an integer transform by scaling its elements appropriately. 
The Orthogonal Polynomial Basis
For the sake of computational simplicity, the finite Cartesian coordinate set X, Y is labeled as {1, 2, 3}. The point spread operator in Equation (3) that defines the linear orthogonal transformation for image coding can be obtained as |M|  |M|, where |M| can be computed and scaled from Equation (4) as follows. 
                  2 2 2 1 2 0 1 2 1 1 1 0 0 2 0 1 0 0 x u x u x u x u x u x u x u x u x u M  = 1 1 1 2 0 1 1 1 1   (7) 
The set of polynomial operators O ij n (0 ≤ i, j ≤ n-1) can be computed as O ij n = û i  û j t where û i is the (i + 1) st column vector of |M|. For example polynomial basis operators of size (2 * 2) are 
         1 1 1 1 2 00 O ,            1 1 1 1 2 01 O ,            1 1 1 1 2 10 O ,           
             1 1 1 1 1 1 1 1 1 3 00 O ,                 1 0 1 1 0 1 1 0 1 3 01 O ,                 1 2 1 1 2 1 1 2 1 3 02 O                 1 1 1 0 0 0 1 1 1 3 10 O ,                1 0 1 0 0 0 1 0 1 3 11 O ,                 1 2 1 0 0 0 1 2 1 3 12 O                 1 0 1 2 2 2 1 1 1 3 20 O ,                 1 0 1 2 0 2 1 0 1 3 21 O ,                  1 2 1 2 4 2 1 2 1 3 22 O 
Having described the orthogonal polynomials based transformation, we propose an authentication scheme built upon these transformed coefficients, in the following section. 
Proposed Authentication Scheme
In this section an image authentication scheme that can be used to verify the content integrity of gray scale images is proposed. This scheme is similar to the DCT based authentication scheme proposed in 
The loss incurred during quantization, is due to the round off operation performed after scaling the transformed coefficients by the quantization value. But the difference between the coefficients belonging to same frequency position of different sub-blocks of the image remains the same before and after quantization. So of this property is exploited to generate a hash to verify the authenticity of the image since such a hash will tolerate a little loss. 
In this proposed system, the image is first partitioned into non-overlapping blocks of size (N x N). The proposed orthogonal polynomials based transformation is then applied to get the OPT coefficients of each block 
      otherwise Q P if h ij ij 0 ) ) (( 1  (8) 
where 0≤ i < b/2 and j = 0 to 3, b is the number of blocks (feature vectors) and τ is the tolerance value introduced in order to prevent the system from reporting some false alarms due to minor acceptable manipulations. The image is then compressed by performing scalar quantization using a quantization factor CQ, entropy coded and then sent to the receiver along with the hash. The same process of selecting the coefficient pairs employed at the sender is used at the receiver to choose the coefficient pairs and the relationship between them is found to generate the hash bits. The hash produced at receiver is then compared with the received hash. The image is declared to be authentic if both the hashes are equal otherwise it is declared to be unauthentic. If the image is unauthentic, the blocks where a mismatch occurs are identified as tampered blocks and indicated. 
The steps involved in this proposed hash generation with the OPT domain at the sender and receiver are presented diagrammatically in 
Hash H 0 Original Image I o ( N x N ) blocks 
Secret Key In the following sub-sections, the hash generation and verification algorithms of the proposed inter-coefficient relationship based authentication scheme are presented. 
Hash Generation Algorithm
The hash generation algorithm generates the hash by transforming the image into OPT domain and then selecting the block pairs using the secret key. After generating the hash value, the image is compressed and sent to the receiver along with the secret key and the computed hash in an encrypted form. 
Hash Verification Algorithm
The verification algorithm decodes and de-quantizes the received compressed image and computes the hash by picking up the blocks pairs with same secret key used at the sender to generate the hash and verifies the authenticity of the received image by comparing it against the received hash. 
In the following section the experimental results of the proposed system is presented. 
Experimental Results
The proposed authentication system is tested with 50 grayscale images of different sizes. Some of the sample test images of size (128 x 128) with gray scale values in the range (0 – 255) are shown in 
The last part of our experiment tests the effect of intentional tampering of the content. The test images intentionally tampered are shown in 
(a) (b) (c) (d) (e) (f) 
Conclusions
In this paper a semi-fragile image authentication system using the orthogonal polynomials based transformation is proposed. In this scheme the input image is first transformed using the orthogonal polynomials based transformation and the coefficients that form the feature vector are selected with a secret key. The system then exploits the inter-coefficient relationship between the orthogonal polynomial based transformed coefficients to form the hash code for authentication. From the experimental results it is evident that, the proposed scheme is effective in discriminating incidental distortions from intentional distortions. The proposed scheme is also compared with its DCT based counterpart and it is observed that the proposed scheme is more robust to additive noise than the DCT based scheme. 
"
"Introduction
 Intrusion detection systems seek to examine all the traffic in a network or system and determine if an attack is in progress. Intrusion prediction systems forecast the unidentified intrusions and try to prevent a compromise before any real damage can be done. As the technology has progressed, the lines between intrusion detection and prediction have blurred somewhat, because traditional detection systems have incorporated the capability not only to alert and advise but to take pro-active steps to prevent a compromise. Traditional detection solutions are necessary to prevent the transfer of malicious codes, but are not sufficient to address the new generation of threats and targeted attacks. Security solutions that proactively protect vital information assets in real time, without waiting for new signature creation and distribution , are needed. The definition of IDP that we are going to use is a system which has ability to detect the known attacks and predict the unknown attacks to prevent the new attacks from being successful 
CoCo-IDP Model
 In this section, we introduce architecture of our cooperative co-evolutionary IDP system. In continue the structure and evaluation parameters are discussed. 
The System Architecture
 Immune system is a biological model which is applicable to many networking and security problems. Genetic algorithms are successful method for optimization problems . In GA, the population repeatedly modifies with genetic operators in a search space and seeking for the answer with the best fitness. GA initializes a population to random individuals of digital values and over successive generations, the population "" evolves "" toward an optimal answer. On the other hand, the co-evolution algorithm is an extended version of GA with multiple groups of populations. Moreover, the cooperative co-evolutionary method includes several genetically isolated groups that evolve in a parallel model. The individual member from each group collaborates with other members through a representative population 10 and improves their fitness according to a specific objective function 
with other groups under master agent management. Each group generates a set of representative which has the best fitness with all pre-selected incidents. The final set of representative will generate from the entire representative set in the second level of the process. As a result, based on final representative set, the best detectors for all known and unknown incidents are generated and they are stored in the master agent. This is the procedure for generating the detectors in CoCo-IDP system. 
The System Structure
 In this part, we introduce the structure of our proposed method and mathematically analyze the system parameters based on cooperative co-evolutionary algo- rithm 
T a = {e 1 , e 2 , e 3 , e 4 , . . . e a } T b = {t 1 , t 2 , t 3 , t 4 , . . . t b } T raf f ic = T a + T b = {e 1 , e 2 , e 3 , e 4 , . . . e a } + {t 1 , t 2 , t 3 , t 4 , . 
. . t b } 
 where T a represents group of incidents e i with "" a "" members , and T b represents groups of normal traffic t i with "" b "" members. Also, the d ij represent a detector and the D i represents a set of detectors: 
D = [D i ] a×n = [d i,j ] a×n . 
Here, "" n "" is the maximum number of detectors for each incident and this value may not be the same for all incidents . The goal is to find the best set of detectors (M) with m i members (representative detectors) where the m i has the best fitness in set of D i and can obtain the maximum successful detection rate. To select the members with the best fitness, we should calculate the matchstrength factor. We define S as the match-strength factor between two binary strings of x and y where x ∈ D and y ∈ (T a and T b ) with size of 64 bits. The value of S can simply obtain by comparison of similar position bits in x and y based on the following equation: 
S(x, y) = l i=1 0 if [x i ] = [y i ] or x i ≡ X 1 else. 
Where l=64 and X refers to the do not care values. In order to find the maximum fitness for incident e j , we obtain a member which has the maximum match-strength based on the following equation: 
S maxi (D i , e i ) = S(d ij , e i )| S(dij ,ei)≥S(D ik ,ei) for i = 1 · · · a & for j, k = 1 . . . n, k = j. 
 In set of detectors which are generated for existing incidents , representative members give a set of members which have the best fitness for each incident. We assume m i has the maximum match-strength (best fitness) in set of detectors for incident e i where: 
M = a i=1 m i . 
In this process M defines a set of detectors with the best fitness. 
The System Parameters
 For evaluation of our proposed system, we have introduced the probability of detection ratio (P DR) and false accuracy ratio (FAR) as the two evaluation parameters. Probability of detection shows the successful detection of a real incident where the probability of false accuracy ratio refers to the probability of selecting a normal traffic instead of an incident or vise versa. In order to calculate the probability of detection ratio, we have defined the following parameters: T is a threshold level which is obtained by dividing the decimal value of threshold field in C-detector to 255. The result gives a real value between zero and one. 
 Hit is a detected target incident which refers to summation of all detected yes-incidents in set of T a . 
We obtain the hit value based on the following equation: 
Hit = Σ a i=1 
Here T (m i ) is the threshold for member m i and Z(m i ) is obtained based on the following equation: Z(m i ) = Σ 64 j=1 1 M askbit(j) = 1 0 else. Probability of detection ratio is calculated based on Equation (1). P DR = Hit/a. 
(1) 
On the other hand, the false rate (F ) is sum of the non-incidents that are detected as the incidents based on the following equation: 
F = Σ a i−1 Σ b j=1 1 if [s(m i , t i )/Z(m i )] ≥ T (m i ) 0 else. 
The probability of false ratio is calculated based on Equation (2). EF R = F/b. 
(2) 
The false accuracy ratio (F AR) is obtained based on the following equation: 
F AR = P F R P F R + P DR . 
Equations (1) 
and 
(2) are defined the two important evaluation parameters for detection procedure. For system evaluation, we have considered the successful ratio based on the following equation: SR = P DR − F AR. 
Prototype Systems
 In this section, we have explained the technical specification of IS and CoCo-IDP prototype systems with more details. We also emphasis on structure of the detectors and describe how the detectors are generated. 
The IS Method
In IS prototype model, the system has implemented based on genetic algorithms. The detectors are mapped into the same form of symbolic representation as CoCo-IDP system , while the IS system evolves in a fixed threshold. The detectors are generated and stores in the system during the initialization phase. In operational phase, the system traces the events which have the best fitness with generated detectors in a fixed threshold. When a detector matches or predict an incident, the detection process will activate. In a successful procedure, the results will store in the system; otherwise, it will remove within a time interval. This technique creates strong pressure for matching and discrimination between incidents and normal traffic. Moreover, the process distinguishes between the known and the un-known incidents. As a result, during the detection process, the detectors compete for successful detection/prediction where the best detectors will win the competition process. 
The CoCo-IDP Method
In CoCo-IDP model, we have implemented the prototype system in a Jini-Grid platform running in a distributed environment. 
Structure of Rules and Detectors
In order to detect the incidents, we should generate the detectors based on technical specification and features of each incident. In this section, we explain how the detectors are generated in CoCo-IDP and IS systems. In specification of CoCo-IDP system, each detector consists of different fields with related features (type of protocol, duration, service, etc). All the fields, features and descriptions are presented in 
System Characteristics
We have implemented the prototype CoCo-IDP system using a jini-grid platform in a set of distributed severs ning Matlab software and supporting MDCE. 12 We have employed several Blade servers with windows operating system. Also, we have used the KDD database records with 31 fields and 132-bit length. The KDD records consist of five classes: Normal, DoS 13 , R2L 14 , U2R 15 and Probing 16 . In the database, the testing incidents have different probability of distribution from training incidents, and there are specific incidents in data test which do not exist in the training data. We assume any un-known incident as a new event. This model creates a scenario very similar to a real application. In our prototype system, the data set contains 22 types of training incidents with additional 17 types of testing data. In the following section, we investigate the cooperation of members in GA for ruling the system to a converged solution. This evaluation proves that the CoCo-IDP algorithm has a solution for detection/prediction problem. Moreover, it is important to show the range of thresholds which are acceptable for the best detection results. In Co-Co algorithm, cooperation of the participated mem- 12 Matlab Distributed Computing Engine 13 Denial of Service attack is an attempt to make a computer resource unavailable to its intended users. 14 Remote to Local is unauthorized access from a remote machine (e.g. guessing password).
Algorithm 
 4.5 Acceptable Affinity/Deviation (Variance δ) Rate 
In fact, the acceptable affinity rate limits the system to an acceptable deviation rate where the maximum deviation refers to minimum matching boundary. The traditional intrusion detection system focuses on an exact signature matching or zero deviation (δ = 0) for detection procedure . As the system becomes more intelligent, the acceptable deviation rate affects the decision criteria where the certain levels of similarity replace with exact matching condition. Thus, the intelligent system has capability to detect the incidents within a reasonable boundary limited to maximum acceptable deviation. In the IS system, the acceptable area is limited within a fixed threshold. The value of threshold is constant for all the incidents where outside this boundary the system is not able to concentrate for a successful detection. On the other hand, for CoCo-IDP the scenario is different and the threshold is more flexible. The threshold level is wider while the maximum level might be different for each type of incident . This is the result of technical specification and constraints in generating the system rules. As a result, the detectors in CoCo-IDP have more diversity, more capability in training and more flexibility compared to IS rules. Moreover, the detectors in CoCo-IDP are able to trace a suspicious incidents with less degree of similarity to the reference patterns compared to the IS system. This characteristic expands the border of activity for the detection algorithms and increases more opportunity for tracking the existing incidents. To show the acceptable area, we have prepared a prototype system similar to the previous model. 
Acceptable Matching Rate 
 4.6 Average Detection/Prediction Process Speed 
1 - S m u r f f 2 - L a n d d 3 - N e p tu n e e 4 - T r e a d r o p p 5 - P o d d 6 - G _ p a s s d d 7 - I m a p p 8 - M u lt ih o p p 9 - F tp _ w r it 
1 - W o r m m 2 - U d p s to r m m 3 - P r o c e s s ta b le e 4 - M a il b o m b b 5 - A p a c h e 2 2 6 - S a in tt 7 - M s c a n n 8 - N a m e d d 9 - X lo c k k 1 0 - X s n o o p p 1 1 - S e n d m a il l 1 2 - H tt p tu n n e ll 1 3 - S n m p g u e s s s 1 4 - S n m - a 
Confirm the Evaluation Results
Evaluation results show that the CoCo-IDP method is a successful technique for intrusion detection and prediction in a distributed system. To confirm validation of the results in detection using a standard confirmation technique , we have prepared a comparison scenario with several well known detection methods using predictive accuracy metric as well as the detection method in 
Conclusions
 Intrusion Detection and Prediction provide the capability of both detecting and predicting against any security threats. Detection system monitors abnormal traffic pattern and reports the suspicious events; however, it is unable to predict any unknown incident. The prediction is an intelligent process that learns and adapts the system for distinguishing the unknown threats. We have presented a detection/prediction system based on a cooperative co-evolutionary immune system and a grid computing technique in a distributed data networks. We have implemented a pure immune system (IS) and a CoCo- IDP system as the prototype models and compared the key parameters of both systems. The results show that the CoCo-IDP system is more successful in both detection and prediction with higher accuracy metric compared to the IS system. Also the system has learning capability to recognize the suspicious events with less fitness compare to the IS system. The probability of detection and the false accuracy rate in the proposed system are compared 
"
"Introduction
Securing real-time multimedia data is a challenging task since the size of data is usually very large and the data needs to be processed in a short time interval. Standard cryptographic algorithms will usually result in a large overhead, rendering them inefficient. Yi, Tan, Siew, and Syed 
Description of FEA-M
FEA-M uses an ID-based Diffie-Hellman key agreement protocol to generate a common secret key, k, an integer, between the sender and the receiver 
P 1 , P 2 · · · P r , 
with the same length, n 2 , where n is 64 and r is an integer 
C i = K · (P i + C i−1 ) · K i + P i−1 P i = K −1 · (C i + P i−1 ) · K −i + C i−1 P 0 = C 0 = V 0 . 
 3 Previous Analyses and Improvements of FEA-M 
 The vulnerability of FEA-M has been identified and improvements have been proposed. Mihaljevic and Kohno point out in 
C i = K · (P i + K · V · K i ) · K i+n + K · V · K i (1) P i = K −1 · (C i + K · V · K i ) · K −(i+n) + K · V · K i (2) 
If C i is a lost block, no further impact on subsequent blocks occurs. 
Further Weaknesses
 In this section, we identify further weaknesses in the original FEA-M and in its improved variant. 
Weakness of the Improvement Pro
posed in 
P i = K −1 · (C i + K · V · K i ) · K −(i+n) + K · V · K i (3) 
The reason this kind of attack works is because the improvement in 
Security Degradation due to the Use of Fixed Pad
 In the original FEA-M, 0s are appended in the last plaintext block so that its length will be exactly n 2 . The obvious disadvantage of this method is that it introduces insecure information redundancy. If the plaintext in the last block is all 0s, after appending the all 0 pad, it will result in an all 0 last bock. As analyzed in 
Impovements to FEA-M
In this section, we propose the following techniques to overcome the weaknesses mentioned above. 
Randomly Generated Bit Streams to Replace the all 0s Pad
 A randomly generated pad can overcome the insecurity introduced by all-zero padding. In our proposal, the Blum- Blum-Shub pseudorandom bit generator 
D i+1 = Hash(D i ), 
 where i is an integer and Hash is a one way hash function such as SHA-256 
Compress Plaintext to Avoid Mihal
jevic's Assumption 
Reliable Transportation to Handle Packet Loss
FEA-M is vulnerable to packet losses. Furthermore, the improved variant in 
2) Correction for improved variant 
In details, improved variant 
Algorithm 3: Correction for improved variant 
3: for 1 ≤ i ≤ r; i++; 4: Hi = M D5 − M AC(Pi, K upper128bits )
; 5: H = Hi||H; 6: Ci = Improved variant in 
 Algorithm 3 provides data source authentication service to persist against packet replay attacks for improved variant 
3) IDA: We propose Algorithm 4 which describes the implementation of the IDA. 
Algorithm 4 focuses on the implementation of the IDA which presents reliable transmission for data packets by introducing some amount of information redundancy. IDA splits the source data, for example, C j , into n pieces, which are, then, encoded by the IDA algorithm. At the receiver end, the IDA can reconstruct C j after receiving any m pieces where m < n. However, guaranteeing zero packet loss comes at the cost of increased communication overhead. For example, for r blocks, assume every block is 4096 bits. So, n is 64, m can be 50. For Algorithm 2, 4096 * r * n/m bits' data are sent over the network and at least 4096 * r bits' data are received. For Algorithm 3, in addition to source data, 64 * r * n/m bits' hash are sent over the network and at least 64 * r bits' hash are received. According to Algorithm 4, we find the computation complexity of IDA is O(n 2 ). 
Conclusion
 After examining the FEA-M algorithm and its improvement , we have identified some of its weaknesses, namely, 1) vulnerability of Mihaljevic's proposal 
Algorithm 4: IDA The Sender Party A: IDA-Encode INPUT: a block of data C j OUTPUT: encoded vectors T 1 , T 2 · · · T n 1: (1) Split C j into N/m pieces where N = n/8: 2: 
C j = (c 1 , · · · , c m ), (c m+1 , · · · , c 2m ), · · · , (c N −m−1 , · · · , c N )
where,c i :byte 3: 
R i = (c (i−1)m+1 , · · · , c im )
,where, i < N/m 4: (2) Process C j : following the specification of IDA 
A i = (a i1 , · · · , a im ), 1 ≤ i ≤ n
,let every subset of m different vectors 6: are linearly independent. Then, process C j : 7: 
T i = A i · (R 1 R 2 · · · R N/m ) = (a i1 · · · a im )·       c 1 , c m+1 , · · · , c N −m+1 · · · c m , c 2m , · · · , c N       
where 1 ≤ i ≤ n (4) 9: 
(3) Send T 1 , T 2 · · · T n to the receiver. The Receiver Party B: IDA-Decode INPUT: encoded vectors T 1 , T 2 · · · T m OUTPUT: a block of data C j ; 1: (1) Assume that the receiver receives T 1 , T 2 · · · T m 2: 
T 1 = A 1 · R 1 , A 1 R 2 · · · A 1 · R N/m 3: T 2 = A 2 · R 1 , A 2 R 2 · · · A 2 · R N/m 4: · · · ; 5: T m = A m · R 1 , A m R 2 · · · A m · R N/m 6: (2) 
Prepare for the calculation of R 1 7: Based on T 1 · · · T m , and Formula (4), we can get: 
8: A g       c 1 · · · c m       =       A 1 gR 1 · · · A m gR 1       where A =       a 11 · · · a 1m · · · · · · · · · a m1 · · · a mm       9: (3) 
Since A is invertible, we can calculate R 1 : 10: 
R 1 =     c 1 · · c m     =     a 11 · · · a 1m · · · · · · a m1 · · · a mm     −1     A 1 gR 1 · · A m gR 1     11: 
(4) Repeat Step 3, we can calculate R 2 · · · R N/m . 12: (5) Reconstruct C j : 13: C j = R 1 ||R 2 K||R N/m where || denotes concatenation. 
"
"Introduction
Smart meters have been widely deployed all over the world. Varieties of security protection technologies have been developed in smart metering. Most of them focus on the smart meter data security protection. However, almost no approaches focus on the security of smart meter parameters. Smart meter parameters, such as the total/sharp/peak/flat/valley period time, should be protected from being modified arbitrarily because they affect the authenticity of users' energy bills. As shown in 
The rest of this paper is organized as follows. Section 2 introduces the preliminary knowledge used in the protocol . Then we investigate the related work in Section 3. We describe the system model and security requirements in Section 4 and then present our proposed protocol in Section 5. Security analysis and experiment evaluation are shown in Sections 6 and 7 respectively. Finally, we conclude the paper. 
Preliminary
 2.1 Shamir's (t, n) Threshold Secret Sharing Scheme (t, n) threshold key sharing scheme based on Lagrange interpolation formula was proposed in 1979 by A. Shamir 
Definition 1. A share is the result value y by computing the following polynomial on inputting a known x. 
f (x) = (K + a 1 x + a 2 x 2 + ... + a t−1 x t−1 ) mod Q 
where a 1 , a 2 , ..., K ∈ F Q , Q is a large prime, F Q is a finite domain on Q, K is the secret value. 
From the definition above we know that n shares are y 1 , y 2 , ..., y n computed from known x 1 , x 2 , ..., x n respectively , and the polynomial y = f (x) can be reconstructed from any t known pairs (x i1 , y i1 ), (x i2 , y i2 ), ..., (x it , y it ) of n pairs (x 1 , y 1 ), ..., (x n , y n ). 
Binomial Distribution
Binomial distribution is a probability distribution with discrete random variables, symbolized by X ∼ B(n, p), where X is the result of the randomized trial, n is the number of independent repeated trials, p is the occurrence probability of an event in one trial. The following is the mathematical definition of binomial distribution. Definition 2. Assuming an event A. The occurrence probability of A is p (0 < p < 1) in one trial, thus the nonoccurrence probability of A in one trial is q = 1 − p. The probability that A occurs k times in n independent repeated trials is: 
P = (X = k) = C k n p k q n−k , k = 0, 1, ..., n (1) 
The probability that A occurs no more than k times in n independent repeated trials is: 
F = (X = k) = P (X ≤ k) = k j=0 C j n p j q n−j (2) 
Related Works
In the studies of smart metering security protection, most of them focus on smart meter data security protection. We briefly review these concerned works from two aspects: Smart meter data privacy protection and smart meter data security defense. 
Data Privacy Protection
 Smart meter data privacy protection aimed at achieving the power company's billing purposes and preserving users' privacy in the meantime. The scheme in paper 
Data Security Defense
Smart meter data security defense aimed at protecting the smart meter data from physical attacks. According to paper 
 4 System Model and Security Re- quirements 4.1 System Model 
 Figure 2 depicts the system architecture of the protocol where three types of participants exist: Smart meter, @BULLET Smart meter. A smart meter locks the smart meter parameter modifying program by using the program lock key K and then divides the program lock key K into n shared keys. Each shared key is distributed to one participant. The smart meter asks for shared keys to recover the program lock key K when receiving a modifying request from one power company. 
Security Requirements
 We aim at designing a program access authorization protocol to protect smart meter parameters from being modified arbitrarily. The security requirements are summarized as follows: 
Request Message Authentication
 Every modifying request message from one power company should be authenticated 
Shared Key Message Confidentiality
Shared keys distributed or recalled by the smart meter should be kept confidential to protect the security of the program lock key K 
Anti-attacking Ability
 The protocol should have anti-attacking abilities to protect the participants against outside attackers and thus protect the security of shared keys. An attacker should not be able to obtain the shared keys through attacking the participants. 
The Protocol
The Notations in the Protocol
The notations used in the protocol are shown in 
P ub R /P ri RSig ( , ) M Pub i i E x y P r Ai i Sig request Pr Pr A U i i i Sig Sig 2 1 1 2 1 ( ) ( ... ) mod t t f x K a x a x a x Q − − = + + + + ( ) k E program ( , ) i i n x y verify 
The Protocol
Generating And Distributing Shared Keys
Each protocol participant has its own private/public key and others' public keys (this process based on public key cryptography is not our focus, so we do not discuss it in detail). Below is the description of generating and distributing shared keys. 
Step 1-1/2. A smart meter M randomly generates a t − 1 power of polynomial, 
f (x) = (K +a 1 x+a 2 x 2 +...+ a t−1 x t−1 ) mod Q (Q is a prime number, K < Q). 
The smart meter uses the program lock key K to lock the smart meter parameter modifying program, E k (program). Then the smart meter selects n different non-zero elements (
x i , y i ) (x i ∈ Q, 1 ≤ i ≤ n, y i = f (x i )
) called secret keys. Anyone who has at least t (t ≤ n) secret keys can recover the program lock key K 
Step 1-3. Upon the power companies and user receive the massages, they first verify the signature of the smart meter. After succeeding, they get their own shared keys E P ub M (x i , y i ) using their own private keys. But they can never know (x i , y i ), the confidential information of the program lock key K, without the private key of the smart meter. 
Modifying Request
If one power company (e.g. A 1 ) wants to modify smart meter parameters, he has to send a modifying request and his own shared key 
Step 2-1. Power company A 1 sends a modifying request to the smart meter. The request is encrypted with the smart meter's public key and sent with the power company's signature. 
Step 2-2. Upon the smart meter receives the request, it first verifies the power company's signature and then gets the request using its private key. 
 Step 2-3. Then the smart meter broadcasts the modifying request and it's signature to all participants asking for shared keys. 
Recovering The Program Lock Key K
Step 3-1. Upon receiving the broadcast, other power companies and user first judge the rationality of the modifying reasons. If anyone considers the reasons are irrationality, he/she does not submit his/her own shared key to the smart meter. Otherwise, he/she verifies the smart meter's signature and sends his/her own shared key with his/her signature to the smart meter. 
Step 3-2. Upon receiving the massages from the power companies and user, the smart meter first verifies their signatures respectively. After succeeding, the smart meter gets the shared keys. If getting less than t shared keys, the smart meter can not recover the program lock key K. This means the modifying request fails. Otherwise, the smart meter recovers the program lock key K and unlocks the program to modify the smart meter parameters according to the modifying request. 
Security Analysis
 In this section, we evaluate the protocol generally according to the security requirements summarized in Section 4. 
Request Message Authentication
 Before a power company sends a modifying request message to the smart meter, the power company has to sign the message using his private key. The private key is only known by the power company. Hence an attacker does not know how to produce the signature of the power company without the power company's private key. Thus an attacker could not able to pretend the power company to transmit the request message. 
Shared Key Message Confidentiality
Shared keys (i.e. E P ub M (x i , y i )) are encrypted using the public key of the smart meter, which is significant from the conventional secret share scheme, no one can get the secret keys (i.e. (x i , y i )) except the smart meter. Thus, an attacker can not acquire the secret keys, the confidential information of the program lock key K, through eavesdropping or intercepting the shared key messages. 
Anti-attacking Abilities Analysis
 The program lock key K is used to lock the smart meter parameter modifying program. If getting the program lock key K, attackers can open the program to do something with smart meter parameters and then achieve their malicious purposes. The security of the program lock key K is crucial to the protocol. According to the targets which attackers attack to, we analyze the anti-attacking abilities from the following two aspects. 
Targets Are Smart Meters
A smart meter randomly generates a polynomial and then gets the program lock key K and shared keys from this polynomial. After using the program lock key K to lock the smart meter parameter modifying program and distributing the shared keys to all participants, the smart meter destroys the program lock key K, the polynomial 
(i.e. f (x) = (K + a 1 x + a 2 x 2 + ... + a t−1 x t−1 
) mod Q), the secret keys (i.e. (x i , y i )) and the shared keys (i.e. 
E P ub M (x i , y i )
). This means that the smart meter does not store any information about the program lock key K. Thus, attackers can not get any information about the program lock key K through attacking 
Targets Are Power Companies/Users
A power company/user receives a shared key from the smart meter and keeps it for future use (i.e. sends the shared key back to the smart meter for modifying smart meter parameters). The shared key (i.e. 
E P ub M (x i , y i )
) is encrypted using the public key of the smart meter, no one except the smart meter can decrypt it. Thus, even though obtaining the shared key through attacking the power company/user, attackers can not get the confidential information (i.e. (x i , y i )) of the program lock key K. 
Less Than t Participants Colluding Together Can Never Modify Smart Meter Parameters
From Section 5, we know that if a smart meter gets less than t shared keys, it can not recover the program lock key K to open up the smart meter parameter modifying  program. So less than t malicious participants colluding together can not unlock the program to modify smart meter parameters. Any one power company/user cannot modify smart meter parameters arbitrarily through sending his/her only one shared key. Moreover, in real life, most power companies are trusted entities. So the probability of the smart meter parameters arbitrarily being modified is very small. In addition, the program lock key K can not be derived out by participants. Since the shared keys are encrypted with the public key of the smart meter, the power companies and user can not get the confidential information of their own stored keys. So the program lock key K can not be deduced out by the participants colluding together through some algorithms (e.g. exhaustive algorithm ), which is superior to the conventional secret share scheme. 
Experiment Evaluation
The Determination of the Optimal t
 We balance the time cost of the program lock key K recovering and the security of the program lock key K to determinate the optimal threshold t. 
sd = P (X ≤ t) − P (X = t) = t j=0 C j n p j (1 − p) n−j − C t n p t (1 − p) n−t (t = 1, ..., n) 
(3) 
The Time Cost for Encryption and Decryption
We deploy the environment with a ThinkPad Core 2 CPU E425 @1.90GHz PC, and choose RSA (1024 bits) as an the asymmetric encryption algorithm, coding in C. The time cost for encryption and decryption in the fist phase of the protocol, namely the generating and distributing shared keys phase, is only about 94 milliseconds . Moreover, the generating and distributing shared keys phase is only carried out once in a smart meter unless the program lock key K leaks. Therefore, The time cost for encryption and decryption in the generating and In general, the second phase and the third phase of the protocol, namely the modifying request phase and the recovering the program lock key K phase, carry out once every three months (i.e. a quarter of a year) in China. We take encrypting and decrypting the longest information for example to test the time cost for encryption and decryption in these two phases during one year. The test results are shown in 
ag 1 = (y 3 − y 1 )/(x 3 − x 1 ), ag 2 = (y 4 − y 2 )/(x 4 − x 2 ), 
where x i and y i are the power consuming time (effective time) and the time cost for encryption and decryption respectively . This ratio is almost a constant value and so small, only about 5.26 × 10 −6 %. So the protocol has good efficiency. 
Conclusions
Smart meter parameters, such as the total/sharp/peak/flat/valley period time, affect the authenticity of energy bills of users. Users or power companies can modify smart meter parameters through sending programming commands. However, they should not arbitrarily modify smart meter parameters for their own purposes. This paper proposes a secret share based program access authorization protocol for smart metering. The protocol realizes that one user and power companies jointly control the modifying permissions on smart meter parameters. Our future work is to construct the hierarchy based administrator domain 
"
"Introduction
Bilinear pairing has attained utmost importance in the field of public key cryptography due to its wide application area. An area of research in this regard has been developed that is known as pairing based cryptog- raphy 
Earlier implementation techniques for computing the Tate pairing such as Barreto, Kim, Lynn, and Scott (BKLS) algorithm 
Background
 The pioneer work in the field of pairing based encryption is proposed by Boneh and Franklin 
Tate Pairing
The name bilinear pairing indicates that it takes a pair of vectors as input and returns a number. It performs a linear transformation on each of its input variables. For example, the dot product of vectors is a bilinear pairing . Similarly, for cryptographic application the bilinear pairing (or pairing) operations are defined on elliptic or hyper-elliptic curves. Pairing is a mapping G 1 ×G 2 → G 3 , where G 1 is a curve group on some field F q , G 2 is another curve group on the lowest extension field F q k , and G 3 is a subgroup of the multiplicative group of F q k . Let, a large odd prime l divides the order of the curve group (#E(F q )). Let, the point P be a l-torsion point for a large prime l|#E(F q ). Here k is the corresponding embedding degree, often referred to as security multiplier in pairing computation. It is the smallest positive integer such that l divides q k − 1. Then the Tate pairing of order l is a map 
e l : E(F q )[l] × E(F q k )[l] → F * q k /(F * q k ) l , 
where E(F q )
P ∈ E(F q )[l], Q ∈ E(F q k )[l] is given by e l (P, Q) = f l,P (D)
 . Here f l,P is a function on E whose divisor is equivalent to l(P )−l(O), D is a divisor equivalent to (Q) − (O), whose support is disjoint from the support of f l,P . The point O represents the point at infinity. For more information regarding divisor, we refer the reader to 
D = i a i P i f l,P (D) = i f l,P (P 
E l (P, Q) = e l (P, Q) (q k −1)/l . 
 The point multiplication based algorithm (Algorithm 1) for pairing computation is given by Miller 
∈ E(F q ), Q ∈ E(F q k ). Output The Tate pairing E l (P, Q) Process 1 i = [log 2 (l)], K ← P, f ← 1. 2 While i ≥ 1 do 3 
Compute equations of l and v arising in the doubling of K. 
4 K ← 2K and f ← f 2 l (Q)/v (Q) 
5 If the i-th bit of l is 1 6 Compute equations of l and v arising in the addition of K and P . 
7 K ← P + K and f ← f l (Q)/v (Q). 8 end 9 i ← i − 1. 10 End While 11 Return f (q k −1)/l 
The Tate pairing can only be computed efficiently if the security parameter k is small. Before the work of Miyaji, Nakabayashi and Takano 
Input P = (x 1 , y 1 ), Q = (x 2 , y 2 ) Output f P (φ(Q)) ∈ µ l ⊂ F * q 6 Process 1 f ← 1 2 For i = 1 to m do 3 x 1 ← x 3 1 , y 1 ← y 3 1 4 µ ← x 1 + x 2 + b 5 λ ← −y1y2σ − µ 2 6 g ← λ − µρ − ρ 2 7 f ← f.g 8 x2 ← x 1/3 2 , y2 ← y 1/3 2 9 
End For 10 Return f q 3 −1 
Fault Attack on Tate Pairing
 Fault attack on pairing computation tries to exploit erroneous results that are produced by the device in presence of some transient fault at loop bound m 
R = R 2 R 1 = e m±r+1 e m±r = g q 3 −1 m±r+1 , where g i = −y 3 i 1 .y 2 σ − µ 2 i − µ i ρ − ρ 2 . 
The value of g i from g q 3 −1 i can be extracted through root finding algorithm and by solving some linear system of equations 
Pairing in Edwards Coordinates
Edwards showed in 
x 2 + y 2 = c 2 (1 + x 2 y 2 ). 
Thereafter Bernstein and Lange 
E d : (1/(1 − d))v 2 = u 3 + 2((1 + d)/(1 − d))u 2 + u via 
the rational map: 
ψ : E d → E (u, v) → 2u v , u − 1 u + 1 . 
The addition formulas on Edwards curve is given by: 
(x 1 , y 1 ), (x 2 , y 2 ) → x 1 y 2 + y 1 x 2 1 + dx 1 x 2 y 1 y 2 , y 1 y 2 − x 1 x 2 1 − dx 1 x 2 y 1 y 2 . 
It is shown in 
R = R 2 R 1 = e m±r+1 (P, Q) e m±r (P, Q) , 
which is exploited to compute the x and y coordinates of secret point P . Therefore, in the countermeasure it is essential to take care of the point such that attacker could not ascertain loop boundary for which the algorithm produces final result. Unfortunately, the countermeasures that are given in 
Countermeasure-1: New Point Blind
ing Technique 
e(P, Q) = e(xP, yQ) = e(P, Q) xy . 
(1) 
In both Duursma-Lee and Kwon-BGOS algorithms, the input points are processed and it produces pairing result after m iterations. Now according to the relationship , which is shown in Equation 1, the pairing result on set of points (P, Q) and (xP, yQ) are equal. However the fault attack exploits the final result, which is remain unchanged in current countermeasure. Thus the fault attack that is defined in 
R 1 = e m±r (x 1 P, y 1 Q) = e m±r (P, Q), R 2 = e m±r+1 (x 2 P, y 2 Q) = e m±r+1 (P, Q). 
The ratio of R 2 and R 1 is nothing but g q 3 −1 m±r+1 , where 
g m±r+1 = −y 3 m±r+1 P .y Q σ − µ 2 m±r+1 − µ m±r+1 ρ − ρ 2 , and µ = x P +x Q +b. The point Q = (x Q , y Q ) 
is known to the attacker. Thus, the above equation can be reduced to a equation of unknown point P = (x P , y P ). Along with the above equation the attacker knows the curve equation, which can be used as second equation for solving the x, y coordinates of the secret point P = (x P , y P ). Hence, the value of P can be computed easily by applying the attacking procedure described in 
 3.2 Countermeasure-2: Altering Traditional Point Blinding 
e(P, Q) = e(P, Q + X).e(P, X) −1 , 
 where X is a random point. It is assumed that P is secret and Q is public. The fault attack described in 
m ± r and m ± r + 1 iterations, which means R 1 = e m±r (P, Q) and R 2 = e m±r+1 (P, Q) for some random fault r and r + 1. The ratio R 2 R 1 can be written in terms of P = (x P , y P ) and Q = (x Q , y Q ) 
as written in Section 3.1. The faulty results can be exploited in the same way as the fault attack described in 
Proposed Countermeasure
This section proposes a suitable countermeasure against fault attack on pairing computation. The underlying principle of fault attack on pairing computation is based on the ability of the attacker to change the value of the loop boundary m. The attacker also has the ability to measure the change from timing or power analysis of the computation . The attacker tries to obtain two pairing computations one for m + r and the other for m + r, augmented by 1 through fault induction. Hence, our countermeasure ensures that even if there is a fault the attacker cannot correlate the pairing output with number of iterations. The objective is to disable the attacker from ascertaining the ratio R 2 /R 1 , as mentioned in Section 2.2. 
Blinding Loop Boundary
The proposed countermeasure blinds the loop boundary m as it is the main factor in fault attacks. It protects the loop boundary so that the attacker cannot guess the number of iterations for which the faulty output is produced . It modifies the Duursma-Lee algorithm for protecting secret point in pairing computation against fault attack. The modified algorithm is shown in Algorithm 3. Other pairing computation procedures, like Kown-BGOS algorithm can be modified by same procedure in order to defend it against fault attack. 
Input P = (x 1 , y 1 ), Q = (x 2 , y 2 ) Output f P (φ(Q)) ∈ µ l ⊂ F * q 6 Process 1 Choose r 1 ∈ R Z * q 6 , and r 2 ∈ R Z, r 2 ≤ m 2 f 0 ← r 1 , f 1 ← 1 3 m ← m + r 2 4 For i = 1 to m do 5 x 1 ← x 3 1 , y 1 ← y 3 1 6 µ ← x1 + x2 + b 7 λ ← −y 1 y 2 σ − µ 2 8 g ← λ − µρ − ρ 2 9 f 1 ← f 1 .g 10 j ← (i == m) 11 f0 ← fj 12 x2 ← x 1/3 2 , y2 ← y 1/3 2 13 End For 14 Return f q 3 −1 0 Correctness: Theorem 1. 
m ← m + r 2 , r 2 ∈ R Z and r 2 ≤ m. 
 It runs for a random number of iterations. However , the intermediate pairing result f 1 is restored into f 0 at the m th iteration only. It is not restored for other iterations. At the end of the execution, i.e. after m iterations f 0 holds the pairing result of m iterations. Hence, the algorithm produces the correct reduced Tate pairing result. Security Against Fault Attack: Security Assumption. The adversary inject random fault into the loop boundary. But the faulty loop boundary value is not known to the adversary. 
Theorem 2. The modified Duursma-Lee algorithm 
against fault attack proposed in 
 5 Fault Attack on Pairing Computation in Edwards Coordinates 
This section attempts to analyze the security of pairing computation in Edwards coordinates that is defined by Ionica and Joux 
Attack Procedure
The fault attack defined in 
on K = (X 1 , Y 1 , Z 1 ) gives 2K = (X 3 , Y 3 , Z 3 )
, and the formulas are: 
X 3 = 2X 1 Y 1 (2Z 2 1 − (X 2 1 + Y 2 1 )), Y 3 = (X 2 1 + Y 2 1 )(Y 2 1 − X 2 1 ), Z 3 = (X 2 1 + Y 2 1 )(2Z 2 1 − (X 2 1 + Y 2 1 )). 
Similarly, during addition K is updated by K + P , which is even more complex than doubling 
l 1 = 2X 1 Y 1 (x/y − y/x)(X 2 1 − Y 2 1 )(X 2 1 + Y 2 1 − Z 2 1 ) − 2(X 2 1 − Y 2 1 ) 2 (X 2 1 + Y 2 1 − Z 2 1 ) − dx 2 y 2 Z 2 1 (X 2 1 + Y 2 1 )(2Z 2 1 − X 2 1 − Y 2 1 ) + (X 2 1 + Y 2 1 )(2Z 2 1 − X 2 1 − Y 2 1 )(X 2 1 + Y 2 1 − Z 2 1 ). 
 The Tate pairing E l (P, Q) is computed by Miller's algorithm on points P, Q such that P is an l-torsion point on the curve E(F q ) and Q ∈ E(F q k ). In order to mount fault attack on Miller's algorithm in Edwards coordinates, we assume that the adversary has ability to inject fault at the register l. We further assume that the adversary can obtain the pairing result E l (P, Q) for l = 2. This may be possible by adopting some powerful fault injection procedure or from a number of trial with the help of timing and simple power analysis 
X 0 , Y 0 , x, y
 , and d, which can be deduced from the equation of 
l 1 by replacing X 1 by X 0 , Y 1 by Y 0 , and Z 1 by 1. 
We can assume that the value of d (curve parameter) and Q = (x, y) are known to the attacker. Thus, f has been simplified and represented by the following equation: The requirement of our fault attack is satisfied by inverting the least-significant-bit of l, l
f = a 1 X 6 0 + a 2 Y 6 0 + a 3 X 5 0 Y 0 + a 4 X 0 Y 5 0 + a 5 X 2 0 Y 4 0 +a 6 X 4 0 Y 2 0 + a 7 X 0 Y 3 0 + a 8 X 3 0 Y 0 + a 9 X 2 0 Y 2 0 +a 10 X 4 0 + a 11 Y 4 0 + a 12 X 2 0 + a 13 Y 2 0 , for constants a 1 , · · · , a 13 . Here a 1 , · · · , 
Countermeasure
In order to resist the above fault attack it is ensured that the Miller's algorithm does not produce a valid pairing result for l = 2, and for the condition that i = 1 and l
Algorithm 4: Fault Attack Resistant Miller's Algorithm 
Input P an l torsion point Proof. The modified Miller's algorithm performs correctly for cryptographic pairing computation. It is automati-cally aborted if l is even. It returns zero if least significant bit (LSB) of l is zero, i.e., l
∈ E(F q ), Q ∈ E(F q k ) Output The Tate pairing E l (P, Q) Process 1 i = [log 2 (l)], K ← P, f ← 1. 2 
Security: 
Theorem 4. 
The fault-attack resistant Miller's algorithm is secure against fault attack described in Section 5. 
Proof. The fault attack described in Section 5 believes that the attacker has ability to inject fault at particular variables during execution. It injects fault at variables i and l. In order to mount the fault attack in pairing computation in Edwards coordinate it is necessary to sets i = 1 and l
Conclusion
 The paper has described the security issues of pairing algorithms in presence of fault. It has shown that the existing countermeasures, which are based on the point blinding technique, are not sufficient for resisting fault attack on pairing algorithms. It has proposed a new countermeasure that resists such kind of fault attacks. A weakness of Miller's algorithm in Edwards coordinates in presence of fault has been also described in this paper. The paper has proposed a suitable countermeasure against such an attack. 
"
"Introduction
Secret sharing is a cryptographic primitive which is used to distribute a secret among a group of players. It is simply a special form of key distribution 
1) (A ∈ Γ) ∧ (A ⊆ B) =⇒ B ∈ Γ; 2) (A ∈ ¯ Γ) ∧ (B ⊆ A) =⇒ B ∈ ¯ Γ. 
We assume that Γ only contains the minimal allowed groups which can recover the secret. Similarly, ¯ Γ only contains maximal adversarial groups which cannot recover the secret. Several access structures have been proposed in the literature. The primitive access structure is the (t, n)threshold access structure. In a (t, n)-threshold access structure, there are n shareholders. An authorized group consists of any t or more participants and any group of at most t − 1 participants is an unauthorized group. Threshold schemes are suitable for situations in which each player is assigned the same trust. In most practical situations, the degree of trust assigned to a player can differ based on the authority of the player. Simmons 
ρ i = log 2 |secret| log 2 |share| . 
The information rate ρ of the scheme is defined as ρ = min{ρ i : i is a participant of the scheme}. A well known fact in secret sharing is that the size of a share is at least the size of the secret. Therefore, the information rate of the participant and hence the information rate of the scheme are both bounded between 0 and 1. Schemes with maximum information rate are desir- able 
Our Contribution
Many applications require that secrets be reconstructed in a well-defined order. For example, in banks, a cheque has to be cleared first by the clerk, then by the cashier and finally by the manager. The order has to be strictly enforced. These applications require ordering theory to be introduced into an access structure. It may appear that this problem can be solved by using Multistage secret sharing, but in fact it is not. Refer Section 2.2 for details and Example 1 for a concrete example. To the best of our knowledge, this is the first paper to bring ordering theory into access structures. A formal definition of proposed Level ordered Access structure (LOAS) is presented in the paper. Also, an ideal secret sharing scheme that realizes this access structure is presented. The scheme is similar in spirit to the compartmented secret sharing scheme proposed by Brickell 
Outline of the Paper
 Formal Definition of level ordered access structure is presented in Section 2. The difference between Level ordered secret sharing schemes (LOSS) and other extensions of Shamir secret sharing especially Hierarchial secret sharing are discussed in Section 2. An interesting relationship between generalized access structures and LOAS is discussed in Appendix 4. LOAS and its properties are discussed in Section 3. Section 3 also discusses the modification of LOAS to include a virtual player, which in turn enables to prove the existence of an ideal scheme for the LOAS. In addition, an ideal scheme and the properties of the LOSS scheme especially homomorphic properties are presented in Section 3. Finally we conclude the paper with possible directions for future work in Section 4. 
Formal Definition of LOAS
In LOAS, a set of players are partitioned into different levels and each level is associated with a threshold. Also there is an ordering defined on the levels. During reconstruction , if the players submit shares according to the specified order, then the actual secret should get reconstructed . Formally the proposed Level ordered Access structure is as follows. 
Definition 2.1 Let U be a set of n participants and let U 1 , U 2 , · · · U m be a partition of the set U. Also let b i be a boolean variable, which we call the activation index associated with the i th level U i , 1 ≤ i ≤ m. Define S i , recursively, to be an authorized set corresponding to the i th level if 
1) S i ⊆ U i and |S i | ≥ t i , 
2) ∃ an authorized set (S i−1 ) whose activation index (b i−1 ) is True, where b 0 = T and S 0 = ∅. I.e., there is an authorized set S i−1 of (i − 1) th level and the truth value of the corresponding activation index b i−1 is true. A authorized sets of LOAS are the authorized sets of level m. 
 2.1 Relationship Between LOAS and Hierarchical and Compartmented Access Structures 
 There are a number of related definitions of access structures like Hierarchial and Compartmented access structures . Following arguments (discussion) explains that these access structures are different from the LOAS defined above. 
Definition 2.2 Disjunctive hierarchical access structure is a multipartite access structure in which each level L i is assigned with a threshold t i , 1 ≤ i ≤ m, and the secret can be reconstructed when, for some i, there are at least t i shareholders who all belong to levels smaller than or equal to L i . Mathematically, 
Γ = {V ⊆ U : |V ∩ (∪ i j=1 U j )| ≥ t i , 
for some i ∈ {1, 2, · · · , m}}. 
Definition 2.3 Conjunctive hierarchical access 
structure is a multipartite access structure in which each level L i is assigned with a threshold t i for 1 ≤ i ≤ m, and the secret can be reconstructed when, for every i, there are at least t i shareholders who all belong to levels smaller than or equal to L i . Mathematically, 
Γ = {V ⊆ U : |V ∩ (∪ i j=1 U j )| ≥ t i , 
for every i ∈ {1, 2, · · · , m}}. Note that in Hierarchical secret sharing, players can be taken from lower levels and this is not permissible in LOAS. Also LOAS defines a sequence of levels where lower levels have to submit their shares before higher levels, whereas such requirement is absent in hierarchical secret sharing. 
Definition 2.4 
Compartmented access structure is a multipartite access structure in which each compartment is assigned with a threshold t i , 1 ≤ i ≤ m, and the secret can be reconstructed when, for every i, there are at least t i shareholders from U i and a total of at least t 0 participants from all the compartments. Mathematically, 
Γ = {V ⊆ U : |V ∩ U i | ≥ t i , for every i ∈ {1, 2, · · · , m} and |V | ≥ t 0 }. 
where t 0 ≥ m i=1 t i . 
Compartmental secret sharing and LOAS bear a similarity. In fact, we'll see in Section 3 that the elementary access structure in Level ordered access structure is a Compartmented access structure. There is no concept of ordering among the compartments in a Compartmented access structure. 
 2.2 Relationship Between Multistage Secret Sharing and LOAS 
In a Multistage secret sharing (MSS) scheme 
We would like to call the above traditional method of multistage secret sharing as "" Loose sequential secret sharing "" as a secret at level L i may be computed without the knowledge of the secret at level L i−1 . Also this method supports parallel secret reconstruction. The LOAS secret sharing scheme described in this paper can be called as "" Strict sequential secret sharing "" as the secret at level L i requires the knowledge of secret at level L i−1 (See Section 2.3 for our idea of realizing LOAS). It is straightforward to infer that strict sequential secret sharing cannot support parallel secret reconstruction. 
Realization of LOAS: An Overview
This section proposes an overview on the realization of LOAS. Specific implementation of the scheme is given in Section 3. In our implementation, a partial secret s i is associated with each level L i . The partial secret in the last level is the actual secret of the scheme i.e s m = s. The players at level L i are allowed to reconstruct the partial secret s i only after the players at level L i−1 have reconstructed the partial secret s i−1 . 
Realization of Level Ordered Access Structure
In this section, the properties of LOAS are examined and a scheme which realizes the Level ordered access structure is given. 
Virtual Player
A way of realizing the level ordered access structure is by adding a virtual player at each level except the first level. The partial secret at each level acts as share of the virtual player in the next level. The virtual player along with the threshold access structure of that level forms the modified access structure at that level. The addition of virtual player ensures that the secrets are reconstructed in specified order. We define an elementary access structure for a level L i to be the conjunction of a virtual player(P i ) and a (t, n) threshold access structure. For example, if a level L i is associated with a (2,3) threshold access structure for players P = {P 1 , P 2 , P 3 } and the virtual player of the level is P then the modified elementary access structure is 
Γ = P (P 1 P 2 + P 1 P 3 + P 2 P 3 ) = P P 1 P 2 + P P 1 P 3 + P P 2 P 3 . 
 One of the widely studied properties of the access structures is whether an ideal scheme exists for a given access structure or not. The following Theorem 2, establishes that the elementary access structure is an ideal access structure. Proof of this theorem is based on the the following theorem, which talks about the existence of an ideal scheme of an access structure, is due to Stinson 
) 
for 1 ≤ i ≤ n, where x i is the x-coordinate given to P i . Also, let φ(D) = (1, 1, 0, · · · , 0) φ(P ) = (1, 0, 0, · · · , 0). Without loss of generality, let (P i1 , P i2 , · · · , P it , P ) be an authorized set. Also let a 1 , · · · , a t , a be the coefficients chosen from GF (q). Hence, 
φ(D) = a 1 φ(P i1 ) + a 2 φ(P i2 ) + · · · + a t φ(P it ) + a φ(P ) (1) 
(1, 1, 0, · · · , 0) = t j=1 a j (0, 1, x ij , x 2 ij , · · · , x t−1 ij ) 
+ a (1, 0, 0, · · · , 0). 
(2) 
It can be easily seen from that a = 1. The remaining set of equations can be expressed in matrix form as follows: 
       1 1 · · · 1 x i1 x i2 · · · x it x 2 i1 x 2 i2 · · · x 2 it . . . . . . . . . x t−1 i1 x t−1 i2 · · · x t−1 it        ×        a 1 a 2 a 3 . . . a t        =        1 0 0 . . . 0        
Since, the coefficient matrix is a Vandermonde matrix, its determinant is non-zero. So, the system has a unique solution. That is the vector (0, 1, 0, · · · , 0) can be expressed as a linear combination of the vectors of an authorized set. 
Proposed Scheme
 A look at virtual player concept reveals that each elementary access structure has two compartments. The first compartment is a (t, n) threshold access structure, and the second compartment has only a virtual player. We denote the j th (j = 1, 2) compartment of a level L i with L ij . So our scheme may be visualized as in the following block diagram. 
Block Diagram
The LOAS can be shown in the form of a block diagram as shown below. 
L 11 L 21 L 31 L 22 L 32 Level L 2 Initialize secret 
In the block diagram, the AND gate symbol is generic and it can be replaced with an XOR gate or an Adder (provides boolean addition of the two inputs) etc. In our algorithms below, we consider it to be an adder. Let F q be the ground field from which the shares and the secrets are chosen. Given the secret, the Algorithm Share assigns partial secrets to the levels of the access structure and subsequently to the players in the levels. 
Algorithm Share
Let s be the secret, and i be the level index. Choose 
s 1 , · · · , s m , so that s m = s. 
1) Initialize the partial secret of the last level s m to s and level index i to m. 
2) For each level L i (i > 1) with partial secret s i do the following a. Assign s i−1 be the share of the virtual player at level i. Shares are assigned to the players in level L i1 based on Shamir's scheme 
b. Decrement the level index by 1 so that i becomes 
i − 1. 
3) Assign shares to players in level L 1 based on Shamir's scheme 
Algorithm Reconstruct
1) The Shamir secret sharing scheme is used to generate partial secret, s 1 from the level L 1 . The generated partial secret is the share of the virtual player in next level L 2 . Initialize level index i to 2. 
2) For each level L i do the following a. The Shamir secret sharing scheme is used to generate secret from the first compartment L i1 , which is added with the share of the virtual player to generate the partial secret of level L i . The generated partial secret is the share of the virtual player in the next level, i.e., the share of 
L (i+1)2 . 
b. Increment the level index i to become i+1, if i < m. Otherwise, return the partial secret of level L m . This partial secret is the desired secret. 
Remark: Note that there are two key operations in the proposed scheme. The first one is assigning the secret of stage i(i < n) as the share of the virtual player in stage i + 1 and the other one is addition of partial secrets of stages i and i + 1 to provide the secret of stage i + 1. Both these operations are required to ensure ordering in the proposed scheme. Two examples are provided, each of which, tries to construct a scheme with only one of the above operations and fails to enforce the ordering. 
 Example 1 (Considers only addition operation and excludes virtual player) Suppose that there are x stages and the order of secret reconstruction is (s 1 , · · · , s m ) from left to right. The actual secret s is recovered only if the partial secrets are recovered in the specified order. Let s m = (t, n) Shamir (s m ) + s m−1 , where s m is the partial secret recovered by stage m using Shamir secret sharing and s 1 = Shamir (s 1 ). From the definition of LOAS we have 
s = s m = (t, n)Shamir(s m ) + s m−1 = (t, n)Shamir(s m ) + (t, n)Shamir(s m−1 ) + s m−2 = (t, n)Shamir(s m ) + (t, n)Shamir(s m−1 ) + · · · (t, n)Shamir(s 1 ). 
Note that the final secret is simply the addition of the partial secrets of all the stages. So the actual secret can be constructed by any of the possible n! permutations with n stages. But according to the definition of LOAS, the secret should be recovered only if the partial secrets are reconstructed in the specified order. 
Lemma 1 The proposed scheme is perfect. 
 Proof: It follows directly from the reconstruction algorithm that an authorized set can recover the secret. Any maximal unauthorized set B consists of 
m i=1 t i − 1 
players, where t i is the threshold of the level L i . So there exists a level L j such that the number of corroborating players from that level fall below the threshold i.e., B ∩ L j < t j . To find the partial secret of the level L j1 , we need t j equations. But, the players from the level L j provide a maximum of t j − 1 shares. As the number of unknowns are less than the number of equations, there exists infinitely many solutions(i.e., |F q |) for the secret value. Hence any maximal unauthorized set cannot obtain any information about the secret.  Proof: We prove the theorem by the induction on levels . If there is only one level, the reconstruction algorithm returns the secret of the first level and terminates. Let the partial secret be recovered correctly for the k th level (induction hypothesis). As per the construction, the first compartment of level k+1 implements Shamir secret sharing and provides the first input to the adder. As per the induction hypothesis, the second input is provided by the partial secret of the k th level. Now the Adder can reconstruct the partial secret s k+1 . Hence, the partial secret in level L k+1 is reconstructed only after the partial secret in level L k . 
Properties of LOSS
 3.6.1 Comparison with the Compartmental Access Structure 
As can be seen from the virtual player concept that each elementary access structure other than the one at first level is a compartmental access structure with two compartments . The first compartment is a (t, n) threshold, the second compartment is a (1, 1) threshold and the global threshold is t + 1. Note that sum of the individual thresholds is the global threshold. The elementary access structure in LOAS is a special case of the compartmental access structure in which sum of the individual thresholds is the global threshold. 
LOSS is a Prepositioned Scheme
Prepositioned schemes 
Privacy. It should be possible to preposition all of the private information needed for the shared control subject to the condition that even if all of the participants were to violate the trust of their position and collaborate with each other, they would have no better chance of recovering the secret information than an outsider has of guessing it. 
Activation. It should be possible to activate the shared control scheme once it is in place by communicating a single share of information, and for many applications , it should also be possible to reveal different secrets (using the same prepositioned private pieces of information) by communicating different activating shares of information. LOSS is one of the best examples of prepositioned secret sharing schemes. The partial secret at level L i is reconstructed only after the partial secret at level L i−1 is reconstructed. The partial secret at level L i−1 together with the activation index acts as activation information for the players at level L i (Activation property). Without the partial secret at level L i−1 , the players at level L i would have no better chance of recovering the secret information than an outsider has of guessing it (Privacy property). 
Homomorphic Property of LOSS
The Homomorphic property of a secret sharing scheme allows to reconstruct the composition of secrets from the composition of corresponding shares without revealing anything about the individual secrets. Recovery of the partial secret at each level L i in the reconstruction alogrithm of LOSS scheme comprises of two steps. 
1) Shamir reconstruction algorithm to reconstruct the secret of the first compartment L i1 ; 
2) Addition of secrets of levels L i−1 and L i2 to calculate the secret of the level L i . 
Shamir's scheme is homomorphic with respect to (+, +) 
Conclusion
 This paper proposed an access structure that closely resembles the known access structures such as conjunctive hierarchial access structure and compartmental access structure. We call the proposed access structure as the Level Ordered Access Structure(LOAS). Unlike existing access structures; wherein there is no concept of ordering, LOAS enforces ordering and it is a sequence of threshold access structures. It is easy to visualize applications of LOAS in variety of areas such as software testing, prepartion of cheques, drafts in banks etc. The paper presented a formal definition of LOAS and a model for realizing LOAS. The paper also analyzed the existence of an ideal scheme for the proposed LOAS and presented an ideal scheme for the same. The side affects of cheating 
V. Ch. Venkaiah obtained his PhD in 1988 
from the Indian Institute of Science (IISc), Bangalore in the area of scientific computing. He worked for several organisations including the Central Research Laboratory of Bharat Electronics, Tata Elxsi India Pvt. Ltd., Motorola India Electronics Limited, all in Bangalore. He then moved onto academics and served IIT, Delhi, IIIT, Hyderabad, and C R Rao Advanced Institute of Mathematics, Statistics, and Computer Science. He is currently serving the Hyderabad Central University. He is a avid researcher. He designed algorithms for linear programming, subspace rotation and direction of arrival estimation, graph colouring, matrix symmetriser, integer factorisation, cryptography, knapsack problem, etc. Allam Appa Rao is a Director at CR Rao Advanced Institute of Mathematics, Statistics, and Computer Science (AIMSCS), University of Hyderabad Campus, Hyderabad . He was the first to receive Ph.D from Andhra University in Computer Engineering in the year 1984. During his more than four decades of professional experience , such as First Vice Chancellor, JNTUK, Kakinada, A.P, Principal, College of Engineering (Autonomous), Andhra University. He shared his wisdom with fellow engineers and scientists across the globe through his innumerable research papers published in international journals and international conference proceedings. Indian Science Congress Association (ISCA) conferred him with "" Srinivas Ramanujan Birth Centenary Award "" Gold medal for his significant and life time contribution to the development of Science and Technology in the country specifically in the area of Computational Biology, Software Engineering and Network Security. 
"
"Introduction
Wireless Mesh Networks (WMN) 
 In this paper, we present SEAODV, a security enhanced version of AODV. We utilize PTK and GTK keys to protect the unicast and broadcast routing messages respectively to ensure that the route discovery process between any two nodes in WMN is secure. We apply BLOM's key pre-distribution scheme in conjunction with the enhanced HELLO message to establish the PTK and use the established PTK to distribute GTK to the node's one-hop neighbors throughout the entire network. 
We also identify various attacking scenarios specifically happened in AODV and present security analysis to prove that our proposed SEAODV is able to effectively defend against most of those identified attacks. Our Scheme is lightweight and computationally efficient due to the symmetric cryptographic operations (e.g., MAC). In addition, SEAODV supports a hop-by-hop authentication as well. 
The rest of the paper is organized as follows. Section 2 discusses related work. In Section 3, we provide background knowledge of Blom's key pre-distribution scheme. Section 4 gives a brief overview of standard AODV and two well known secure routing protocols in MANETs, named SAODV and ARAN. Details of our SEAODV protocol will be presented in Section 5. Section 6 identifies various potential attacking scenarios in AODV and presents the security analysis. The performance evaluation is explained in Section 7. Finally, Section 8 concludes the paper. 
Related Work
 So far, there has been tremendous research on layer 3 secure routing for wireless networks. Each secure routing protocol is tailored to a specific type of wireless networks, such as ad hoc networks or wireless sensor networks. All of them have similar properties, and thus some routing protocols of ad hoc networks can be applied to wireless mesh networks. However, they may not provide specific security features (such as hop-by-hop authentication) for mesh networks and still vulnerable to various types of routing attacks such as flooding, route re-direction, spoofing etc. Depending on when routes are required to be calculated , routing protocols can be divided into two categories: proactive routing and on-demand routing. In proactive routing, every node maintains one or more tables containing routing information to every other node in the network. All nodes in the network update their tables to maintain a consistent and up-to-date view of the network whenever the network topology changes or a node's routing table is updated. Example of proactive routing is Link Quality Source Routing (LQSR) 
In HWMP 
Blom'S Key Pre-distribution Scheme
Blom's key pre-distribution scheme in 
Overview of AODV, SAODV and ARAN
 SEAODV is based on the standard AODV. ARAN (Authenticated Routing for Ad hoc Networks) and SAODV (Secure Ad hoc On-demand Distance Vector) are two well known secure routing protocols for Ad hoc networks. Both of them are also based on AODV, although ARAN presents different message format in route discovery pro- cess. 
Standard AODV
 The Ad Hoc On-Demand Distance Vector (AODV) algorithm is an on-demand reactive routing protocol, which means it seeks for routes only when required. AODV makes use of sequence numbers to avoid forming routing loops. The standard operations of AODV are described as follows. When a source node wants to communicate with a destination node, it broadcasts a RREQ (Route Request) message to all its one-hop neighbors if it cannot find an active route in its routing table. Upon receiving the broadcasted messages, its neighbors check their routing tables and see whether there exists a route to the destination node. If not, they will forward the RREQ message to their neighbors until the RREQ reaches the designated destination or an intermediate node know the route to the destination. In that case, the destination node or the intermediate node updates its reverse route to the source node from which they received the RREQ, generates a RREP (Route Reply) message and unicasts it back to the source node. When the source node or the intermediate node receives a RREP message, they update their forward route to the destination, use the neighbors from which they receive their RREP and update their routing tables accordingly. To maintain connectivity information, each node detects possible link breakage to its immediate neighbors with periodical HELLO message. In the case a broken link is detected for the next hop of an active route, a RERR (Route Error message) is sent to all its neighbors who are using that specific route. 
SAODV
SAODV is a secure version of AODV. SAODV uses hash chains to secure hop count field and digital signatures to protect the non-mutable fields in both RREQ and RREP messages. The following explains how SAODV works in detail. During route discovery process, a random seed number is generated by the source node and the maximum hop count (MHC) value is set to be the Time-To-Live (TTL) value from the IP header. Source node then computes the hash value Hash = h(seed) and T OP Hash as h MHC (seed). Upon receiving an RREQ message, an intermediate node verifies whether T OP Hash equals to h MHC−Hopcount (Hash). If the two values are completely identical, the hop count is presumed to be unaltered. Moreover, before rebroadcasting the RREQ to its onehop neighbors, the intermediate node increases the hop count by one and computes the new hash value h(Hash). Except the hop count field, all other fields in the RREQ are considered to be non-mutable and secured by using digital signature. When RREQ meets the destination, destination generates a RREP in the same way towards to the source. 
ARAN
The entire routing message in ARAN 
Blom's key pre-distribution scheme and enhanced HELLO message to compute the pairwise transient key (PTK), which subsequently being used to distribute the group transient key (GTK). PTK and GTK are used to secure the unicast and broadcast routing messages respectively. 
Use of Keys
SEAODV is built on the existing AODV. Choosing AODV as our protocol's foot stone is due to its simplicity, maturity , popularity and availability in the research over the past few years. SEAODV requires each node in the network to maintain two key hierarchies. One is the broadcast key hierarchy, which includes all the broadcast keys from its active one hop neighbors. The other hierarchy is called unicast hierarchy and it stores all the secret pairwise keys that this node shares with its one hop neighbors . Every node uses keys in its broadcast hierarchy to authenticate the incoming broadcast routing messages (e.g., RREQ) from its one hop neighbors and applies secret pairwise keys in the unicast hierarchy to verify the incoming unicast messages such as RREP. 
Enhanced HELLO Message (HELLO RREQ, HELLO RREP)
In AODV, HELLO message 
Exchange Public Seed G and GTK by Using Enhanced HELLO Message
During the key pre-distribution phase, every legitimate node in the wireless mesh network knows and stores its public known Seed G (seed of the column of public G matrix ) and the corresponding private row of the generated A matrix. The entire exchange process can be depicted in the following three major steps. 
Step 1: Exchange of Seed G of public G matrix. When node A wants to exchange its Seed G with its one-hop neighbors, it looks for its public Seed G from its key pool, and broadcasts the enhanced HELLO RREQ message. Node B will do the same as A if B is A's one-hop neighbor. Upon finishing step 1, every node in the network possesses the public Seed G of all its one-hop neighbors. 
Step 2: Derivation of PTK (Pairwise Transient Key). Each node uses the Seed G it received from its neighbors and the node's corresponding private row of matrix A to compute PTK. Upon finishing Step 2, every node has stored the public known Seed G of its neighbors and derived the PTK it shares with each of its one-hop neighbors. 
Step 3: Exchange of GTK (Group Transient Key) through HELLO RREP. Upon receiving HELLO RREQ from node A, node B encrypts GT K B with its private P T K B and unicasts the corresponding HELLO RREP message back to A. The encrypted GT K B is also attached in the unicast HELLO RREP message. Once A receives HELLO RREP from B, A applies its private P T K A to decrypt the GT K B and stores it in the database. The same process applies to node B as well. Eventually, every node possesses the GTK keys from all its one-hop neighbors and the group of secret pairwise PTK keys that it share with each of its one-hop neighbor. 
Securing Route Discovery
In order to implement a hop-by-hop authentication, each node must verify the incoming message from its one-hop neighbors before re-broadcasting or unicasting the message . The trust relationship between each pair of nodes relies on their shared GTK and PTK keys, which have been obtained during the key exchange process. Route discovery process of SEAODV is similar to that of standard AODV, but a MAC extension is appended to the end of the AODV routing message. The new format of the RREQ in SEAODV is given in 
Securing Route Setup
Eventually, the RREQ message reach the destination or an intermediate node which has a fresh route to the destination . The destination node or an intermediate node can generate a modified RREP and unicast it back to the next hop from which it received the RREQ towards to the originator of the RREQ (the source). Since RREP message is authenticated at each hop due to the use of PTK keys, adversary has no opportunity to re-direct the traffic. Before unicasting the modified RREP back to the originator of the RREQ, the node first needs to check its routing table to identify the next hop from which it received the broadcast RREQ; then the node applies the PTK key it privately shares with the identified next hop to compute the M AC(P T K, M ) and affixes this MAC to the end of RREP as shown in 
Securing Route Maintenance
 In SEAODV, a node generates a RERR message if it receives data packet destined to another node for which it does not have an active route in its routing table or the node detects a broken link for the next hop of an active route or a node receives a RERR from a neighbor for one or more active routes. The format of a modified RERR message is shown in 
Attacking Scenarios in AODV and Security Analysis
This section presents possible attacks launched in AODV during a route discovery process and compare the security analysis results of our SEAODV with ARAN, SAODV and LHAP. 
Attacking Scenarios in AODV
RREQ Flooding Attack Flooding is one of the simplest attacks that a malicious node could have launched. An attacker tries to flood the entire network with RREQ message destines to a known or an unknown address. As a consequence, this causes a mass of unnecessary broadcasts and force the neighbors to process these flooding route requests, the aim is to consume the energy of the nodes in the network and the network bandwidth. Therefore, the whole network communication may be breakdown and the throughput is dropped dramatically. 
RREP Routing Loop Attack A routing loop is a path that goes through the same nodes over and over again. As a consequence, this kind of attack will deplete the resources of every node in the loop and cause the isolation of the destination and few packets can eventually reach the destination. Both RREQ flooding and RREP routing loop attacks are also called Denial-of-Service (DoS) attacks. DoS attacks do not intend to destroy the data message, but try to consume and compromise the scarce resource that available to nodes in the network. They can even disrupt the usability of the network.  These mutable fields include hop count, sequence numbers and other metric related fields. A malicious node M could divert the traffic through itself by advertising a route to the destination with a larger destination sequence number (DSN) than the one it received from the destination. In case B, route re-direction attack maybe launched by modifying the metric field in the AODV routing message, which is the hop count field in this case. A malicious node M simply modifies the hop count field to zero in order to claim that it has a better path to the destination. Both Case A and Case B belong to the category of modification attacks in AODV. Fabrication Attack Adversary may fabricate the routing messages to disorder the routing decisions. For instance, a malicious node could simply fabricate a route error message in AODV protocol, this will put all the upstream nodes in the network into a very embarrassment situation since all of them now believe that a certain number of destination are unable to reach. This may result in these upstream nodes to re-initiate a route request to those unreachable destinations so as to discover and build another possible route to them. This brings the energy consuming issue on the table again and significantly degrades the performance of the routing protocol. 
Route Re-direction Attack 
Tunnelling Attack 
In Ad hoc network, a node can be located adjacent to other nodes. A tunnelling attack is referred to two or more malicious nodes in the network may collude and cooperate with each other to encapsulate and exchange routing messages between them by either using the existing data routes or potentially high power transceiver 
Security Analysis
We analyze our proposed SEAODV in terms of defending against those attacking scenarios presented in Section 6.1 and compare the security analysis results against other three secure routing protocols: ARAN, SAODV and LHAP. RREQ Flooding ARAN suffers badly from continuously verifying digital signatures, while SAODV also incurs massive overhead in signature verification process. Contrarily, LHAP offers better immunity due to its light-weight nature by using one-way hash chain and only authenticates RREQ from its one-hop neighbors. The number of hash operations required to verify the authenticity of a message is from single hash operation up to maximum number of tolerance in terms of packet loss. SEAODV only authenticates RREQ from nodes that are in the list of its active one-hop neighbors. Hash operations are required in SEAODV and re-creation of MAC is simple, fast and one time only. 
RREP Routing Loop 
In ARAN, every transmission of signed routing message makes impersonation and modification of sequence numbers impossible. SAODV does not support hop-byhop authentication. It is based on source-destination authentication and any intermediate nodes could be impersonated by any chance during the flying of RREP. LHAP uses one-way hash chain to protect the message by simply appending traffic key right after the raw message. Malicious node can simply block the wireless transmission between two neighboring nodes, modifies the messages, put the corresponding intercepted traffic keys right after the messages and send them back to the wireless channel. SEAODV is a hop-by-hop authentication . GTK and PTK keys are used to secure the broadcast and unicast routing messages respectively. The entire routing message is MACed, therefore, possibilities of impersonation and modification are eliminated. 
Route Re-direction 
Both ARAN and SEAODV can defeat this type of attack. ARAN employs digital signature to sign every single routing message in a hop-by-hop fashion, while in SEAODV, GTK and PTK keys are used to compute the MAC, which secures all the fields in the entire routing message. SAODV cannot effectively prevent the metric field (in this case hop count) from being increased by malicious nodes. This increases the chances of the route being de-selected from the potential candidate routes, which is another form of route re-direction attack. In LHAP, again malicious nodes can use the exact technique 
Formation of Routing Loops 
Two conditions need to be satisfied in order to launch this attack. The malicious node has to impersonate a legitimate node in the network and is able to modify the metric such as hop count to be a better value in terms of less hop count in this case. SAODV is able to prevent the hop-count from decreasing, hence avoiding this attack. ARAN and SEAODV can also defeat this type of attack due to its hop-by-hop authentication. However, in LHAP, as long as the malicious node gets a chance to intercept the effective traffic keys and re-use them in a timely manner, there is a possibility to launch this type of attack. RERR Fabrication In ARAN, messages can only be fabricated by nodes with valid certificates and ARAN offers non-repudiation. Nodes keep sending fabricated routing messages might get excluded from the future route computation. While in SAODV, malicious node may simply impersonate nodes other than the one initiates the original RERR and forward the signed RERR to other nodes in the network. By doing do, malicious nodes can not only deplete the energy of the nodes, but also successfully defeat the routing protocol. LHAP also suffers from this type of attack; malicious node could use the captured traffic key to be attached after the modified RERR as long as the captured traffic keys are still "" fresh "" enough to be authenticated by the receivers. SEAODV experiences least negative impact against this attack since a receiving node only authenticates the RERR that comes from its active one-hop neighbors. This forces malicious node can only forward the replayed RERRs come from the receiving nodes' one-hop neighbors in order to launch this type of attack. Tunnelling ARAN uses the total time consumed in seeking a route as physical metric. It does not guarantee the shortest path in terms of hop count, but does offer the quickest path. This is still not enough to defeat the tunnelling attack because malicious nodes can simply adopt high-power gain transceiver to tunnel the routing messages such as RREP in order to make the source believe that the "" tunnelled path "" is the quickest one. As a consequence, malicious nodes would have been included on the final route towards destination and gained all the subsequent data packets passed through them. Similar methodology would be taken by malicious nodes to launch this attack on SAODV and SEAODV with the difference that now the actual routing metric is misrepresented in terms of hop counts. LHAP only authenticates messages from its one-hop neighbors, it makes tunnelling attack become more tougher to be launched since malicious nodes now have to intercept the "" fresh enough "" traffic keys at both ends of the tunnel. 
 Summaries for each routing protocol in terms of defending against those identified attacks are presented in 
Performance Evaluation
Performance evaluation is presented to prove that SEAODV is superior against ARAN and SAODV in terms of computation cost and route acquisition latency. 
Computation Cost
Computational cost is measured and computed at every node in the network. Since every node in the wireless mesh network can be looked upon as both the sender and the receiver, the total computation cost incurred at each node is going to be the cost of this node being as a sender plus the cost of the node being as a receiver. This methodology is applied to the evaluation of the computation cost for the three secure routing protocols: ARAN, SAODV and SEAODV. Variables and notations used for computing computation and communication cost are shown in 
1) ARAN ARAN is a hop-by-by secure routing protocol for Ad hoc networks; it uses public key cryptography to guarantee integrity of routing message. However, a major drawback of ARAN is that it requires extensive signature generation and verification during the routing discovery process. In ARAN, all computation cost experienced at each hop comes from the extensive signature generation and verification. To be more detailed, during the routing discovery process , each sender generates its own signature and uses it to sign the entire routing message before sending it back to the wireless channel. Once the message is received on its fly to the destination, the receiver has to verify the signature(s) first before updating its routing table. According to the operation of ARAN, receivers are required to be classified into two different categories. Receivers that are only one-hop away from the originator of the RREQ or RREP fall into the first category and those are more than one-hop away are referred to the second category. The reason is that receivers in different category incur different computational cost in terms of number of signature verifications. Being a receiver in the first category, which means node is only one-hop away from the originator of RREQ or RREP, node is required to do two times of signature verifications if the routing message comes from the originator of the RREQ or RREP. The first signature verification is used for verifying the certificate of the originator of RREQ or RREP and obtaining the public key of the originator. The second one is required to verify the signature of the originator by using the public key of the originator . However, the node still needs to perform four times of signature verifications should the routing message come from node other than the originator of RREQ or RREP. In contrast to the node being a receiver in the first category, node in the second category experiences four times of signature verifications when receives a RREQ or RREP from its onehop neighbor. In addition to two times of signature verifications described in the last paragraph, another extra two times of signature verifications are a must due to the verification of both certificate and signature of the node from which it receives the RREQ or RREP. 
 Now the computation cost in terms of number of signature generation and verification can be derived and given below. 
Signature Gen 
(
× Signature
(Receivers that are more than one-hop away from the originator of RREQ or RREP) 
Therefore, the computation cost for an established route of N nodes between source S and destination D is expressed as 
−Hop Count ) +2 × Signatue V er 
The equation indicates that as the total number of nodes on the finalized route increase, more hash operations and signature verifications are required to be performed during the route set up process. 
3) SEAODV The computation cost of SEAODV is simple and straightforward in contrast to that of ARAN and SAODV. In SEAODV, the computation cost involved to every node on the route is exactly the same whenever the node acts to be a sender or a receiver. The computation cost for a finalized route with N nodes can be deduced 2×(N −2)×2×M AC +2×2×M AC. In this equation, our scheme only involves the operation of MAC and with no signature at all. 
In this part, three secure routing protocols (ARAN, SAODV and SEAODV) are evaluated in terms of computation cost. The computation cost for computing signature, hash operation and MAC are listed in By applying the cryptographic costs from 
@BULLET Computation cost is computed in terms of timing expense in millisecond, indicates that SEAODV performs excellent in authentication latency due to its efficient cryptographic operation. 
Communication Cost
ARAN, SAODV, and our SEAODV are similar in the way of discovering routes. RREQ is broadcast by the originator of the on-demand node towards the destination . Upon receiving the RREQ, destination unicasts the RREP back to the source from which the RREQ is generated by using the reverse path which has been set up during the flooding of the RREQ. All of these routing protocols apply the same methodology in their routing mechanisms. Therefore, their communication costs (the number of routing messages) are the same. However, ARAN, SAODV, and SEAODV experience various number of control bytes within RREQ and RREP. The more number of control bytes incurred in a single routing message , the larger the entire routing message. Therefore, routing message with bigger size in terms of bytes tends to have a lower probability of successful reception at the destination and suffer longer delay. Before computing the latency produced by the communication overhead for each of the routing protocols mentioned above, the following assumptions are made: 1) Network throughput is 400 Kbps for a single flow 
3) In ARAN, the size of RREQ or RREP generated by the source or destination is smaller than those forwarded by intermediate nodes, which include two signatures and two certificates. While the RREQ or RREP originates by either the source or destination is only comprised of one signature and one certificate. Presume that the route discovery packet (RDP) in ARAN is the same size as that of used in AODV, which is 24 bytes. Therefore, for RREQ and RREP with single signature and certificate, the total size is 312 bytes which is the same as that of in SAODV given below. For RREQ or RREP with double signatures and double certificates, the total size is extended to 568 bytes; 4) In SAODV, 312 bytes in total for both RREQ and RREP, which include original AODV message (24 bytes), signature (128 bytes), top hash (16 bytes), hash (16 bytes) and certificate (128 bytes); 
5) In SEAODV, there are totally 40 bytes for either RREQ or RREP. The AODV message costs 24 bytes and the HMAC is 16 bytes. 
There are two routing messages in ARAN with single signature, others are double signatures. The total number of bytes required to be transmitted in order to ensure a secure route set up is bytes. In SAODV, all routing messages are the same size, hence the total number of bytes required 568×(N −2)×2+312×2 to be transmitted is 312 × (N − 2) × 2 + 312 × 2 bytes. Similarly, the total number of bytes of SEAODV incurred during the route set up process is 40×(N −2)×2+40×2 bytes. Therefore, the number of RREQ and RREP incurred during the entire route setup process can be derived for ARAN, SAODV and SEAODV. For an established route with N nodes on it, the total number of RREQ and RREP is (N −2)×2+2. The number of bytes that are required to be transmitted in order to safely setup a route for ARAN, SAODV and SEAODV can be computed. The communication cost of transmitting those required bits can be calculated below:  Now the average route acquisition latency can be derived by using the following equation: Average Route Acquisition Latency 
= Computation Cost + Communication Cost 
The following Tables 4, 5, and 6 summarize the total cost required to safely setup a route between a source and a destination for the three routing protocols in terms of route acquisition latency in millisecond.  The average route acquisition latency (the total computation and communication costs) is defined as the average delay between sending a RREQ packet by a source for discovering a route to a destination and the receipt of the first corresponding RREP. 
"
"Introduction
 A botnet is a coordinated group of compromised machines controlled via Command & Control (C&C) communication channels that are connected to some C & C servers/peers and managed by botmasters/botherders. It can be used in performing various malicious activities like sending spam mails, distributed denial-of-service (DDoS) attacks, phishing attacks and click frauds 
1) Bots exchange binary and configuration updates with each other. P2P Zeus bots check the responsiveness of their neighbors every 30 minutes. Each neighbor is contacted in turn, and given 5 opportunities to reply . If a neighbor does not reply within 5 retries, it is deemed unresponsive, and is discarded from the peer list. During this verification round, every neighbor is asked for its current binary and configuration file version numbers. If a neighbor has an update available, the probing bot spawns a new thread to download the update. 
 2) Bots exchange list of proxy bots, which are designated bots where stolen data can be dropped and command can be retrieved. Additionally, bots exchange neighbor lists (Peer lists) with each other to maintain a coherent network. 
Apart from P2P botnets, a new trend in the evolution of botnets is the rise of botnets that spread through social networking sites. One of the largest social networking botnet is KOOBFACE 
1) A bot is a program and therefore has a limited set of commands and every command issued by a bot in its normal C & C operations is followed by a response from either a server in its hierarchy in the botnet or from some other bot in its peer group. In other word, C & C interactions in P2P botnets must follow a strict command-response pattern and the manner in which a bot responds to a specific set of commands are also more-or-less uniform. 
2) A P2P bot needs to keep itself updated about other bots that are still active in its network and therefore needs to keep communicating with them. 
3) In normal C & C operations, a P2P bot establishes numerous small sessions. More specifically, they keep changing communicating ports for normal C & C interaction or until they lunch attack. Therefore, the number of packets in each of the bot generated flow during normal C & C operation is usually small. 4) We also observe that most of the packets in bot generated flows are small in size i.e. the size of the largest packet in most of the bot generated flows is less than the MTU. This is to keep privacy and to avoid detection by not influencing normal internet services. 5) Among the few packets transferred in a bot generated flow,the largest sized packets are transferred at a specific proportion (usually < 1), whereas, the normal P2P traffic carries most of the packets to the size of MTU. 
 6) Finally, we observed that each bot generates mutually similar communicating flows to its peers in the same P2P botnet. Rest of the paper is organized as follows: Section 2 provides a brief overview of related works. In Section 3, we provide a brief overview of botnet detection problem using network flows and the proposed architectural overview. In Section 4 we discuss our approach for dataset preparation and description of features selected for classification. In Section 5, we briefly describe the fuzzy rule generation algorithms used for botnet C & C traffic classification. In Section 6, we provide a detail analysis of results obtained from our classification models. In Section 7, we elaborate on future works and also the conclusion. 
Related Works
Botnet threats have been continuously growing with adoption of newer technologies and propagation techniques by the bot-masters. Much resiliency has been achieved by recent botnets through migration from purely centralized C & C architecture to a partly or wholly decentralized architecture. New botnets have emerged on other digital devices like mobile phones or Smartphones. Mobile devices could send SMS and MMS to connect to their C & C proxy servers. Emergence of Online Social Network (OSNs) botnets is another recent development. A recently published survey paper 
In our architectural framework shown in 
Feature Selection
Overview of FURIA
FURIA 
I F = (Φ s,L , Φ c,L , Φ c,U , Φ s,U ) 
to represent fuzzy intervals. The trapezoidal membership function for fuzzy sets (or fuzzy intervals) is given by: 
I F (v) df =          1 Φ c,L ≤ v ≤ Φ c,U v−Φ s,L Φ c,L −Φ s,L Φ s,L < v < Φ c,L Φ s,U −v Φ s,U −Φ c,U Φ c,U < v < Φ s,U 0 else          (1) 
Φ c,L and Φ c,U are, respectively, lower and upper bound of the core (elements with membership 1) of the fuzzy set; likewise, Φ s,L and Φ s,U are, respectively, the lower and upper bound of the support(elements with membership > 0). Thus, Φ s,L and Φ s,U are the fuzzy extensions of original RIPPER intervals [Φ c,L , Φ c,U ] that are considered as core. Rules are fuzzified in a greedy way through fuzzification of every antecedent in a rule or in other word, through replacement of sharp boundaries of a rule with soft boundaries. Fuzzification of each antecedent is done by testing all relevant values {x i |x = (x 1 · · · x k ) ∈ D T i , x i < Φ c,L i } as candidates for Φ s,L i and for all values {x i |x = (x 1 . . . x k ) ∈ D T i , x i > Φ c,U i } as candidates for Φ s,U i . Here, relevant data for each antecedent (A i ∈ I i ) is the one considered by ignoring all those instances that are excluded by any other antecedent (A j ∈ I F j ), 
j = i: D i T = {x = (x 1 . . . x k ) ∈ D T |I F j (x j ) 
> 0 for all j = i} ⊆ D T . FURIA being a fuzzy rule generating algorithm is characterized by its core and its support. It is valid inside the core and invalid outside the support; in-between, the validity drops in a gradual way. Apart from having this definite advantage of fuzzy rule generation over other conventional rule generation algorithms such as RIPPER and C4.5, FURIA generates unordered rule set instead of rule lists and provides an efficient rule stretching method to deal with uncovered instances. All these features of the algorithm make it most suitable for rule generation for network security threat detection. 
Results and Analysis
We use WEKA 
Analysis of Rule Sets
Unlike sharp boundaries generated by RIPPER, C4.5 etc. a fuzzy rule is characterized by soft boundaries. Each fuzzy rule consists of two parts: its "" core "" and its "" support "" . For example, one of the rules generated from C & C traffic of Nugache botnet is: (TBLSP in 
Analysis of Classification Results
Final datasets prepared from botnet C & C traffic of the three bots under consideration are being used to build classification models using WEKA machine learning tools. We randomized flow instances in our datasets by passing it through Randomize filter available with WEKA's unsupervised instance filter category. This was necessitated because our original datasets are imbalanced having less normal web flows. While constructing classifier, we used 10-fold cross validation so that there is no over-fitting of our training set. Results of Classification task by any classification algorithm during testing are usually displayed in a confusion matrix. A confusion matrix holds the count of the correct and incorrect classification from each class or the differences between the true and predicted classes for a set of labelled instances. 
Accuracy = T P + T N T P + T N + F P + F N (2) 
Sensitivity = T P T P + F N (3) P ositiveP redictiveV alue(P P V ) = T P T P + F P (4) F alseP ositiveRate = F P F P + T N (5) 
Sensitivity (or True Positive Rate) is the proportion of correctly identified bot flows out of total flows labelled as bot. Similarly, PPV or Precision is the proportion of correctly identified bot flows out of total predicted bot flows. 
 3) We then calculated the percentage of distinct combinations having more than 1000 bytes in LSP for each dataset. We found that none of the packets in Nugache and Waledac datasets carry a payload of greater than or equal to 1000 bytes. For Zeus, the percentage of distinct combinations having more than 1000 bytes in LSP is 0.733% and for Normal the value is 7.64%. From Step 2 we find that Zeus has a significantly higher percentage of distinct combinations compared to Nugache and Waledac. Similarly, from Step 3 we find that Zeus also has a good number of flows with LSP having more than 1000 bytes. Therefore, it is not difficult to ascertain that the classification error rate of Zeus is bound to be more compared to Nugache or Waledac. The accuracy (the rate of correct classification) measure of a classifier is often used for comparison of predictive ability of learning algorithms. However, the accuracy measure completely ignores the probability estimations of the classification systems. Probability estimations generated by most classifiers can be used for ranking instances which gives likelihood estimations of instances and is therefore more desirable than just a classification. The AUC (area under the curve) of the ROC (Receiver Operating Characteristic) curve provides an alternative and better measure for machine learning algorithms by being invariant to the decision criterion selected, prior probabilities and is easily extendable to include cost/benefit analy- sis 
With a varied decision threshold and already obtained number of points on the ROC curve [FP rate = α, TP rate = 1 -β], the area under the ROC curve can be calculated by using the trapezoidal integration as follows: 
AU C = i {(1 − β i .α + 1 2 [(1 − β).α]} (6) 
where, 
(1 − β) = (1 − β i ) − (1 − β i−1 ), α = α i − α i−1 . 
In case of perfect predictions the AUC is 1 and if AUC is 0.5 the prediction is random. We provide a comparative analysis of our classification models using AUC val- ues. 
Conclusions
A fuzzy rule based detection framework for P2P botnets is presented here. The proposed approach leverages on flow level features and packet level features of network traffic to build excellent classification model for P2P botnet C & C traffic. The accuracy achieved by our system is as good as 99.745%, 99.715%, and 99.105% for Nugache, Waledac and Zeus botnet samples respectively. The fuzzy rule based approach is a supervised one and hence can detect known botnet traces only. P2P botnets have distributed C & C architecture and therefore complete annihilation of existing botnets is not easy. However , using our fuzzy rule based classification model, we can track botnet C & C traffic pro-actively as well as with high accuracy. In future, our effort will be to build similar detection model for botnets using other protocols and communication technologies such as social network based botnets, mobile botnets etc. in the above areas and published 126 technical papers in various national and international journals in addition to presentation/publication in several international/national conferences. Till date, he has produced 8 Ph.Ds and research assistance given for 2 Ph.Ds. Presently 11 scholars are pursuing Ph.D work under his guidance. Dr. Ghose is having 8 sponsored projects worth of 1 crore (INR). Dr Ghose also served as technical consultant to various reputed organizations like IIT Chennai , IIT Kharagpur, WRI, Tricy, SCIMST, KELTRON, HLL, Trivandrum. Dr. Ghose can also be reached at mkghosesmu.edu,in; headcse.smit@gmail.com. 
"
"Introduction
The rapid development of processing data technologies and internet applications has improved the ease of access to information online. It also increases the problem of illegal copying and redistribution of digital media. Encryption and Stenography are the two techniques introduced to solve data on line. In 1992, the research suggested to use the watermarking technique in data protection. Nowadays, image watermarking is a protection technology that has attracted a lot of attention. The basic idea of watermarking involves integrating a message into a digital content. This last covers the information to be transmitted in a holder in a way to be invisible and correctly reversible (an algorithm allows the exact extraction of the embedded watermark). Its algorithm requires equilibrium between three constraints: imperceptibility, robustness and embedding capacity 
Mathematical Recall of Radon Transform
Generalized Radon Transform
 In 1917, Radon, Austrian mathematician, defined the theory of Radon Transform. He proved the possibility to reconstruct a function of a space from knowledge of its integration along the hyper-plans in the same space. This theory establishes the reversibility of the Radon transform and the transition between the native function space and the Radon space, or the space of projections 
R(ρ, θ) = +∞ −∞ +∞ −∞ f (x, y)δ(x cos θ + y sin θ − ρ)dxdy, 
where ρ represents the perpendicular distance of a straight line from the origin, and represents the angle between the distance vector and the x-axis. The literature proposed two categories of onedimensional Radon transformation; the first is based on Radial Integration Transform (RIT) and the second is based on the Circular Integration Transform (CIT). 
One-Dimensional Radial Integration Transform
The RIT of a function f (x, y) is defined as the integral of f (x, y) along a straight line that begins from the origin (x 0 , y 0 )and has angle θ with respect to the horizontal axis (see 
R f (θ) = +∞ 0 f (x 0 + u cos θ, y 0 + u sin θ)du, 
where u is the distance from the origin (x 0 , y 0 )and f (x, y) is presented by the integral along a straight line that begins from the origin (x 0 , y 0 ) and has an angle with respect to the horizontal axis. 
One-Dimensional Circular Integration Transform
The CIT of a function f (x, y) is defined as the integral of f (x, y) along a circle curve with center x 0 , y 0 and radius ρ(see 
C f (ρ) = 2π 0 f (x 0 + ρ cos θ, y 0 + ρ sin θ)ρdθ, 
where dθ is the corresponding elementary angle and f (x, y) represents the circle integrated function around the center (x 0 , y 0 )and by the radius ρ. where dθ represents the angular variation step, ∆s is the scaling step, k∆p represents the radius of the smallest circle that encircles the image, J represents the number of samples on the radius with orientation θ, t = 1, ..., 360 ∆θ and k = 1, ..., 360 ∆θ . The Radon transformation of the image I(x, y) of size 
N ρ = √ N 2 + M 2 + 1 M ρ = θmax ∆θ . 
(1) 
The Proposed Watermarking Approach
 The literature suggested that the Radon Transform properties are much recommended in watermarking applications in which resistance to geometric attacks. A watermark embedding and detection scheme using these properties are described in the proposed watermarking approach . Also, due to the expansion of the projected image matrix from its size [M, N ] to [M ρ , N e ] this filed allows a higher amount of embedded data. In fact the DRT increases the size of the transformed image (see Equation (1)). The proposed method consists in embedding the watermark in selected coefficients in the radon field. These coefficients are chosen from the area of maximal energy. They represent maxims in the Radon coefficients and must respect the following three essential character- istics: 
 @BULLET These coefficients are set on the integral line of projection , so they will be well recovered from the inverse radon transform. @BULLET Secondly, they contain the most important details of the original images. Consequently, they are the most adapted to a code in a watermark with better imperceptibility. @BULLET Their high values enable us to hide the binary coefficients of the watermark without any perceptual degradation. 
Details of the Proposed Algorithm
 In the following Sections, we will note the original watermark as W o , the encrypted embedded watermark as W , the recovered encrypted watermark as W ,the recovered decrypted watermark as W o , the Original RGB color image (support or host image) as I, the original blue matrix of the host image as I b ,the transformed channel blue (matrix blue) of the host image in Radon field as R b , the watermarked channel blue (matrix blue) of the host image in Radon field as R bw , the watermarked blue matrix in spatial field as I bw , I w represents the watermarked spatial image and the transformed channel blue (matrix blue) of the watermarked host image in Radon field as R bw . Likewise, (x, y) represents the spatial coordinates of the original image, (ρ, θ) represents the coordinates of the color image in the radon field and k and l are the coded bits representing the watermark, [M w N w ] represents the size of the original image, 
Watermark Embedding Process
The main concept of the watermark embedding process is shown in 
Step 1: Encrypt the watermark. In order to encrypt the watermark, we use the following steps: 
1) Transform the original watermark W o into a one dimensional vector V water by the following equa- tion: 
W o (x, y) → V water (l). 
2) Decompose the watermark in N equal blocks B i , where V water (N ) = {B 1 , B 2 , ..., B N . 
3) Generate a key key0 witch its length is equal to the length of the block B i . 
4) Encrypt the first block B 1 by using the following equation: 
Bc 1 = B 1 ⊕ key0. 
 5) Encrypt the second block B 2 by using the following equation: 
Bc 2 = B 2 ⊕ Bc 1 . 
6) Generally, after each encrypting iterates of each block B i , the resulting encrypted blocks Bc i is used to encrypt the next block Bc i+1 , where i = 3, 4, 5, ..., N . 
Bc i = B i ⊕ Bc i−1 . 
7) After encrypting the watermark by using the function "" XOR "" , we applied S max iteratively permutations on the encrypted watermark vector in order to improve the encryption system. The first permutated iteration is defined by the following equation: P s=1 (B 1 , ..., B N ) = W o : B N/2 , ..., B 1 , B N , ..., B ((N/2)+1) . 
 We continue the permutation process by applying the defined function as follows: 
P s=α (Bc 1 , ..., Bc N ) = P s=α−1 Bc N , ..., Bc (N/2) , Bc 1 , ..., Bc ((N/2)+1) , 
where α = 2 → S max and S max represents the number of the permutation iteration. The encrypted watermark vector V c is obtained after S max permutations and it is defined as follows: 
V c (l) = P s=20 (l). 
 To obtain the encrypted watermark, we transformed the vector V c to matrix with size equal to 
V c (l) → W (x, y). 
Step 2: Select the radon coefficients to embed the watermark. In this step, a discrete radon transform is applied only on the blue channel I b of the color image I. A selection of a set of coefficients having the higher energy from this transformed matrix called R b is done. The number of the selected coefficient is equal to M w ×N w which represents the length of the encrypted watermark . For this reason, we transform the image matrix into a vector V by using the following equation: 
Rb(ρ, θ) → V (k). 
Then, the vector V is organized in downward order. It is defined as fellows: 
V (1) > V (2) > V (3) > ... > V (k − 1) > V (k), 
 where K = M θ * N θ Next, we select the used coefficients to embed the watermark. They represent the M w × N w first highest coefficients in the matrix R b . For this process, we use the following steps: 1) Define the threshold λ opt : It represents the coefficient number M w × N w in the vector V : 
λ opt = V (M w * N w ). 
2) Select the coefficients to be used for watermark coding: 
R E (ρ, θ) = R b (ρ, θ) whereR b (ρ, θ) ≥ λ opt . 
 So, the selected coefficient to encode the watermark represent the M w × N w first coefficients in the vector V : 
R E (ρ, θ) = V (l). 
Step 3: Embedding process. The embedding process is described in 
 Each selected coefficient coded one bits of the encrypted watermark vector V w . To embed watermark in the selected coefficient of the matrix R b , we used the following equation: 
   E θ (y) = θ E ρ (x) = ρ. 
Step 4: Watermarking image in spatial field. We transform the watermarking blue matrix R bw by the inverse DRT (IDRT) and we combine the RGB channels of the image to create the watermarked color image in spatial domain I w . 
Watermark Recovering Process
This algorithm represents the second principal algorithm in every watermarking approach. It serves to recover the embedded information with minimal loss. For this reason , it is necessary to respect the used parameter of the processing field and to use the details of the embedding program in this approach. This process is detailed by the illustrated in 
Er (cl) = R bw (E ρ (cl), E θ (cl)), 
where cl = 1, 2, 3, ..., M w × N w This vector contains the used coefficients in embedding the watermark. In order to decide if the embedding bits is equal to 1 or 0. We The recovered watermark W is a one-dimensional vector which represents the encrypted watermark. To decrypt it, we use the same algorithm used to encrypt the original watermark with the same parameters and steps. It is defined as follows: 
 1) Decompose the recovered encrypted watermark vector in N equal blocks B i with length equal to the length of the blocks B i where i = 1, 2, ..., N : 
V water (N ) = B 1 , B 2 , ..., B N . 
2) In order to recover the original watermark, we apply S max iteratively inverses permutations to the recover encrypted watermark vector.The first inverse permutation is defined as follows: 
P −1 S =1 (B 1 , B 2 , ..., B N ) = V water (B ((N/2)+1 , ..., B N , B (N/2) , ..., B 1 ). 
The next of the inverse permutation process defined by the following equation: 
P −1 S =α (B 1 , B 2 , ..., B N ) = P −1 S =α−1 (B ((N/2)+1 , ..., B N , B (N/2) , ..., B 1 ), 
 where α = 2 → S max and S max represents the number of the permutation iteration used in the embedding watermark process. The decrypted watermark vector after S max inverse permutation V d it is defined as follows: 
V d (l) = P −1 S=Smax (l). 
3) Devise V d into blocks B i with equal length equal to the length of the blocks B i where i = 1, 2, ..., N . Use the same key sing in the embedding process key0 to decrypt the first block Br 1 by using the following equation: 
Br 1 = B 1 ⊕ key0. 
4) Decrypt the second block B 
2 by using the following equation: 
Br 2 = B 2 ⊕ B 1 . 
5) Generally, after each decrypting iterates of each block Br i , the resulting decrypted blocks is used to decrypt the next block Br i+1 where i = 3, 4, ..., N − 1, N . 
Br i = B i ⊕ B i−1 . 
6) The decrypted watermark after application of XOR function is defined as follows: 
V d (N ) = Br 1 , Br 2 , ..., Br N . 
Finally, to obtain the decrypted watermark, we transformed the vector V d to matrix with size equal to 
M w N w ] defined as follows: V d (N ) → W O (x, y). W o 
is the recover watermark. 
Experimental Results
 To evaluate the performance of the proposed watermarking scheme, we use a data base composed with 100 logical watermarks coded on 0 and 1 binary (d = 2) and 50 host cover images. Different tests give results close to the present results in this paper. In this work, we present the results of the standard Lena image RGB color (see 
M 2 + N 2 + 1]. 
The similitude rate between the extracted watermark and the original watermark is continuously computed to test the robustness of this approach. This is done by the normalized Cross-correlation presented in the following equation: 
N C = Mq i=1 Nq i=1 W o W o Mq 1 [ Nq 1 W 2 o ] Mq 1 [ Nq 1 W 2 o ] 
. 
 On the other hand, the imperceptibility of the embedded watermark is a constraint that must be respected. A measure of similarity rate based on the PSNR described by the following equation is computed after each watermarking process with respect to the gain factor used. A threshold of 37 dB is fixed to verify if some distortions begin to appear on the watermarked image in addition to a psycho-visual decision. P SN R = 10 log( 
d 2 M SE ). 
Where d represents the maximal image intensities (d = 256 for the host cover image and d = 2 for the used logical watermark in our case) and MSE is calculated in the following equation: 
M SE = 1 M w × N w Mw×Nw i=1 (W o − W o ) 2 . 
The first step of this simulation study consists to select the threshold λ uses to select the radon coefficients used to embed the watermark. 
Selection of the Threshold λ
The threshold λ which is chosen for selecting the radon coefficients will be used to embed the watermark in order to improve the robustness of the proposed watermarking scheme against different attacks categories. For that purpose , it is provided in the following five different values of λ, (λ 0 , λ min , λ means−inf ,λ means−max and λ max ), used to select the encoding coefficients. A comparative study is performed to select the optimum threshold used to select the embedding coefficients. 1) Watermarking in coefficients equal to 0. To select the encoded coefficients R E (ρ, θ), we used a threshold λ 0 = 0. The number of the selected coefficient is equal to M w * N w where: 
if R b (ρ, θ) = λ 0 then R E (ρ, θ) = R b (ρ, θ). 
The simulations tests show that we cannot recover the embedding watermark in the coefficients equal to 0. So, the comparative parameters give the following results: C = N aN , P SN R = N aN and BER = 6400bits = 100%. 
2) Watermarking in minimal coefficients different to 0. In this algorithm, the coefficients whose values are minimal and different from zero are selected. For this reason we used a counter "" count "" to compute the number of zero in the matrix R b (ρ, θ): 
   count = 0; if R b (ρ, θ) = 0 then count = count + 1; end. 
The threshold λ min is selected as follows: 
λ min = V (((len − count) − (M w × N w )) + 1), 
where len is the length V of the vector representing the coefficients of the host radon image organized in downward order. The selected coefficient to embed watermark are: R E (ρ, θ) = R b (ρ, θ) where R b (ρ, θ) ≤ λ min and R b (ρ, θ) = 0. 
The simulations tests give a threshold λ min = 5073 and they show that we can recover the embedding watermark by the following parameter quality results: C = 0.3023, P SN R = 52.3670dB, BER = 2413bits = 37.70 %. The visual results given in The simulations tests give a threshold λ max = 32208 and they show that we can recover the embedding watermark by the following parameters quality results: C = 1, P SN R = Inf and BER = 0bits = 0 %. The visual results given in Figures 8 and 9. 
R E (ρ, θ) = R b (ρ
: R E (ρ, θ) = R b (ρ, θ) where R b (ρ, θ) < λ means and R b (ρ, θ) > λ min . 
The used threshold in this algorithm is noted by λ means−inf . The simulations results give λ means = λ means−inf = 18842 and λ min = 5073. The simulation tests show that we can recover the embedding watermark by the following parameter quality results: C = 0.7957, P SN R = 58.8606 and BER = 541bits = 8.45%. The visual results given Figures 12 and 13.  14, 15, and 16 show the variation of the correlation , PSNR and BER of the recovered watermark for different values of threshold λ. Figures 14, 15, and 16 show that the more threshold λ increases the better the correlation of the recovered watermark becomes. So, the optimum threshold λ is λ opt = λ max .Generally, the robustness of the proposed method against synchronous and asynchronous attacks for the higher coefficient depends on the highest energy of the radon region. The important peaks of radon domain are located at the points corresponding to the projection parameter. Besides, the coefficients of the radon region in which the highest energy is located and set on the line of projection contain the significant information of the original image. So, they allow a good recovery of the transformed information with inverse radon transform. 
Robustness of the Proposed Watermarking Approach
An example of an original and a watermarked image is illustrated in 
I (x, y) − I w (x, y) = ε(x, y) 
or the Weber law imposes that: 
M i=1 N j=1 I (i, j) − I(i, j) I (i, j) ≤ τ where τ ∼ = (2% − 3%). 
 Since the simulation results proved that ε < τ . So, perceptually we can say that no visible changes are engendered by the watermarking approach in radon domain, then: 
I = I and ε → 0. 
Figures 24, 25, and 26 illustrate an original and Radon transformed image followed by the positions (In red) of the imperceptible distortions introduced by applying the radon transform and recovering the image by the inverse Radon transform without any watermark embedding. In these Sections, we improve that the watermarking image in radon field presenting a more robustness against asynchronous attacks presented in table I is proved. In addition, when dealing with image, this transform doesn't engender any perceptual degradation. 
Justifying Robustness Against Asynchronous Attacks
 In this Section, we will detail why the proposed approach resists against the geometric attacks. This is done through the mathematical characteristics of the Radon transform. The robustness of the proposed approach and its resistance against asynchronous attacks can be justified only if we prove mathematically that a change of the Radon matrix coefficients presented by a watermark insertion is invariant against geometric transforms and has no mathematical or visual impact on the image in its spatial representation . In addition, this transform has to be entirely reversible and conservative. In the case it is almost conservative , we have a loss of data in the inverse process when applying the inverse DRT. The data loss does not affect in any way the perceptibility of the watermarked image in the spatial domain based on the Weber law. All these constraints must be tested and proved in the following Section. Given an image I(x, y) defined in + 2 , R(ρ, θ) represents its radon projection in + 2 . (x, y) and (ρ, θ) represent respectively the coordinates of the image in spatial and Radon domain.
Linearity
Given g(x, y) = βI(x, y) where β is a constant. The DRT of g(x, y) gives the following relation: 
DRT [g(x, y)] = R 1 (ρ, θ) where R 1 (ρ, θ) R(ρ, θ) = β , or β = β + ∆β 
through different tests we find that: 
∆β <<<<<<< β so β ∼ = β. Do : R 1 (ρ, θ) = βR(ρ, θ). So : DRT [g(x, y)] = βDRT [I(x, y)]. 
Similarly, we define the following relationship: 
If : g(x, y) = β 1 I 1 (x, y) + β 2 J(x, y). 
 The Radon Transformation of J gives the following re- sults: 
DRT [g(x, y)] = β 1 DRT [I 1 (x, y)] + β 2 DRT [J(x, y)], where [I1(x, y)] and [J(x, y)] 
 are two image defined in spatial field and β1 and β2 are two constants. This relation proves that the DRT is linearly invariant. 
Image Scaling
A scaling on the − → X and − → Y axis of the image is applied as presented in 
g(x, y) = I(x − x 0 , y − y 0 ). 
DRT [g(x, y)] = R 1 (ρ 1 , θ) = DRT [I(x − x 0 , y − y 0 )], where ρ 1 = (x − x 0 ) cos θ + (y − y 0 ) sin θ = x cos θ − x 0 cos θ + y sin θ − y 0 sin θ = x cos θ + y sin θ ρ −x 0 cos θ − y 0 sin θ = ρ − x 0 cos θ − y 0 ) sin θ. So: DRT [g(x, y)] = R 1 (ρ 1 , θ) = R(ρ − x 0 cos θ − y 0 sin θ, θ). 
Also, the error defined by ∆ρ = −x 0 cos θ − y 0 sin θ, θ is lower than the value of ρ. Since the variation ∆ρ cannot allow the projection of a pixel neighbor defined by its coordinates (ρ, θ) due to its size ∆ρ <<<< ρ then, the radon transformation depends only on the value of ρ. 
Image Rotation
Supposing that K(ρ, θ) represents the polar coordinate of I(x, y) and g(ρ, θ) = K(ρ, θ − ϕ) with ϕ represents the angle of circular shifting. The results of rotating a spatial image in the radon field is studied and presented in 
DRT [g(ρ, θ)] = DRT [(ρ, θ − ϕ)] = R 1 (ρ 1 , θ 1 ) = I(x 
cos θ cos ϕ + y sin θ sin ϕ, −y cos θ sin ϕ + y sin θ cos ϕ). This relation shows that the radon transform depends only on the value of θ − ϕ and its magnitude is constant. Consequently the projected pixel will change its location in the Radon field according to the angular rotation applied . This proves that angular rotations are conserved to generate the correspondent Radon coefficients. 
R 1 (ρ 1 , θ 1 ) = I(x cos(θ − ϕ), y sin(θ − ϕ)). = R(ρ, θ − ϕ). ρ 1 = ρ θ 1 = θ − ϕ. So, DRT [g(ρ, θ)] = R 1 (ρ 1 , θ 1 ) = R(ρ, θ − ϕ). 
Cropping Rotation and Scaling
 In this section, we test the invariance of the DRT if different asynchronous attacks are combined simultaneously such as rotation and scaling or cropping without changing the axis projection. 
Case 1. Scaling on Y axis and circular shifting by an angle (See 
DRT [g(ρ, θ)] = DRT [K(ρ, θ − ϕ)] = R 1 (ρ 1 , θ 1 ) = I(x cos θ cos ϕ + x sin θ sin ϕ, −(y − y 0 )cosθ sin ϕ +(y − y 0 ) sin θ cos ϕ. R 1 (ρ 1 , θ 1 ) = I(x cos(θ − ϕ), (y − y 0 ) sin(θ − ϕ)). = R(ρ − y 0 sin(θ − ϕ), θ − ϕ). ρ 1 = ρ − y 0 sin(θ − ϕ) θ 1 = θ − ϕ. 
So, in this case: 
DRT [K(ρ, θ − ϕ)] = R(ρ − y 0 sin(θ − ϕ), θ − ϕ). 
The error is defined by: 
∆ρ = −y 0 sin(θ − ϕ) <<<< ρ. Experimental test proved that the error found is very small compared with ρ (∆ρ <<<< ρ). 
DRT [g(ρ, θ)] = DRT [K(ρ, θ − ϕ)] = R 1 (ρ 1 , θ 1 ) = I((x − x 0 ) cos θ cos ϕ +(x − x 0 ) sin θ sin ϕ, −y cos θ sin ϕ + y sin θ cos ϕ. R 1 (ρ 1 , θ 1 ) = I((x − x 0 ) cos(θ − ϕ), ysin(θ − ϕ)) = R(ρ − x 0 cos(θ − ϕ), θ − ϕ) ρ 1 = ρ − x 0 cos(θ − ϕ) θ 1 = θ − ϕ. So, DRT [K(ρ, θ − ϕ)] = R(ρ − x 0 cos(θ − ϕ), θ − ϕ). 
The error is ∆ρ = −x 0 cos(θ − ϕ) <<<< ρ. 
DRT [g(ρ, θ)] = DRT [K(ρ, θ − ϕ)] = R 1 (ρ 1 , θ 1 ) = I((x − x 0 ) cos θ cos ϕ +(x − x 0 ) sin θ sin ϕ, −(Y − Y 0 ) cos θ sin ϕ +(Y − Y 0 ) sin θ cos ϕ). R 1 (ρ 1 , θ 1 ) = I((x − x 0 ) cos(θ − ϕ), (Y − Y 0 ) sin(θ − ϕ)). = R(ρ − x 0 cos(θ − ϕ) −y 0 sin(θ − ϕ), θ − ϕ). ρ 1 = ρ − x 0 cos(θ − ϕ) − y 0 sin(θ − ϕ) θ 1 = θ − ϕ. 
So, in this case: 
DRT [K(ρ, θ − ϕ)] = R(ρ − x 0 cos(θ − ϕ), −y 0 sin(θ − ϕ), θ − ϕ). 
The error is defined by: Due to its feeble value, the error ∆ρ cannot change the original integer quantized coefficient in Radon field. So, the used coefficient to code the watermark does not change and the watermark is correctly recovered. As proved above, face to singular or composed geometric transform, the Radon transform offers invariance to the transformed image. Consequently, the distortions and geometric transforms applied on the spatial watermarked image have generally no effect on the embedded watermark in the radon field since these variations are conserved over the Radon coefficients where the watermark is coded. 
∆ρ = x 0 cos(θ − ϕ)y 0 sin(θ − ϕ). 
Comparative Study
In order to prove the efficiency and high robustness of the proposed method, a comparison study illustrated in Figures 32 and 33 is conducted with recent proposed approach in the literature exploiting the Radon domain 
Conclusions
In this paper a watermarking approach based on the Radon transform is presented. The watermark is coded in selected coefficients with respect to specific mathematical characteristics and the energy is characterized by higher robustness against various attacks types. The proposed scheme presents high robustness especially against asynchronous attacks. This resistance against these geometric attacks is studied and proved mathematically. Accordingly, the equilibrium between watermarking constraints is achieved. Robustness and imperceptibility are respected and the embedding capacity is increased. transformations. He has over 14 international journals papers and 65 conference papers. His domain of interest is: Audio-image and video processing applied in filtering, encryption and watermarking. He belongs to the CEREP research unit and supervises actually five thesis and 08 masters in the field. Ezzedine Ben Braiek obtained his HDR on 2008 in Electrical engineering from ENSET Tunisia. He is, presently, full professor in the department of electrical engineering at the National High School of Engineering of Tunis (ENSIT) and manager of the research group on vision and image processing at the CEREP. His fields of interest include automatics, electronics, control, computer vision, image processing and its application in handwritten data recognition. 
"
"Introduction
Unlike traditional wireless networks, wireless mobile ad hoc networks (MANETs) are infrastructure less networks consisting of a set of handled devices such as lap tops, PDA, mobile phones…etc, often mobile and moving in a limited area. Accordingly mobile nodes within an ad hoc network must collaborate to execute some of the habitual network services such as routing and security 
Routing in MANETs
In order to treat routing in MANETs lot of algorithms are proposed toward the specificities of MANETs such as node mobility, devices' constraints as well as the used medium. According to the strategy followed by the routing algorithms several categories exist: 
Proactive Protocols
This category of routing is inherited from the conventional ones, since it keeps the whole topology of the network by each node over the network in the routing table. Node mobility and topology changing are treated by periodically exchanging the routing tables between neighbors. Routes are found immediately however the maintenance of the routing tables consumes the network resources in high mobility networks 
Reactive Protocols
In order to treat the disadvantage of the proactive routing and minimize the overhead due to topology changing the reactive routing protocols propose to occasionally react to the demand of route establishment by launching a route discovery mechanism to find routes only when needed, since at a given moment only a subset of the network nodes are communicating at the same time. This kind of routing is very suitable for MANETs however the overhead during route discovery can block the network due to flooding 
Hybrid Protocols
Tacking advantages of the reactive and the proactive routing protocols, the hybrid ones use a hierarchical structure of the network to ensure routing, in the way that the entire network is organized into clusters or regions in which we use a proactive strategy to ensure routing inside each cluster and a reactive one to ensure inter-cluster communication 
Swarm Intelligence based Routing
This category of routing is recently developed for MANETs inspired from insect communities such as bees and ants which very often collectively execute smart actions with only a little intelligence at each insect, which can be very practice from the MANETs perspective since each node within a MANET can be viewed as an ant in a hostile environment with limited capabilities and trying to find its way to food or to the nest. Using artificial pheromone and probability computing this kind of routing finds the best path according to a variety of parameters defined according to the network context 
Swarm Intelligence based Routing
As devoted in the previous section, the swarm intelligence routing paradigm is completely inspired from insect swarms such as ants and bees. Since these swarms of insects have lot of desired characteristics compared to MANETs, in the way that ants or honeybees swarms are made up of hundreds to thousands of small insects with little intelligence and communication capabilities in an unpredictable environment, however always generate smart solutions and achieve the objectives of the community such as finding or optimizing the path to the food 
The most known swarm intelligence based routing algorithms are inspired from the ant colony, the swarm intelligence in this community is achieved using a special form of communication based on pheromone which is a substance related to hormones produced by each ant during its movement, this pheromone is sensed by other ants within the community in order to find their route in the nature, since ants are attracted by pheromone which leads it to the food or the nest. Ants always follow the higher pheromone concentration which often leads to follow the shortest trail and causes a self-accelerated reaction without any centralized intervention, which ensures path optimization, because the shortest path have always the greatest concentration of pheromone. 
The Ant Routing Algorithm
Using the same idea devoted in the previous section to find food and optimize the route from the nest to food, the Ant Routing Algorithm ARA 
Generally, ARA uses two kinds of artificial ants for route discovery and establishment: 
The first one is called forward ant (FANT), which is used to discover routes from the source to the destination node. Therefore this ant travels over the entire network in order to find any possible route from S to D, similar to the discoverer ants in real ant colony which go far in the nature in order to find any possible source of food, during her trip over the network each ant deposits a constant amount of artificial pheromone used after by the intermediate nodes to shorten paths. 
The second kind of ants are called backward ants (BANT), these ants follow the same path established by the FANTs in order to establish the final route from S to D. Therefore the BANT travels over the same path discovered by the corresponding FANT from D to S in order to inform S about all the possible routes. During their lifetime the BANT and the FANT modify at each hop the artificial pheromone for each edge over the network by adding a constant amount of pheromone ∆φ at each visit to any intermediate node emulating real ants. Consequently, ARA uses a pheromone table in which it saves the level of pheromone for each edge. So, each node has a record in this table for each edge, the pheromone table is increased by FANT and BANT and accordingly decreased due to time. Using the value of pheromone for each edge, each node computes the probability to use one of these edges for routing using the following equation: 
            i i N j j i j i j i N j N j p i 0 , , ,   (1) 1 ,    i N j j i p 
As we can observe ARA is a distributed routing algorithm inspired from ant colony to establish routes within a MANET. ARA has lot of suitable characteristics for MANETs which makes it one of the most favorite algorithms for large scale ad hoc networks. However, from the security point of view, it seems that ARA does not implement any security mechanism to ensure the integrity, authentication, confidentiality of the exchanged data over the network, in the way that ARA defines only the necessary mechanisms and procedures to establish and maintain routes. 
PKI in MANET
A public key infrastructure is a set of components for managing digital certificates in a given community or network, PKI is recognized as the most powerful tool providing trust and security in conventional networks. PKI relies on asymmetric cryptography to ensure authentication and non-repudiation over the whole network based on a trusted authority called certificate authority CA which signs and verifies certificates of the entire network, therefore the robustness and availability of the PKI services depends on the availability and the security of the CA, consequently the greatest intention must be given to the CA when deploying a PKI 
The trivial implementation of PKI for any network is to affect the whole services of the CA to one node in the network to be responsible of certificate publishing, renewal, revocation…etc, which is the case in all the conventional networks. Nevertheless deploying such architecture for a mobile network creates a point of failure which is the node handling the CA services, since the disappearance of this node due to mobility for example causes the service desertion. Therefore, affecting all CA services to only one node in a MANET is not efficient; consequently the CA's services must be distributed to more than one node in order to ensure the continuation of the security service. Although, in literature lot of propositions have been deployed in order to implement a PKI for MANET by distributing the CA services over the network's nodes. 
Partially Distributed Certificate Authority
This solution proposed by Zhou and Hass is based on a (k, n) threshold scheme to distribute the CA's signing key to k nodes over the network using Shamir's secret sharing scheme 
Cluster based PKI
The authors in 
PKI-DSR
The authors in 
The example given for the application of this solution is built over the dynamic source routing DSR, the proposed protocol is called PKI-DSR according to the simulation results of PKI-DSR compared to the original DSR, PKI- DSR add a little overhead to the network whenever there is attacks against the network, otherwise there is no overhead. 
µPKI
µPKI is a lightweight implementation developed especially for wireless sensor networks 
Attacks against routing in MANETs
Due to the nature of the used medium which is opened to everyone with the adequate hardware and the network stack, as well as the nature the underlying technique of communication which is based on multi hop routing in which every node is responsible of executing routing primitives to ensure the network connectivity. A large variety of attacks against MANETs exists: 
Black hole: the objective of this attack is to attract the traffic from a particular node or region through the attacker 
Routing table poisoning: 
This attack is performed against table driven routing protocols, in the way that the attacker diffuses false routing information to its neighbors in order to disturb or block the traffic over the network 
Denial of service attacks: in this kind of attacks 
the attacker tries to disrupt, deny or degrades the service of the network, it is executed in different ways and decreases network performances, this kind of attacks is the most dangerous since all attacks defined previously can be subject of denial of service attacks if they are executed permanently against the network 
Spoofing attack: also called impersonate attack is executed in the absence of an authentication mechanism, in the way that the attacker spoofs the identity of a legitimate node in order to gain access to the network or to execute malicious actions using the spoofed identity such as black hole, replay or data modification like denial of service attack, this attack can be avoided using a mechanism of authentication such as digital certificate 
AntPKI
As we have described above, lot implementation of PKI have been proposed in literature in order to make in practice an effective security solution that take into consideration all the characteristics of MANETs, however the majority of these solutions give a set of a stand-alone specifications that implements independently the PKI without taking into consideration the underlying routing protocol in order to optimize the PKI's operations by using some of the routing procedures for PKI's services implementation such as in PKI-DSR, since the exploitation of the routing operations may minimizes the overhead due to PKI's services. 
In the same context, in the following sections we are going to present an implementation of a PKI over ant colony based routing protocols; this kind of routing knows a great development due to its characteristics which are very suitable for MANETs, however all the recent proposed protocols do not give any consideration to security. Therefore, we are going to propose an implementation of PKI called AntPKI to be used over this kind of routing; our proposed implementation will make use of the specifications of underlying protocols in order to publish, revoke and secure end to end communications. 
ANTPKI Strategy
As mentioned above, ant colony based routing protocols generally use a set of request known under the name of ants used to discover and establish the path between two nodes in the network. ARA for example uses two kinds of ants. The FANT used for route discovery, it is forwarded by each node over the network until it arrives to the destination node, which replies with a BANT which is sent to the source node in order to establish the final path. Consequently each node in the network may forward at a given time a FANT or BANT during the routing process. Accordingly, any implementation of a PKI over ant based routing must use these ants in order to implement the security procedures of a PKI and publish self-issued certificates of the network nodes. 
System Bootstrapping
In a conventional PKI there is a centralized server responsible of the generation, publication, renewal and revocation of each certificate within a given community, however due to the characteristics of a MANET these services cannot be done in the same way since there is no concept of centralized authority which can accomplish this task, consequently the network nodes must collaborate between themselves in order to accomplish this task. To overcome the absence of the certificate authority, in AntPKI the certificate are self-issued by each node in the network, hence each node is responsible of the information contained in its certificate as well we suppose that each node have the capability of generating and keeping in secret its certificate, private and public key. The certificate structure is inherited from X.509 V3 standard by adding some additional fields which can be useful in our case such as the IP or MAC address. So, when any node joins the network for the first time it must generate a certificate and fills it with the adequate information such as the user name, validity period, public key, MAC and IP address, signs this certificate with its private key, and waits for the appropriate moment to publish the certificate for the rest of nodes. 
In order to handle the revocation and the publication service of PKI, each node must have the capacity to handle and manage two directories, the first one is for saving revoked certificate and the second one is to save valid certificate. 
PKI Management and CA Services
The most important services of a PKI is the publication and the revocation lists, because they are visited by each entity in a given community in order to verifies the validity of the certificates and consequently ensure the security of the network. Therefore in AntPKI we have defined a mechanism for certificate publishing which uses the underlying protocol to publish certificate over the entire network. Therefore, we are going to make use of the mechanism of route discovery in order to publish the certificate of each node for the rest of nodes over the network. 
(1) Certificate publishing 
The ant colony based routing is based on ants in order to establish and maintain routes over the network. Therefore, in AntPKI we are going to utilize these ants in order to publish and secure links over the network. AntNET or ARA is based on the FANT in order to discover routes over the network; this ant visits each node over the network deposing a kind of substance called pheromone used after to compute probability in order to evaluate routes. Our interest in this ant is that it visits each node over the network during the period of route discovery, which makes it the preferred period to publish certificate. Our proposed solution to publish certificate is by using the FANT to handle the certificate publishing over the network, thus we propose to create a new field in the FANT which will contain the certificate of the source node. Consequently, each node when launches a route discovery, it inserts its certificate into the FANT before launching it. As a result, the certificate of the corresponding node is diffused over the entire network; consequently each intermediate node retrieves from the FANT the certificate of the source node and saves it in the corresponding directory for future use, in the same process of route discovery each intermediate node use the FANT as support to transfer its certificate to its neighbors. The structure of the forward ant is changed by creating new fields, the first one is used by the source node in order to publish its certificate for the rest of nodes and the second one is used by intermediate nodes to publish their certificate for their neighbors. 
(2) Certificate Revocation 
The revocation mechanism is used in a PKI in order to cancel some certificates from use for validity reason since each certificate is valid only within a given period, or for security reasons due to an attempt from the corresponding node to build up an attack against the network, if the network has an intrusion detection system. For this reason each node, keeps a revocation list where it saves the revoked certificate, the certificate revocation list is updated by an accusation mechanisms defined by AntPKI. Since, in AntPKI each node when detects any malicious attention from a neighboring node using an underlying intrusion detection system, it sends an accusation request which contains the certificate of that node otherwise if it has not its certificate it send an accusation with the IP or MAC address of that node. The accusation request is diffused over the entire network; the corresponding certificate is automatically revoked whenever the number of accusations reaches a predefined value. Whenever the certificate is revoked due to Source node certificate the detection of an attack AntPKI does not revoke only the certificate of the corresponding node however it revokes all the information contained in the certificate such as names, IP and MAC address, accordingly the corresponding node is excluded from all the possible services of the network. 
(3) Certificate renewal 
This mechanism is launched by the interesting node, whenever a node wishes to renew its certificate due to the expiration of its delay or the modification of the certificate fields, it generates the new certificate involve it in a renewal request and diffuses it over the entire network, each node when receiving this request changes the old certificate with the new one. 
AntPKI Functioning
In the previous section we have presented the operations that manage digital certificates using AntPKI; however distributing and publishing certificate is only useful if they are used for securing links between each pair of nodes in the network. 
In order to ensure security services such as integrity, confidentiality and non-repudiation using AntPKI we propose to use three mechanisms of cryptography: Symmetric encryption: this method of cryptography is based on a single key shared between two entities in a given community, it is very practice to encrypt big amount of data in a short time, which makes it the preferred solution to encrypt ordinary traffic over the network. 
Message authentication code 
"" MAC "" : usually this solution is used to provide integrity, which can be viewed as a hash function applied on a data packet, resulting on a digest. This digest is encrypted using a symmetric encryption with a key shared between the communicating parties. The MAC is always joined to the original packet and verified by the destination node using the same key to detect any alteration to the packet integrity. 
Session Key Establishment
In order to ensure the confidentiality over the network we propose to encrypt the ordinary traffic over the network using symmetric encryption which is very suitable regarding its costless compared to the asymmetric one. Thus, we propose to establish a session key between each two communicating entities to encrypt ordinary traffic, this session key is established by the cooperation of the entities using their public key and using as support the backward ant BANT to optimize the network resources. 
As we have presented in the previous section, the certificate of the source node is diffused over the entire network using the FANT, using the same idea the destination node uses the BANT to publish its certificate along the discovered route. Consequently, the BANT structure is modified by including a new field which will contain the certificate of the destination node. We also use BANT as support for establishing the session key between the communicating parties (source and destination); accordingly the original BANT packet structure is modified to include another field used to transmit the session key by the destination node to the source node. Whenever, the FANT arrives to the its destination, the destination node saves the certificates contained in this request, generates a BANT and inserts its certificate in the appropriate fields, then it generates a random session key, encrypts it using the public key of the source node obtained from its certificate and sends the BANT over the discovered route. 
Whenever the source node receives the BANT, it can conclude three kinds of information from this request:  The session key encrypted by the destination node's public key. The source node retrieves the session key from the BANT, decrypts it using its private key and according to the specifications of the underlying routing protocol it waits for the adequate moment to begin using it for data encryption and authentication. In order to ensure more security and to resist against long term attacks which tries to conclude the used encryption key by analyzing the exchanged traffic for a long period we propose to use a periodic key update to periodically update the session key. The key update is launched by one of the two communicating parties after the expiration of the period and consisting on the creation of a new random session key encrypts it with the public key of the other node and sends it over the same route, when receiving this request the second node decrypts it using its private key and begin immediately using the new key. 
Integrity and Authentication
As we have implicitly mentioned above, integrity is ensured using MAC (Message authentication code), joined to each packet transmitted over the network. The MAC is joined to the packet either this packet is encrypted or not, because some control messages do not need to be encrypted however they need to be authenticated. The MAC can be encrypted using the session key shared between the communication parties and established during the route discovery or using the public keys. A MAC using public key encryption ensures both authentication and integrity which makes it the preferred mechanism to be used over AntPKI. 
Analysis
Key Management Specifications
In this section we are going to analyze AntPKI regarding the habitual evaluation criteria of key management schemes: Security: this aspect deals with the confidentiality and integrity of the exchanged data over the network, as we have presented above AntPKI uses two methods of cryptography to ensure and enforce security. The first one is used for authentication and integrity using MAC (Message authentication Code) signed by the public keys or by the session key of the communicating parties which guaranties both integrity and authentication. The second method is using symmetric encryption to guaranty confidentiality of the exchanged data using a session key established during the route discovery period Network performance: regarding the network performance AntPKI does not add any overhead to the network because it use as support the underlying routing protocol to handle the specifications of AntPKI such as certificate publishing, which guaranties the efficiency of AntPKI. However, during the detection of attackers a small overhead is added to the network in order to revokes the certificate of this node and exclude him from the network. 
Resistance Againnst Attacks
As given in the previous section AntPKI guaranties all the key management criteria such as availability, scalability…etc, which are very important to deploy a key management scheme, however AntPKI must ensure more security by resisting against all known attacks, therefore in this paragraph, we try to evaluate the robustness of AntPKI face to the majority of existed attacks: 
Passive attacks: These kinds of attacks like eavesdropping permanently capture and analyze the exchanged data over the network in order to retrieve information like the network architecture, security or routing mechanisms. Passive attacks cannot be executed against AntPKI because the exchanged data over the network is encrypted using symmetric encryption, which makes it out from any tentative of eavesdropping. To ensure more security we have used periodic key update in order to resist against long term attacks executed by collecting a great amount of encrypted data. 
Modification, replay and Insertion: 
These kinds of attacks alter the integrity of the exchanged data, using the key agreement defined above each communicating nodes share a symmetric key used to ensure data confidentiality enforced by a periodic key update, in the other hands data authentication and integrity is guaranteed using digital signature. 
Black hole attack: 
In this attack, the malicious node tries to attract the exchanged traffic over itself; by replying with false BANT which drives all the exchanged data to this node giving him the possibility to execute any other attacks. Using AntPKI black hole attack is not possible since AntPKI is based on digital certificate giving the possibility to both source and destination nodes to authenticate each other and verify the validity of discovered routes. 
Spoofing attacks: Using our approach it seems that this attack can't be executed since the mechanism of certificate publishing using forward and backward ants ensure the publication of nodes' certificate over the Original packet MAC (using session or public key) network used after for ensuring the authentication and the security of nodes over the network similar to conventional network. 
Conclusion
In this paper we have presented an implementation of PKI for MANETs over ant based routing algorithm, as it is known PKI is the most secure and popular solution to guaranty security in conventional networks however it is unclear if it can be efficiently implemented in MANETs due to the characteristics and the constraints of this kind of networks. Our proposed scheme called AntPKI is a simplified implementation of PKI for MANETs over swarm intelligence routing. AntPKI uses the mechanism of route discovery as support for certificate publishing using FANT and BANT which guaranties that the certificates of both communicating parties reach the majority of the network nodes. Using the underlying routing protocol as support for security management optimizes the network performances and leads to an efficient PKI implementation. 
In addition AntPKI guaranties data confidentiality using a session key established at the same moment of the certificate publishing and using the same requests (FANT and BANT). 
Reference 
"
"Introduction
 A batch verification of digital signatures provides better computational complexity when several signatures are verified together. Several batch verification algorithms have been proposed for various digital signature schemes, e.g. DSA or RSA. Having n message/signature pairs (m 1 , s 1 ), . . . , (m n , s n ) the batch verification algorithm answers the following question: "" Are all the signatures correct (valid)? "" In the negative case, further investigation (and computation) is necessary in order to identify bad signature or signatures . Lee at al. 
LCCC Method and its Security Problems
 Let N be a public RSA modulus, i.e. N = pq for sufficiently large primes p and q. Let e be a public exponent for this RSA instance, i.e. e is relatively prime to (p − 1)(q − 1). A signature s of a message m is valid if and only if s e ≡ m (mod N ). In order to simplify notation, we use m instead of H(m), i.e. m denotes a hash of an actual message. The LCCC method uses a standard "" generic test "" to test the validity of a batch (m 1 , s 1 ), . . . , (m n , s n ). The generic test (GT) can be instantiated as a Random Subset Test or Small Exponents Test, see 
Since the choice of GT does not affect our analysis, we do not describe it in greater detail. Lee at al. 
Single Bad Signature
 The algorithm called DBI basic is aimed at identifying single bad signature in a batch (if such a signature exists), see 
1) If all signatures in the batch x are valid, DBI basic (x) returns "" true "" . 
2) If there is exactly one bad signature in the batch x, DBI basic (x) returns the index of this bad message/signature pair. 
3) If there are more then one bad signature in the batch x, DBI basic (x) returns "" false "" (This is the reason for the test GT(x (m k , s k ))). 
DBI basic (x) input: x = ((m 1 , s 1 ), . . . , (m n , s n )) 
if GT(x) then return "" true "" ; 
M ← n i=1 m i ; M * ← n i=1 m i i ; S ← n i=1 s i ; S * ← n i=1 s i i ; find k ∈ {1, . . . , n} such that S e M k ≡ (S * ) e M * (mod N ); ( * ) 
if k does not exist then return "" false "" ; if GT(x (m k , s k )) then return k (index of bad signature); return "" false "" ; 
The Problem 
We show that the Property 2 can be easily attacked (and thus, Theorem 1 in 
Let x = ((m 1 , s 1 ), . . . , (m n , s n )
) be a batch where all signatures are valid. Let j be an arbitrary even number from the set {1, . . . , n}. Let us replace the pair (m j , s j ) with pair (m j , −s j ). We denote this new batch as x . Since the public exponent e is odd number, we get 
(−s j ) e ≡ −(s e j ) ≡ −m j ≡ m j (mod N ). 
Hence, batch x contains exactly one bad signature. When evaluating DBI basic (x ), the property ( * ) is satisfied for every even k from the set {1, . . . , n}: left side: 
S e M k ≡ (−s j ) e · i∈{1,...,n}−{j} s e i m j · i∈{1,...,n}−{j} m i k ≡ (−s j ) e m j k ≡ s e j m j k ≡ 1 (mod N ) 
right side: 
(S * ) e M * ≡ ((−s j ) j ) e · i∈{1,...,n}−{j} (s i i ) e m j j · i∈{1,...,n}−{j} m i i ((−s j ) j ) e m j j ≡ ((s j ) j ) e m j j ≡ 1 (mod N ). 
When simplifying left and right sides of ( * ) we make use of the fact that k and j are even numbers, respectively. Hence, the first tested even k (probably k = 2 when implemented in a standard for-loop) will be determined as an index of bad signature. If k = j, the subsequent test GT(x (m k , s k )) returns "" false "" . Let us summarize: the batch x contains single bad signature, but DBI basic was unable to find it. 
Remark. The DBI basic method cannot be easily fixed. Testing every candidate k satisfying ( * ) will destroy intended efficiency of the method. Moreover, testing whether bad signature is just −1 multiple of the valid signature is computationally as demanding as simply checking the signature alone. 
 Remark. Modifying ( * ) in such way that only odd exponents are used, i.e. 
M * ← n i=1 m 2i−1 i , S * ← n i=1 s 2i−1 i , and ( * ) transforms into S e M 2k−1 ≡ (S * ) e M * (mod N ), 
would prevent our attack. However, the security of this modification should be investigated closer. 
Multiple Bad Signatures
Lee at al. extended their DBI basic method to identify multiple bad signatures in a batch. The authors used divide-and-conquer approach and denoted their method DBI α (see 
The Problem 
The idea of our attack from Section 2.1 can be used to increase the DBI α complexity. Let us illustrate this increase on case of single bad signature and α = 2. Result can be easily generalized for multiple bad signatures and other values of α. Let n = 2 m , and let us assume that ( * ) is tested in a standard for-loop. The adversary modifies signature s n to −s n . Then the ( * ) is satisfied for k = 2, but GT(x (m 2 , s 2 )) returns "" false "" , thus forcing division of x, and recursive calls of DBI 2 (x 1 ) and DBI 2 (x 2 ). DBI 2 (x 1 ) requires one GT. However , DBI 2 (x 2 ) requires two GT, since ( * ) is satisfied for DBI α (x) input: 
x = ((m 1 , s 1 ), . . . , (m n , s n )) 
if n = 1 then if GT(x) return "" true "" ; else return {1} (index of bad signature); if n = 2 then if GT(x) return "" true "" ; else find k ∈ {1, 2} such that 
(s1s2) e m1m2 k ≡ (s1s 2 2 ) e m1m 2 2 (mod N ); ( * * ) 
if k = 1 return {1}; if k = 2 return {2}; else return {1, 2}; /* case n > 2 */ if GT(x) then return "" true "" ;  k = 2 m−1 + 2, and leads to further recursive calls. Counting all GT's (regardless of their input size) performed during DBI 2 computation gives 3(m − 1) invocations of GT. On the other hand, standard divide-and-conquer method requires only 2m + 1 invocations of GT. 
M ← n i=1 m i ; M * ← n i=1 m i i ; S ← n i=1 s i ; S * ← n i=1 s i i ; find k ∈ {1, . . . , n} such that S e M k ≡ (S * ) e M * (mod N ); ( * ) 
Remark. Using odd exponents, as proposed in Section 2.1, would prevent this "" complexity attack "" . 
"
"Introduction
3D information hiding algorithms 
Adaptive information hiding algorithms 
The Proposed Algorithm
This section illustrates the proposed algorithm, including the data embedding and data extraction procedure. The flowchart of the proposed algorithm is shown in 
The Data Embedding Procedure
The data embedding procedure begins by preprocessing the topological information of the input model. To efficiently derive the referencing neighbors for each embedding vertex in the modified BFS process, a vertex neighboring table records the indices of the actual neighbors based on the polygonal information appearing in the model file. Furthermore, the model information, including the diagonal length of the bounding volume, the number of vertices, and the number of faces, can be also derived in this process. 
The breadth first search is a search method that begins from the root node of a graph and explores all its neighboring nodes. For each neighboring node, the algorithm explores unexplored neighboring nodes iteratively. The algorithm stops after all nodes have been traversed. All neighboring nodes derived by expanding a node are added to a queue with a first-in-first-out (FIFO) property. However, the breadth first search may not only have a unique search result for a graph. The search results depend on how the user chooses the neighboring nodes of each node. In the proposed technique, the search order is based on the sequence appearing in the vertex neighboring table for each corresponding vertex. This vertex neighboring table is robust against similarity transformation and vertex ordering attacks. Thus, the same search order can be derived in the data extraction procedure. 
The modified BFS process determines the referencing neighbors for complexity estimation and data embedding. Each vertex can have three different statuses "" NS, "" "" SS, "" and "" S, "" representing the corresponding vertex as being unsearched, semi-searched, and searched. Each vertex is initialized as "" NS. "" The initial vertex for performing this process is resolved by the first index of the polygon intersected by the principal axis and regarded as the root node for performing the BFS algorithm. In the first iteration, the process enqueues the initial vertex, dequeues it, and then enqueues its unsearched neighbors. Note that, the order of the enqueued neighbors is not arbitrary, but based on the sequence appearing in the vertex neighboring table for each corresponding vertex. When the vertex is enqueued, its status is modified to "" SS, "" whereas the status is modified to "" S "" when the corresponding vertex is dequeued. In the second iteration, the process dequeues one vertex from the queue and euqueues its unsearched neighbors. The order of the enqueued neighbors is still based on the sequence appearing in the vertex neighboring table. This iteration is repeated until the queue is empty. The algorithm then proceeds until all vertices are searched. In each iteration, the neighbors with the status "" NS "" or "" SS "" are included in the referencing list for each dequeued vertex. 
To be robust against similarity transformation attacks, the coordinate transformation process collects the vertex whose referencing list is empty. Thereafter, principal component analysis 
1 2 EC ET  to 2 EC ET   . Equation (2) 
shows the derivation for the data-embedded distance d'. When the calculated embedding capacity EC 1 is smaller than EC  , secret message SM 2 with length EC 1 is extracted from the stream of secret message with binary form and SM 10 is the decimal value from a binary-to-decimal format transformation of SM 2 . 
). 
Model Name V N F N BV DL N V DL N V N 
The Data Extraction Procedure
During the data extraction procedure, the following processes are performed sequentially. First, the preprocessing process deals with the topological information of the input model. A vertex neighboring table is then constructed. As mentioned before, the construction of this table is based on the topological property of each vertex. However, the topological information is never modified in the data embedding procedure. Therefore, this table can be correctly reconstructed according to the same processes in the data embedding procedure. Second, the modified BFS process iteratively determines the referencing neighbors for each embedding vertex of the stego model. Third, we collect all the vertices whose referencing list is an empty set. Thereafter, a principal component analysis is performed on the above vertices to produce a coordinate system and transform all vertices of the stego model from the Cartesian coordinate system to the new one. Fourth, the embedding threshold can be also derived in this process based on the secret key and the diagonal length DL of the bounding volume of the vertices with no referencing neighbors. Finally, the data-embedded distance d' can be calculated. The secret message with a binary format can be easily derived based on the embedding capacity and decimal-to-binary format transformation of SM 10 derived from Equation (3). 
  1 11 10 2 2 2 / 2 2 EC EC EC EC EC d ET SM = dd ET ET                     (3) 
Experimental Results
This section presents the experimental results obtained from eight 3D common polygonal models: "" Armadillo, "" "" Brain, "" "" Cow, "" "" Golfball, "" "" Lucy, "" "" Maxplanck, "" "" Dragon, "" and "" Hand. "" The proposed algorithm was implemented in Microsoft Visual C++ programming language on a personal computer with an Intel Core i7 2.67 GHz processor and 3 GB of memory. 
10 ET
() V ON for determining the referencing neighbors for each embedding vertex. The proposed algorithm adopts a modified breadth first search scheme that explores all edges of the polygonal model to determine the referencing neighbors. Thus, the complexity is only 
() E ON , whether E N 
is the number of edges in the cover model. However, the proposed technique lowers the number of the referencing neighbors for each embedding vertex at approximately half of the actual neighbors. Despite this, the referencing ratio is still superior to that of Cheng and Wang's algorithm. 
Conclusions
This study proposes an adaptive information hiding algorithm for polygonal models. The main point of this algorithm is to use a modified BFS method to efficiently derive the referencing neighbors for each embeddable vertex. Thereafter, the surface complexity of the embedding vertex is then estimated by the distance from the center of the referencing neighbors. Different amounts of secret messages are embedded according to the surface properties of each vertex. To decrease the model distortion caused by a large embedding capacity, a constant threshold is employed to control the maximum embedding capacity for each vertex. The proposed algorithm can provide a higher embedding capacity, higher robustness, and a lower model distortion under acceptable estimation accuracy. Most importantly, the performance for determining the referencing neighbors of each embeddable vertex can be significantly improved. With the help of experimental results, this study demonstrates the feasibility of this technique for 3D adaptive information hiding. 
"
"Introduction
The most important contribution of modern cryptography is the invention of a way to create digital signatures. A digital signature is an electronic analogue of a written signature to be used by the recipient or a third party to identity of the signatory or to verify the integrity of the data. To deal with specific application scenarios, digital signature schemes have evolved into many different variants. Among them, aggregate signature schemes, which allow a collection of individual signatures to be compressed into a single short signature, are most useful for reducing the size of certificate verification chains and for reducing message size in secure routing protocols 
Preliminaries
 2.1 Generic Construction of a Certificateless Aggregate Signature Scheme 
 A certificateless aggregate signature (CLAS) scheme consists of three parts, initial setup InitSetup, signature generation and aggregation CL-Sign, and signature verification CL-Verify: 
 InitSetup. This part consists of the following algo- rithms: 
 Setup: This algorithm, run by the KGC, takes a security parameter as input, then outputs masterkey and system parameter params. Partial-Private-Key-Extract: This algorithm, run by the KGC, takes params, master-key and a user's identity ID as inputs, then outputs a partial-private-key D ID to that user. Set-Secret-Value: This algorithm, run by a user, returns a secret value x. Set-Private-Key: This algorithm, run by a user, takes the user's partial-private-key D ID and his secret value as inputs, and outputs the full private key. Set-Public-Key: This algorithm, run by a user, takes params and the user's full private key as inputs, and outputs a public key pk ID for that user. CL-Sign. This part consists of an individual signature generation algorithm and a signature aggregation al- gorithm. 
 IndiSign: The individual signature generation algorithm , run by a signer, takes params, a message m, and the user's full private key as inputs, and outputs σ as the signature for the message m. 
SignAggr: The signature aggregation algorithm, run by any user or a third party, takes n individual signatures σ i on messages m i generated by users of identities ID i where i = 1, · · · , n, as input and returns an aggregate signature σ. CL-Verify. This part consists of an individual signature verification algorithm and an aggregate signature verification algorithm. 
 IndiVeri: The individual signature verification algorithm , run by a verifier, takes params, a public key pk ID , a message m, a user's identity ID, and a signature S as inputs. The verifier accepts signature S if and only if S is the signature of the message m for the public key pk ID of the user with identity ID. 
 SignVeri: The aggregate signature verification algorithm , run by a verifier, takes an aggregate signatures σ i on messages m i generated by users of identities ID i and public key pk IDi where i = 1, · · · , n, as input and accepts the aggregate signature σ if it is valid. 
Security Models
Traditionally, a digital signature scheme is secure if it is existentially unforgeable against adaptive chosen message attacks. The attack methods are centered on querying signatures for adaptive chosen messages. For a CLS scheme, the situation is more complicated since the attackers can do a lot more than merely querying signatures. For example , they can query for the partial private key of any user. Therefore, when discussing the security issues of a certificateless signature scheme, there are two types of adversaries , A I and A II corresponding to two types of attack models Type-I and Type-II respectively. A Type-I attack model is used to model the case when an adversary A I has compromised the user secret value or replace the user public key. However, he cannot compromise the master-key nor access the user partial key. Whereas a Type-II attack model is used to model the case when an adversary A II (the malicious-but-passive KGC) has gained access to the master key but cannot perform public key replacement of the user being attacked. Since our attack is of Type-I, we describe the attack model in more detail. We refer the readers to 
Initialization. C runs Setup algorithm to generate the master key and public parameters to A I . 
 Queries. A I can adaptively perform the following polynomially bounded queries. Partial-Private-Key query: A 1 can query for the partial private key of any user with identity ID. C will return the partial private key D ID to A 1 . Public-Key query: A 1 can query for the public key of any user with identity ID. C will return the public key pk ID of that user. Secret-Value query: A 1 can query for the secret value of any user with identity ID. C will return the secret value x ID of that user to A 1 . Public-Key-Replacement: For any user with identity ID and public key pk, A can set a new public key pk , and then C replaces pk with pk . 
IndiSign query: A 1 can query for the signature σ i corresponding to a message m i , a user with identity ID i and public key pk i . C will generate σ i , and return it to A 1 . 
 SignAggr query: A 1 can query aggregate signature for multiple signatures, C will return an aggregate signature σ by the SignAggr algorithm and return it to A 1 . Forgery. A 1 outputs an aggregate signature 
σ * = (R * , S * ) of n individual signatures σ i on messages m i generated by users of identities ID * i where i = 1, · · · , n. A 1 
 wins the game if and only if the following conditions hold. 
1) The forged aggregate signature σ * is valid. 
2) For each i, 1 ≤ i ≤ n, at least one of the secret value or the partial private key of ID * i has not been queried. 
3) σ * has never been queried by the IndiSign and SignAggr oracles. 
CCLAS
 Most certificateless signature schemes are based on bilinear pairing 
G 1 × G 1 → G 2 
, where G 1 is an additive cyclic group of prime order q, and G 2 is a multiplicative cyclic group of the same order q. We are interested in bilinear maps with the following properties: 
 1) Computable: given P, Q ∈ G 1 , there exists a polynomial time algorithm to computê e(P, Q) ∈ G 2 . 
2) Bilinear: for any x, y ∈ Z * q , we havê e(xP, yP ) = ˆ e(P, P ) xy for any P ∈ G 1 . 3) Non-degenerate: if P is a generator of G 1 , thenê thenˆthenê(P, P ) is a generator of G 2 . The CCLAS scheme consists of eight probabilisticpolynomial time algorithms, namely Setup, PartialKey- Gen, UserKeyGen, IndiSign, IndiVeri, SignAggr, SignVeri and ExtAggr. Setup: The KGC determines a bilinear mapêmapˆmapê : G 1 × G 1 → G 2 where G 1 is a cyclic additive group of prime order q with a generator P , G 2 is a cyclic multiplicative group of the same order, and three hash functions H 1 : {0, 1} * → Z * q , H 2 : {0, 1} * → Z * q , and H 3 : {0, 1} * → G 1 . Then it randomly chooses s ∈ Z * q as master-key, and then sets P pub as the master-public-key where P pub = sP . Finally , it publishes the system parameter params 
= G 1 , G 2 , ˆ e, q, P, P pub , H 1 , H 2 , H 3 . 
Partial-Private-Key-Extract: The KGC, based on params, master-key s and user's identity ID i , computes and returns a partial-private-key D i = sQ i to the user with identity ID i where 
Q i = H 1 (ID i ). 
UserKeyGen: A user with identity ID i , sets a random value x i ∈ Z * q as his secret value and public key P i = x i P . The pair (D i , x i ) is the user's full secret key 
SK i . 
 IndiSign: To facilitate the aggregation of individual signatures , a random string ω, called state string, is chosen by the first signer. Each subsequent signer checks that it has not used the string ω before. To sign a message m i using the full secret key (x i , D i ), the signer with identity ID i should perform the following steps: 
1) Compute P ω = H 2 (ω); 2) Pick a random number from r i Z * q and compute R i = r i P ; 3) Compute h i = H 3 (ω); 4) Compute P ω = H 2 (m i , ID i , ω); 5) Compute S i = r i P ω + D i + x i h i ; 6) Output σ i =< R i , S i >. 
IndiVeri: To verify a signature σ i =< R i , S i > on the state string ω and the message m i , the verifier should perform the following steps: 
1) Compute P ω = H 2 (ω); 2) Compute h i = H 3 (m i , ID i , ω); 
3) Accept the signature if and only ifêifˆifê(P, 
S i ) = ˆ e(R i , P ω )ˆ e(P pub , Q i )ˆ e(P i , h i ). 
SignAggr: For i = 1, · · · , n, to aggregate signatures σ i =< R i , S i > on state string ω and messages m i signed by users with identities ID i , one should perform the following: 
1) Compute S = Σ n i=1 S i and R = Σ n i=1 R i ; 
2) Output the aggregate signature σ =< R, S >. SignVeri: To verify a signature σ i =< R i , S i > on the state string ω and the message m i , the verifier should perform the following steps: 
1) Compute P ω = H 2 (ω); 2) Compute Q i = H 1 (ID i ) and h i = H 3 (m i , ID i , ω) for i = 1, · · · , n; 
3) Accept the aggregate signature if and only if 
ˆ e(P, S) = ˆ e(R, P ω )ˆ e(P pub , Σ n i=1 Q i )ˆ e(Π n i=1 P i , h i ). 
The aggregate signature is compact in a sense that its length is the same as that of an individual signatures. Furthermore , CCLAS scheme introduces another algorithm called ExtAggr which can be used to extract a valid individual signature. When an individual signature is be extracted from the aggregate signature the remaining part is also a valid aggregate signature. 
Cryptanalysis of CCLAS
A Type I Attack
In this section we will show that is in fact forgeable under Type I attack. The attack goes as follows. Suppose an adversary, say Alice, knows the secret value x i of a user with identity ID i through the Public-Key- Replacement query or the Secret-Value query query. Then Alice can issue an IndiSign query to obtain a signature σ i on a message m i and a state string ω such that 
σ i = (R i , S i ) where R i = r i P , S i = r i P ω + D i + x i h i , and h i = H 3 (m i , ID i , ω
). Note that Alice cannot compute the partial private key D i directly. However, from σ i , Alice can compute T = r i P ω +D i = S i −x i h i since x i is known. Now it is very simple for Alice to forge a signature σ = (R , S ) for any message m under the same state string ω. She only needs to set R = R and S = T + x i h where 
h = H 3 (m , ID i , ω). Sincê e(P, S ) = ˆ e(P, T + x i h ) = ˆ e(P, r i P ω +D i +x i h ) = ˆ e(R , P ω )ˆ e(P pub , Q i )ˆ e(P i , h ), σ = (R , S 
) is indeed a valid signature for message m . Hence, given an aggregate signature σ which includes σ i , Alice can use ExtAggr algorithm to extract σ i from σ followed by adding σ to it to obtain a forged aggregate signature σ * . 
Discussion
The linear equation used to construct the second part of a signature in CCLAS is similar to that of the CLS short signature scheme proposed in 
Conclusions
The integration of certificateless public key cryptography and aggregate signature has many potential applications. However, for a certificateless aggregate signature scheme to be used in application environments, we must make sure that it is secure against attacks. Therefore cryptanalysis plays a vital role for a cryptographic protocol to be successfully applied in the real world. In this paper, we have analyzed CCLAS scheme and showed that it is not secure against strong Type-I attacks. 
"
"Introduction
 Data hiding refers to the general process by which a discrete information stream is embedded within a multimedia signal by imposing nearly invisible changes on the host signal, such as text, audio, image, or video. There is a variety of data one may want to hide in such data sets. The hidden information may be a textual description of image features, some complementary information (words, sound, etc.) about the original scene, or something has nothing to do with the host data. Information hiding has many application areas, such as the copyright protection for digital media watermarking, fingerprinting, steganography and data embedding. In data hiding applications, the hidden data can represent authorship information, a time stamp, or copyright information. Mostly, information hiding method is applied when people try to transmit some information secretly, but there must be some malicious opponents apply various operations to interfere with the process, they want to get this information, or corrupt this information. Transmitter must be able to recognize the presence of an attacker who attempts to disrupt the communication of hidden data despite that information is corrupted only by un-malicious manipulation. Our focus is on transferring significant amounts of information to a decoder. Channel capacity is the maximum data transmission rate across a communication channel with the probability of decoding error-approaching zero, and the rate distortion function is the minimum rate needed to describe a source under a distortion constraint. 
Channels Model
Moulin and O'Sullivan have set up a channel model in 
Encoder N f N S M N K N X ) | ( N N N x y A N Y Attack channel Decoder N S N f N X M ^ ^ 
Figure 1: Information communication problem M is the hiding information, 
S N = (S 1 , S 2 , · · · , S N ) is host data set, K N = (K 1 , K 2 , · · · , K N ) 
 is side information . M is to be hidden into S N through the side information K N . Encoder must select a good method to implement this hiding work, i.e. to find a good expression for f N = (M, S N , K N ). After encoding, we get embedded information X N , which is certain to suffer from some attack in information communication, then the decoder receives Y N from the communication channel A N (y N | x N ), the decoder must try his best to get the hidden information through φ N (Y N , K N ). The messagê M ( ˆ M ∈ ℘) must have some difference with M due to the attack. To the encoder, he must guarantee the transparency and robustness of information hiding, so X N should have a little difference with S N , in order to control the difference ; the encoding progress should be subject to the expected-distortion constraint as Equation (1): 
Ed N (S N , X N ) ≤ D 1 . 
(1) 
Meanwhile, there is a relative formula as follows: 
d N (x N , y N ) = 1 N N k=1 d(x k , y k ). 
 To the attacker, he must guarantee his attack is acceptable , otherwise Y N has nothing to do with X N . In that way, his attack loses meaning, so the attack progress should be subject to some distortion constraint. 
Ed N (X N , Y N ) ≤ D 2 . 
(2) 
Then C(D 1 , D 2 ) is defined as the super-mum of all achievable rates for distortions D 1 , D 2 . A rate R = 1 N log |℘| is achievable for distortions (D 1 , D 2 ), too. Lit- erature 
s N ∈S N k N ∈K N m∈℘ 1 |℘| p(s N , k N )d N 1 (s N , fN (s N , m, k N )) ≤ D1. 
Equation 
(2) can be expressed as: 
x N ∈X N y N ∈Y N d N 2 (x N , y N )A(y N |x N )p(x N ) ≤ D 2 . 
The average probability of error is described as: 
P e,N = 1 |℘| m∈℘ P r[φ N (Y N , K N ) = m|M = m] = 1 |℘| m s N (y N ,k N ):φN (y N ,k N ) =m A N (y N |f N (S N , m, k N ))p(s N , k N ). 
In the recent years, many models have been set up, but they are all subject to analogy distortion constraint. For example, M. Barni et al have put forward their ideas in 
x i = s i + γm i |s i |. 
(3) 
Where s i indicates the original DCT, DFT, or DWT coefficients, x i indicates the hided coefficients, m i is the i-th component of the hiding information, i presents the position of the marked coefficients within the frequency spectrum, and γ is a parameter controlling the hiding strength. Cover and Chiang have designed four channel particularities in 
2) Channel C 11 : both the encoder and the decoder know the side information. 
 3) Channel C 01 : only the decoder knows the side infor- mation. 4) Channel C 10 : only the encoder knows the side infor- mation. 
Then Cover and Chiang research on hiding capacity of the four channels according to the character of them. A N (y N |x N ) depicts the statistics dependence between input signals and output ones. The decoder who has plentiful experiences can deduce which kind of attack has taken place, and then he may extract hidden information more exactly. The capacity estimates of data hiding systems under some practical attacks must be considered because the attack is inevitable. Information hiding can be thought as a game among two cooperative players (the information hider and the decoder) and an opponent (the attacker). The first player tries to maximize a payoff function, the opponent tries his best to minimize it, so the function should have connection with encode f N , decode φ N , attack A N . Surely, the game must obey the two expecteddistortion constraints (1) and (2), so the hiding-capacity is the function of f N , φ N , A N , D 1 , D 2 , we define payoff function as follows: 
J(f N , φ N , A N , D 1 , D 2 ) = max fN ,D1,φN { min AN ,D2 {C(f N , φ N , A N , D 1 , D 2 )}}. If D 1 , D 2 is decided at first, then J(f N , φ N , A N ) = J(f N , φ N , A N , D 1 , D 2 ). 
Here, let's define the support set of p(s, k), Ω = {(s, k) ∈ S × K : p(s, k) > 0}. We introduce an auxiliary variable U (Maybe U includes some information about host data, side information , attack channel, or nothing important). We use Q(x, u|s, k) denote a conditional probability distribution function (pdf) from S × K to X × U . The function Q(x, u|s, k) has depicted all information about encoding procession. Furthermore, it includes some other information with respect to U . Additionally, φ N is determined by f N in a certain degree, so Q(x, u|s, k) can substitute f N and φ N , we can derive that: 
J(Q, A) = J(f N , φ N , A N ). 
 Then we can get data hiding -capacity by the application of information theory: 
J(Q, A) = I(U ; Y |k) − I(U ; S|K). 
This equation denotes that payoff function is equal to the discrimination between the quantity of information that can be got about U from Y and the quantity of information that can be got about U from S under the same assumption that side information is known. 
Conclusion 1: Assume that for any N ≥ 1, the attacker knows f N , the decoder knows both f N and the attack channel, a rate R is achievable for distortion D 1 and attacks in the class {A(f N )}, if and only if R < C, where C = max Q(x,u|s,k)∈Q min A(y|x)∈A(Q) J(Q, A) and U is a random variable defined over an alphabet U of cardinality of |U | ≤ |X||Ω| + 1. 
 4 Estimates of Data Hiding -Ca- pacity 
 Recent research has shown that the data hiding capacity is the value of a mutual-information game among the encoder, decoder and attacker. The capacity of channel is given as C = max p(M) {I(M ; Y N )} by Claude Shannon in 
Non-Blind Channels
A Simple Non-Blind Case
Let's first discuss a simple non-blind case, the data hiding capacity can be written as follows: 
C = max p(M) {I(M, Y N |S N )} (4) = max p(M) {H(M, S N ) − H(M |S N , Y N )} (5) ≤ H(M * |S N ). 
(6) 
Equation (4) is the definition of the capacity for the non-blind case (decoder knows host data set S N ). Equation (5) is the definition of the mutual-information by the theory of information. Equation (6) is reached because of the non-negativeness of entropy. 
Gauss Channels
If S follows a Gaussian distribution, let S ∼ N (0, 
δ 2 ), d(x, y) = (x − y) 2 
, this model is very special and widely used. Literature 
Conclusion 2: Let S = X = Y = R (R 
is the set of real number) and d(x, y) = (x − y) 2 be the squared-error distortion measure. Assume that K = S, let α be the maximized of the following function: 
f (α) = [(2α − 1)δ 2 − D 2 + D 1 ][D 1 − (α − 1) 2 δ 2 ] [D 1 + (2α − 1)δ 2 ]D 2 , in the interval (α inf , 1 + √ D 1 /δ), where α inf = max(1, δ 2 +D2−D1 2δ 2 
). Then we have: 
1) If D 2 ≥ (δ + √ D 1 ) 2 
, the hiding capacity is C = 0. 
2) If S is non-Gaussian with mean zero and standard 
deviation δ > √ D 2 − √ D 1 , 
the hiding capacity is upper-bounded by 
CG = 1 2 log(1 + [(2α − 1)δ 2 − D2 + D1][D1 − (α − 1) 2 δ 2 ] [D1 + (2α − 1)δ 2 ]D2 ). (7) 3) If S ∼ N (0, δ 2 ) and D 2 < (δ + √ D 1 ) 2 
 , the hiding capacity is given by Equation (7). The optimal covert channel is given by X = αS + Z, where Z ∼ N (0, 
D 1 − (α − 1) 2 δ 2 ) 
is independent of S. The optimal attack is the Gaussian test channel from ratedistortion theory, 
A * (y|x) = N (β −1 x, β −1 D 2 ), (8) where β = δ 2 x δ 2 x −D2 , and δ 2 x = D 1 + (2α − 1)δ 2 . 
The role of the host-signal scaling parameter α ≥ 1 in Conclusion 2 is to increase the value of δ 2 x and thereby to reduce the effective noise variance δ 2 w of the Gaussian test channel. From Equation (7) we can give 
C = 1 2 log(1 + δ 2 z δ 2 w 
), where δ 2 w decreases as α increases, and δ 2 z increases as tends to 1. Hence the optimal value of α results from a tradeoff. 
Parallel Gauss Channels
 Parallel Gaussian models are useful in that they are reasonably tractable and provide capacity expressions for realistic signal models. They also provide upper bounds on capacity if the actual distribution of S differs from the model are a more important reason. For instance, any correlation between subsignals S k would decrease capacity, as well as any deviation from a Gaussian distribution with the same second-order statistics 
Spike Models
Let us consider a rate-distortion bound for still-image compression, under a so-called spike model that captures the sparsity of wavelet image representations 
δ 2 k >> D 1 , D 2 for 1 ≤ k ≤ K * and δ 2 k << D 1 , D 2 for K * < k < K. 
Then, we will get 
C = 1 2 r * log(1 + D1 D2−D1 ), where r * = K * k=1 r k ∈ 
1) The optimal power allocations by the data hider and the attacker are independent of the signal variances in the strong channels, provided that these variances are much relative to D 1 and D 2 . 
2) The optimal data-hiding strategy equalizes the power among the strong channels, and likewise, the optimal attack strategy equalizes the noise power among strong channels. Negligible power is allocated to weak channels. 
3) The (per-sample) capacity C k is the same for all strong channels and is negligible for the weak channels . The capacities {C k } are in the strong channels and C = k r k C k depends only on the distortion levels D 1 and D 2 , rather than the variances δ 2 k . 
AR-1 Models
AR-1 model is a classical model. First, we assume the host-image source is a separable AR-1 Gaussian process 
S(f x , f y ) = δ 2 (1 − ρ 2 x )(1 − ρ 2 y ) |1 − ρ x e −j2πfx | 2 |1 − ρ y e −j2πfy | 2 , − 1 2 ≤ f x , f y ≤ 1 2 . 
Compute data-hiding capacity estimates for image sources characterized by different values of ρ x , ρ y and δ 2 . The capacities are computed using the numerical algorithm mentioned in 
D 2 /D 1 − 1). 
2) There is a saturation of the capacity C k in a given channel when the variance δ 2 k in that channel increases , and C k is proportional to δ 2 k for small δ 2 k . 
3) For relatively low values of ρ x = ρ y (say less than 0.8), capacity is essentially the same as in the i.i.d. Gaussian case: 
C ≈ 1 2 log(1 + D1 D2−D1 ). 4) As ρ x = ρ y approaches 1
, capacity tends to zero, as more and more channels (frequencies) become weak and hence are unable to hide significant information. 
Blind Channels
Rate of reliable transmission for blind information hiding clearly cannot be higher than the rate in the case that the decoder has access to side information and host data. So hiding-capacity is upper-bounded by Equation 
(7) for any p(s). In the following, Theorem 5.3 in 
"
"Introduction
 Secret sharing is an important research content of modern cryptography, it is a method of increasing the security of cryptography system. The earliest secret sharing schemes were proposed by Shamir 
In order to prevent malicious behavior of the dealer and participants, a new type of secret sharing scheme was first proposed by Feldman 
Preliminaries
 In this section, we briefly describe the definition of multilinear map, the related security assumptions, and recall the publicly verifiable secret sharing (PVSS) scheme. 
Multilinear Maps
Boneh and Silverberg (BS) 
· · · , a n ∈ Z * q , we have e n (a 1 g 1 , a 2 g 2 , · · · , a n g n ) = e n (g 1 , g 2 , · · · , g n ) a1a2···an ; 
2) Non-degenerate: If g ∈ G 1 is a generator of G 1 , then e n (g, g, · · · , g) is a generator of G 2 ; 
3) Computable: For all g 1 , g 2 , · · · , g n ∈ G 1 , there is an efficient algorithm to compute e n (g 1 , g 2 , · · · , g n ). 
Security Problems and Assumptions
Computational Diffie-Hellman (CDH) problem: Given g, ag, bg ∈ G 1 for some a, b ∈ Z * q , it is difficult to compute abg ∈ G 1 . Discrete logarithm (DL) problem: Given g, ag ∈ G 1 , it is hard to compute a ∈ Z * q . 
Multilinear discrete logarithm (MDL) problem: Let G be a finite cyclic group with prime order q, for all k > 1, 1 ≤ i ≤ k and g i ∈ G, given (i, g i , ag i ) for some a ∈ Z * q , it is hard to compute a. n-Multilinear computational Diffie-Hellman (n- MDH) problem: Given g, a 1 g, a 2 g, ..., a n g ∈ G 1 for some random selective a 1 , a 2 , · · · , a n ∈ Z p , where g is a generator of group G 1 , it is hard to compute 
e n (g, g, · · · , g) a1a2···an ∈ G 2 . 
MDH assumption: No PPT algorithm can solve the MDH problem with a non-negligible advantage. 
Model of PVSS
 In this section, the model of (t, n) threshold publicly verifiable secret sharing (PVSS) scheme is presented. Let t and n be two positive integers such that 1 ≤ t ≤ n. Let U 1 , ..., U n denote the participants, and D denotes the dealer. An access structure can be a (t, n) threshold scheme for 1 ≤ t ≤ n, it means that any subset of t or more participants is able to reconstruct the secret, while the subset of at most t-1 participants cannot recover the secret and has no information about it. The system of a PVSS scheme consists of three phases are described be- low. 
1) Initialization phase: On input the number n of participants, a threshold t, it outputs all public parameters as well as participants' private keys and the corresponding public keys as part of the system pa- rameters. 
 2) Distribution phase: On input a secret s, the distribution phase consists of two steps as follows. 
a. Share distribution: The dealer D distributes a secret s among n participants, the dealer uses the participants' private keys and public parameters to encrypt secret and then publishes some specific value Y i (the shares are embedded into these specific values Y i ) to the participants U i , where for i = 1, 2, · · · , n. b. Public verification: This step can be executed by a third party and determines whether the distributed shares are valid. Anyone not just the participants can verify these specific values Y i by checking some equations. If all the checking equations hold, then these specific values Y i are believed to be correctly published by the dealer, and the shares included in Y i are valid. Once the equations do not hold, we say that the dealer fails to distribute a secret, and then break the scheme. 
3) Reconstruction phase: The reconstruction phase contains decryption of the shares and reconstruction of the secret: a. Decryption of the shares: Each participant uses his/her own private key to obtain the corresponding share s i from the specific value Y i , respectively. b. Reconstruction of the secret: When the qualified participants offered at least t correct shares s i , then the secret s can be recovered from these shares s i by threshold technique such as Lagrange interpolation. 
Proposed PVSS Scheme
 In this section, we present our non-interactive and effective PVSS scheme based on multiple linear pairing. First, the key generation center (KGC) generates m public parameters P 
(1) pub , P (2) pub , ..., P (m) 
pub , m ∈ R Z * q . We assume that the secret S = e m (P 
(1) pub , P (2) pub , ..., P (m) 
pub ) a will be distributed by the dealer D among n participants, where 
a ∈ Z * q . Let U = {U 1 , U 2 , · · · , U n } 
 be a set of n qualified participants. The PVSS scheme consists of three phases: Initialization phase, Distribute phase and Reconstruct phase. 
1) Initialization phase Let G 1 and G 2 be two groups, separately denote additive cyclic group and multiplicative cyclic group which have the same prime order q. Assuming that there exists a multilinear map e : 
G n 1 → G 2 among G 1 and G 2 . 
The independently generators P, Q of groups G 1 and G 2 are selected using appropriate public procedure. Each participant U i chooses a private key d i ∈ Z * q and compute the corresponding public key 
P i = d i P 
(i) pub for i = 1, 2, ..., n. 2) Distribute phase The distribution phase consists of two steps as fol- lowing: a. Distribution of the shares: The dealer D wishes to distribute a secret among n participants . The dealer D first chooses a 
dom polynomial f (x) = t−1 j=0 a j x j of 
degree at most t − 1 with coefficients in Z q . Here f (0) = a 0 = a. And then the dealer keeps this polynomial secretly but computes and publishes the following values: the related commitments C j = a j · P , for j = 0, 1, ..., t − 1, 
X i = f (i) · P and γ i = f (i) · P (i) 
pub . The dealer also publishes the encrypted shares 
Y i = e m (P i , P (1) pub , · · · , P (i−1) pub , P (i+1) pub ..., P (m) pub ) f (i) for i = 1, 2
, ..., n. Each X i can be constructed by all public values C j as follows: 
X i = f (i) · P = t−1 j=0 a j · (i j ) · P = t−1 j=0 (i j ) · a j · P = t−1 j=0 (i j ) · C j b
 . Verification of shares: Anyone first can 
cover X i = t−1 j=0 (i j ) · C j from 
the value C j and then checks equation (1) by public values C j and X i , γ i , for j = 0, 1, ..., t − 1, i = 1, 2, ..., n. Equation (1): 
e m (γ 1 , ..., γ j−1 , P (j) pub , γ j+1 , ..., γ m−1 , X j ) = e m+1 (γ 1 , γ 2 , ..., γ m−1 , P ) (1) 
 If the Equation (1) holds, then the verifier believes that these specific values Y i correctly published by the dealer D and the verifier can confirm that each Y i holds for i = 1, 2, ..., n. The proof is as follows: 
Y i = e m (P i , P (1) pub , ..., P (i−1) pub , P (i+1) pub , ..., P (m) pub ) f (i) = e m (d i · P (i) pub , P (1) pub , ..., P (i−1) pub , P (i+1) pub , ..., P (m) pub ) f (i) = e m (P (i) pub , P (1) pub , ..., P (i−1) pub , P (i+1) pub , ..., P (m) pub ) di·f (i) = e m (P (1) pub , P (2) pub , ..., P (m) pub ) di·f (i) . 
3) Reconstruct phase 
This phase is divided into decryption of the shares and the reconstruction of the secret: a. Decryption of the shares: Each participant U i uses his/her own private key d i to compute the corresponding share 
S i = e m (P (1) pub , P (2) pub , · · · , P (m) 
pub ) f (i) by computing the following equation: 
Y i d −1 i = e m (P i , P (1) pub , ..., P (i−1) pub , P (i+1) pub , · · · , P (m) pub ) f (i)·d −1 i = e m (d i · P (i) pub , P (1) pub , ..., P (i−1) pub , P (i+1) pub , · · · , P (m) pub ) f (i)·d −1 i = e m (P (i) pub , P (1) pub , ..., P (i−1) pub , P (i+1) pub , · · · , P (m) pub ) f (i) = e m (P (1) pub , P (2) pub , ..., P (m) pub ) f (i) = S i . 
b. Reconstruct of the secret: Any t shareholders U i with the correct shares S i can reconstruct the secret 
S = e m (P (1) pub , P (2) pub , ..., P (m) pub ) a , for i = 1, 2
, ..., t. The secret S is obtained by Lagrange interpolation as Equation (2): 
S = t i=1 S λi i = e m (P (1) pub , P (2) pub , ..., P (m) pub ) a (2) 
Where λ i = j =i i j−i is Lagrange coefficient. 
Scheme Analysis
This section introduced the proof of the correctness and security of the proposed scheme, and we make performance analysis mainly in the computation and communication aspects. 
Correctness Analysis
Lemma 1. First, we verify the equation e m (γ 1 , ..., γ j−1 , P pub , γ j+1 , ..., γ m−1 , 
X j ) = e m (γ 1 , γ 2 , ..., γ m−1 , P ) (1). 
Proof. From the public values 
X i = f (i) · P , γ i = f (i) · P (i) 
pub . we can gain that 
e m (γ 1 , γ 2 , ..., γ j−1 , P (j) pub , γ j+1 , ..., γ m−1 , X j ) = e m (γ 1 , γ 2 , ..., γ j−1 , P (j) pub , γ j+1 , ..., γ m−1 , f (j) · P ) = e m (γ 1 , γ 2 , ..., γ j−1 , f (j) · P (j) pub , γ j+1 , ..., γ m−1 , P ) = e m (γ 1 , γ 2 , ..., γ j−1 , γ j , γ j+1 , ..., γ m−1 , P ) = e m (γ 1 , γ 2 , ..., γ m−1 , P ). 
Hence, Equation (1) holds, the shares distributed by the dealer are valid. Lemma 2. And then verify that the method of reconstructing the secret is correct. In other words, it is need to 
verify equation S = t i=1 S λi i = e m (P (1) pub , P (2) pub , ..., P (m) pub ) a . 
Proof. From the known share value 
S i = e m (P (1) pub , P (2) pub , · · · , P (m) pub ) f (i) 
, which is computed from private key d i and specific public value Y i , we can get that 
t i=1 S λi i = t i=1 (e m (P (1) pub , P (2) pub , ..., P (m) pub ) f (i) ) λi = e m (P (1) pub , P (2) pub , ..., P (m) pub ) t i=1 f (i)·λi = e m (P (1) pub , P (2) pub , ..., P (m) pub ) f (0) = e m (P (1) pub , P (2) pub , ..., P (m) pub ) a = S The Equation (2) 
 holds, so the method of secret reconstruction is correct. 
Security Analysis
In this section, we present security analysis of our proposed scheme under the multilinear Diffie-Hellman (MDH) assumption. We first consider the security of the shares 
S i = e m (P (1) pub , P (2) pub , ..., P (m) pub ) f (i) . Given the public values P (i) 
pub , P i , X i and Y i for i = 1, 2, ..., n, we observe that the difficulty of computing the share S i is equivalent to solve the multilinear Diffie-Hellman (MDH) problem as described in Section 2. Consequently, we have the following lemma. Lemma 3. The encryption of shares is security in the proposed PVSS scheme if and only if the MDH assumption holds. Proof. ⇐ By contradiction proof. Assuming that the MDH assumption holds but the encryption of shares is not security. Since the method of share encryption does not hold, then there exists an Algorithm A can compute the shares 
S i = e m (P (1) pub , P (2) pub , ..., P (m) 
pub ) f (i) with a non-negligible probability ε for the given public value P 
(i) pub , P i , X i and Y i . Then we want to prove that an attacker can solve the MDH problem with the same probability using the Algorithm A. The MDH problem is that given a 1 P, a 2 P, ..., a m P for some a 1 , a 2 , · · · , a m ∈ Z * q , it is hard to compute e m (P, P, · · · , P ) a1a2···am . Hence, we try to compute the value e m (P, P, · · · , P ) a1a2···am using A in the following. The attacker chooses random elements a 1 , a 2 , 
· · · , a m , b ∈ Z * q and a 1 , a 2 , · · · , a 
m , b ∈ Z * q . For the given values Q 1 = a 1 P, Q 2 = a 2 P, ..., Q m = a m P, Q = bP , the attacker first computes and feeds the values P 
(i) 
pub = a i ·Q i , P i = a i · P (i) pub , X i = b · Q and Y i = e m (P (1) pub , P (2) pub , ..., P (m) 
pub ) ai·bb to A, where i = 1, 2, ..., n. Since the input of A is uniformly distributed and X i = b · Q = b · bP = f (i)P is known, we obtain that 
S i = e m (P (1) pub , P (2) pub , ..., P (m) pub ) f (i) = e m (a 1 a 1 ·P, a 2 a 2 ·P, ..., a m a m ·P ) f (i) = e m (P, P, ..., P ) a1a 1 ·a2a 2 ·ama m ·f (i) = e m (P, P, ..., P ) a1a 1 ·a2a 2 ·ama m ·bb with 
 the same nonnegligible probability ε. By taking 
(S i ) 1/(a 1 a 2 ···a m bb ) = e m (P, P
, ..., P ) a1a2···am , we know that the attacker is able to compute e m (P, P, · · · , P ) a1a2···am with the probability ε. It is a contradiction to the above MDH assump- tion. It shows that the MDH assumption holds, then the encryption of shares is secure. ⇒ By contradiction proof. Assuming that the encryption of shares is secure but the MDH assumption does not hold. Because the MDH assumption does not hold, then there exists an algorithm B can compute e m (P, P, · · · , P ) a1a2···am with a non-negligible probability ε for m random elements a 1 P, a 2 P, ..., a m P ∈ G 1 , where 
a 1 , a 2 , · · · , a m ∈ Z * q . 
 The attacker chooses random 
ments β 1 , β 2 , · · · , β m , b ∈ Z * q and β 1 , β 2 , · · · , β m , b ∈ Z * q . 
 When feeding Q = bP, X i = b Q to B, the attacker computes and inputs 
Q 1 = β 1 ·P, Q 2 = β 2 ·P, ..., Q m = β m · P , P (i) pub = β i P for i = 1, 2
, ..., n. Then the share S i must satisfy that 
S i = e m (P (1) pub , P (2) pub , · · · , P (m) pub ) f (i) = e m (Q 1 , Q 2 , · · · , Q m ) = e m (β 1 P, β 2 P..., β m P ). 
Since the input of B is uniformly distributed, we can compute X i = b Q = b bP = f (i)P with the same probability ε because of Q = bP, X i = b Q. Therefore , we can obtain that 
e m (β 1 P, β 2 P, ..., β m P ) f (i) = e m (β 1 P, β 2 P..., β m P ). Which produces e m (P, P..., P ) β 1 β 2 ···β m bb = e m (P, P..., P ) β1β2···βm . 
 Due to the MDH assumption does not hold, So the algorithm B can compute e m (P, P..., P ) β1β2···βm with the same non-negligible probability ε, and then the share S i can be computed by algorithm B. Hence, the encryption of shares is not secure. It shows that the encryption of shares is secure, the MDH assumption must hold. Lemma 4. If only t-1 participants can work together to recover the secret in the proposed scheme, then the Multilinear Diffie-Hellman (MDH) problem can be solved. 
 Proof. At first, we recall that the MDH problem is to compute 
e m (P, P, · · · , P ) a1a2···am for given P, a 1 P, a 2 P, ..., a n P ∈ G 1 for some random choices a 1 , a 2 , · · · , a n ∈ Z p . 
As in Section 2, solving the MDH problem is to compute e m (P, P, · · · , P ) a1a2···am with the non-negligible probability ε. Without loss of generality, we assume that t-1 participants U 1 , U 2 , ..., U t−1 are able to pool their valid shares and recover the secret. Now we need to prove that adversary Λ can compute 
e m (P, P, · · · , P ) a1a2···am 
 by using t-1 participants as oracle . In the following, we will set up the system to simulate PVSS for adversary Λ such that this system enables the adversary Λ to compute e m (P, P, · · · , P ) a1a2···am when t-1 participants are seen as oracle. The Setup system consists of six steps as follows: 
1) Adversary Λ sets P (i) pub = a i P, C 0 = bP (= f (0)P ) for i = 1, 2, ..., n, where a i ∈ Z * q , b ∈ Z * q . 
2) Taking t-1 values: The values f (1), f (2), ..., f (t − 1) are chosen at random from Z * q , and previous fixed f (0) such that a polynomial f (x) can be fixed. 
3) Adversary Λ compute forward t-1 values of X i and Y i as follows: 
X i = f (i)P , Y i = e m (P i , P (1) pub , · · · , P (i−1) pub , P (i+1) pub ..., P (m) pub ) f (i) , i = 1, 2, ..., t − 1. 4) f (0) 
is hiding fixed, so Λ is not able to compute the following values: 
f (t), f (t + 1), · · · , f (n)
. However, we can use 
X i for i = 1, 2, · · · , t − 1 to obtain C j by solving t-1 simultaneous equations X i = t−1 j=0 (i j ) · C j for j = 1, 2, · · · , t−1. 
When we have computed these values C j , we can obtain 
X i for i = t, t − 1, ..., n by Lagrange interpolation formula. 5) First compute C j (i = 1, ..., t − 1). Since f (x) = t−1 i=1 a i · x i , 
then there is the following linear system of equations: 
         f (0) = a 0 f (1) = a 0 + a 1 · 1 + · · · + a t−1 · 1 t−1 . . . f (t − 1) = a 0 + a 1 · (t − 1) 1 + · · · + a t−1 · (t − 1) t−1 
In this linear system of equations, adversary Λ knows the values of f 
(1), f (2), · · · , f (t−1)
 , while f (0) is unknown , so it is unable to compute the coefficient a i of the polynomial f (x). However, adversary Λ can compute values of C j by the linear system of equations and public values C 0 , 
X j for i = 0, 1, · · · , t − 1, j = 1, 2, ..., t − 1. 
6) Now, adversary Λ computes the public keys 
P i of participants U i as P i = v i · P (i) pub for i = 0, 1, ..., n, where v i ∈ Z * q . 
In particular, we 
set Y i = e m (X i , P (1) pub , · · · , P (m) pub ) ai·vi such that Y i = e m (P i , P (1) pub , · · · , P (i−1) pub , P (i+1) pub ..., P (m) pub ) f (i) , as required. 
Now, the complete view for the system is defined. Which is consistent with the private view of participants U 1 , U 2 , ..., U t−1 , and the view comes from the right distribution . Supposing that they are able to obtain the secret 
S = e m (P (1) pub , P (2) pub , ..., P (m) pub ) f (0) . Since P (i) pub = a i P and f (0) = b for i = 0, 1, · · · 
, n, we can compute 
e m (P (1) pub , P (2) pub , ..., P (m) pub ) f (0) = e m (P, P, · · · , P ) a1a2···amb . 
It contradicts the MDH assumption. So far we have ignored the proofs that are required at several points in the protocol. However, in the random oracle model, these proofs can easily be simulated. By the above two lemmas, we can draw the following theorem. 
Theorem 1. Under the MDH assumption, the proposed scheme is a secure PVSS scheme in the random oracle model. That is, (1) only qualified participants can compute the valid shares; (2) any subset of t-1 participants is unable to recover the secret. (3) The proposed PVSS scheme must provide publicly verifiable property. 1) From Lemma 3 and the scheme's construction method in Section 3, we know that Y i = S di i , then any attacker is unable to compute the corresponding shares S i from these specific values Y i because of the hardness of the MDH and discrete logarithm. 
2) By Lemma 4, any t participants with shares S i can obtain the secret 
S = e m (P (1) pub , P (2) pub , ..., P (m) 
 pub ) a by Lagrange interpolation method. And any subset of t-1 or less participants is unable to recover the secret unless the MDH problem is solved. 
3) From Section 3, it is easy to know that anyone not just the participants can verify each Y i whether it is equal to 
e m (P (1) pub , P (2) pub , ..., P (m) pub ) dif (i) with the dealer's secret f (i) for i = 0, 1, · · · , n. In this 
 tion, we also have verified that each qualified participant U i can use his/her private key d i to compute the share 
S i = Y i d −1 i = e m (P (1) pub , P (2) pub , ..., P (m) pub ) f (i) . Each S i also 
contains the factor f (i). So the proposed scheme must provide publicly verifiable property. 
Performance Analysis
In this subsection, we mainly analyze the computation overhead and communication overhead. The performance analysis shows that our scheme is effective when comparing with previous schemes. For convenience to evaluate the computational cost, we define the following notations: T em : The time of executing a multiple linear pairing operation e m : 
G m 1 → G 2 . 
 T G mul : The time of executing a scalar multiplication operation of points in G 1 . T exp : The time of executing a modular exponent operation of points in Z q . 
 T mul : The time of executing a modular multiplication operation of points in Z q . 
T Lag : The time of using the Lagrange interpolating method to construct the secret. T pol : The time of computing the polynomial value f (
x) = t−1 i=0 a i x i in Z q . 
1) From the computation aspect. As we all know, the most time consuming is power modular operation in the scheme based on Discrete Logarithm. The most time consuming is a modular multiplication operation of points in the scheme based on ECDLP. While the most time consuming mainly contains T em , T G mul in the scheme based on multiple linear pair- ing. Hence, we only consider these time-consuming operations T em , T G mul and T exp in the performance analysis of the proposed PVSS scheme. In our scheme, there is no need for the dealer to compute the corresponding shares for the participants, compared with the references 
Discussion
In this section, the application of our publicly verifiable secret sharing scheme is presented in electronic voting. By using our PVSS scheme as a basic tool, we get a simple and efficient voting scheme. At last, we analyze the advantages of this electronic voting scheme. From the model for universally verifiable elections as introduced by Hwang et al. 
V 1 , · · · , V l , 
and each of them acts as a dealer in our PVSS scheme, as well as a set of passive observers. These sets need not be disjoint, each player may be both a voter and a tallier. Assuming that each tallier T i has registered a public key 
P i = a i P (i) 
pub for the randomly selected private key a i ∈ R Z q , where i = 1, 2, · · · , n. The designed electronic voting scheme consists of two phases: Ballot casting and Tallying. 1) Ballot casting. A voter V casts a vote v ∈ {0, 1} by running the distribution protocol for our PVSS scheme from Section 3, using a random secret value a ∈ R Z q , the voter can compute the value 
U = e m (P (1) pub , P (2) pub , · · · , P (m) 
pub ) a+v . In addition, the voter constructs a proof P ROOF U showing that indeed v ∈ {0, 1} without revealing any information on v. P ROOF U refers to the commitment value of C 0 = a 0 P = aP which is published as part of the PVSS distribution protocol. And then each voter proves that: 
e m (P i , P (1) pub , · · · , P (i−1) pub , P (i+1) pub , · · · P (m) pub ) = e m (P (1) pub , P (2) pub , · · · , P (m) 
pub ) ai+v . Due to the publicly verifiability of the proposed PVSS scheme and the known value of P ROOF U , the ballots can be checked by using the above equation by the bulletin board when the voters submit their ballots. What's more, the ballot for voter V consists of the output values U and P ROOF U of the PVSS distribution protocol. 2) Tallying. Supposing that voters V j have all cast valid ballots, where j = 1, · · · , k and k ≤ l. The tallying protocol uses the reconstruction protocol of our PVSS scheme. We first accumulate all the respective encrypted shares, that is, we compute the values Y * i , where 
Y * i = k j=1 Y ij = e m (P i , P (1) pub , · · · , P (i−1) pub , P (i+1) pub , · · · , P (m) pub ) k j=1 fj (i) . 
And then each tallier T i applies the reconstruction protocol to the value Y * i , which will produce 
e m (P i , P (1) pub , · · · , P (i−1) pub , P (i+1) pub , · · · , P (m) pub ) k j=1 fj (0) = e m (P i , P (1) pub , · · · , P (i−1) pub , P (i+1) pub , · · · , P (m) pub ) aj 
Next, by combining with the equation 
k j=1 U j = e m (P i , P (1) pub , · · · , P (i−1) pub , P (i+1) pub , · · · , P (m) pub ) k j=1 aj +vj . We obtain e m (P i , P (1) pub , · · · , P (i−1) pub , P (i+1) pub , · · · , P (m) pub ) k j=1 vj , 
from which the tally T = k j=1 v j , 0 ≤ T ≤ k can be computed efficiently. 
The advantages of this electronic protocol: 1) In the ballot casting phase, the voters' ballots contain the votes in encrypted form and the voters need not be anonymous in this protocol. In tallying phase, the talliers use their private keys to collectively compute the final tally corresponding with the accumulation of all the valid ballots. 
2) The above electronic voting scheme achieves the same level of security with regard to publicly verifiability, privacy, and robustness. 
3) Our scheme does not require a shared-key generation protocol for a threshold decryption scheme, which avoids the interaction between the voters and the interaction among the talliers. 4) Compared with 
Analysis results show that our PVSS scheme can be used in elections for computational privacy without needing a private channel. 
Conclusion
In this paper, we proposed a non-interactive, simple and effective publicly verifiable secret sharing based on multiple linear pairing. In our PVSS scheme, not just the participants, anyone is able to verify whether the shares distributed by the dealer are correctly at the secret distribution phase and whether each participant releases valid shares at the reconstruction phase. We use multiple linear property of multilinear map and the batch verification technique to reduce the computational overhead at verification phase. The computation cost and communication overhead are lower than the previous PVSS schemes which are based on bilinear paring or discrete logarithm. In addition , under the multilinear Diffie-Hellman assumption, we have shown our PVSS scheme is security in the random oracle model. In the discussion section, we present the application of our PVSS scheme in electronic voting and analyze the advantages of this protocol. Our next work is to apply the proposed PVSS scheme in secure multi-party computation and other practical protocols. 
"
"Introduction
Random subsets of the positive integers not exceeding a certain fixed integer N have many applications in the fields of network security, cryptography and other security issues. Many researchers find out that using random subsets can improve the efficiency and security performance in the key pre-distribution procedure, then propose key management and broadcasting authentication protocol in P2P, ad hoc network and wireless sensor network 
E N = E N (R) = {e 1 , . . . , e N } ∈ 1 − | R | N , − | R | N N with e m = 1 − |R| N for m ∈ R − |R| N otherwise (m = 1
, . . . , N ). 
(1) 
Then the well-distribution measure of the subset R of {1, 2, . . . , N } is defined by 
W (R, E N ) = max a,b,t t−1 j=0 
e a+jb 
where the maximum is taken over all a, b, t such that a, b, t ∈ N and 1 ≤ a ≤ a + (t − 1)b ≤ N , while the correlation measure of order k of R is defined as 
C k (R, E N ) = max M,D M m=1 e m+d1 e m+d2 · · · e m+d k 
where the maximum is taken over all D = (d 1 , . . . , d k ) with non-negative integers 0 ≤ d 1 < · · · < d k and M such that M + d k ≤ N . One would expect that these measures are "" small "" . Thus we may consider a subset R of {1, 2, . . . , N } as a "" good "" pseudo-random subset if W (R, E N ) and C k (R, E N ) (at least for small k) are small; they must be O(N ) and ideally, they are O(N 1/2+ε ) 
1, · · · , p − 1}, F * p 
the set of non-zero elements of F p . Let E be an elliptic curve over F p , given by an affine Weierstrass equation of the standard form 
y 2 = x 3 + Ax + B 
with coefficients A, B ∈ F p and nonzero discriminant, see 
|#E(F p ) − p − 1| ≤ 2p 1/2 , 
 where #E(F p ) is the number of F p -rational points, including the point at infinity O. The translation map by 
W ∈ E(F p ) on E(F p ) is defined as τ W : P → P ⊕ W. 
It is obvious that (f @BULLET 
τ W )(P ) = f (τ W (P )) = f (P ⊕ W ). 
In this article, for convenience, we always suppose that E(F p ) is a cyclic group of order N and 
G ∈ E(F p ) is a generator, i.e., E(F p ) = G. 
In particular, N ∼ p in this case. A multiple of G is taken by 
nG = ⊕ n i=1 G. We write nG = (x n , y n ) ∈ F p × F p on E for all 1 ≤ n ≤ N − 1 and set X(nG) = x n and Y (nG) = y n . 
Construction of subsets. We would like to study the pseudorandom properties of the subset R of {1, 2, . . . , N } defined by 
R := {n | 1 ≤ n ≤ N, X(nG) ≡ h (mod p) for any h ∈ H} (2) where r ∈ Z, s ∈ N, s < p/2 and H = {r, r + 1, . . . , r + s − 1}. 
 We remark that R can be defined in several different ways using elliptic curves, we refer to a preprint version of 
The Cardinality of R
Exponential sums play an important role in the proofs to estimate the cardinality of R and its pseudo-random measures. For any positive integer m, we identify Z m with the residue ring modulo m. Put e m (z) = exp(2πiz/m). The exponential sums enter into our problem by means of the following well known basic identity. Lemma 1 (
For any element c ∈ Z m , we have 
z∈Z m e m (cz) = m, if c = 0 0, otherwise. 
We also need the following statement. Lemma 2 (
v z=u f (zG) =∞ ψ(λf (zG)) ≤ 2deg(f )p 1/2 (1 + log N ) holds for all λ ∈ F * p and integers 0 ≤ u < v ≤ N − 1. 
We now present a bound on the cardinality of R. 
Proof. From the definition of R in Equation (2) and 
Lemma 1, we have 
r+s−1 h=r λ∈Fp ψ(λ(X(nG) − h)) = p, if n ∈ R 
0, otherwise. 
Hence by Lemmas 2 and 3 we obtain 
|R| = N n=1 1 p r+s−1 h=r λ∈Fp ψ(λ(X(nG) − h)) = s(N − 1) p + 1 p λ∈F * p r+s−1 h=r ψ(−λh) N n=1 ψ(λX(nG)) ≤ s(N − 1) p + 1 p λ∈F * p r+s−1 h=r ψ(−λh) · N n=1 ψ(λX(nG)) 
≤ sN p + 4p 1/2 (1 + log p). 
We complete the proof of Theorem 1. 
Pseudo-random Measures of R
Now we present upper bounds on the well-distribution measure and the correlation measure of order k of R defined in Equation (2). The associated sequence E N defined by Equation 
(1) is 
e m = 1 − α for m ∈ R −α otherwise, where α = | R | N = s p + 8θp −1/2 (1 + log p) 
with some θ satisfying |θ| < 1, since N ∼ p. 
Let β = s p −α. 
Throughout this paper, the implied constant in the symbol "" "" is absolute. 
Theorem 2. Let R be a subset of {1, . . . , N } defined as in Equation (2), we have W (R, E N ) p 1/2 (1 + log p)(1 + log N ). Proof. For 1 ≤ n ≤ N − 1
, it is easy to see that 
e n = (1 − α) 1 p r+s−1 h=r λ∈Fp ψ(λ(X(nG) − h)) −α   1 − 1 p r+s−1 h=r λ∈Fp ψ(λ(X(nG) − h))   = 1 p r+s−1 h=r λ∈Fp ψ(λ(X(nG) − h)) − α = s p − α + 1 p λ∈F * p r+s−1 h=r ψ(−λh)ψ(λX(nG)) = β + 1 p λ∈F * p r+s−1 h=r ψ(−λh)ψ(λX(nG)). 
(3) However e N = −α, since N G = O. Assume that a, b, t ∈ N and 1 ≤ a ≤ a + b(t − 1) ≤ N . According to Equation (3), we obtain t−1 i=0 e a+ib ≤ 1 p t−1 i=0 λ∈F * p r+s−1 h=r ψ(−λh)ψ(λX((a + ib)G)) + t−1 i=0 β + 1 ≤ 1 p λ∈F * p r+s−1 h=r ψ(−λh) · t−1 i=0 ψ(λX((a + ib)G)) + |tβ| + 1 ≤ 4p 1/2 (1 + log p)(1 + log N ) + |tβ| + 1 
by Lemma 2 and Lemma 3 or 
|tβ| = 8tθp −1/2 (1 + log p) ≤ 16p 1/2 (1 + log p) 
since t ≤ N ∼ p. So we have t−1 i=0 e a+ib p 1/2 (1 + log p)(1 + log N ). 
We complete the proof of Theorem 2. 
Theorem 3. Let R be a subset of {1, . 
. . , N } defined as in Equation (2), for k < p, we have 
C k (R, E N ) kp 1/2 (2 + log p) k (1 + log N ). 
Proof. Assume that integers d 1 , . 
. . , d k and M ∈ N with 
Now using Equation (3), we obtain 
M m=1 e m+d 1 e m+d 2 · · · e m+d k ≤ M m=1 k i=1   β + 1 p λ∈F * p r+s−1 h=r ψ(−λh)ψ(λX((m + d i )G)) + 1 = 1 + 1 p k M m=1 k i=1   pβ + λ∈F * p r+s−1 h=r ψ(−λh)ψ(λX((m + d i )G)) = 1 + 1 p k M m=1 k u=0 1≤j 1 <...<j u ≤k (pβ) k−u u i=1   λ∈F * p r+s−1 h=r ψ(−λh)ψ(λX((m + d j i )G))   = 1 + 1 p k k u=0 (pβ) k−u 1≤j 1 <...<j u ≤k λ 1 ∈F * p · · · λ u ∈F * p r+s−1 h=r ψ(−h(λ 1 + . . . + λ u )) M m=1 ψ u v=1 λ v X((m + d j v )G) = 1 + 1 p k k u=0 (pβ) k−u 1≤j1<...<ju≤k λ 1 ∈F * p r+s−1 h=r ψ(−hλ 1 ) · · · λ u ∈F * p r+s−1 h=r ψ(−hλ u ) M m=1 ψ((λ 1 X @BULLET τ dj 1 G + . . . + λ u X @BULLET τ dj u G )(mG)) 
≤ 1 + 1 p k k u=0 k u (pβ) k−u p u (1 + log p) u Z = 1 + 1 p k (pβ + p(1 + log p)) k Z = 1 + (β + 1 + log p) k Z ≤ (2 + log p) k Z where M m=1 ψ((λ 1 X @BULLET τ dj 1 G + . . . + λ u X @BULLET τ dj u G )(mG)) ≤ Z. 
It suffices to estimate the value of Z, i.e., the upper bound of 
M m=1 ψ((λ 1 X @BULLET τ d j 1 G + . . . + λ u X @BULLET τ d ju G )(mG)) 
for any λ 1 , . . . , λ u ∈ F * p and 1 ≤ u ≤ k. By 
λ 1 X @BULLET τ d j 1 G + . 
. . + λ u X @BULLET τ d ju G 
is a nonconstant rational function of degree at most 2u. So by Lemma 3 again, we obtain 
M m=1 ψ((λ 1 X @BULLET τ dj 1 G + . . . + λ u X @BULLET τ dj u G )(mG)) ≤ 4up 1/2 (1 + log N ) ≤ 4kp 1/2 (1 + log N ). 
We complete the proof of Theorem 3 by setting 
Z = 4kp 1/2 (1 + log N ). 
Conclusion
2007F3086, 2008J0014, 2008F5049 and the Funds of the Education Department of Fujian Province under grant JA07164. The authors wish to thank the referees for their valuable comments. 
"
"Introduction
 A digital signature is one of the most important security primitives in modern cryptography. In a traditional public key signature scheme, methods to guarantee the authenticity of a public key are required, since the public key of the signer is actually a type of random string. To provide the binding between a signer and his public key, the traditional public key signature uses a certificate that is a digitally signed statement issued by the CA (Certification Authority). The need for public key infrastructure (PKI) supporting certificates is considered the main difficulty in the deployment and management of public key signature schemes. First proposed by Shamir 
Preliminaries
Bilinear Pairings
Bilinear pairing is an important cryptographic primitive. Let (G 1 , +) and (G 2 , ·) be two cyclic groups of the same prime order q. The bilinear pairing is a map e : G 1 ×G 1 → G 2 , which satisfies the following properties: 
@BULLET Bilinear: e(aP, bQ) = e(P, Q) ab for all P, Q ∈ G 1 and a, b ∈ Z * q . 
@BULLET Non-degenerate: If P is a generator of G 1 , then e(P, P ) is a generator of G 2 . In other words, 
e(P, P ) = 1 G 2 . 
@BULLET Computable: There exists an efficient algorithm to 
compute e(P, Q) for all P, Q ∈ G 1 . 
Typically, the map e will be derived from either Weil or Tate pairing on a elliptic curve over a finite field. 
Diffie-Hellman Problems
We also introduce here the computational problems that will form the basis of security for the proposed certificateless signature scheme. Discrete Logarithm Problem (DLP): Given two group elements P and Q in G 1 , find an integer n, such that Q = nP whenever such an integer exists. Computational Diffie-Hellman Problem (CDHP): For any a, b ∈ Z * q , given (P, aP, bP ), compute abP . Decisional Diffie-Hellman Problem (DDHP): For any a, b, c ∈ Z * q , given (P, aP, bP, cP ), decide whether c = ab mod q. Gap Diffie-Hellman (GDH) Group: We define G 1 as a GDH group if G 1 is a group such that DDHP can be solved in polynomial time, but no algorithm can solve CDHP with non-negligible advantage within polynomial- time. The q-Strong Diffie-Hellman problem (q-SDHP): 
Given a (q + 2)-tuple (P, Q, αQ, α 2 Q, · · · , α q Q), find a pair (c, (c + α) −1 P ) with c ∈ Z * q . 
Setup: This algorithm takes as input a security parameter k and returns the system parameters and master key. More specially , this algorithm runs as follows. Let G 1 be a cyclic additive group generated by P , whose order is a prime q, G 2 be a cyclic multiplicative group of the same order q, and e : 
G 1 × G 1 → G 2 be a bilinear pairing. 1) Choose s ∈ R Z * q and set P pub = sP and compute g = e(P, P ). 2) Choose cryptographic hash functions H 1 : {0, 1} * → Z * q and H 2 : {0, 1} * × G 2 → Z * q . 
3) Set the system parameters as 
{G 1 , G 2 , q, P, P pub , g, H 1 , H 2 } 
and keep the master key s secret. 
The system parameters are distributed to the users of the system through a secure authenticated channel. Partial-Private-Key-Extract: This algorithm takes as input the system parameters, the master key, and an identifiable information and returns its corresponding partial private key. More formally, to construct the partial private key for Alice with identifiable information ID A , we adopt the blind technique as in 
1) Alice chooses a value k ∈ R Z * q to compute kP , then Alice sends his identity ID A and kP to the KGC. 
2) KGC checks that Alice has a claim to a particular online identifier ID A . If they do, the KGC computes D 
ID A = (H 1 (ID A ) + s) −1 P + s(kP )
, then sends it to Alice through an open channel. 
3) Alice computes D ID A = D ID A − k(sP ) = (H 1 (ID A ) + s) −1 P . Alice KGC k ∈ R Z * q ID A kP ID A kP − −−−−−− → D ID A = (H 1 (ID A ) + s) −1 P +s(kP ) D ID A ← −− − D ID A = D ID A − kP pub = (H 1 (ID A ) + s) −1 P 
 Anyone else cannot get Alice's private key unless he can get ksP from kP and sP , which is a hard CDH problem. Alice can get his private key by D ID A − kP pub because k is chosen by himself. Notice that Alice can verify the correctness of the Partial-Private-Key-Extract algorithm output by 
checking that e(D ID A , H 1 (ID A )P + P pub ) = g. 
Set-Secret-Value: This algorithm takes as input the system parameters and an identifiable information and returns its corresponding secret value. More specially, to set the secret value for Alice, choose x A ∈ R Z * q , and output x A as her secret value. 
Set-Private-Key: 
 This algorithm takes as input the system parameters , a partial private key, and a secret value and returns corresponding private key. More specially, to construct the private key for Alice, compute 
SK ID A = x A D ID A = x A (H 1 (ID A 
) + s) −1 P as her private key. 
Set-Public-Key: 
This algorithm takes as input the system parameters and a secret value and outputs corresponding public key. More specially, to construct the public key for Alice, compute 
X A = x −1 A P , Y A = x −1 A P pub , and set P K ID A =< X A , Y A > 
as her public key. Sign: Given a message m and a private key SK ID A , perform the following steps. 
1) Choose a ∈ R Z * q . 2) Compute r = g a ∈ G 2 . 3) Set v = H 2 (m r) ∈ Z * q . 4) Compute U = (a + v)SK ID A ∈ G 1 . 5) Set σ = (U, v) ∈ G 1 × Z * 
2) Compute r = e(U, H 1 (ID A )X A + Y A )g −v 3) Check if v = H 2 (m r) 
holds. If it does, accept the signature. Otherwise, stop and reject the signature. This completes the description of our proposed certificateless signature scheme. In the following section, we analyze the scheme from performance and security points of view. 
 4 Analysis of the Proposed Certificateless Signature Scheme 
Correctness Analysis
Consistency of the proposed scheme is satisfied. In effect, if σ = (U, v) is a valid signature of a message m for Alice with public key 
P K ID A =< X A , Y A >, then e(X A , P pub ) = e(X A , sP ) = e(sX A , P ) = e(Y A , P ) (1) r = e(U, H 1 (ID A )X A + Y A )g −v = e((a + v)SK ID A , H 1 (ID A )x −1 A P + x −1 A sP )g −v = e((a + v)(H 1 (ID A ) + s) −1 x A P, (H 1 (ID A ) + s)x −1 A P )g −v = e(P, P ) a+v g −v = g a (2) 
4.
Performance Analysis
According to the state-of-the-art results in 
Security Analysis
The proposed scheme is unforgeable under the hardness assumption of the q-strong Diffie–Hellman problem and Computational Diffie–Hellman problem. On the one hand, even the KGC who knows the master key s, the partial private key of Alice, and the public key < X A , Y A > of Alice, cannot compute a valid signature. If he can compute x A from the equalities X A = x A P or Y A = x A sP , then he can forge BLS signatures 
v = H 2 (m e(U, H 1 (ID A )x A + Y A )g −v ) holds. 
@BULLET Secondly, the adversary can choose v at random and try to compute U such that the equation 
v = H 2 (m e(U, H 1 (ID A )x A + Y A )g −v ) holds. 
However, due to the hardness of the q-strong Diffie– Hellman problem, computational Diffie–Hellman problem and the one-way property of cryptographic hash function, the adversary can not forge a valid signature by this two ways. 
+1)(q s +q h2 )/2 k . 
Then, there exists an algorithm B that is able to solve the q-SDHP for q = q h 1 in an expected time 
t ≤ 120686q h1 q h2 (t+O(q s , τ p ))/((1−q/2 k ))+O(q 2 τ mult ). 
(3) 
where τ 
mult and τ p respectively denote the cost of a scalar multiplication in G 2 and the required time for a pairing evaluation. 
The formal security analysis is the same as Barreto et al.'s provably-secure identity-based signatures 
Conclusions
In order to avoid the inherent escrow of identity-based cryptography and yet not requiring certificates to guarantee the authenticity of public keys, Certificateless public key cryptography was first introduced by Al-Riyami and Paterson in Asiacrypt 2003, and has received a significant attention in recent years. In this paper, we proposed a new certificateless signature scheme based on bilinear pairings, the proposed scheme is more efficient than those of previous schemes by pre-computing the pairing e(P, P ) = g and publishing as the system parameters. The scheme is proved to be secure under under the hardness assumption of the bilinear pairing inversion problem and Computational Diffie-Hellman problem. 
"
"Introduction
MANET has been a challenging research area for the last few years because of its dynamic topology, power constraints and limited range of each mobile host's wireless transmissions and security issues. MANET is a rapidly deployable, self-organized and multi-hop wireless network. It is typically set up for limited periods of time and particular applications such as military, disaster areas and medical applications. Nodes in MANET may move arbitrarily while communicating over wireless links. This network is typically used in situations where there is no centralized administration or support from networking infrastructure such as routers or base stations. Many up-to-date researches pay attention to MANET as a new technology with specific characteristics which distinguish it from other types of networks, such as openness which simplifies the way for external attacks to join the trusted nodes easily, resource limitation in power and bandwidth, mobility and dynamicity, flexibility, distributed computation and decentralizatio 
This paper is structured as follows: Section 2 introduces a literature review for the previous works. Section 3 presents AODV routing protocol. Then, Section 4 explains the AODV vulnerability to RCA. After that, Section 5 highlights the simulation environment. Section 6 presents the simulation results and finally Section 7 presents the research conclusion and future work. 
Related Work
Ning and Sun in 
Simulation Enviroment
This section explains the environment where our experimental simulations are performed. Specifically, the simulation parameters and main performance metrics used are discussed. 
The simulation experiments were carried out using Qualnet 5.0.2 
The performance metrics used are: 1. Packet delivery ratio: Packet delivery ratio is the ratio of the number of data packets successfully delivered to the destination to those generated by CBR source. 
2. Delay jitter: Delay jitter is the variation in the time between packets arriving at the destination, caused by network congestion, timing drift, or route changes. In our experiments' results, each scenario result represents the average of five simulation runs. 
Results and Discussion
Figure 6 and 7 depicts that the average results of the attackers' placements show that near receiver placement scenario outperforms the others in degrading the packet delivery ratio in the network. This is because packet delivery ratio means the ratio of the number of packets received by the receiver to the number of the packets actually sent by the sender. Therefore, when attackers are located near receiver, they reduce the possibility of receiving the already produced and sent packets which leads to an overall packet delivery ratio degradation. 
Conclusion and Future Work
In this paper, we studied the impact of RCA over AODV routing protocol in MANET. The study focused on assessing the effect of varying the number of attackers and their placements on two performance metrics which are packet delivery ratio and delay jitter. The study opens the door to the researchers to suggest solutions which could mitigate the impact of RCA. 
In the future, we aim to study the impact of RCA on other performance metrics such as: throughput, end-to-end delay, energy consumption and routing packets overhead. Also, the study could include more attacking scenarios such testing the effect of attackers' radio range and flooding rate. Our concern also to modify AODV protocol to have security protection against RCA. protocols, simulation, and wireless communications. He also has a growing interest in multihop and network architecture for intelligent cloud base station. 
Mueen 
"
"Introduction
 In this paper we present a biometric identity based signature scheme (BIO-IBS). Traditional public key cryptosystems use very long integers, typically 2048 bits, as public keys. These systems rely on digital certificates to connect an identity like a person or a machine to a public key. Identity based systems have the advantage that the public key is the identity, usually an arbitrary string like an email address. In our case we use a biometric measurement of an individual. A significant problem here is the fact that biometric identities tend to vary over time. Obviously this causes problems for key generation. We discuss how to counteract this problem below. One of the main uses of signature schemes is in the area of non-repudiation of documents. Our scheme is particularly useful in this area as biometric measurements such as fingerprints have been established for a long time as evidential tools 
Elliptic Curve Background
We use the symbol ⊕ to denote bitwise exclusive or, XOR. We represent the finite field with p elements as F p = {0, 1, 2, 3, . . . , p − 2, p − 1}. For the remainder of this paper we assume p ≡ 3 mod 4. The extension field F 2 p can be defined as F 2 p = {a + bi} where a, b ∈ F p and i = √ −1. The basic units for elliptic curve arithmetic are points (x, y) on an elliptic curve, E, over a finite field, F p , denoted E(F p ), of the form 
y 2 = x 3 + ax + b with x, y, a, b ∈ F p . 
We define abstract concepts of addition, P + Q, and scalar multiplication by an integer, sQ, on the points of E(F p ). We also define a special point at infinity, ∞, which is not a solution to the equation given above. These operations combine to make E(F p ) a finite abelian group with ∞ behaving as the identity element. Details of how these concepts are implemented appear in 
τ q : E(F p )[q] × E(F 2 p )/qE(F 2 p ) → µ(q) τ q (P, Q) = a + bi ∈ µ(q). 
Note that we can make computational savings by using a modified version of the Tate pairing. We define the map 
φ : E(F p ) → E(F 2 p ) as follows: 
φ(x, y) = (−x + 0i, 0 + yi). 
For example, given (3, 12) ∈ E(F 19 ), φ(3, 12) = (16 + 0i, 0 + 12i). We then define the modified Tate pairing 
τ q : E(F p )[q] × E(F p ) → µ(q) 
as follows: 
τ q (P, Q) = τ q (P, φ(Q)). 
This allows us to pick Q in E(F p ) and thus avoid the complexity of dealing with cosets in E(F 2 p ). A more detailed account of the modified Tate pairing appears in 
 3 Generating Key Data from Bio- metrics 
Using biometric data as a basis for cryptographic keys is problematic as biometric measurement is not perfectly reproducible. Recent work 
Fuzzy Extraction
Obtaining a biometric reading is a variable process that introduces errors. Examples of such errors include variation in the physical biometric (e.g. a cut on a finger) or bad placement of the finger on the reading device. The reader and matching software can attempt to fix these errors using various techniques. These techniques such as feature extraction may also introduce variation 
We now outline the construction of a fuzzy extractor for the space M under the Hamming Distance metric. This construction is built using error correcting codes. We now define what we mean by an error correcting code. Let the set C be a subset of n-bit words (i.e. C ⊆ {0, 1} n ), with n > v and having at least 2 k elements for some positive integer k. Let C e : M → C be a one-to-one encoding function and also let C d : {0, 1} n → C be a decoding function that has an error threshold of t (can correct up to t-bit errors). The decoding function C d will take an arbitrary n-bit string and "" correct "" it to the nearest codeword in C. Using coding theory notation the combination of C, C e , and C d gives us an [n, k, 2t + 1] binary error correcting code 
Choice of Error Correcting Codes
In this section we will give the reader an overview of the specific error correcting code we use in the implementation . We chose to use a BCH (Bose-Chaudhuri- Hocquenghem) code because it is an excellent general purpose binary code. The fact that the BCH class of codes have polynomials as codewords makes them more complex and powerful compared to simpler codes like Hamming codes, as they can be constructed to be multi-error correcting. They can be designed to correct errors up to about half the code's block length. We now look at the actual process of constructing a BCH code. The variable t is defined to be the number of errors to be corrected and n = 2 m − 1 is the length of the code, where m, n are positive integers. The polynomial f (x) ∈ Z 2 
The generator polynomial g(x) must be computed in a particular way in order to construct a valid BCH code. If p(x) is a primitive polynomial of degree m in Z 2 
The Extraction Process
 In this section we will give an overview of the implementation of the process of acquiring a unique string from a varying biometric. For the purposes of this paper this implementation will take place using a standard personal computer and a biometric reader connected via a USB port using BioAPI 1 compliant readers. The first step is to obtain b from the biometric reader. We outline the approach in Appendix 7. The next step is to use an error correcting code such as a BCH code to fuzzy extract some data from the biometric input. We now describe how such a code is used to build a fuzzy extractor by defining the Gen and Rep functions. The Gen function takes the biometric input b and outputs ID = H(b) that is used for key generation and a publicly accessible reproducer PAR = b ⊕ C e (ID) which is used in conjunction with another biometric input b to recover ID. The Rep function takes b and PAR as input and outputs ID = C d (b ⊕ PAR) = C d (b ⊕ b ⊕ C e (ID)) which is equal to ID if and only if dis(b, b ) ≤ t. We illustrate the process below in 
 4 Biometric Identity Based Signature Scheme 
 Shamir first proposed the concept of identity-based cryptography in 
Modified SOK-IBS
The modified SOK-IBS scheme 
Setup
Given a security parameter k, the Private Key Generator (PKG) selects groups G 1 and G 2 of prime order q > 2 k , a generator P of G 1 , a randomly chosen master key s ∈ Z * q and the associated public key P pub = sP . It also selects cryptographic hash functions of the same domain and range H 1 , H 2 : {0, 1} * → G * 1 . A more detailed discussion on the workings of H 1 and H 2 are given in Section 4.3. The system's public parameters are 
params = (G 1 , G 2 , ˆ e, P, P pub , H 1 , H 2 ). 
Key Generation
Given a user's identity Φ, the PKG computes Q Φ = H 1 (Φ) ∈ G 1 and the associated private key d Φ = sQ Φ ∈ G 1 that is transmitted to the user. 
Sign
In order to sign a message M , 1) Pick a random integer r ∈ Z q and compute U = rP ∈ 
G 1 . Then H = H 2 (Φ, M, U ) ∈ G 1 . 
2) Compute V = d Φ + rH ∈ G 1 where "" + "" indicates addition on the group G 1 . 
The signature on M is the pair σ = U, V ∈ G 1 × G 1 . 
Verify
To verify a signature σ = U, V ∈ G 1 × G 1 on a message M , the verifier first obtains the signer's identity Φ and computes Q Φ = H 1 (Φ) ∈ G 1 . The verifier recalculates H = H 2 (Φ, M, U ) ∈ G 1 . He then accepts the signature ifê ifˆifê(P, V ) = ˆ e(P pub , Q Φ )ˆ e(U, H) and rejects it otherwise. 
BIO-IBS
Our proposed scheme will incorporate fuzzy extractors into the SOK-IBS scheme. Like SOK-IBS, the BIO-IBS scheme consists of four parts. 
Setup
 Given a security parameter k, the Private Key Generator (PKG) selects groups G 1 and G 2 of prime order q > 2 k , a generator P of G 1 , a randomly chosen master key s ∈ Z * q and the associated public key P pub = sP . It also selects cryptographic hash functions of the same domain and range H 1 , H 2 : {0, 1} * → G * 1 . The PKG picks H 3 : b → {0, 1} * , an encoding function C e and a decoding function C d . It also selects a method of extracting the features of a biometric, F e . The system's public parameters are 
params = (G 1 , G 2 , ˆ e, P, P pub , H 1 , H 2 , H 3 , C e , C d , F e ). 
Key Generation
 On obtaining a user's biometric b using the feature extractor F e , the identity string can be calculated as ID = H 3 (b). The PKG computes the public key Q ID = H 1 (ID) ∈ G 1 and the associated private key d ID = sQ ID ∈ G 1 . 
Sign
In order to sign a message M , 1) Pick a random integer r ∈ Z q and compute U = rP ∈ 
G 1 . Then H = H 2 (ID, M, U ) ∈ G 1 . 2) Compute V = d ID + rH ∈ G 1 . 
3) The value PAR = b ⊕ C e (ID) is included as part of the signature. 
The signature on M is the triple σ = U, V, P AR. 
Verify
To verify a signature σ = U, V, P AR on a message M for an identity ID, the verifier performs the following steps. 
= Rep(b , P AR). 2 2) Calculate Q ID = H 1 (ID ) ∈ G 1 and H = H 2 (ID , M, U ) ∈ G 1 . 
3) Signature is verified ifêifˆifê(P, 
V ) = ˆ e(P pub , Q ID )ˆ e(U, H) 
and rejected otherwise. 
Map To Point
For H 1 and H 2 mentioned above, a binary string {0, 1} * has to be embedded onto a point Q on the elliptic curve E(F p ). This requires the use of a function f : {0, 1} * → G * 1 where G 1 is a subgroup of the points on an elliptic curve. Rather than map directly onto G * 1 we use a standard hash function, H, to hash to a set A ⊆ {0, 1} * . Then we use a deterministic encoding function, G, to map A 
onto G * 1 so f (ID) = G(H(ID)
). See 
Security Issues
Malicious signing
 It is relatively easy to obtain biometric data from an individual . For example fingerprints can be lifted from a glass. Other examples appear in 
Disavowal
In our situation recovery of the original biometric is not a problem as we use it to generate the public key. However tampering with β would be useful in an attempt by a legitimate signer to later disavow their signature. Recent work by Boyen et al. 
Security of the Signature Scheme
A framework for analysing the security of Identity Based Signature schemes was given in 
Implementation of BIO-IBS
In this section, we discuss the design and implementation issues involved in extending an existing IBE implementation to incorporate the BIO-IBS system. An overview of the IBE implementation and the BIO-IBS extension is given in 
 6.1 An Existing Identity Based Encryption System 
One of the core technologies required for our BIO-IBS is a bilinear mapping over an elliptic curve. Such a mapping, the Tate pairing, was previously developed as part of an implementation, 
Fuzzy Extractors
 The fuzzy extractor class performs two functions, the generation function Gen and the reproduction function Rep. Since the generation function returns two strings ID and P AR, those strings are encapsulated in the Generator class. The reproduction function result is encapsulated in the Reproducer class. A binary error correcting code and a metric space make up the attributes of the fuzzy extractor. These attributes are represented by the interfaces BinaryError CorrectingCode and MetricSpace respectively, allowing for choice in the type of error correcting codes and metric spaces to be used. We extend the MetricSpace class with a HammingDistance MetricSpace that implements the Hamming Distance metric. 
Conclusion
We have presented a biometric identity based signature scheme. We have utilised, extended, and implemented ideas in the areas of error corrected string construction from biometric data, key generation, and pairing based signature schemes to form the components of our system. We presented the application of such a scheme to repudiation situations. We discussed the advantage of using the biometric data in the public key and described the utility of using biometric evidence in disputes that may arise. We also discussed the main security issues around this system. Finally we described how such a biometric signature scheme was incorporated into an existing IBE software package. The pluggable architecture provides for easy incorporation of different implementations of component algorithms. This will facilitate inclusion of future performance enhancements to existing algorithms or inclusion of new algorithms to the system. found in fingerprint biometrics, where the position and orientation of the finger results in significant differences between samples. To get around this problem distinguishing features of the sample are identified using a feature extraction process to form a template. Examples of feature extraction techniques appear in 
Key Generation 
 Bob uses the biometric reader to sign the document. Before the document is signed, his public and private keys are generated as in 
Signing and Verification 
V ) = τ q (P pub , Q ID )τ q (U, H) 
where P , P pub and H are system parameters, U and V are taken from the signature and Q ID is the recreated public key based on the biometric reading. 
Verification Failure 
"
"Introduction
 Nowadays, in order to research, marketing or provide better service, a numbers of enterprises would collect customers' relevance data, such as personal information, service experience, and desired functions. However, since the occurrences of deceptive crime and personal information disclosure happened frequently, privacy protection has been paid much attention by consumers, companies, researchers, and legislators. Victims not only receive annoying advertisements and reluctant marketing tricks, but also face to the threat of life and property 
 User: User is a human being related to the entire internal and external enterprise system. All users, such as employees, customers, and business partners, have their own position and duty in an enterprise. 
 Role: Role is a named job title or job function which defines an authority level. If an user has been assigned and authorized a role (User Assignment; UA), he/she can exercise a permission to access specific data. The relations between users and roles are many-to-many, in other word, an user can belong to many roles, and a roles can assigned to many users. 
¡ ¢ £ ¡ ¤ ¥ ¦ § ¨ ¢ ¡ ¤ ¦ ¥ © ¤ ¥ ¢ ¡ ¡ § ! ¡ ¤ ¥ ¡ ¢ £ "" ¡ ¡ # ! $ ¢ ! % ¤ & ¥ ¢ £ $ ¡ ¡ § ! "" ¡ ¡ # ! $ ¢ ! % ¤ & ¥ ¦ § ¨ ¢ ' ¢ £ "" £ ( ) 0 ¤ ¦ ' ¥ 
 Permission: A permission can be the terms authorization , access right, and privilege in the literature. Permissions confer on their holder the ability to perform an action such as read, write, and execute in the system. After assigned specific permissions (Permission assignment; PA), a role has rights to do operations designated in permissions to data. Based on many-to-many relation, A role can hold many permissions , and the same permission can be appointed to many permissions. If the data provider want to protect his/her sensitive privacy data, he/she could edit permissions for restricting rights or specific conditions to access data he/she own. For instance, a data provider, Bob can set a permission like "" only the salesman can read Bob's personal information for marketing at 9 to 17 o'clock. In this example, Bob sets four restrictions on his personal information , including access operation "" read "" , role assignment "" salesman "" , specific purpose "" marketing "" ,and time period "" 9 to 17 o'clock "" . By this way to protect privacy of data providers, they could rely on their intention to adjusting permissions flexibly. Session: Users can activate a subset of the roles simultaneously for establishing sessions. A user invoke the roles they belong to enable to accomplish tasks in a session. 
RBAC is a wildly used approach to restricting system access to authorized users in computer system security. The permissions which perform certain operations of resources are assigned to specific roles. It means that RBAC regulates users' access based on rights and authorization of their roles. One of the reasons why we adopt RABC to  protect privacy is its authorization management mechanism . RBAC is designed to meet the need of relieving the authorization management and immediately offering access control policies 
 Data quality: Although privacy protection is an important issue for users, we are unwilling to see that enterprises could not collect enough information for researches or provide better services because of privacy protection policies. We consider that the methods we survey could either retain the data quality or protect a data provider's privacy. 
Simplicity: The great majority of data providers are common people, but not program developers. The policy construction should be simple and easy to use, so that normal users can edit their own privacy policy for access control. Next, we briefly introduce three directions of implementing data access control "" Role-based "" , "" Purposebased "" , and "" Extension "" . In Section 3, we utilize three criterions to put across features of each category. And then, we bring up future research issues in Section 4. Conclusions are in Section 5. 
¡ ¢ £ ¤ ¥ ¦ § ¨ © £ ¢ ¢ ¡ ! "" # ¡ ! $ % & ' ( ) 0 1 2 ¤ ¥ ¦ § ¨ 
Privacy Protection Data Access Control
Role-based Data Access Control
In 
Purpose-based Data Access Control
In 
Prohibited Intended Purpose (PIP) indicates that the data provider absolutely disallows accessing the data for a specific purpose. 
The three components (AIP, CIP and PIP) are set as the intended purpose by the data provider. Access purpose must comply with the intended purpose for authorization of data access. Combining the advantages of RBAC 
Extension Mechanisms
In this category, there are different methods from the mechanism on the basic of the role-based and the purposebased data access control. The papers, 
Element-based is a relation scheme Rel(A 1 , l 1 , A 2 , l 2 , . . . , A n , l n ), such that R = A 1 ,...A n 
(Rel). The l i governs the access to data element A i in each instance of R (See 
{A, P, S}, 0 Order Status {A, P }, 0 Access-Log ALL {A, P }, 0 
Comparison
 In this paper, we define three directions of the way of preserving privacy, role-based, purpose-based, and extension data access control. There are both positive and negative effects in these three direction. Therefore, we synthesize three criterions of privacy protection data management system implementation. In this paper, we classify the data privacy protection schemes into three categories, "" Role-based "" , "" Purposebased "" , and "" Extension "" . Role-based schemes have developed for a long time, so their capability, expansibility, and development degree are integral. Purpose-based schemes utilize the notion of purpose limitation, they are clear and easy to understand for customers, employees, and correlative data providers. Extension schemes direct toward the improvement of efficiency and efficacy on incorporating privacy protection. After researching on these three domains of schemes, there are three significant and major factors we synthesize on constructing an data access control system for preserving privacy of data providers. Finally, The comparison of these three factors with above classifications are shown in 
Future Research
In this article, we introduce the recent development of privacy-aware data access control, in order to be suitable for enterprise applications and users' requirement, researchers proposing a specific scheme such as additional condition and obligation support, conditional data presentation , and efficacy promotion in a relational database system. However, there are several issues worth researching and developing in the future as followings. 
Data Security: No matter how secure a system is, it must exist some security loopholes or weakness which might be attacked or invading in the system. We should establish a data protection mechanism for sensitive and private data such as encryption. For another situation, a user belonging the role can do definite operations to specific data if the role have particular permissions, but if users need to store data in files for particular situations like a travel on business , the portability of data might be a secret worry.  Deception: Most schemes of privacy-aware access control are based on the user of trust; therefore, purposes in access control policy are used by data requesters oneself. If data requesters lie for illegal objectives, it could be difficult to detect and dangerous for user privacy data. It needs an entire mechanism to ensure that data requesters certainly utilize data for specific purposes they assign. Nevertheless, even a perfect mechanism has been constructed, and it cannot entirely stand against the malicious attack from adversaries . Therefore, we can adopt passive detection instead of active limitation. For instance, recording each private data access for tracking, it would be described later. 
 Infringement Detection: Like every protection system such as IDS, IPS, and firewall, etc., if we apply an infringement detection to a privacy protection in data access, it could strengthen the trust of data providers. Generally, a detection system includes four steps. At first, it collects all related data records, and we could follow it as an example to collect all data access records including a privacy infringement behavior . Second, it should define characteristics which can represent the features of infringement caution like times of data access, user login record, and error occurrence. Third, it analyzes statistics for a possible malicious behavior such as massive data access, irregular authorization, and periodical error, etc.. Finally , it configures the setting of thresholds to warn system administrators, related employees, and even data providers. professor in the Department of Computer Science and Information Engineering, Chaoyang University of Technology . Currently, he is a professor in the Department of Management Information Systems, National Chung Hshing University. His research interests include network security, mobile computing, and distributed system. 
"
